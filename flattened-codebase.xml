<?xml version="1.0" encoding="UTF-8"?>
<files>
  <file path="setup.py"><![CDATA[
    #!/usr/bin/env python3
    """
    å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿå®‰è£…è„šæœ¬
    """
    
    from setuptools import setup, find_packages
    from pathlib import Path
    
    # è¯»å–READMEæ–‡ä»¶
    readme_path = Path(__file__).parent / "README.md"
    long_description = readme_path.read_text(encoding="utf-8") if readme_path.exists() else ""
    
    # è¯»å–requirementsæ–‡ä»¶
    requirements_path = Path(__file__).parent / "requirements.txt"
    requirements = []
    if requirements_path.exists():
        with open(requirements_path, "r", encoding="utf-8") as f:
            requirements = [
                line.strip() 
                for line in f 
                if line.strip() and not line.startswith("#")
            ]
    
    setup(
        name="rl-trading-system",
        version="0.1.0",
        author="RL Trading Team",
        author_email="team@rltrading.com",
        description="åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿ",
        long_description=long_description,
        long_description_content_type="text/markdown",
        url="https://github.com/rl-trading/rl-trading-system",
        package_dir={"": "src"},
        packages=find_packages(where="src"),
        classifiers=[
            "Development Status :: 3 - Alpha",
            "Intended Audience :: Financial and Insurance Industry",
            "License :: OSI Approved :: MIT License",
            "Operating System :: OS Independent",
            "Programming Language :: Python :: 3",
            "Programming Language :: Python :: 3.8",
            "Programming Language :: Python :: 3.9",
            "Programming Language :: Python :: 3.10",
            "Topic :: Office/Business :: Financial :: Investment",
            "Topic :: Scientific/Engineering :: Artificial Intelligence",
        ],
        python_requires=">=3.8",
        install_requires=requirements,
        extras_require={
            "dev": [
                "pytest>=7.1.0",
                "pytest-cov>=3.0.0",
                "black>=22.6.0",
                "flake8>=5.0.0",
                "mypy>=0.971",
                "pre-commit>=2.20.0",
            ],
            "docs": [
                "sphinx>=5.0.0",
                "sphinx-rtd-theme>=1.0.0",
                "myst-parser>=0.18.0",
            ],
        },
        entry_points={
            "console_scripts": [
                "rl-trading-train=scripts.train:main",
                "rl-trading-evaluate=scripts.evaluate:main",
                "rl-trading-deploy=scripts.deploy:main",
                "rl-trading-monitor=scripts.monitor:main",
            ],
        },
        include_package_data=True,
        package_data={
            "rl_trading_system": ["config/*.yaml"],
        },
        zip_safe=False,
    )
    ]]></file>
  <file path="requirements.txt"><![CDATA[
    # æ ¸å¿ƒä¾èµ–
    torch>=1.12.0
    numpy>=1.21.0
    pandas>=1.3.0
    scipy>=1.7.0
    scikit-learn>=1.0.0
    
    # å¼ºåŒ–å­¦ä¹ 
    gym>=0.21.0
    stable-baselines3>=1.6.0
    
    # é‡åŒ–äº¤æ˜“
    qlib>=0.8.0
    akshare>=1.8.0
    
    # æ·±åº¦å­¦ä¹ 
    transformers>=4.20.0
    torch-audio>=0.12.0
    torch-vision>=0.13.0
    
    # æ•°æ®å¤„ç†
    pyarrow>=8.0.0
    h5py>=3.7.0
    tables>=3.7.0
    
    # ç›‘æŽ§å’Œæ—¥å¿—
    prometheus-client>=0.14.0
    loguru>=0.6.0
    psutil>=5.9.0
    
    # Webæ¡†æž¶
    fastapi>=0.78.0
    uvicorn>=0.18.0
    pydantic>=1.9.0
    
    # æ•°æ®åº“
    influxdb-client>=1.30.0
    redis>=4.3.0
    aioredis>=2.0.0
    sqlalchemy>=1.4.0
    psycopg2-binary>=2.9.0
    asyncpg>=0.27.0
    
    # å¯è§†åŒ–
    matplotlib>=3.5.0
    seaborn>=0.11.0
    plotly>=5.9.0
    dash>=2.5.0
    
    # é…ç½®ç®¡ç†
    pyyaml>=6.0
    python-dotenv>=0.20.0
    hydra-core>=1.2.0
    
    # æµ‹è¯•
    pytest>=7.1.0
    pytest-cov>=3.0.0
    pytest-mock>=3.8.0
    pytest-asyncio>=0.19.0
    hypothesis>=6.50.0
    pytest-benchmark>=3.4.0
    
    # ä»£ç è´¨é‡
    black>=22.6.0
    flake8>=5.0.0
    mypy>=0.971
    isort>=5.10.0
    pre-commit>=2.20.0
    
    # æ¨¡åž‹è§£é‡Š
    shap>=0.41.0
    lime>=0.2.0
    
    # éƒ¨ç½²
    docker>=5.0.0
    kubernetes>=24.2.0
    gunicorn>=20.1.0
    
    # å…¶ä»–å·¥å…·
    tqdm>=4.64.0
    click>=8.1.0
    rich>=12.5.0
    typer>=0.6.0
    ]]></file>
  <file path="pytest.ini"><![CDATA[
    [tool:pytest]
    testpaths = tests
    python_files = test_*.py *_test.py
    python_classes = Test*
    python_functions = test_*
    addopts = 
        --strict-markers
        --strict-config
        --verbose
        --cov=src/rl_trading_system
        --cov-report=term-missing
        --cov-report=html:htmlcov
        --cov-report=xml
        --cov-fail-under=80
        --tb=short
    markers =
        unit: å•å…ƒæµ‹è¯•
        integration: é›†æˆæµ‹è¯•
        e2e: ç«¯åˆ°ç«¯æµ‹è¯•
        slow: æ…¢é€Ÿæµ‹è¯•
        gpu: éœ€è¦GPUçš„æµ‹è¯•
    filterwarnings =
        ignore::UserWarning
        ignore::DeprecationWarning
    ]]></file>
  <file path="pyproject.toml"><![CDATA[
    [build-system]
    requires = ["setuptools>=45", "wheel", "setuptools_scm[toml]>=6.2"]
    build-backend = "setuptools.build_meta"
    
    [tool.black]
    line-length = 88
    target-version = ['py38', 'py39', 'py310']
    include = '\.pyi?$'
    extend-exclude = '''
    /(
      # directories
      \.eggs
      | \.git
      | \.hg
      | \.mypy_cache
      | \.tox
      | \.venv
      | build
      | dist
    )/
    '''
    
    [tool.isort]
    profile = "black"
    multi_line_output = 3
    line_length = 88
    known_first_party = ["rl_trading_system"]
    known_third_party = ["torch", "numpy", "pandas", "qlib", "akshare"]
    
    [tool.mypy]
    python_version = "3.8"
    warn_return_any = true
    warn_unused_configs = true
    disallow_untyped_defs = true
    disallow_incomplete_defs = true
    check_untyped_defs = true
    disallow_untyped_decorators = true
    no_implicit_optional = true
    warn_redundant_casts = true
    warn_unused_ignores = true
    warn_no_return = true
    warn_unreachable = true
    strict_equality = true
    
    [[tool.mypy.overrides]]
    module = [
        "qlib.*",
        "akshare.*",
        "gym.*",
        "stable_baselines3.*",
        "shap.*",
        "lime.*",
        "prometheus_client.*"
    ]
    ignore_missing_imports = true
    
    [tool.coverage.run]
    source = ["src/rl_trading_system"]
    omit = [
        "*/tests/*",
        "*/test_*.py",
        "*/__init__.py",
    ]
    
    [tool.coverage.report]
    exclude_lines = [
        "pragma: no cover",
        "def __repr__",
        "if self.debug:",
        "if settings.DEBUG",
        "raise AssertionError",
        "raise NotImplementedError",
        "if 0:",
        "if __name__ == .__main__.:",
        "class .*\\bProtocol\\):",
        "@(abc\\.)?abstractmethod",
    ]
    ]]></file>
  <file path="package.json"><![CDATA[
    {
      "devDependencies": {
        "bmad-method": "^4.33.1"
      }
    }
    
    ]]></file>
  <file path="docker-compose.yml"><![CDATA[
    version: '3.8'
    
    services:
      # ä¸»åº”ç”¨æœåŠ¡
      rl-trading-system:
        build: .
        container_name: rl-trading-system
        ports:
          - "8000:8000"  # ç›‘æŽ§ç«¯å£
          - "8888:8888"  # Jupyterç«¯å£
          - "6006:6006"  # TensorBoardç«¯å£
        volumes:
          - ./data:/app/data
          - ./logs:/app/logs
          - ./checkpoints:/app/checkpoints
          - ./outputs:/app/outputs
          - ./config:/app/config
        environment:
          - PYTHONPATH=/app/src
          - LOG_LEVEL=INFO
        depends_on:
          - redis
          - influxdb
        networks:
          - trading-network
        restart: unless-stopped
    
      # Redisç¼“å­˜æœåŠ¡
      redis:
        image: redis:7-alpine
        container_name: rl-trading-redis
        ports:
          - "6379:6379"
        volumes:
          - redis_data:/data
        command: redis-server --appendonly yes
        networks:
          - trading-network
        restart: unless-stopped
    
      # InfluxDBæ—¶åºæ•°æ®åº“
      influxdb:
        image: influxdb:2.7-alpine
        container_name: rl-trading-influxdb
        ports:
          - "8086:8086"
        volumes:
          - influxdb_data:/var/lib/influxdb2
        environment:
          - DOCKER_INFLUXDB_INIT_MODE=setup
          - DOCKER_INFLUXDB_INIT_USERNAME=admin
          - DOCKER_INFLUXDB_INIT_PASSWORD=password123
          - DOCKER_INFLUXDB_INIT_ORG=rl-trading
          - DOCKER_INFLUXDB_INIT_BUCKET=trading-data
        networks:
          - trading-network
        restart: unless-stopped
    
      # Prometheusç›‘æŽ§
      prometheus:
        image: prom/prometheus:latest
        container_name: rl-trading-prometheus
        ports:
          - "9090:9090"
        volumes:
          - ./monitoring/prometheus.yml:/etc/prometheus/prometheus.yml
          - prometheus_data:/prometheus
        command:
          - '--config.file=/etc/prometheus/prometheus.yml'
          - '--storage.tsdb.path=/prometheus'
          - '--web.console.libraries=/etc/prometheus/console_libraries'
          - '--web.console.templates=/etc/prometheus/consoles'
          - '--storage.tsdb.retention.time=200h'
          - '--web.enable-lifecycle'
        networks:
          - trading-network
        restart: unless-stopped
    
      # Grafanaå¯è§†åŒ–
      grafana:
        image: grafana/grafana:latest
        container_name: rl-trading-grafana
        ports:
          - "3000:3000"
        volumes:
          - grafana_data:/var/lib/grafana
          - ./monitoring/grafana/dashboards:/etc/grafana/provisioning/dashboards
          - ./monitoring/grafana/datasources:/etc/grafana/provisioning/datasources
        environment:
          - GF_SECURITY_ADMIN_USER=admin
          - GF_SECURITY_ADMIN_PASSWORD=admin123
          - GF_USERS_ALLOW_SIGN_UP=false
        networks:
          - trading-network
        restart: unless-stopped
    
      # Jupyter Lab (å¼€å‘çŽ¯å¢ƒ)
      jupyter:
        build: .
        container_name: rl-trading-jupyter
        ports:
          - "8889:8888"
        volumes:
          - ./notebooks:/app/notebooks
          - ./data:/app/data
          - ./src:/app/src
          - ./config:/app/config
        environment:
          - PYTHONPATH=/app/src
        command: jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root --NotebookApp.token=''
        networks:
          - trading-network
        restart: unless-stopped
    
    volumes:
      redis_data:
      influxdb_data:
      prometheus_data:
      grafana_data:
    
    networks:
      trading-network:
        driver: bridge
    ]]></file>
  <file path="README.md"><![CDATA[
    # å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿ
    
    åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé‡‡ç”¨SACï¼ˆSoft Actor-Criticï¼‰ç®—æ³•ä½œä¸ºå†³ç­–å¼•æ“Žï¼Œä½¿ç”¨Transformeræž¶æž„æ•æ‰é•¿æœŸæ—¶åºä¾èµ–ã€‚
    
    ## ðŸŽ¯ é¡¹ç›®ç›®æ ‡
    
    - **å¹´åŒ–æ”¶ç›Šç›®æ ‡**: 8%-12%
    - **æœ€å¤§å›žæ’¤æŽ§åˆ¶**: â‰¤15%
    - **å¤æ™®æ¯”çŽ‡**: â‰¥1.0
    - **ä¿¡æ¯æ¯”çŽ‡**: â‰¥0.5
    
    ## ðŸ—ï¸ ç³»ç»Ÿæž¶æž„
    
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿ                      â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
    â”‚  â”‚  æ•°æ®å¤„ç†å±‚  â”‚  â”‚  ç‰¹å¾å·¥ç¨‹å±‚   â”‚  â”‚    æ—¶åºç¼–ç å±‚        â”‚   â”‚
    â”‚  â”‚             â”‚  â”‚              â”‚  â”‚                      â”‚   â”‚
    â”‚  â”‚ â€¢ Qlib     â”‚â”€â–¶â”‚ â€¢ æŠ€æœ¯æŒ‡æ ‡   â”‚â”€â–¶â”‚ â€¢ Transformer       â”‚   â”‚
    â”‚  â”‚ â€¢ Akshare  â”‚  â”‚ â€¢ åŸºæœ¬é¢å› å­ â”‚  â”‚ â€¢ Multi-Head Attn   â”‚   â”‚
    â”‚  â”‚ â€¢ å®žæ—¶è¡Œæƒ…  â”‚  â”‚ â€¢ å¸‚åœºå¾®è§‚   â”‚  â”‚ â€¢ Positional Enc    â”‚   â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
    â”‚                                              â”‚                  â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚                    å¼ºåŒ–å­¦ä¹ å†³ç­–å±‚                          â”‚ â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚ â”‚
    â”‚  â”‚  â”‚Portfolio Env â”‚  â”‚Actor Networkâ”‚  â”‚Critic Network   â”‚  â”‚ â”‚
    â”‚  â”‚  â”‚              â”‚  â”‚             â”‚  â”‚                 â”‚  â”‚ â”‚
    â”‚  â”‚  â”‚State Space  â”‚â—€â–¶â”‚Policy Head  â”‚  â”‚Value Head       â”‚  â”‚ â”‚
    â”‚  â”‚  â”‚Action Space â”‚  â”‚(SAC)        â”‚  â”‚Q-Function       â”‚  â”‚ â”‚
    â”‚  â”‚  â”‚Reward Func  â”‚  â”‚             â”‚  â”‚                 â”‚  â”‚ â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â”‚                              â”‚                                â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚                    æ‰§è¡Œä¸Žç›‘æŽ§å±‚                           â”‚ â”‚
    â”‚  â”‚  â€¢ äº¤æ˜“æˆæœ¬æ¨¡åž‹  â€¢ é£Žé™©æŽ§åˆ¶  â€¢ å®žæ—¶ç›‘æŽ§  â€¢ å®¡è®¡æ—¥å¿—      â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    
    ## ðŸš€ å¿«é€Ÿå¼€å§‹
    
    ### çŽ¯å¢ƒè¦æ±‚
    
    - Python 3.8+
    - PyTorch 1.12+
    - CUDA 11.0+ (å¯é€‰ï¼Œç”¨äºŽGPUåŠ é€Ÿ)
    
    ### å®‰è£…
    
    ```bash
    # å…‹éš†é¡¹ç›®
    git clone https://github.com/rl-trading/rl-trading-system.git
    cd rl-trading-system
    
    # åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
    python -m venv venv
    source venv/bin/activate  # Linux/Mac
    # æˆ– venv\Scripts\activate  # Windows
    
    # å®‰è£…ä¾èµ–
    pip install -r requirements.txt
    
    # å®‰è£…é¡¹ç›®
    pip install -e .
    ```
    
    ### é…ç½®
    
    1. å¤åˆ¶é…ç½®æ¨¡æ¿ï¼š
    ```bash
    cp config/model_config.yaml.example config/model_config.yaml
    cp config/trading_config.yaml.example config/trading_config.yaml
    ```
    
    2. ä¿®æ”¹é…ç½®æ–‡ä»¶ä¸­çš„å‚æ•°
    
    ### è®­ç»ƒæ¨¡åž‹
    
    ```bash
    # ä½¿ç”¨é»˜è®¤é…ç½®è®­ç»ƒ
    python scripts/train.py
    
    # æŒ‡å®šé…ç½®æ–‡ä»¶å’Œè®­ç»ƒè½®æ•°
    python scripts/train.py --config config/model_config.yaml --episodes 5000
    ```
    
    ### å›žæµ‹è¯„ä¼°
    
    ```bash
    # è¯„ä¼°æ¨¡åž‹æ€§èƒ½
    python scripts/evaluate.py --model-path ./checkpoints/best_model.pth
    
    # æŒ‡å®šå›žæµ‹æ—¶é—´æ®µ
    python scripts/evaluate.py \
        --model-path ./checkpoints/best_model.pth \
        --start-date 2022-01-01 \
        --end-date 2023-12-31
    ```
    
    ### éƒ¨ç½²æ¨¡åž‹
    
    ```bash
    # é‡‘ä¸é›€éƒ¨ç½²ï¼ˆæŽ¨èï¼‰
    python scripts/deploy.py --model-path ./checkpoints/best_model.pth --deployment-type canary
    
    # å…¨é‡éƒ¨ç½²
    python scripts/deploy.py --model-path ./checkpoints/best_model.pth --deployment-type full
    ```
    
    ### å¯åŠ¨ç›‘æŽ§
    
    ```bash
    # å¯åŠ¨ç›‘æŽ§æœåŠ¡
    python scripts/monitor.py
    
    # è®¿é—®ç›‘æŽ§é¢æ¿
    # Prometheus: http://localhost:8000/metrics
    # Grafana: http://localhost:3000 (éœ€è¦å•ç‹¬é…ç½®)
    ```
    
    ## ðŸ“Š æ ¸å¿ƒåŠŸèƒ½
    
    ### æ•°æ®å¤„ç†
    - **å¤šæ•°æ®æºæ”¯æŒ**: Qlibã€Akshareã€å®žæ—¶è¡Œæƒ…
    - **ç‰¹å¾å·¥ç¨‹**: æŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬é¢å› å­ã€å¸‚åœºå¾®è§‚ç»“æž„
    - **æ•°æ®è´¨é‡æŽ§åˆ¶**: å¼‚å¸¸æ£€æµ‹ã€ç¼ºå¤±å€¼å¤„ç†ã€æ•°æ®éªŒè¯
    
    ### æ¨¡åž‹æž¶æž„
    - **Transformerç¼–ç å™¨**: æ•æ‰é•¿æœŸæ—¶åºä¾èµ–
    - **SACæ™ºèƒ½ä½“**: è¿žç»­åŠ¨ä½œç©ºé—´çš„å¼ºåŒ–å­¦ä¹ 
    - **æ³¨æ„åŠ›æœºåˆ¶**: å¤šå¤´æ³¨æ„åŠ›å’Œæ—¶é—´æ³¨æ„åŠ›èšåˆ
    
    ### äº¤æ˜“çŽ¯å¢ƒ
    - **Portfolio Environment**: ç¬¦åˆOpenAI Gymè§„èŒƒ
    - **äº¤æ˜“æˆæœ¬æ¨¡åž‹**: Almgren-Chrisså¸‚åœºå†²å‡»æ¨¡åž‹
    - **Aè‚¡è§„åˆ™**: T+1ã€æ¶¨è·Œåœã€äº¤æ˜“æ—¶é—´é™åˆ¶
    
    ### é£Žé™©æŽ§åˆ¶
    - **æŒä»“é™åˆ¶**: å•è‚¡æœ€å¤§æŒä»“ã€è¡Œä¸šæš´éœ²æŽ§åˆ¶
    - **æ­¢æŸæœºåˆ¶**: åŠ¨æ€æ­¢æŸã€æœ€å¤§å›žæ’¤æŽ§åˆ¶
    - **å®žæ—¶ç›‘æŽ§**: é£Žé™©æŒ‡æ ‡å®žæ—¶è®¡ç®—å’Œå‘Šè­¦
    
    ## ðŸ§ª æµ‹è¯•
    
    ```bash
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    pytest
    
    # è¿è¡Œå•å…ƒæµ‹è¯•
    pytest tests/unit
    
    # è¿è¡Œé›†æˆæµ‹è¯•
    pytest tests/integration
    
    # ç”Ÿæˆè¦†ç›–çŽ‡æŠ¥å‘Š
    pytest --cov=src/rl_trading_system --cov-report=html
    ```
    
    ## ðŸ“ˆ æ€§èƒ½æŒ‡æ ‡
    
    ### å›žæµ‹ç»“æžœï¼ˆç¤ºä¾‹ï¼‰
    - **å¹´åŒ–æ”¶ç›ŠçŽ‡**: 10.5%
    - **æœ€å¤§å›žæ’¤**: 12.3%
    - **å¤æ™®æ¯”çŽ‡**: 1.25
    - **ä¿¡æ¯æ¯”çŽ‡**: 0.68
    - **èƒœçŽ‡**: 52.3%
    - **å¹³å‡æŒä»“æœŸ**: 5.2å¤©
    
    ### ç³»ç»Ÿæ€§èƒ½
    - **æ¨¡åž‹æŽ¨ç†å»¶è¿Ÿ**: <50ms
    - **æ•°æ®å¤„ç†åžå**: >1000 stocks/s
    - **å†…å­˜ä½¿ç”¨**: <4GB
    - **CPUä½¿ç”¨çŽ‡**: <80%
    
    ## ðŸ”§ å¼€å‘æŒ‡å—
    
    ### ä»£ç è§„èŒƒ
    - ä½¿ç”¨ `black` è¿›è¡Œä»£ç æ ¼å¼åŒ–
    - ä½¿ç”¨ `flake8` è¿›è¡Œä»£ç æ£€æŸ¥
    - ä½¿ç”¨ `mypy` è¿›è¡Œç±»åž‹æ£€æŸ¥
    - éµå¾ª PEP 8 ç¼–ç è§„èŒƒ
    
    ### æµ‹è¯•é©±åŠ¨å¼€å‘
    1. å…ˆç¼–å†™æµ‹è¯•ç”¨ä¾‹
    2. è¿è¡Œæµ‹è¯•ï¼ˆåº”è¯¥å¤±è´¥ï¼‰
    3. ç¼–å†™æœ€å°å®žçŽ°ä»£ç 
    4. è¿è¡Œæµ‹è¯•ï¼ˆåº”è¯¥é€šè¿‡ï¼‰
    5. é‡æž„å’Œä¼˜åŒ–ä»£ç 
    
    ### æäº¤è§„èŒƒ
    ```bash
    # å®‰è£…pre-commité’©å­
    pre-commit install
    
    # æäº¤ä»£ç å‰ä¼šè‡ªåŠ¨è¿è¡Œæ£€æŸ¥
    git commit -m "feat: æ·»åŠ æ–°åŠŸèƒ½"
    ```
    
    ## ðŸ“š æ–‡æ¡£
    
    - [APIæ–‡æ¡£](docs/api/) - è¯¦ç»†çš„APIæŽ¥å£æ–‡æ¡£
    - [ç”¨æˆ·æŒ‡å—](docs/user_guide/) - ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ
    - [å¼€å‘è€…æŒ‡å—](docs/developer_guide/) - å¼€å‘è€…æ–‡æ¡£
    - [éƒ¨ç½²æŒ‡å—](docs/deployment/) - ç³»ç»Ÿéƒ¨ç½²æ–‡æ¡£
    
    ## ðŸ¤ è´¡çŒ®æŒ‡å—
    
    1. Fork é¡¹ç›®
    2. åˆ›å»ºç‰¹æ€§åˆ†æ”¯ (`git checkout -b feature/AmazingFeature`)
    3. æäº¤æ›´æ”¹ (`git commit -m 'Add some AmazingFeature'`)
    4. æŽ¨é€åˆ°åˆ†æ”¯ (`git push origin feature/AmazingFeature`)
    5. æ‰“å¼€ Pull Request
    
    ## ðŸ“„ è®¸å¯è¯
    
    æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ - æŸ¥çœ‹ [LICENSE](LICENSE) æ–‡ä»¶äº†è§£è¯¦æƒ…ã€‚
    
    ## âš ï¸ å…è´£å£°æ˜Ž
    
    æœ¬ç³»ç»Ÿä»…ä¾›å­¦ä¹ å’Œç ”ç©¶ä½¿ç”¨ï¼Œä¸æž„æˆæŠ•èµ„å»ºè®®ã€‚ä½¿ç”¨æœ¬ç³»ç»Ÿè¿›è¡Œå®žé™…äº¤æ˜“çš„é£Žé™©ç”±ç”¨æˆ·è‡ªè¡Œæ‰¿æ‹…ã€‚
    
    ## ðŸ“ž è”ç³»æˆ‘ä»¬
    
    - é¡¹ç›®ä¸»é¡µ: https://github.com/rl-trading/rl-trading-system
    - é—®é¢˜åé¦ˆ: https://github.com/rl-trading/rl-trading-system/issues
    - é‚®ç®±: team@rltrading.com
    
    ## ðŸ™ è‡´è°¢
    
    æ„Ÿè°¢ä»¥ä¸‹å¼€æºé¡¹ç›®çš„æ”¯æŒï¼š
    - [Qlib](https://github.com/microsoft/qlib) - é‡åŒ–æŠ•èµ„å¹³å°
    - [PyTorch](https://pytorch.org/) - æ·±åº¦å­¦ä¹ æ¡†æž¶
    - [OpenAI Gym](https://gym.openai.com/) - å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒ
    - [Stable Baselines3](https://stable-baselines3.readthedocs.io/) - å¼ºåŒ–å­¦ä¹ ç®—æ³•åº“
    ]]></file>
  <file path="Makefile"><![CDATA[
    # å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿ Makefile
    
    .PHONY: help install install-dev test test-unit test-integration test-e2e lint format type-check clean docs serve-docs build docker-build docker-run
    
    # é»˜è®¤ç›®æ ‡
    help:
    	@echo "å¯ç”¨çš„å‘½ä»¤ï¼š"
    	@echo "  install        - å®‰è£…é¡¹ç›®ä¾èµ–"
    	@echo "  install-dev    - å®‰è£…å¼€å‘ä¾èµ–"
    	@echo "  test           - è¿è¡Œæ‰€æœ‰æµ‹è¯•"
    	@echo "  test-unit      - è¿è¡Œå•å…ƒæµ‹è¯•"
    	@echo "  test-integration - è¿è¡Œé›†æˆæµ‹è¯•"
    	@echo "  test-e2e       - è¿è¡Œç«¯åˆ°ç«¯æµ‹è¯•"
    	@echo "  lint           - è¿è¡Œä»£ç æ£€æŸ¥"
    	@echo "  format         - æ ¼å¼åŒ–ä»£ç "
    	@echo "  type-check     - è¿è¡Œç±»åž‹æ£€æŸ¥"
    	@echo "  clean          - æ¸…ç†ä¸´æ—¶æ–‡ä»¶"
    	@echo "  docs           - ç”Ÿæˆæ–‡æ¡£"
    	@echo "  serve-docs     - å¯åŠ¨æ–‡æ¡£æœåŠ¡å™¨"
    	@echo "  build          - æž„å»ºé¡¹ç›®"
    	@echo "  docker-build   - æž„å»ºDockeré•œåƒ"
    	@echo "  docker-run     - è¿è¡ŒDockerå®¹å™¨"
    
    # å®‰è£…ä¾èµ–
    install:
    	pip install -r requirements.txt
    	pip install -e .
    
    install-dev:
    	pip install -r requirements.txt
    	pip install -e ".[dev]"
    	pre-commit install
    
    # æµ‹è¯•
    test:
    	pytest tests/ --cov=src/rl_trading_system --cov-report=term-missing --cov-report=html
    
    test-unit:
    	pytest tests/unit/ -v
    
    test-integration:
    	pytest tests/integration/ -v
    
    test-e2e:
    	pytest tests/e2e/ -v
    
    # ä»£ç è´¨é‡
    lint:
    	flake8 src/ tests/ scripts/
    	mypy src/rl_trading_system/
    
    format:
    	black src/ tests/ scripts/
    	isort src/ tests/ scripts/
    
    type-check:
    	mypy src/rl_trading_system/
    
    # æ¸…ç†
    clean:
    	find . -type f -name "*.pyc" -delete
    	find . -type d -name "__pycache__" -delete
    	find . -type d -name "*.egg-info" -exec rm -rf {} +
    	rm -rf build/
    	rm -rf dist/
    	rm -rf htmlcov/
    	rm -rf .coverage
    	rm -rf .pytest_cache/
    	rm -rf .mypy_cache/
    
    # æ–‡æ¡£
    docs:
    	cd docs && make html
    
    serve-docs:
    	cd docs/_build/html && python -m http.server 8080
    
    # æž„å»º
    build:
    	python setup.py sdist bdist_wheel
    
    # Docker
    docker-build:
    	docker build -t rl-trading-system:latest .
    
    docker-run:
    	docker run -it --rm -p 8000:8000 rl-trading-system:latest
    
    # è®­ç»ƒå’Œè¯„ä¼°
    train:
    	python scripts/train.py
    
    evaluate:
    	python scripts/evaluate.py --model-path ./checkpoints/best_model.pth
    
    deploy:
    	python scripts/deploy.py --model-path ./checkpoints/best_model.pth
    
    monitor:
    	python scripts/monitor.py
    
    # å¼€å‘å·¥å…·
    jupyter:
    	jupyter lab --ip=0.0.0.0 --port=8888 --no-browser --allow-root
    
    tensorboard:
    	tensorboard --logdir=./logs --port=6006
    
    # æ•°æ®å¤„ç†
    download-data:
    	python -c "import qlib; qlib.init(); from qlib.data import D; print('Qlibæ•°æ®åˆå§‹åŒ–å®Œæˆ')"
    
    # çŽ¯å¢ƒæ£€æŸ¥
    check-env:
    	python -c "import torch; print(f'PyTorchç‰ˆæœ¬: {torch.__version__}')"
    	python -c "import torch; print(f'CUDAå¯ç”¨: {torch.cuda.is_available()}')"
    	python -c "import qlib; print('Qlibå¯¼å…¥æˆåŠŸ')"
    	python -c "import akshare; print('Akshareå¯¼å…¥æˆåŠŸ')"
    ]]></file>
  <file path="Dockerfile"><![CDATA[
    # å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿ Docker é•œåƒ
    FROM python:3.9-slim
    
    # è®¾ç½®å·¥ä½œç›®å½•
    WORKDIR /app
    
    # è®¾ç½®çŽ¯å¢ƒå˜é‡
    ENV PYTHONPATH=/app/src
    ENV PYTHONUNBUFFERED=1
    ENV DEBIAN_FRONTEND=noninteractive
    
    # å®‰è£…ç³»ç»Ÿä¾èµ–
    RUN apt-get update && apt-get install -y \
        build-essential \
        curl \
        git \
        && rm -rf /var/lib/apt/lists/*
    
    # å¤åˆ¶requirementsæ–‡ä»¶
    COPY requirements.txt .
    
    # å®‰è£…Pythonä¾èµ–
    RUN pip install --no-cache-dir -r requirements.txt
    
    # å¤åˆ¶é¡¹ç›®æ–‡ä»¶
    COPY src/ src/
    COPY config/ config/
    COPY scripts/ scripts/
    COPY setup.py .
    COPY README.md .
    
    # å®‰è£…é¡¹ç›®
    RUN pip install -e .
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    RUN mkdir -p /app/logs /app/data /app/checkpoints /app/outputs
    
    # è®¾ç½®æƒé™
    RUN chmod +x scripts/*.py
    
    # æš´éœ²ç«¯å£
    EXPOSE 8000 8888 6006
    
    # å¥åº·æ£€æŸ¥
    HEALTHCHECK --interval=30s --timeout=30s --start-period=5s --retries=3 \
        CMD curl -f http://localhost:8000/health || exit 1
    
    # é»˜è®¤å‘½ä»¤
    CMD ["python", "scripts/monitor.py"]
    ]]></file>
  <file path=".pre-commit-config.yaml"><![CDATA[
    repos:
      - repo: https://github.com/pre-commit/pre-commit-hooks
        rev: v4.4.0
        hooks:
          - id: trailing-whitespace
          - id: end-of-file-fixer
          - id: check-yaml
          - id: check-added-large-files
          - id: check-merge-conflict
          - id: debug-statements
          - id: check-docstring-first
    
      - repo: https://github.com/psf/black
        rev: 22.6.0
        hooks:
          - id: black
            language_version: python3
    
      - repo: https://github.com/pycqa/isort
        rev: 5.10.1
        hooks:
          - id: isort
            args: ["--profile", "black"]
    
      - repo: https://github.com/pycqa/flake8
        rev: 5.0.4
        hooks:
          - id: flake8
            args: [--max-line-length=88, --extend-ignore=E203,W503]
    
      - repo: https://github.com/pre-commit/mirrors-mypy
        rev: v0.971
        hooks:
          - id: mypy
            additional_dependencies: [types-PyYAML, types-requests]
            args: [--ignore-missing-imports]
    
      - repo: local
        hooks:
          - id: pytest-check
            name: pytest-check
            entry: pytest
            language: system
            pass_filenames: false
            always_run: true
            args: [tests/unit, --tb=short]
    ]]></file>
  <file path=".gitignore"><![CDATA[
    # Byte-compiled / optimized / DLL files
    __pycache__/
    *.py[cod]
    *$py.class
    
    # C extensions
    *.so
    
    # Distribution / packaging
    .Python
    build/
    develop-eggs/
    dist/
    downloads/
    eggs/
    .eggs/
    lib/
    lib64/
    parts/
    sdist/
    var/
    wheels/
    share/python-wheels/
    *.egg-info/
    .installed.cfg
    *.egg
    MANIFEST
    
    # PyInstaller
    #  Usually these files are written by a python script from a template
    #  before PyInstaller builds the exe, so as to inject date/other infos into it.
    *.manifest
    *.spec
    
    # Installer logs
    pip-log.txt
    pip-delete-this-directory.txt
    
    # Unit test / coverage reports
    htmlcov/
    .tox/
    .nox/
    .coverage
    .coverage.*
    .cache
    nosetests.xml
    coverage.xml
    *.cover
    *.py,cover
    .hypothesis/
    .pytest_cache/
    cover/
    
    # Translations
    *.mo
    *.pot
    
    # Django stuff:
    *.log
    local_settings.py
    db.sqlite3
    db.sqlite3-journal
    
    # Flask stuff:
    instance/
    .webassets-cache
    
    # Scrapy stuff:
    .scrapy
    
    # Sphinx documentation
    docs/_build/
    
    # PyBuilder
    .pybuilder/
    target/
    
    # Jupyter Notebook
    .ipynb_checkpoints
    
    # IPython
    profile_default/
    ipython_config.py
    
    # pyenv
    #   For a library or package, you might want to ignore these files since the code is
    #   intended to run in multiple environments; otherwise, check them in:
    # .python-version
    
    # pipenv
    #   According to pypa/pipenv#598, it is recommended to include Pipfile.lock in version control.
    #   However, in case of collaboration, if having platform-specific dependencies or dependencies
    #   having no cross-platform support, pipenv may install dependencies that don't work, or not
    #   install all needed dependencies.
    #Pipfile.lock
    
    # poetry
    #   Similar to Pipfile.lock, it is generally recommended to include poetry.lock in version control.
    #   This is especially recommended for binary packages to ensure reproducibility, and is more
    #   commonly ignored for libraries.
    #   https://python-poetry.org/docs/basic-usage/#commit-your-poetrylock-file-to-version-control
    #poetry.lock
    
    # pdm
    #   Similar to Pipfile.lock, it is generally recommended to include pdm.lock in version control.
    #pdm.lock
    #   pdm stores project-wide configurations in .pdm.toml, but it is recommended to not include it
    #   in version control.
    #   https://pdm.fming.dev/#use-with-ide
    .pdm.toml
    
    # PEP 582; used by e.g. github.com/David-OConnor/pyflow and github.com/pdm-project/pdm
    __pypackages__/
    
    # Celery stuff
    celerybeat-schedule
    celerybeat.pid
    
    # SageMath parsed files
    *.sage.py
    
    # Environments
    .env
    .venv
    env/
    venv/
    ENV/
    env.bak/
    venv.bak/
    
    # Spyder project settings
    .spyderproject
    .spyproject
    
    # Rope project settings
    .ropeproject
    
    # mkdocs documentation
    /site
    
    # mypy
    .mypy_cache/
    .dmypy.json
    dmypy.json
    
    # Pyre type checker
    .pyre/
    
    # pytype static type analyzer
    .pytype/
    
    # Cython debug symbols
    cython_debug/
    
    # PyCharm
    #  JetBrains specific template is maintained in a separate JetBrains.gitignore that can
    #  be added to the global gitignore or merged into this project gitignore.  For a PyCharm
    #  project, it is recommended to ignore the entire .idea directory.
    .idea/
    
    # é¡¹ç›®ç‰¹å®šæ–‡ä»¶
    # æ•°æ®æ–‡ä»¶
    data_cache/
    *.h5
    *.hdf5
    *.parquet
    
    # æ¨¡åž‹æ–‡ä»¶
    checkpoints/
    *.pth
    *.pkl
    *.joblib
    
    # æ—¥å¿—æ–‡ä»¶
    logs/
    *.log
    
    # è¾“å‡ºæ–‡ä»¶
    outputs/
    results/
    backtest_results/
    reports/
    
    # ä¸´æ—¶æ–‡ä»¶
    tmp/
    temp/
    
    # é…ç½®æ–‡ä»¶ï¼ˆåŒ…å«æ•æ„Ÿä¿¡æ¯ï¼‰
    config/local_*.yaml
    config/*_local.yaml
    .env.local
    .env.production
    
    # æ•°æ®åº“æ–‡ä»¶
    *.db
    *.sqlite
    
    # Jupyterç›¸å…³
    .jupyter/
    jupyter_config/
    
    # TensorBoardæ—¥å¿—
    runs/
    tensorboard_logs/
    
    # MLflow
    mlruns/
    
    # DVC
    .dvc/
    *.dvc
    
    # ç³»ç»Ÿæ–‡ä»¶
    .DS_Store
    Thumbs.db
    
    # IDEæ–‡ä»¶
    .vscode/
    *.swp
    *.swo
    *~
    
    # æ–‡æ¡£æž„å»º
    docs/_build/
    docs/build/
    
    # æµ‹è¯•ç›¸å…³
    .coverage.*
    coverage.xml
    htmlcov/
    .pytest_cache/
    
    # æ€§èƒ½åˆ†æž
    *.prof
    *.profile
    ]]></file>
  <file path="tests/__init__.py"><![CDATA[
    # æµ‹è¯•æ¨¡å—
    ]]></file>
  <file path="scripts/train.py"><![CDATA[
    #!/usr/bin/env python3
    """
    è®­ç»ƒè„šæœ¬
    
    ç”¨äºŽè®­ç»ƒå¼ºåŒ–å­¦ä¹ äº¤æ˜“æ™ºèƒ½ä½“
    """
    
    import argparse
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root / "src"))
    
    from rl_trading_system.config import ConfigManager
    from rl_trading_system.training import RLTrainer
    
    
    def main():
        parser = argparse.ArgumentParser(description="è®­ç»ƒå¼ºåŒ–å­¦ä¹ äº¤æ˜“æ™ºèƒ½ä½“")
        parser.add_argument("--config", type=str, default="config/model_config.yaml",
                           help="é…ç½®æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--data-config", type=str, default="config/trading_config.yaml",
                           help="äº¤æ˜“é…ç½®æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--episodes", type=int, default=None,
                           help="è®­ç»ƒè½®æ•°")
        parser.add_argument("--output-dir", type=str, default="./outputs",
                           help="è¾“å‡ºç›®å½•")
        
        args = parser.parse_args()
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        model_config = config_manager.load_config(args.config)
        trading_config = config_manager.load_config(args.data_config)
        
        # è¦†ç›–é…ç½®å‚æ•°
        if args.episodes:
            model_config["training"]["n_episodes"] = args.episodes
        
        print("å¼€å§‹è®­ç»ƒå¼ºåŒ–å­¦ä¹ äº¤æ˜“æ™ºèƒ½ä½“...")
        print(f"é…ç½®æ–‡ä»¶: {args.config}")
        print(f"è®­ç»ƒè½®æ•°: {model_config['training']['n_episodes']}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        
        # TODO: å®žçŽ°è®­ç»ƒé€»è¾‘
        # trainer = RLTrainer(model_config, trading_config)
        # trainer.train()
        
        print("è®­ç»ƒå®Œæˆï¼")
    
    
    if __name__ == "__main__":
        main()
    ]]></file>
  <file path="scripts/monitor.py"><![CDATA[
    #!/usr/bin/env python3
    """
    ç›‘æŽ§è„šæœ¬
    
    ç”¨äºŽå¯åŠ¨ç³»ç»Ÿç›‘æŽ§æœåŠ¡
    """
    
    import argparse
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root / "src"))
    
    from rl_trading_system.config import ConfigManager
    from rl_trading_system.monitoring import TradingSystemMonitor
    
    
    def main():
        parser = argparse.ArgumentParser(description="å¯åŠ¨ç³»ç»Ÿç›‘æŽ§")
        parser.add_argument("--config", type=str, default="config/monitoring_config.yaml",
                           help="ç›‘æŽ§é…ç½®æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--port", type=int, default=8000,
                           help="ç›‘æŽ§æœåŠ¡ç«¯å£")
        
        args = parser.parse_args()
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        config = config_manager.load_config(args.config)
        
        # è¦†ç›–ç«¯å£é…ç½®
        if args.port:
            config["monitoring"]["prometheus"]["port"] = args.port
        
        print("å¯åŠ¨ç³»ç»Ÿç›‘æŽ§...")
        print(f"é…ç½®æ–‡ä»¶: {args.config}")
        print(f"ç›‘æŽ§ç«¯å£: {config['monitoring']['prometheus']['port']}")
        
        # TODO: å®žçŽ°ç›‘æŽ§é€»è¾‘
        # monitor = TradingSystemMonitor(config)
        # monitor.start()
        
        print("ç›‘æŽ§æœåŠ¡å·²å¯åŠ¨ï¼")
        print(f"PrometheusæŒ‡æ ‡åœ°å€: http://localhost:{config['monitoring']['prometheus']['port']}/metrics")
    
    
    if __name__ == "__main__":
        main()
    ]]></file>
  <file path="scripts/evaluate.py"><![CDATA[
    #!/usr/bin/env python3
    """
    è¯„ä¼°è„šæœ¬
    
    ç”¨äºŽè¯„ä¼°è®­ç»ƒå¥½çš„æ¨¡åž‹æ€§èƒ½
    """
    
    import argparse
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root / "src"))
    
    from rl_trading_system.config import ConfigManager
    from rl_trading_system.backtest import MultiFrequencyBacktest
    
    
    def main():
        parser = argparse.ArgumentParser(description="è¯„ä¼°äº¤æ˜“æ¨¡åž‹æ€§èƒ½")
        parser.add_argument("--model-path", type=str, required=True,
                           help="æ¨¡åž‹æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--config", type=str, default="config/trading_config.yaml",
                           help="é…ç½®æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--start-date", type=str, default=None,
                           help="å›žæµ‹å¼€å§‹æ—¥æœŸ")
        parser.add_argument("--end-date", type=str, default=None,
                           help="å›žæµ‹ç»“æŸæ—¥æœŸ")
        parser.add_argument("--output-dir", type=str, default="./backtest_results",
                           help="ç»“æžœè¾“å‡ºç›®å½•")
        
        args = parser.parse_args()
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        config = config_manager.load_config(args.config)
        
        # è¦†ç›–é…ç½®å‚æ•°
        if args.start_date:
            config["backtest"]["start_date"] = args.start_date
        if args.end_date:
            config["backtest"]["end_date"] = args.end_date
        
        print("å¼€å§‹æ¨¡åž‹è¯„ä¼°...")
        print(f"æ¨¡åž‹è·¯å¾„: {args.model_path}")
        print(f"å›žæµ‹æœŸé—´: {config['backtest']['start_date']} - {config['backtest']['end_date']}")
        print(f"è¾“å‡ºç›®å½•: {args.output_dir}")
        
        # TODO: å®žçŽ°è¯„ä¼°é€»è¾‘
        # backtest_engine = MultiFrequencyBacktest(config)
        # results = backtest_engine.run_backtest(model, start_date, end_date)
        
        print("è¯„ä¼°å®Œæˆï¼")
    
    
    if __name__ == "__main__":
        main()
    ]]></file>
  <file path="scripts/deploy.py"><![CDATA[
    #!/usr/bin/env python3
    """
    éƒ¨ç½²è„šæœ¬
    
    ç”¨äºŽéƒ¨ç½²äº¤æ˜“æ¨¡åž‹åˆ°ç”Ÿäº§çŽ¯å¢ƒ
    """
    
    import argparse
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root / "src"))
    
    from rl_trading_system.config import ConfigManager
    from rl_trading_system.deployment import CanaryDeployment
    
    
    def main():
        parser = argparse.ArgumentParser(description="éƒ¨ç½²äº¤æ˜“æ¨¡åž‹")
        parser.add_argument("--model-path", type=str, required=True,
                           help="æ¨¡åž‹æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--deployment-type", type=str, choices=["canary", "full"],
                           default="canary", help="éƒ¨ç½²ç±»åž‹")
        parser.add_argument("--canary-ratio", type=float, default=0.05,
                           help="é‡‘ä¸é›€éƒ¨ç½²æ¯”ä¾‹")
        parser.add_argument("--config", type=str, default="config/trading_config.yaml",
                           help="é…ç½®æ–‡ä»¶è·¯å¾„")
        
        args = parser.parse_args()
        
        # åŠ è½½é…ç½®
        config_manager = ConfigManager()
        config = config_manager.load_config(args.config)
        
        print("å¼€å§‹æ¨¡åž‹éƒ¨ç½²...")
        print(f"æ¨¡åž‹è·¯å¾„: {args.model_path}")
        print(f"éƒ¨ç½²ç±»åž‹: {args.deployment_type}")
        
        if args.deployment_type == "canary":
            print(f"é‡‘ä¸é›€æ¯”ä¾‹: {args.canary_ratio}")
            # TODO: å®žçŽ°é‡‘ä¸é›€éƒ¨ç½²é€»è¾‘
            # canary = CanaryDeployment(config)
            # canary.deploy_new_model(model, production_model)
        else:
            print("æ‰§è¡Œå…¨é‡éƒ¨ç½²...")
            # TODO: å®žçŽ°å…¨é‡éƒ¨ç½²é€»è¾‘
        
        print("éƒ¨ç½²å®Œæˆï¼")
    
    
    if __name__ == "__main__":
        main()
    ]]></file>
  <file path="monitoring/prometheus.yml"><![CDATA[
    # Prometheusé…ç½®æ–‡ä»¶
    global:
      scrape_interval: 15s
      evaluation_interval: 15s
    
    rule_files:
      # - "first_rules.yml"
      # - "second_rules.yml"
    
    scrape_configs:
      # Prometheusè‡ªèº«ç›‘æŽ§
      - job_name: 'prometheus'
        static_configs:
          - targets: ['localhost:9090']
    
      # äº¤æ˜“ç³»ç»Ÿç›‘æŽ§
      - job_name: 'rl-trading-system'
        static_configs:
          - targets: ['rl-trading-system:8000']
        metrics_path: '/metrics'
        scrape_interval: 10s
    
      # Redisç›‘æŽ§
      - job_name: 'redis'
        static_configs:
          - targets: ['redis:6379']
    
      # InfluxDBç›‘æŽ§
      - job_name: 'influxdb'
        static_configs:
          - targets: ['influxdb:8086']
    ]]></file>
  <file path="examples/transformer_usage_example.py"><![CDATA[
    """
    Transformerç¼–ç å™¨ä½¿ç”¨ç¤ºä¾‹
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨æ—¶åºTransformerå¤„ç†é‡‘èžæ•°æ®
    """
    
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    import torch
    import numpy as np
    from src.rl_trading_system.models.transformer import TimeSeriesTransformer, TransformerConfig
    
    
    def main():
        """ä¸»å‡½æ•°ï¼šæ¼”ç¤ºTransformerç¼–ç å™¨çš„ä½¿ç”¨"""
        
        print("=== Transformerç¼–ç å™¨ä½¿ç”¨ç¤ºä¾‹ ===\n")
        
        # 1. åˆ›å»ºé…ç½®
        config = TransformerConfig(
            d_model=256,        # æ¨¡åž‹ç»´åº¦
            n_heads=8,          # æ³¨æ„åŠ›å¤´æ•°
            n_layers=6,         # ç¼–ç å™¨å±‚æ•°
            d_ff=1024,         # å‰é¦ˆç½‘ç»œç»´åº¦
            dropout=0.1,        # dropoutæ¦‚çŽ‡
            max_seq_len=252,    # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆä¸€å¹´äº¤æ˜“æ—¥ï¼‰
            n_features=50,      # è¾“å…¥ç‰¹å¾æ•°
            activation='gelu'   # æ¿€æ´»å‡½æ•°
        )
        
        print(f"é…ç½®ä¿¡æ¯:")
        print(f"  æ¨¡åž‹ç»´åº¦: {config.d_model}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {config.n_heads}")
        print(f"  ç¼–ç å™¨å±‚æ•°: {config.n_layers}")
        print(f"  ç‰¹å¾æ•°: {config.n_features}")
        print()
        
        # 2. åˆ›å»ºæ¨¡åž‹
        transformer = TimeSeriesTransformer(config)
        transformer.eval()  # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
        
        # æ‰“å°æ¨¡åž‹ä¿¡æ¯
        model_info = transformer.get_model_size()
        print(f"æ¨¡åž‹ä¿¡æ¯:")
        print(f"  æ€»å‚æ•°æ•°: {model_info['total_parameters']:,}")
        print(f"  å¯è®­ç»ƒå‚æ•°æ•°: {model_info['trainable_parameters']:,}")
        print(f"  æ¨¡åž‹å¤§å°: {model_info['model_size_mb']:.2f} MB")
        print()
        
        # 3. åˆ›å»ºç¤ºä¾‹æ•°æ®
        batch_size = 2      # æ‰¹æ¬¡å¤§å°
        seq_len = 60        # åºåˆ—é•¿åº¦ï¼ˆ60ä¸ªäº¤æ˜“æ—¥ï¼‰
        n_stocks = 10       # è‚¡ç¥¨æ•°é‡
        n_features = 50     # ç‰¹å¾æ•°é‡
        
        # æ¨¡æ‹Ÿé‡‘èžæ—¶åºæ•°æ®
        # å½¢çŠ¶: [batch_size, seq_len, n_stocks, n_features]
        x = torch.randn(batch_size, seq_len, n_stocks, n_features)
        
        print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {x.shape}")
        print(f"  æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"  åºåˆ—é•¿åº¦: {seq_len}")
        print(f"  è‚¡ç¥¨æ•°é‡: {n_stocks}")
        print(f"  ç‰¹å¾æ•°é‡: {n_features}")
        print()
        
        # 4. å‰å‘ä¼ æ’­
        with torch.no_grad():
            output = transformer(x)
        
        print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"  æ‰¹æ¬¡å¤§å°: {output.shape[0]}")
        print(f"  è‚¡ç¥¨æ•°é‡: {output.shape[1]}")
        print(f"  ç¼–ç ç»´åº¦: {output.shape[2]}")
        print()
        
        # 5. æ¼”ç¤ºå¸¦æŽ©ç çš„å¤„ç†
        print("=== å¸¦æŽ©ç çš„å¤„ç† ===")
        
        # åˆ›å»ºåºåˆ—æŽ©ç ï¼ˆæŽ©ç›–æœ€åŽ10ä¸ªæ—¶é—´æ­¥ï¼‰
        mask = torch.zeros(batch_size, seq_len)
        mask[:, -10:] = float('-inf')  # æŽ©ç›–æœ€åŽ10ä¸ªæ—¶é—´æ­¥
        
        with torch.no_grad():
            masked_output = transformer(x, mask=mask)
        
        print(f"æŽ©ç å½¢çŠ¶: {mask.shape}")
        print(f"æŽ©ç è¾“å‡ºå½¢çŠ¶: {masked_output.shape}")
        print(f"è¾“å‡ºæ˜¯å¦ç›¸åŒ: {torch.allclose(output, masked_output)}")
        print()
        
        # 6. æ¼”ç¤ºæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
        print("=== æ³¨æ„åŠ›æƒé‡å¯è§†åŒ– ===")
        
        with torch.no_grad():
            attention_info = transformer.get_attention_weights(x[:1])  # åªå–ç¬¬ä¸€ä¸ªæ ·æœ¬
        
        print(f"æ—¶é—´æ³¨æ„åŠ›æƒé‡ä¿¡æ¯:")
        print(f"  è‚¡ç¥¨æ•°é‡: {attention_info['n_stocks']}")
        print(f"  åºåˆ—é•¿åº¦: {attention_info['seq_len']}")
        print(f"  æ³¨æ„åŠ›å¤´æ•°: {len(attention_info['temporal_attentions'][0])}")
        print()
        
        # 7. æ¼”ç¤ºä¸åŒè¾“å…¥å°ºå¯¸çš„å¤„ç†
        print("=== ä¸åŒè¾“å…¥å°ºå¯¸çš„å¤„ç† ===")
        
        test_cases = [
            (1, 30, 5),    # å°æ‰¹æ¬¡ï¼ŒçŸ­åºåˆ—ï¼Œå°‘è‚¡ç¥¨
            (4, 120, 20),  # å¤§æ‰¹æ¬¡ï¼Œé•¿åºåˆ—ï¼Œå¤šè‚¡ç¥¨
        ]
        
        for batch_size, seq_len, n_stocks in test_cases:
            test_x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            with torch.no_grad():
                test_output = transformer(test_x)
            
            print(f"è¾“å…¥ {test_x.shape} -> è¾“å‡º {test_output.shape}")
        
        print()
        
        # 8. æ¼”ç¤ºåºåˆ—ç¼–ç ï¼ˆä¸è¿›è¡Œæ—¶é—´èšåˆï¼‰
        print("=== åºåˆ—ç¼–ç ï¼ˆä¸èšåˆï¼‰===")
        
        x_small = torch.randn(1, 30, 5, n_features)
        
        with torch.no_grad():
            encoded_sequence = transformer.encode_sequence(x_small)
        
        print(f"è¾“å…¥å½¢çŠ¶: {x_small.shape}")
        print(f"ç¼–ç åºåˆ—å½¢çŠ¶: {encoded_sequence.shape}")
        print("æ³¨æ„ï¼šç¼–ç åºåˆ—ä¿æŒäº†æ—¶é—´ç»´åº¦ï¼Œæ²¡æœ‰è¿›è¡Œèšåˆ")
        print()
        
        # 9. æ€§èƒ½æµ‹è¯•
        print("=== æ€§èƒ½æµ‹è¯• ===")
        
        import time
        
        test_x = torch.randn(2, 60, 10, n_features)
        
        # é¢„çƒ­
        with torch.no_grad():
            for _ in range(5):
                _ = transformer(test_x)
        
        # æµ‹è¯•æŽ¨ç†æ—¶é—´
        start_time = time.time()
        with torch.no_grad():
            for _ in range(100):
                _ = transformer(test_x)
        end_time = time.time()
        
        avg_time = (end_time - start_time) / 100
        print(f"å¹³å‡æŽ¨ç†æ—¶é—´: {avg_time*1000:.2f} ms")
        print(f"æ¯ç§’å¯å¤„ç†æ ·æœ¬æ•°: {1/avg_time:.1f}")
        print()
        
        print("=== ç¤ºä¾‹å®Œæˆ ===")
    
    
    if __name__ == "__main__":
        main()
    ]]></file>
  <file path="examples/data_interface_usage_example.py"><![CDATA[
    """
    æ•°æ®æŽ¥å£ä½¿ç”¨ç¤ºä¾‹
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨QlibDataInterfaceå’ŒAkshareDataInterfaceèŽ·å–æ•°æ®
    """
    
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
    
    import logging
    from datetime import datetime, timedelta
    from src.rl_trading_system.data.qlib_interface import QlibDataInterface
    from src.rl_trading_system.data.akshare_interface import AkshareDataInterface
    from src.rl_trading_system.data.data_cache import get_global_cache
    from src.rl_trading_system.data.data_quality import get_global_quality_checker
    
    # é…ç½®æ—¥å¿—
    logging.basicConfig(level=logging.INFO)
    logger = logging.getLogger(__name__)
    
    
    def demo_qlib_interface():
        """æ¼”ç¤ºQlibæ•°æ®æŽ¥å£ä½¿ç”¨"""
        logger.info("=== Qlibæ•°æ®æŽ¥å£æ¼”ç¤º ===")
        
        try:
            # åˆ›å»ºQlibæŽ¥å£å®žä¾‹
            qlib_interface = QlibDataInterface()
            
            # èŽ·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿï¼Œå®žé™…éœ€è¦qlibçŽ¯å¢ƒï¼‰
            logger.info("èŽ·å–è‚¡ç¥¨åˆ—è¡¨...")
            # stock_list = qlib_interface.get_stock_list('A')
            # logger.info(f"èŽ·å–åˆ°{len(stock_list)}åªè‚¡ç¥¨")
            
            # èŽ·å–ä»·æ ¼æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
            symbols = ['000001.SZ', '000002.SZ']
            start_date = '2023-01-01'
            end_date = '2023-01-10'
            
            logger.info(f"èŽ·å–ä»·æ ¼æ•°æ®: {symbols}")
            # price_data = qlib_interface.get_price_data(symbols, start_date, end_date)
            # logger.info(f"èŽ·å–åˆ°{len(price_data)}æ¡ä»·æ ¼æ•°æ®")
            
            logger.info("QlibæŽ¥å£æ¼”ç¤ºå®Œæˆï¼ˆéœ€è¦qlibçŽ¯å¢ƒæ‰èƒ½å®žé™…è¿è¡Œï¼‰")
            
        except ImportError as e:
            logger.warning(f"Qlibæœªå®‰è£…: {e}")
        except Exception as e:
            logger.error(f"QlibæŽ¥å£æ¼”ç¤ºå¤±è´¥: {e}")
    
    
    def demo_akshare_interface():
        """æ¼”ç¤ºAkshareæ•°æ®æŽ¥å£ä½¿ç”¨"""
        logger.info("=== Akshareæ•°æ®æŽ¥å£æ¼”ç¤º ===")
        
        try:
            # åˆ›å»ºAkshareæŽ¥å£å®žä¾‹
            akshare_interface = AkshareDataInterface(rate_limit=0.5)
            
            # èŽ·å–è‚¡ç¥¨åˆ—è¡¨ï¼ˆæ¨¡æ‹Ÿï¼Œå®žé™…éœ€è¦akshareçŽ¯å¢ƒï¼‰
            logger.info("èŽ·å–è‚¡ç¥¨åˆ—è¡¨...")
            # stock_list = akshare_interface.get_stock_list('A')
            # logger.info(f"èŽ·å–åˆ°{len(stock_list)}åªè‚¡ç¥¨")
            
            # èŽ·å–ä»·æ ¼æ•°æ®ï¼ˆæ¨¡æ‹Ÿï¼‰
            symbols = ['000001', '000002']
            start_date = '2023-01-01'
            end_date = '2023-01-10'
            
            logger.info(f"èŽ·å–ä»·æ ¼æ•°æ®: {symbols}")
            # price_data = akshare_interface.get_price_data(symbols, start_date, end_date)
            # logger.info(f"èŽ·å–åˆ°{len(price_data)}æ¡ä»·æ ¼æ•°æ®")
            
            logger.info("AkshareæŽ¥å£æ¼”ç¤ºå®Œæˆï¼ˆéœ€è¦akshareçŽ¯å¢ƒæ‰èƒ½å®žé™…è¿è¡Œï¼‰")
            
        except ImportError as e:
            logger.warning(f"Akshareæœªå®‰è£…: {e}")
        except Exception as e:
            logger.error(f"AkshareæŽ¥å£æ¼”ç¤ºå¤±è´¥: {e}")
    
    
    def demo_cache_usage():
        """æ¼”ç¤ºç¼“å­˜ä½¿ç”¨"""
        logger.info("=== ç¼“å­˜ä½¿ç”¨æ¼”ç¤º ===")
        
        # èŽ·å–å…¨å±€ç¼“å­˜å®žä¾‹
        cache = get_global_cache()
        
        # è®¾ç½®ç¼“å­˜
        test_data = "è¿™æ˜¯æµ‹è¯•æ•°æ®"
        cache.set("test_key", test_data, ttl=60)  # ç¼“å­˜60ç§’
        logger.info("æ•°æ®å·²ç¼“å­˜")
        
        # èŽ·å–ç¼“å­˜
        cached_data = cache.get("test_key")
        logger.info(f"ä»Žç¼“å­˜èŽ·å–æ•°æ®: {cached_data}")
        
        # èŽ·å–ç¼“å­˜ä¿¡æ¯
        cache_info = cache.get_cache_info()
        logger.info(f"ç¼“å­˜ä¿¡æ¯: {cache_info}")
        
        # æ¸…ç†ç¼“å­˜
        cache.clear()
        logger.info("ç¼“å­˜å·²æ¸…ç†")
    
    
    def demo_quality_checker():
        """æ¼”ç¤ºæ•°æ®è´¨é‡æ£€æŸ¥"""
        logger.info("=== æ•°æ®è´¨é‡æ£€æŸ¥æ¼”ç¤º ===")
        
        import pandas as pd
        import numpy as np
        
        # èŽ·å–å…¨å±€è´¨é‡æ£€æŸ¥å™¨
        checker = get_global_quality_checker()
        
        # åˆ›å»ºæµ‹è¯•ä»·æ ¼æ•°æ®
        price_data = pd.DataFrame({
            'open': [10.0, 11.0, 12.0, 13.0, 14.0],
            'high': [11.0, 12.0, 13.0, 14.0, 15.0],
            'low': [9.0, 10.0, 11.0, 12.0, 13.0],
            'close': [10.5, 11.5, 12.5, 13.5, 14.5],
            'volume': [1000, 1200, 1100, 1300, 1150],
            'amount': [10500, 13800, 13750, 17550, 16675]
        })
        
        # æ£€æŸ¥æ•°æ®è´¨é‡
        quality_report = checker.check_data_quality(price_data, 'price')
        logger.info(f"æ•°æ®è´¨é‡çŠ¶æ€: {quality_report['status']}")
        logger.info(f"æ•°æ®è´¨é‡åˆ†æ•°: {quality_report['score']:.2f}")
        logger.info(f"é—®é¢˜æ•°é‡: {len(quality_report['issues'])}")
        logger.info(f"è­¦å‘Šæ•°é‡: {len(quality_report['warnings'])}")
        
        # åˆ›å»ºæœ‰é—®é¢˜çš„æ•°æ®
        bad_data = pd.DataFrame({
            'open': [10.0, -1.0, 12.0],  # åŒ…å«è´Ÿå€¼
            'high': [11.0, 12.0, 11.0],  # æœ€é«˜ä»·ä½ŽäºŽæœ€ä½Žä»·
            'low': [9.0, 10.0, 13.0],
            'close': [10.5, np.nan, 12.5],  # åŒ…å«ç¼ºå¤±å€¼
            'volume': [1000, 1200, 1100],
            'amount': [10500, 13800, 13750]
        })
        
        bad_quality_report = checker.check_data_quality(bad_data, 'price')
        logger.info(f"é—®é¢˜æ•°æ®è´¨é‡çŠ¶æ€: {bad_quality_report['status']}")
        logger.info(f"é—®é¢˜æ•°æ®è´¨é‡åˆ†æ•°: {bad_quality_report['score']:.2f}")
        logger.info(f"å‘çŽ°çš„é—®é¢˜: {bad_quality_report['issues']}")
        
        # æ¸…æ´—æ•°æ®
        cleaned_data = checker.clean_data(bad_data, 'price', 'conservative')
        logger.info(f"æ¸…æ´—å‰æ•°æ®è¡Œæ•°: {len(bad_data)}")
        logger.info(f"æ¸…æ´—åŽæ•°æ®è¡Œæ•°: {len(cleaned_data)}")
    
    
    def main():
        """ä¸»å‡½æ•°"""
        logger.info("æ•°æ®æŽ¥å£ä½¿ç”¨ç¤ºä¾‹å¼€å§‹")
        
        # æ¼”ç¤ºå„ä¸ªåŠŸèƒ½
        demo_qlib_interface()
        print()
        
        demo_akshare_interface()
        print()
        
        demo_cache_usage()
        print()
        
        demo_quality_checker()
        
        logger.info("æ•°æ®æŽ¥å£ä½¿ç”¨ç¤ºä¾‹ç»“æŸ")
    
    
    if __name__ == "__main__":
        main()
    ]]></file>
  <file path="examples/config_usage_example.py"><![CDATA[
    #!/usr/bin/env python3
    """
    é…ç½®ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹
    
    æ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ConfigManageråŠ è½½ã€éªŒè¯å’Œç®¡ç†é…ç½®æ–‡ä»¶
    """
    
    import os
    import sys
    from pathlib import Path
    
    # æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°Pythonè·¯å¾„
    project_root = Path(__file__).parent.parent
    sys.path.insert(0, str(project_root))
    
    from src.rl_trading_system.config import (
        ConfigManager, 
        MODEL_CONFIG_SCHEMA, 
        TRADING_CONFIG_SCHEMA,
        ConfigLoadError,
        ConfigValidationError
    )
    
    
    def main():
        """ä¸»å‡½æ•°"""
        print("=== é…ç½®ç®¡ç†å™¨ä½¿ç”¨ç¤ºä¾‹ ===\n")
        
        # åˆ›å»ºé…ç½®ç®¡ç†å™¨å®žä¾‹
        config_manager = ConfigManager()
        
        # ç¤ºä¾‹1ï¼šåŸºæœ¬é…ç½®åŠ è½½
        print("1. åŸºæœ¬é…ç½®åŠ è½½")
        try:
            model_config = config_manager.load_config("config/model_config.yaml")
            print(f"   æ¨¡åž‹ç»´åº¦: {model_config['model']['transformer']['d_model']}")
            print(f"   å­¦ä¹ çŽ‡: {model_config['model']['sac']['lr_actor']}")
        except ConfigLoadError as e:
            print(f"   é…ç½®åŠ è½½å¤±è´¥: {e}")
        
        print()
        
        # ç¤ºä¾‹2ï¼šçŽ¯å¢ƒå˜é‡è¦†ç›–
        print("2. çŽ¯å¢ƒå˜é‡è¦†ç›–")
        
        # è®¾ç½®çŽ¯å¢ƒå˜é‡
        os.environ['MODEL_TRANSFORMER_D_MODEL'] = '512'
        os.environ['MODEL_SAC_LR_ACTOR'] = '1e-3'
        
        try:
            model_config_with_env = config_manager.load_config(
                "config/model_config.yaml", 
                enable_env_override=True
            )
            print(f"   åŽŸå§‹æ¨¡åž‹ç»´åº¦: 256")
            print(f"   çŽ¯å¢ƒå˜é‡è¦†ç›–åŽ: {model_config_with_env['model']['transformer']['d_model']}")
            print(f"   åŽŸå§‹å­¦ä¹ çŽ‡: 3e-4")
            print(f"   çŽ¯å¢ƒå˜é‡è¦†ç›–åŽ: {model_config_with_env['model']['sac']['lr_actor']}")
        except ConfigLoadError as e:
            print(f"   é…ç½®åŠ è½½å¤±è´¥: {e}")
        
        # æ¸…ç†çŽ¯å¢ƒå˜é‡
        del os.environ['MODEL_TRANSFORMER_D_MODEL']
        del os.environ['MODEL_SAC_LR_ACTOR']
        
        print()
        
        # ç¤ºä¾‹3ï¼šé…ç½®éªŒè¯
        print("3. é…ç½®éªŒè¯")
        try:
            trading_config = config_manager.load_config("config/trading_config.yaml")
            config_manager.validate_config(trading_config, TRADING_CONFIG_SCHEMA)
            print("   âœ“ äº¤æ˜“é…ç½®éªŒè¯é€šè¿‡")
        except ConfigValidationError as e:
            print(f"   âœ— é…ç½®éªŒè¯å¤±è´¥: {e}")
        except ConfigLoadError as e:
            print(f"   âœ— é…ç½®åŠ è½½å¤±è´¥: {e}")
        
        print()
        
        # ç¤ºä¾‹4ï¼šåº”ç”¨é»˜è®¤å€¼
        print("4. åº”ç”¨é»˜è®¤å€¼")
        try:
            # åˆ›å»ºä¸€ä¸ªä¸å®Œæ•´çš„é…ç½®
            incomplete_config = {
                'model': {
                    'transformer': {
                        'd_model': 128  # åªè®¾ç½®éƒ¨åˆ†å€¼
                    }
                }
            }
            
            # åº”ç”¨é»˜è®¤å€¼
            complete_config = config_manager.apply_defaults(incomplete_config, MODEL_CONFIG_SCHEMA)
            
            print(f"   è®¾ç½®çš„å€¼: d_model = {complete_config['model']['transformer']['d_model']}")
            print(f"   é»˜è®¤å€¼: n_heads = {complete_config['model']['transformer']['n_heads']}")
            print(f"   é»˜è®¤å€¼: dropout = {complete_config['model']['transformer']['dropout']}")
            
        except Exception as e:
            print(f"   åº”ç”¨é»˜è®¤å€¼å¤±è´¥: {e}")
        
        print()
        
        # ç¤ºä¾‹5ï¼šå®Œæ•´å·¥ä½œæµç¨‹
        print("5. å®Œæ•´å·¥ä½œæµç¨‹ï¼ˆåŠ è½½ + éªŒè¯ + é»˜è®¤å€¼ï¼‰")
        try:
            complete_config = config_manager.load_and_validate_config(
                "config/model_config.yaml",
                MODEL_CONFIG_SCHEMA,
                enable_env_override=True
            )
            print("   âœ“ å®Œæ•´å·¥ä½œæµç¨‹æ‰§è¡ŒæˆåŠŸ")
            print(f"   æœ€ç»ˆé…ç½®åŒ…å« {len(complete_config)} ä¸ªé¡¶çº§é…ç½®é¡¹")
            
        except (ConfigLoadError, ConfigValidationError) as e:
            print(f"   âœ— å®Œæ•´å·¥ä½œæµç¨‹å¤±è´¥: {e}")
        
        print()
        
        # ç¤ºä¾‹6ï¼šå¤šé…ç½®æ–‡ä»¶åˆå¹¶
        print("6. å¤šé…ç½®æ–‡ä»¶åˆå¹¶")
        try:
            config_files = [
                "config/model_config.yaml",
                "config/trading_config.yaml"
            ]
            
            merged_config = config_manager.load_configs(config_files)
            
            print(f"   åˆå¹¶åŽåŒ…å«é…ç½®é¡¹: {list(merged_config.keys())}")
            print("   âœ“ å¤šé…ç½®æ–‡ä»¶åˆå¹¶æˆåŠŸ")
            
        except ConfigLoadError as e:
            print(f"   âœ— å¤šé…ç½®æ–‡ä»¶åˆå¹¶å¤±è´¥: {e}")
        
        print()
        
        # ç¤ºä¾‹7ï¼šé…ç½®ç¼“å­˜
        print("7. é…ç½®ç¼“å­˜")
        try:
            # é¦–æ¬¡åŠ è½½ï¼ˆä»Žæ–‡ä»¶ï¼‰
            config1 = config_manager.load_config("config/model_config.yaml", use_cache=True)
            
            # å†æ¬¡åŠ è½½ï¼ˆä»Žç¼“å­˜ï¼‰
            config2 = config_manager.load_config("config/model_config.yaml", use_cache=True)
            
            print("   âœ“ é…ç½®ç¼“å­˜åŠŸèƒ½æ­£å¸¸")
            print(f"   ä¸¤æ¬¡åŠ è½½çš„é…ç½®å†…å®¹ç›¸åŒ: {config1 == config2}")
            
        except ConfigLoadError as e:
            print(f"   âœ— é…ç½®ç¼“å­˜æµ‹è¯•å¤±è´¥: {e}")
        
        print("\n=== ç¤ºä¾‹å®Œæˆ ===")
    
    
    if __name__ == "__main__":
        main()
    ]]></file>
  <file path="docs/README.md"><![CDATA[
    # å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿæ–‡æ¡£
    
    ## é¡¹ç›®æ¦‚è¿°
    
    åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿï¼Œé‡‡ç”¨SACï¼ˆSoft Actor-Criticï¼‰ç®—æ³•ä½œä¸ºå†³ç­–å¼•æ“Žï¼Œä½¿ç”¨Transformeræž¶æž„æ•æ‰é•¿æœŸæ—¶åºä¾èµ–ã€‚
    
    ## æ–‡æ¡£ç»“æž„
    
    - [APIæ–‡æ¡£](api/) - ç³»ç»ŸAPIæŽ¥å£æ–‡æ¡£
    - [ç”¨æˆ·æŒ‡å—](user_guide/) - ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ
    - [å¼€å‘è€…æŒ‡å—](developer_guide/) - å¼€å‘è€…æ–‡æ¡£
    - [éƒ¨ç½²æŒ‡å—](deployment/) - ç³»ç»Ÿéƒ¨ç½²æ–‡æ¡£
    - [æ•…éšœæŽ’é™¤](troubleshooting/) - å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ
    
    ## å¿«é€Ÿå¼€å§‹
    
    è¯·å‚è€ƒ [å¿«é€Ÿå¼€å§‹æŒ‡å—](user_guide/quick_start.md) äº†è§£å¦‚ä½•å¿«é€Ÿéƒ¨ç½²å’Œä½¿ç”¨ç³»ç»Ÿã€‚
    
    ## ç³»ç»Ÿæž¶æž„
    
    è¯·å‚è€ƒ [ç³»ç»Ÿæž¶æž„æ–‡æ¡£](developer_guide/architecture.md) äº†è§£ç³»ç»Ÿçš„è¯¦ç»†è®¾è®¡ã€‚
    ]]></file>
  <file path="src/__init__.py"><![CDATA[
    # å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿ
    # åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“
    ]]></file>
  <file path="config/trading_config.yaml"><![CDATA[
    # äº¤æ˜“é…ç½®æ–‡ä»¶
    # æ”¯æŒçŽ¯å¢ƒå˜é‡è¦†ç›–ï¼Œä¾‹å¦‚ï¼šTRADING_ENVIRONMENT_INITIAL_CASH=2000000.0
    trading:
      environment:
        stock_pool: []              # è‚¡ç¥¨æ± ï¼Œå°†åœ¨è¿è¡Œæ—¶å¡«å……ï¼Œå¯é€šè¿‡ TRADING_ENVIRONMENT_STOCK_POOL è¦†ç›–
        lookback_window: 60         # å›žçœ‹çª—å£ï¼Œå¯é€šè¿‡ TRADING_ENVIRONMENT_LOOKBACK_WINDOW è¦†ç›–
        initial_cash: 1000000.0     # åˆå§‹èµ„é‡‘ï¼Œå¯é€šè¿‡ TRADING_ENVIRONMENT_INITIAL_CASH è¦†ç›–
        commission_rate: 0.001      # æ‰‹ç»­è´¹çŽ‡ 0.1%ï¼Œå¯é€šè¿‡ TRADING_ENVIRONMENT_COMMISSION_RATE è¦†ç›–
        stamp_tax_rate: 0.001       # å°èŠ±ç¨ŽçŽ‡ 0.1%ï¼Œå¯é€šè¿‡ TRADING_ENVIRONMENT_STAMP_TAX_RATE è¦†ç›–
        risk_aversion: 0.1          # é£Žé™©åŽŒæ¶ç³»æ•°ï¼Œå¯é€šè¿‡ TRADING_ENVIRONMENT_RISK_AVERSION è¦†ç›–
        max_drawdown_penalty: 1.0   # æœ€å¤§å›žæ’¤æƒ©ç½šï¼Œå¯é€šè¿‡ TRADING_ENVIRONMENT_MAX_DRAWDOWN_PENALTY è¦†ç›–
        
      cost_model:
        almgren_chriss:
          permanent_impact: 0.1     # æ°¸ä¹…å†²å‡»ç³»æ•°ï¼Œå¯é€šè¿‡ TRADING_COST_MODEL_ALMGREN_CHRISS_PERMANENT_IMPACT è¦†ç›–
          temporary_impact: 0.05    # ä¸´æ—¶å†²å‡»ç³»æ•°ï¼Œå¯é€šè¿‡ TRADING_COST_MODEL_ALMGREN_CHRISS_TEMPORARY_IMPACT è¦†ç›–
        
      data:
        provider: "qlib"            # æ•°æ®æä¾›å•† qlib æˆ– akshareï¼Œå¯é€šè¿‡ TRADING_DATA_PROVIDER è¦†ç›–
        cache_enabled: true         # æ˜¯å¦å¯ç”¨ç¼“å­˜ï¼Œå¯é€šè¿‡ TRADING_DATA_CACHE_ENABLED è¦†ç›–
        cache_dir: "./data_cache"   # ç¼“å­˜ç›®å½•ï¼Œå¯é€šè¿‡ TRADING_DATA_CACHE_DIR è¦†ç›–
        
    backtest:
      freq: "1d"                    # å›žæµ‹é¢‘çŽ‡ 1d æˆ– 1minï¼Œå¯é€šè¿‡ BACKTEST_FREQ è¦†ç›–
      rebalance_freq: "1d"          # å†å¹³è¡¡é¢‘çŽ‡ï¼Œå¯é€šè¿‡ BACKTEST_REBALANCE_FREQ è¦†ç›–
      start_date: "2020-01-01"      # å›žæµ‹å¼€å§‹æ—¥æœŸï¼Œå¯é€šè¿‡ BACKTEST_START_DATE è¦†ç›–
      end_date: "2023-12-31"        # å›žæµ‹ç»“æŸæ—¥æœŸï¼Œå¯é€šè¿‡ BACKTEST_END_DATE è¦†ç›–
      benchmark: "000300.SH"        # åŸºå‡†æŒ‡æ•°ï¼ˆæ²ªæ·±300ï¼‰ï¼Œå¯é€šè¿‡ BACKTEST_BENCHMARK è¦†ç›–
    ]]></file>
  <file path="config/monitoring_config.yaml"><![CDATA[
    # ç›‘æŽ§é…ç½®æ–‡ä»¶
    monitoring:
      prometheus:
        port: 8000
        metrics_path: "/metrics"
        
      thresholds:
        max_drawdown: 0.15
        min_sharpe_ratio: 0.5
        max_var_95: 0.03
        lookback_days: 90
        
      alerts:
        channels:
          - email
          - dingtalk
        email:
          smtp_server: "smtp.gmail.com"
          smtp_port: 587
          username: ""
          password: ""
          recipients: []
        dingtalk:
          webhook_url: ""
          
    logging:
      level: "INFO"
      format: "{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}"
      rotation: "1 day"
      retention: "30 days"
      compression: "gz"
    ]]></file>
  <file path="config/model_config.yaml"><![CDATA[
    # æ¨¡åž‹é…ç½®æ–‡ä»¶
    # æ”¯æŒçŽ¯å¢ƒå˜é‡è¦†ç›–ï¼Œä¾‹å¦‚ï¼šMODEL_TRANSFORMER_D_MODEL=512
    model:
      transformer:
        d_model: 256                # æ¨¡åž‹ç»´åº¦ï¼Œå¯é€šè¿‡ MODEL_TRANSFORMER_D_MODEL è¦†ç›–
        n_heads: 8                  # æ³¨æ„åŠ›å¤´æ•°ï¼Œå¯é€šè¿‡ MODEL_TRANSFORMER_N_HEADS è¦†ç›–
        n_layers: 6                 # ç¼–ç å™¨å±‚æ•°ï¼Œå¯é€šè¿‡ MODEL_TRANSFORMER_N_LAYERS è¦†ç›–
        d_ff: 1024                  # å‰é¦ˆç½‘ç»œç»´åº¦ï¼Œå¯é€šè¿‡ MODEL_TRANSFORMER_D_FF è¦†ç›–
        dropout: 0.1                # DropoutçŽ‡ï¼Œå¯é€šè¿‡ MODEL_TRANSFORMER_DROPOUT è¦†ç›–
        max_seq_len: 252            # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆä¸€å¹´äº¤æ˜“æ—¥ï¼‰ï¼Œå¯é€šè¿‡ MODEL_TRANSFORMER_MAX_SEQ_LEN è¦†ç›–
        n_features: 50              # ç‰¹å¾æ•°é‡ï¼Œå¯é€šè¿‡ MODEL_TRANSFORMER_N_FEATURES è¦†ç›–
      
      sac:
        state_dim: 256              # çŠ¶æ€ç»´åº¦ï¼Œå¯é€šè¿‡ MODEL_SAC_STATE_DIM è¦†ç›–
        action_dim: 100             # åŠ¨ä½œç»´åº¦ï¼ˆè‚¡ç¥¨æ•°é‡ï¼‰ï¼Œå¯é€šè¿‡ MODEL_SAC_ACTION_DIM è¦†ç›–
        hidden_dim: 512             # éšè—å±‚ç»´åº¦ï¼Œå¯é€šè¿‡ MODEL_SAC_HIDDEN_DIM è¦†ç›–
        lr_actor: 0.0003            # Actorå­¦ä¹ çŽ‡ï¼Œå¯é€šè¿‡ MODEL_SAC_LR_ACTOR è¦†ç›–
        lr_critic: 0.0003           # Criticå­¦ä¹ çŽ‡ï¼Œå¯é€šè¿‡ MODEL_SAC_LR_CRITIC è¦†ç›–
        lr_alpha: 0.0003            # Alphaå­¦ä¹ çŽ‡ï¼Œå¯é€šè¿‡ MODEL_SAC_LR_ALPHA è¦†ç›–
        gamma: 0.99                 # æŠ˜æ‰£å› å­ï¼Œå¯é€šè¿‡ MODEL_SAC_GAMMA è¦†ç›–
        tau: 0.005                  # è½¯æ›´æ–°ç³»æ•°ï¼Œå¯é€šè¿‡ MODEL_SAC_TAU è¦†ç›–
        alpha: 0.2                  # ç†µæ­£åˆ™åŒ–ç³»æ•°ï¼Œå¯é€šè¿‡ MODEL_SAC_ALPHA è¦†ç›–
        target_entropy: -100.0      # ç›®æ ‡ç†µï¼Œå¯é€šè¿‡ MODEL_SAC_TARGET_ENTROPY è¦†ç›–
        buffer_size: 1000000        # ç»éªŒå›žæ”¾ç¼“å†²åŒºå¤§å°ï¼Œå¯é€šè¿‡ MODEL_SAC_BUFFER_SIZE è¦†ç›–
        batch_size: 256             # æ‰¹æ¬¡å¤§å°ï¼Œå¯é€šè¿‡ MODEL_SAC_BATCH_SIZE è¦†ç›–
        
    training:
      n_episodes: 10000             # è®­ç»ƒè½®æ•°ï¼Œå¯é€šè¿‡ TRAINING_N_EPISODES è¦†ç›–
      eval_freq: 100                # è¯„ä¼°é¢‘çŽ‡ï¼Œå¯é€šè¿‡ TRAINING_EVAL_FREQ è¦†ç›–
      patience: 50                  # æ—©åœè€å¿ƒå€¼ï¼Œå¯é€šè¿‡ TRAINING_PATIENCE è¦†ç›–
      min_delta: 0.001              # æœ€å°æ”¹è¿›é˜ˆå€¼ï¼Œå¯é€šè¿‡ TRAINING_MIN_DELTA è¦†ç›–
      checkpoint_dir: "./checkpoints"  # æ£€æŸ¥ç‚¹ç›®å½•ï¼Œå¯é€šè¿‡ TRAINING_CHECKPOINT_DIR è¦†ç›–
    ]]></file>
  <file path="config/compliance_config.yaml"><![CDATA[
    # åˆè§„é…ç½®æ–‡ä»¶
    compliance:
      audit:
        retention_years: 5
        database_url: "influxdb://localhost:8086/trading_audit"
        
      risk_control:
        max_position_concentration: 0.1  # å•åªè‚¡ç¥¨æœ€å¤§æŒä»“æ¯”ä¾‹
        max_sector_exposure: 0.3  # å•ä¸ªè¡Œä¸šæœ€å¤§æš´éœ²
        stop_loss_threshold: -0.05  # æ­¢æŸé˜ˆå€¼
        
      explainability:
        shap_enabled: true
        lime_enabled: true
        attention_visualization: true
        
      reporting:
        daily_report: true
        weekly_report: true
        monthly_report: true
        report_recipients: []
    ]]></file>
  <file path="archive/task.md"><![CDATA[
    # å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿå¼€å‘ä»»åŠ¡æ¸…å•
    
    ## é¡¹ç›®æ¦‚è¿°
    åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“å¼€å‘ä»»åŠ¡åˆ†è§£
    
    ## é˜¶æ®µä¸€ï¼šåŸºç¡€è®¾æ–½æ­å»ºï¼ˆç¬¬1-2å‘¨ï¼‰
    
    ### 1. çŽ¯å¢ƒé…ç½®ä¸Žé¡¹ç›®åˆå§‹åŒ–
    - [ ] åˆ›å»ºé¡¹ç›®ç›®å½•ç»“æž„
    - [ ] é…ç½®Pythonè™šæ‹ŸçŽ¯å¢ƒï¼ˆPython 3.8+ï¼‰
    - [ ] å®‰è£…æ ¸å¿ƒä¾èµ–ï¼šPyTorch, Qlib, Akshare, OpenAI Gym
    - [ ] é…ç½®Gitä»“åº“å’Œ.gitignore
    - [ ] ç¼–å†™requirements.txtå’Œsetup.py
    - [ ] é…ç½®æ—¥å¿—ç³»ç»Ÿï¼ˆä½¿ç”¨loguruï¼‰
    
    ### 2. é…ç½®ç®¡ç†ç³»ç»Ÿ
    - [ ] åˆ›å»ºYAMLé…ç½®æ–‡ä»¶æ¨¡æ¿ï¼ˆmodel_config.yaml, trading_config.yamlç­‰ï¼‰
    - [ ] å®žçŽ°é…ç½®åŠ è½½å™¨ï¼ˆæ”¯æŒçŽ¯å¢ƒå˜é‡è¦†ç›–ï¼‰
    - [ ] è®¾ç½®é»˜è®¤å‚æ•°å€¼
    - [ ] é…ç½®éªŒè¯æœºåˆ¶
    
    ### 3. æ•°æ®æŽ¥å…¥å±‚
    - [ ] å®žçŽ°Qlibæ•°æ®æŽ¥å£å°è£…
    - [ ] å®žçŽ°Akshareå®žæ—¶è¡Œæƒ…æŽ¥å£
    - [ ] è®¾è®¡ç»Ÿä¸€çš„æ•°æ®æ ¼å¼æ ‡å‡†
    - [ ] å®žçŽ°æ•°æ®ç¼“å­˜æœºåˆ¶ï¼ˆRedis/æœ¬åœ°æ–‡ä»¶ï¼‰
    - [ ] æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·
    
    ## é˜¶æ®µäºŒï¼šæ ¸å¿ƒç»„ä»¶å¼€å‘ï¼ˆç¬¬3-5å‘¨ï¼‰
    
    ### 4. å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒå®žçŽ°
    - [ ] å®žçŽ°PortfolioEnvåŸºç±»ï¼ˆç»§æ‰¿gym.Envï¼‰
    - [ ] å®šä¹‰çŠ¶æ€ç©ºé—´ï¼ˆåŽ†å²ç‰¹å¾ã€æŒä»“ã€å¸‚åœºçŠ¶æ€ï¼‰
    - [ ] å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆæŠ•èµ„ç»„åˆæƒé‡ï¼‰
    - [ ] å®žçŽ°å¥–åŠ±å‡½æ•°ï¼ˆè€ƒè™‘æ”¶ç›Šã€é£Žé™©ã€æˆæœ¬ï¼‰
    - [ ] å®žçŽ°reset()å’Œstep()æ–¹æ³•
    - [ ] æ·»åŠ Aè‚¡äº¤æ˜“è§„åˆ™çº¦æŸï¼ˆT+1ã€æ¶¨è·Œåœç­‰ï¼‰
    
    ### 5. äº¤æ˜“æˆæœ¬æ¨¡åž‹
    - [ ] å®žçŽ°å›ºå®šæˆæœ¬è®¡ç®—ï¼ˆæ‰‹ç»­è´¹ã€å°èŠ±ç¨Žï¼‰
    - [ ] å®žçŽ°Almgren-Chrisså¸‚åœºå†²å‡»æ¨¡åž‹
    - [ ] æ·»åŠ æ»‘ç‚¹ä¼°è®¡
    - [ ] æ”¯æŒä¸åŒå¸‚åœºæ¡ä»¶ä¸‹çš„æˆæœ¬è°ƒæ•´
    
    ### 6. ç‰¹å¾å·¥ç¨‹æ¨¡å—
    - [ ] æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ï¼ˆRSI, MACD, Bollinger Bandsç­‰ï¼‰
    - [ ] åŸºæœ¬é¢å› å­æå–
    - [ ] å¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
    - [ ] ç‰¹å¾æ ‡å‡†åŒ–å’Œç¼ºå¤±å€¼å¤„ç†
    - [ ] ç‰¹å¾é€‰æ‹©å’Œé™ç»´å·¥å…·
    
    ## é˜¶æ®µä¸‰ï¼šæ¨¡åž‹å¼€å‘ï¼ˆç¬¬6-8å‘¨ï¼‰
    
    ### 7. Transformerç¼–ç å™¨
    - [ ] å®žçŽ°TimeSeriesTransformeråŸºç¡€æž¶æž„
    - [ ] ä½ç½®ç¼–ç ï¼ˆæ”¯æŒç›¸å¯¹ä½ç½®ï¼‰
    - [ ] å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    - [ ] æ—¶é—´æ³¨æ„åŠ›èšåˆå±‚
    - [ ] æ”¯æŒå¯å˜é•¿åº¦åºåˆ—
    
    ### 8. SACæ™ºèƒ½ä½“å®žçŽ°
    - [ ] Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
    - [ ] åŒCriticç½‘ç»œï¼ˆQå‡½æ•°ï¼‰
    - [ ] ç›®æ ‡ç½‘ç»œå’Œè½¯æ›´æ–°æœºåˆ¶
    - [ ] æ¸©åº¦å‚æ•°è‡ªåŠ¨è°ƒæ•´
    - [ ] åŠ¨ä½œçº¦æŸï¼ˆç¡®ä¿æƒé‡å’Œä¸º1ï¼‰
    
    ### 9. ç»éªŒå›žæ”¾ç¼“å†²åŒº
    - [ ] å®žçŽ°ReplayBufferç±»
    - [ ] æ”¯æŒä¼˜å…ˆçº§é‡‡æ ·
    - [ ] å†…å­˜æ•ˆçŽ‡ä¼˜åŒ–
    - [ ] æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œé‡‡æ ·
    
    ## é˜¶æ®µå››ï¼šè®­ç»ƒç³»ç»Ÿï¼ˆç¬¬9-11å‘¨ï¼‰
    
    ### 10. è®­ç»ƒæµç¨‹ç®¡ç†
    - [ ] å®žçŽ°RLTrainerä¸»ç±»
    - [ ] æ•°æ®é›†åˆ’åˆ†ç­–ç•¥ï¼ˆè®­ç»ƒ/éªŒè¯/æµ‹è¯•ï¼‰
    - [ ] æ»šåŠ¨çª—å£è®­ç»ƒæ”¯æŒ
    - [ ] æ—©åœæœºåˆ¶å®žçŽ°
    - [ ] å­¦ä¹ çŽ‡è°ƒåº¦å™¨
    
    ### 11. æ¨¡åž‹æ£€æŸ¥ç‚¹ç®¡ç†
    - [ ] è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡åž‹
    - [ ] æ¨¡åž‹ç‰ˆæœ¬æŽ§åˆ¶
    - [ ] è®­ç»ƒçŠ¶æ€æ¢å¤
    - [ ] æ¨¡åž‹åŽ‹ç¼©å’Œä¼˜åŒ–
    
    ### 12. åˆ†å¸ƒå¼è®­ç»ƒæ”¯æŒ
    - [ ] å¤šGPUæ•°æ®å¹¶è¡Œ
    - [ ] å¼‚æ­¥çŽ¯å¢ƒé‡‡æ ·
    - [ ] æ¢¯åº¦ç´¯ç§¯ä¼˜åŒ–
    - [ ] è®­ç»ƒè¿›åº¦å¯è§†åŒ–ï¼ˆtqdm, tensorboardï¼‰
    
    ## é˜¶æ®µäº”ï¼šå›žæµ‹ç³»ç»Ÿï¼ˆç¬¬12-14å‘¨ï¼‰
    
    ### 13. å›žæµ‹å¼•æ“Žå¼€å‘
    - [ ] å®žçŽ°MultiFrequencyBacktestç±»
    - [ ] æ”¯æŒæ—¥é¢‘å’Œåˆ†é’Ÿé¢‘å›žæµ‹
    - [ ] äº¤æ˜“æ‰§è¡Œæ¨¡æ‹Ÿï¼ˆè€ƒè™‘å®žé™…æˆäº¤ï¼‰
    - [ ] æ”¯æŒå¤šç§æˆäº¤ä»·æ ¼ï¼ˆå¼€ç›˜ä»·ã€æ”¶ç›˜ä»·ã€VWAPï¼‰
    
    ### 14. è¯„ä¼°æŒ‡æ ‡è®¡ç®—
    - [ ] æ”¶ç›ŠæŒ‡æ ‡ï¼ˆæ€»æ”¶ç›Šã€å¹´åŒ–æ”¶ç›Šã€æœˆåº¦æ”¶ç›Šï¼‰
    - [ ] é£Žé™©æŒ‡æ ‡ï¼ˆæ³¢åŠ¨çŽ‡ã€æœ€å¤§å›žæ’¤ã€VaRã€CVaRï¼‰
    - [ ] é£Žé™©è°ƒæ•´æŒ‡æ ‡ï¼ˆSharpeã€Sortinoã€Calmarï¼‰
    - [ ] äº¤æ˜“æŒ‡æ ‡ï¼ˆæ¢æ‰‹çŽ‡ã€æˆæœ¬åˆ†æžï¼‰
    
    ### 15. å›žæµ‹æŠ¥å‘Šç”Ÿæˆ
    - [ ] è‡ªåŠ¨ç”ŸæˆHTMLæŠ¥å‘Š
    - [ ] æ”¶ç›Šæ›²çº¿å¯è§†åŒ–
    - [ ] æŒä»“åˆ†æžå›¾è¡¨
    - [ ] é£Žé™©åˆ†è§£æŠ¥å‘Š
    
    ## é˜¶æ®µå…­ï¼šç›‘æŽ§ç³»ç»Ÿï¼ˆç¬¬15-16å‘¨ï¼‰
    
    ### 16. Prometheusç›‘æŽ§é›†æˆ
    - [ ] å®šä¹‰ç›‘æŽ§æŒ‡æ ‡ï¼ˆGaugeã€Counterã€Histogramï¼‰
    - [ ] å®žçŽ°TradingSystemMonitorç±»
    - [ ] æŒ‡æ ‡é‡‡é›†å’Œå¯¼å‡º
    - [ ] Grafanaä»ªè¡¨æ¿é…ç½®
    
    ### 17. å‘Šè­¦ç³»ç»Ÿ
    - [ ] åŠ¨æ€é˜ˆå€¼ç®¡ç†å™¨
    - [ ] å‘Šè­¦è§„åˆ™é…ç½®
    - [ ] å¤šæ¸ é“é€šçŸ¥ï¼ˆé‚®ä»¶ã€é’‰é’‰ã€ä¼ä¸šå¾®ä¿¡ï¼‰
    - [ ] å‘Šè­¦èšåˆå’Œé™é»˜
    
    ### 18. æ—¥å¿—ç³»ç»Ÿ
    - [ ] ç»“æž„åŒ–æ—¥å¿—æ ¼å¼
    - [ ] æ—¥å¿—åˆ†çº§å’Œè·¯ç”±
    - [ ] æ—¥å¿—è½®è½¬å’Œå½’æ¡£
    - [ ] ELKæ ˆé›†æˆï¼ˆå¯é€‰ï¼‰
    
    ## é˜¶æ®µä¸ƒï¼šéƒ¨ç½²ç³»ç»Ÿï¼ˆç¬¬17-18å‘¨ï¼‰
    
    ### 19. Canaryéƒ¨ç½²
    - [ ] å®žçŽ°CanaryDeploymentç®¡ç†å™¨
    - [ ] ç°åº¦å‘å¸ƒæµç¨‹
    - [ ] A/Bæµ‹è¯•æ¡†æž¶
    - [ ] è‡ªåŠ¨å›žæ»šæœºåˆ¶
    
    ### 20. å®¹å™¨åŒ–éƒ¨ç½²
    - [ ] ç¼–å†™Dockerfile
    - [ ] Docker Composeé…ç½®
    - [ ] Kuberneteséƒ¨ç½²æ–‡ä»¶
    - [ ] CI/CDæµæ°´çº¿ï¼ˆGitHub Actions/Jenkinsï¼‰
    
    ### 21. ç”Ÿäº§çŽ¯å¢ƒä¼˜åŒ–
    - [ ] æ¨¡åž‹æŽ¨ç†ä¼˜åŒ–ï¼ˆONNXã€TorchScriptï¼‰
    - [ ] å†…å­˜ä½¿ç”¨ä¼˜åŒ–
    - [ ] å¹¶å‘æŽ§åˆ¶å’Œé™æµ
    - [ ] æœåŠ¡å¥åº·æ£€æŸ¥
    
    ## é˜¶æ®µå…«ï¼šåˆè§„ä¸Žå®¡è®¡ï¼ˆç¬¬19-20å‘¨ï¼‰
    
    ### 22. å®¡è®¡æ—¥å¿—ç³»ç»Ÿ
    - [ ] å®žçŽ°AuditLoggerç±»
    - [ ] å†³ç­–è®°å½•å­˜å‚¨ï¼ˆæ—¶åºæ•°æ®åº“ï¼‰
    - [ ] å®¡è®¡æŸ¥è¯¢æŽ¥å£
    - [ ] åˆè§„æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆ
    
    ### 23. æ¨¡åž‹å¯è§£é‡Šæ€§
    - [ ] SHAPå€¼è®¡ç®—é›†æˆ
    - [ ] LIMEè§£é‡Šå™¨é›†æˆ
    - [ ] æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
    - [ ] ç‰¹å¾é‡è¦æ€§åˆ†æž
    
    ### 24. é£Žé™©æŽ§åˆ¶æ¨¡å—
    - [ ] æŒä»“é›†ä¸­åº¦é™åˆ¶
    - [ ] è¡Œä¸šæš´éœ²æŽ§åˆ¶
    - [ ] æ­¢æŸæœºåˆ¶
    - [ ] å¼‚å¸¸äº¤æ˜“æ£€æµ‹
    
    ## é˜¶æ®µä¹ï¼šæµ‹è¯•ä¸Žæ–‡æ¡£ï¼ˆæŒç»­è¿›è¡Œï¼‰
    
    ### 25. å•å…ƒæµ‹è¯•
    - [ ] çŽ¯å¢ƒç»„ä»¶æµ‹è¯•
    - [ ] æ¨¡åž‹ç»„ä»¶æµ‹è¯•
    - [ ] æ•°æ®å¤„ç†æµ‹è¯•
    - [ ] æµ‹è¯•è¦†ç›–çŽ‡ > 80%
    
    ### 26. é›†æˆæµ‹è¯•
    - [ ] ç«¯åˆ°ç«¯è®­ç»ƒæµç¨‹æµ‹è¯•
    - [ ] å›žæµ‹ç³»ç»Ÿæµ‹è¯•
    - [ ] éƒ¨ç½²æµç¨‹æµ‹è¯•
    - [ ] æ€§èƒ½åŸºå‡†æµ‹è¯•
    
    ### 27. æ–‡æ¡£ç¼–å†™
    - [ ] APIæ–‡æ¡£ï¼ˆSphinxï¼‰
    - [ ] ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œ
    - [ ] éƒ¨ç½²æŒ‡å—
    - [ ] æ•…éšœæŽ’æŸ¥æ‰‹å†Œ
    
    ## é˜¶æ®µåï¼šä¼˜åŒ–ä¸Žæ‰©å±•ï¼ˆç¬¬21å‘¨+ï¼‰
    
    ### 28. æ€§èƒ½ä¼˜åŒ–
    - [ ] æ•°æ®åŠ è½½ä¼˜åŒ–ï¼ˆå¤šè¿›ç¨‹ã€é¢„å–ï¼‰
    - [ ] æ¨¡åž‹æŽ¨ç†åŠ é€Ÿ
    - [ ] å†…å­˜æ± ä¼˜åŒ–
    - [ ] ç¼“å­˜ç­–ç•¥ä¼˜åŒ–
    
    ### 29. åŠŸèƒ½æ‰©å±•
    - [ ] æ”¯æŒæ›´å¤šRLç®—æ³•ï¼ˆPPOã€A2Cã€DDPGï¼‰
    - [ ] å¤šèµ„äº§ç±»åˆ«æ”¯æŒï¼ˆæœŸè´§ã€æœŸæƒï¼‰
    - [ ] ç»„åˆä¼˜åŒ–ç­–ç•¥
    - [ ] å¸‚åœºçŠ¶æ€è¯†åˆ«
    
    ### 30. ç”¨æˆ·ç•Œé¢
    - [ ] Webç›‘æŽ§ç•Œé¢ï¼ˆReact/Vueï¼‰
    - [ ] ç­–ç•¥é…ç½®ç•Œé¢
    - [ ] å®žæ—¶äº¤æ˜“çœ‹æ¿
    - [ ] ç§»åŠ¨ç«¯æ”¯æŒï¼ˆå¯é€‰ï¼‰
    
    ## ä»»åŠ¡ä¼˜å…ˆçº§è¯´æ˜Ž
    
    ### P0 - æ ¸å¿ƒåŠŸèƒ½ï¼ˆå¿…é¡»å®Œæˆï¼‰
    - å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒ
    - SACæ™ºèƒ½ä½“
    - åŸºç¡€å›žæµ‹ç³»ç»Ÿ
    - äº¤æ˜“æˆæœ¬æ¨¡åž‹
    
    ### P1 - é‡è¦åŠŸèƒ½ï¼ˆåº”è¯¥å®Œæˆï¼‰
    - Transformerç¼–ç å™¨
    - ç›‘æŽ§ç³»ç»Ÿ
    - Canaryéƒ¨ç½²
    - å®¡è®¡æ—¥å¿—
    
    ### P2 - å¢žå¼ºåŠŸèƒ½ï¼ˆå¯ä»¥æŽ¨è¿Ÿï¼‰
    - åˆ†å¸ƒå¼è®­ç»ƒ
    - é«˜çº§å¯è§†åŒ–
    - å¤šç®—æ³•æ”¯æŒ
    - UIç•Œé¢
    
    ## å¼€å‘å»ºè®®
    
    1. **è¿­ä»£å¼€å‘**ï¼šé‡‡ç”¨æ•æ·å¼€å‘æ–¹æ³•ï¼Œæ¯2å‘¨ä¸€ä¸ªè¿­ä»£
    2. **ä»£ç å®¡æŸ¥**ï¼šæ‰€æœ‰ä»£ç å¿…é¡»ç»è¿‡å®¡æŸ¥æ‰èƒ½åˆå¹¶
    3. **æŒç»­é›†æˆ**ï¼šæ¯æ¬¡æäº¤è§¦å‘è‡ªåŠ¨æµ‹è¯•
    4. **æ€§èƒ½ç›‘æŽ§**ï¼šä»Žç¬¬ä¸€å¤©å¼€å§‹ç›‘æŽ§ç³»ç»Ÿæ€§èƒ½
    5. **æ–‡æ¡£å…ˆè¡Œ**ï¼šå…ˆå†™æ–‡æ¡£ï¼Œå†å†™ä»£ç 
    
    ## é‡Œç¨‹ç¢‘
    
    - **M1ï¼ˆç¬¬5å‘¨ï¼‰**ï¼šå®ŒæˆåŸºç¡€çŽ¯å¢ƒå’Œæ•°æ®æŽ¥å…¥
    - **M2ï¼ˆç¬¬8å‘¨ï¼‰**ï¼šå®Œæˆæ ¸å¿ƒæ¨¡åž‹å¼€å‘
    - **M3ï¼ˆç¬¬11å‘¨ï¼‰**ï¼šå®Œæˆè®­ç»ƒç³»ç»Ÿ
    - **M4ï¼ˆç¬¬14å‘¨ï¼‰**ï¼šå®Œæˆå›žæµ‹ç³»ç»Ÿ
    - **M5ï¼ˆç¬¬18å‘¨ï¼‰**ï¼šå®Œæˆéƒ¨ç½²ç³»ç»Ÿ
    - **M6ï¼ˆç¬¬20å‘¨ï¼‰**ï¼šç³»ç»Ÿä¸Šçº¿è¯•è¿è¡Œ
    
    ## é£Žé™©ä¸Žåº”å¯¹
    
    1. **æ•°æ®è´¨é‡é—®é¢˜**ï¼šå»ºç«‹æ•°æ®éªŒè¯å’Œæ¸…æ´—æµç¨‹
    2. **æ¨¡åž‹è¿‡æ‹Ÿåˆ**ï¼šä¸¥æ ¼çš„éªŒè¯æµç¨‹å’Œæ­£åˆ™åŒ–
    3. **ç³»ç»Ÿå»¶è¿Ÿ**ï¼šä¼˜åŒ–å…³é”®è·¯å¾„ï¼Œä½¿ç”¨ç¼“å­˜
    4. **åˆè§„é£Žé™©**ï¼šå®Œæ•´çš„å®¡è®¡æ—¥å¿—å’Œå†³ç­–è§£é‡Š
    
    ## èµ„æºéœ€æ±‚
    
    - **å¼€å‘äººå‘˜**ï¼š3-4åï¼ˆMLå·¥ç¨‹å¸ˆ2åï¼ŒåŽç«¯1åï¼Œå‰ç«¯1åï¼‰
    - **ç¡¬ä»¶èµ„æº**ï¼šGPUæœåŠ¡å™¨ï¼ˆè®­ç»ƒï¼‰ï¼ŒCPUæœåŠ¡å™¨ï¼ˆæŽ¨ç†ï¼‰
    - **ç¬¬ä¸‰æ–¹æœåŠ¡**ï¼šæ•°æ®æºè®¢é˜…ï¼Œäº‘æœåŠ¡ï¼ˆå¯é€‰ï¼‰
    ]]></file>
  <file path="archive/desgin.md"><![CDATA[
    # åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“è®¾è®¡æ–¹æ¡ˆ
    
    ## ä¸€ã€é¡¹ç›®æ¦‚è¿°
    
    ### 1.1 æ ¸å¿ƒæŠ€æœ¯æž¶æž„
    - **å¼ºåŒ–å­¦ä¹ æ¡†æž¶**ï¼šé‡‡ç”¨SACï¼ˆSoft Actor-Criticï¼‰/PPOï¼ˆProximal Policy Optimizationï¼‰ä½œä¸ºå†³ç­–å¼•æ“Ž
    - **æ—¶åºå»ºæ¨¡**ï¼šä½¿ç”¨Transformer/Informeræž¶æž„æ•æ‰é•¿æœŸæ—¶åºä¾èµ–
    - **çŽ¯å¢ƒè®¾è®¡**ï¼šåŸºäºŽOpenAI Gymè§„èŒƒæž„å»ºPortfolio Environment
    - **ç›®æ ‡**ï¼šåœ¨è€ƒè™‘äº¤æ˜“æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œå®žçŽ°å¹´åŒ–æ”¶ç›Š8%-12%ï¼Œæœ€å¤§å›žæ’¤æŽ§åˆ¶åœ¨15%ä»¥å†…
    
    ### 1.2 ç³»ç»Ÿæž¶æž„å›¾
    
    ```
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚                    å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿæž¶æž„                             â”‚
    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
    â”‚  â”‚   æ•°æ®å¤„ç†å±‚     â”‚  â”‚  ç‰¹å¾å·¥ç¨‹å±‚       â”‚  â”‚   æ—¶åºç¼–ç å±‚           â”‚    â”‚
    â”‚  â”‚                 â”‚  â”‚                  â”‚  â”‚                        â”‚    â”‚
    â”‚  â”‚ â€¢ Qlib Data    â”‚â”€â–¶â”‚ â€¢ æŠ€æœ¯æŒ‡æ ‡       â”‚â”€â–¶â”‚ â€¢ Transformer Encoder  â”‚    â”‚
    â”‚  â”‚ â€¢ Akshare API  â”‚  â”‚ â€¢ åŸºæœ¬é¢å› å­     â”‚  â”‚ â€¢ Positional Encoding  â”‚    â”‚
    â”‚  â”‚ â€¢ å®žæ—¶è¡Œæƒ…     â”‚  â”‚ â€¢ å¸‚åœºå¾®è§‚ç»“æž„   â”‚  â”‚ â€¢ Multi-Head Attention â”‚    â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
    â”‚                                                         â”‚                    â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
    â”‚  â”‚                        å¼ºåŒ–å­¦ä¹ å†³ç­–å±‚                                  â”‚  â”‚
    â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚  â”‚
    â”‚  â”‚  â”‚ Portfolio Env  â”‚  â”‚  Actor Network   â”‚  â”‚  Critic Network    â”‚    â”‚  â”‚
    â”‚  â”‚  â”‚                â”‚  â”‚                  â”‚  â”‚                    â”‚    â”‚  â”‚
    â”‚  â”‚  â”‚ State Space    â”‚â—€â–¶â”‚ Transformer Base â”‚  â”‚ Transformer Base   â”‚    â”‚  â”‚
    â”‚  â”‚  â”‚ Action Space   â”‚  â”‚ Policy Head      â”‚  â”‚ Value Head         â”‚    â”‚  â”‚
    â”‚  â”‚  â”‚ Reward Functionâ”‚  â”‚ (SAC/PPO)        â”‚  â”‚ Q-Function         â”‚    â”‚  â”‚
    â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚  â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
    â”‚                                     â”‚                                        â”‚
    â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
    â”‚  â”‚                    æ‰§è¡Œä¸Žç›‘æŽ§å±‚                                         â”‚ â”‚
    â”‚  â”‚  â€¢ äº¤æ˜“æˆæœ¬æ¨¡åž‹  â€¢ é£Žé™©æŽ§åˆ¶  â€¢ å®žæ—¶ç›‘æŽ§  â€¢ å®¡è®¡æ—¥å¿—                   â”‚ â”‚
    â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    ```
    
    ## äºŒã€å¼ºåŒ–å­¦ä¹ çŽ¯å¢ƒè®¾è®¡
    
    ### 2.1 Portfolio Environmentå®šä¹‰
    
    ```python
    import gym
    import numpy as np
    from gym import spaces
    import qlib
    from qlib.data import D
    
    class PortfolioEnv(gym.Env):
        """Aè‚¡æŠ•èµ„ç»„åˆçŽ¯å¢ƒï¼Œç¬¦åˆOpenAI Gymè§„èŒƒ"""
        
        def __init__(self, config):
            super().__init__()
            
            # å¸‚åœºé…ç½®
            self.stock_pool = config['stock_pool']  # è‚¡ç¥¨æ± 
            self.lookback_window = config['lookback_window']  # åŽ†å²çª—å£
            self.n_stocks = len(self.stock_pool)
            
            # äº¤æ˜“æˆæœ¬å‚æ•°
            self.commission_rate = 0.001  # æ‰‹ç»­è´¹çŽ‡ 0.1%
            self.stamp_tax_rate = 0.001   # å°èŠ±ç¨ŽçŽ‡ 0.1%ï¼ˆä»…å–å‡ºï¼‰
            self.slippage_model = AlmgrenChrissModel()  # å¸‚åœºå†²å‡»æ¨¡åž‹
            
            # çŠ¶æ€ç©ºé—´ï¼š[åŽ†å²ç‰¹å¾çª—å£, å½“å‰æŒä»“, å¸‚åœºçŠ¶æ€]
            self.observation_space = spaces.Dict({
                'features': spaces.Box(
                    low=-np.inf, high=np.inf, 
                    shape=(self.lookback_window, self.n_stocks, config['n_features'])
                ),
                'positions': spaces.Box(
                    low=0, high=1, shape=(self.n_stocks,)
                ),
                'market_state': spaces.Box(
                    low=-np.inf, high=np.inf, shape=(config['market_features'],)
                )
            })
            
            # åŠ¨ä½œç©ºé—´ï¼šç›®æ ‡æƒé‡ï¼ˆè¿žç»­åŠ¨ä½œï¼‰
            self.action_space = spaces.Box(
                low=0, high=1, shape=(self.n_stocks,)
            )
            
            # é£Žé™©å‚æ•°
            self.risk_aversion = config['risk_aversion']
            self.max_drawdown_penalty = config['max_drawdown_penalty']
            
        def step(self, action):
            """æ‰§è¡Œäº¤æ˜“åŠ¨ä½œ"""
            # 1. æ ‡å‡†åŒ–åŠ¨ä½œï¼ˆç¡®ä¿æƒé‡å’Œä¸º1ï¼‰
            target_weights = action / (action.sum() + 1e-8)
            
            # 2. è®¡ç®—äº¤æ˜“é‡
            current_weights = self._get_current_weights()
            trade_weights = target_weights - current_weights
            
            # 3. è®¡ç®—äº¤æ˜“æˆæœ¬
            transaction_cost = self._calculate_transaction_cost(
                current_weights, target_weights
            )
            
            # 4. æ‰§è¡Œäº¤æ˜“ï¼ˆè€ƒè™‘Aè‚¡è§„åˆ™ï¼‰
            executed_weights = self._execute_trades(target_weights)
            
            # 5. è®¡ç®—ä¸‹ä¸€æœŸæ”¶ç›Š
            next_returns = self._get_next_returns()
            portfolio_return = np.dot(executed_weights, next_returns)
            
            # 6. è®¡ç®—é£Žé™©è°ƒæ•´åŽçš„å¥–åŠ±
            reward = self._calculate_reward(
                portfolio_return, 
                transaction_cost,
                executed_weights
            )
            
            # 7. æ›´æ–°çŠ¶æ€
            self.current_step += 1
            next_state = self._get_observation()
            done = self.current_step >= self.max_steps
            
            # 8. è®°å½•ä¿¡æ¯ï¼ˆç”¨äºŽç›‘æŽ§å’Œå®¡è®¡ï¼‰
            info = {
                'portfolio_return': portfolio_return,
                'transaction_cost': transaction_cost,
                'executed_weights': executed_weights,
                'sharpe_ratio': self._calculate_sharpe(),
                'max_drawdown': self._calculate_max_drawdown(),
                'timestamp': self.current_date
            }
            
            return next_state, reward, done, info
        
        def _calculate_transaction_cost(self, current_weights, target_weights):
            """è®¡ç®—äº¤æ˜“æˆæœ¬ï¼ˆåŒ…æ‹¬æ‰‹ç»­è´¹ã€å°èŠ±ç¨Žã€æ»‘ç‚¹ï¼‰"""
            trade_weights = np.abs(target_weights - current_weights)
            
            # æ‰‹ç»­è´¹ï¼ˆåŒè¾¹ï¼‰
            commission = np.sum(trade_weights) * self.commission_rate
            
            # å°èŠ±ç¨Žï¼ˆä»…å–å‡ºï¼‰
            sell_weights = np.maximum(current_weights - target_weights, 0)
            stamp_tax = np.sum(sell_weights) * self.stamp_tax_rate
            
            # å¸‚åœºå†²å‡»æˆæœ¬ï¼ˆä½¿ç”¨Almgren-Chrissæ¨¡åž‹ï¼‰
            volumes = self._get_current_volumes()
            slippage = self.slippage_model.calculate_impact(
                trade_weights, volumes, self.total_value
            )
            
            return commission + stamp_tax + slippage
        
        def _calculate_reward(self, portfolio_return, transaction_cost, weights):
            """è®¡ç®—é£Žé™©è°ƒæ•´åŽçš„å¥–åŠ±"""
            # åŸºç¡€æ”¶ç›Š
            net_return = portfolio_return - transaction_cost
            
            # é£Žé™©æƒ©ç½š
            portfolio_volatility = self._calculate_portfolio_volatility(weights)
            risk_penalty = self.risk_aversion * portfolio_volatility
            
            # æœ€å¤§å›žæ’¤æƒ©ç½š
            current_drawdown = self._calculate_current_drawdown()
            drawdown_penalty = self.max_drawdown_penalty * max(0, current_drawdown - 0.1)
            
            # æœ€ç»ˆå¥–åŠ±
            reward = net_return - risk_penalty - drawdown_penalty
            
            return reward
    ```
    
    ### 2.2 Almgren-Chrisså¸‚åœºå†²å‡»æ¨¡åž‹
    
    ```python
    class AlmgrenChrissModel:
        """å¸‚åœºå†²å‡»æˆæœ¬æ¨¡åž‹"""
        
        def __init__(self, permanent_impact=0.1, temporary_impact=0.05):
            self.gamma = permanent_impact  # æ°¸ä¹…å†²å‡»ç³»æ•°
            self.eta = temporary_impact    # ä¸´æ—¶å†²å‡»ç³»æ•°
            
        def calculate_impact(self, trade_weights, volumes, total_value):
            """
            è®¡ç®—å¸‚åœºå†²å‡»æˆæœ¬
            å‚è€ƒï¼šAlmgren & Chriss (2001)
            """
            # è®¡ç®—äº¤æ˜“å å¸‚åœºæˆäº¤é‡çš„æ¯”ä¾‹
            market_participation = trade_weights * total_value / (volumes + 1e-8)
            
            # æ°¸ä¹…å†²å‡»ï¼ˆçº¿æ€§ï¼‰
            permanent_cost = self.gamma * np.sum(market_participation * trade_weights)
            
            # ä¸´æ—¶å†²å‡»ï¼ˆå¹³æ–¹æ ¹ï¼‰
            temporary_cost = self.eta * np.sum(np.sqrt(market_participation) * trade_weights)
            
            return permanent_cost + temporary_cost
    ```
    
    ## ä¸‰ã€Transformerå¼ºåŒ–å­¦ä¹ æ¨¡åž‹
    
    ### 3.1 æ—¶åºTransformerç¼–ç å™¨
    
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    class TimeSeriesTransformer(nn.Module):
        """æ—¶åºæ•°æ®çš„Transformerç¼–ç å™¨"""
        
        def __init__(self, config):
            super().__init__()
            
            self.d_model = config['d_model']
            self.n_heads = config['n_heads']
            self.n_layers = config['n_layers']
            self.dropout = config['dropout']
            
            # è¾“å…¥åµŒå…¥å±‚
            self.input_projection = nn.Linear(
                config['n_features'], self.d_model
            )
            
            # ä½ç½®ç¼–ç 
            self.positional_encoding = PositionalEncoding(
                self.d_model, config['max_seq_len']
            )
            
            # Transformerç¼–ç å™¨å±‚
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=self.d_model,
                nhead=self.n_heads,
                dim_feedforward=config['d_ff'],
                dropout=self.dropout,
                activation='gelu',
                batch_first=True
            )
            
            self.transformer_encoder = nn.TransformerEncoder(
                encoder_layer, 
                num_layers=self.n_layers,
                norm=nn.LayerNorm(self.d_model)
            )
            
            # æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
            self.temporal_attention = TemporalAttention(self.d_model)
            
        def forward(self, x, mask=None):
            """
            x: [batch_size, seq_len, n_stocks, n_features]
            """
            batch_size, seq_len, n_stocks, n_features = x.shape
            
            # é‡å¡‘ä¸º [batch_size * n_stocks, seq_len, n_features]
            x = x.view(batch_size * n_stocks, seq_len, n_features)
            
            # è¾“å…¥æŠ•å½±
            x = self.input_projection(x)
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            x = self.positional_encoding(x)
            
            # Transformerç¼–ç 
            x = self.transformer_encoder(x, mask=mask)
            
            # æ—¶é—´æ³¨æ„åŠ›èšåˆ
            x = self.temporal_attention(x)
            
            # é‡å¡‘å›ž [batch_size, n_stocks, d_model]
            x = x.view(batch_size, n_stocks, self.d_model)
            
            return x
    
    class TemporalAttention(nn.Module):
        """æ—¶é—´ç»´åº¦çš„æ³¨æ„åŠ›èšåˆ"""
        
        def __init__(self, d_model):
            super().__init__()
            self.attention = nn.MultiheadAttention(
                d_model, num_heads=8, batch_first=True
            )
            self.query = nn.Parameter(torch.randn(1, 1, d_model))
            
        def forward(self, x):
            """èšåˆæ—¶é—´åºåˆ—ä¸ºå•ä¸€è¡¨ç¤º"""
            batch_size = x.size(0)
            query = self.query.expand(batch_size, -1, -1)
            
            # ä½¿ç”¨å­¦ä¹ çš„queryå‘é‡èšåˆæ—¶é—´ä¿¡æ¯
            out, _ = self.attention(query, x, x)
            return out.squeeze(1)
    ```
    
    ### 3.2 SAC Actor-Criticç½‘ç»œ
    
    ```python
    class TransformerSACAgent(nn.Module):
        """åŸºäºŽTransformerçš„SACæ™ºèƒ½ä½“"""
        
        def __init__(self, config):
            super().__init__()
            
            # å…±äº«çš„Transformerç¼–ç å™¨
            self.encoder = TimeSeriesTransformer(config['transformer'])
            
            # Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
            self.actor = Actor(
                input_dim=config['transformer']['d_model'],
                n_stocks=config['n_stocks']
            )
            
            # Criticç½‘ç»œï¼ˆQå‡½æ•°ï¼‰
            self.critic_1 = Critic(
                state_dim=config['transformer']['d_model'],
                action_dim=config['n_stocks']
            )
            self.critic_2 = Critic(
                state_dim=config['transformer']['d_model'],
                action_dim=config['n_stocks']
            )
            
            # ç›®æ ‡ç½‘ç»œ
            self.target_critic_1 = Critic(
                state_dim=config['transformer']['d_model'],
                action_dim=config['n_stocks']
            )
            self.target_critic_2 = Critic(
                state_dim=config['transformer']['d_model'],
                action_dim=config['n_stocks']
            )
            
            # æ¸©åº¦å‚æ•°ï¼ˆç”¨äºŽç†µæ­£åˆ™åŒ–ï¼‰
            self.log_alpha = nn.Parameter(torch.zeros(1))
            
        def get_action(self, state, deterministic=False):
            """èŽ·å–åŠ¨ä½œï¼ˆæŠ•èµ„ç»„åˆæƒé‡ï¼‰"""
            # ç¼–ç çŠ¶æ€
            encoded_state = self.encoder(state['features'])
            
            # ç»“åˆå…¶ä»–çŠ¶æ€ä¿¡æ¯
            full_state = torch.cat([
                encoded_state,
                state['positions'].unsqueeze(1).expand(-1, encoded_state.size(1), -1),
                state['market_state'].unsqueeze(1).expand(-1, encoded_state.size(1), -1)
            ], dim=-1)
            
            # ç”ŸæˆåŠ¨ä½œ
            action, log_prob = self.actor(full_state, deterministic)
            
            return action, log_prob
    
    class Actor(nn.Module):
        """ç­–ç•¥ç½‘ç»œ"""
        
        def __init__(self, input_dim, n_stocks):
            super().__init__()
            
            self.mlp = nn.Sequential(
                nn.Linear(input_dim * n_stocks, 512),
                nn.LayerNorm(512),
                nn.ReLU(),
                nn.Linear(512, 256),
                nn.LayerNorm(256),
                nn.ReLU()
            )
            
            # è¾“å‡ºå‡å€¼å’Œæ ‡å‡†å·®
            self.mean_head = nn.Linear(256, n_stocks)
            self.log_std_head = nn.Linear(256, n_stocks)
            
            # æƒé‡çº¦æŸï¼ˆç¡®ä¿å’Œä¸º1ï¼‰
            self.softmax = nn.Softmax(dim=-1)
            
        def forward(self, state, deterministic=False):
            """ç”ŸæˆæŠ•èµ„ç»„åˆæƒé‡"""
            x = state.flatten(start_dim=1)
            x = self.mlp(x)
            
            mean = self.mean_head(x)
            log_std = self.log_std_head(x)
            log_std = torch.clamp(log_std, -20, 2)
            
            if deterministic:
                # ç¡®å®šæ€§åŠ¨ä½œ
                action = self.softmax(mean)
                log_prob = None
            else:
                # éšæœºåŠ¨ä½œï¼ˆé‡å‚æ•°åŒ–æŠ€å·§ï¼‰
                std = log_std.exp()
                normal = torch.distributions.Normal(mean, std)
                x_t = normal.rsample()
                action = self.softmax(x_t)
                
                # è®¡ç®—å¯¹æ•°æ¦‚çŽ‡
                log_prob = normal.log_prob(x_t)
                log_prob -= torch.log(1 - action.pow(2) + 1e-6)
                log_prob = log_prob.sum(1, keepdim=True)
            
            return action, log_prob
    ```
    
    ## å››ã€è®­ç»ƒä¸ŽéªŒè¯æµç¨‹
    
    ### 4.1 æ•°æ®é›†åˆ’åˆ†ç­–ç•¥
    
    ```python
    class DataSplitStrategy:
        """æ—¶åºæ•°æ®çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†"""
        
        def __init__(self):
            # å›ºå®šåˆ’åˆ†ç‚¹
            self.train_period = ('2009-01-01', '2017-12-31')
            self.valid_period = ('2018-01-01', '2021-12-31')
            self.test_period = ('2022-01-01', '2025-12-31')
            
            # æ»šåŠ¨çª—å£å‚æ•°
            self.rolling_window = 252 * 4  # 4å¹´è®­ç»ƒçª—å£
            self.retraining_freq = 63      # æ¯å­£åº¦é‡è®­ç»ƒ
            
        def get_rolling_splits(self, start_date, end_date):
            """ç”Ÿæˆæ»šåŠ¨çª—å£çš„æ•°æ®åˆ’åˆ†"""
            splits = []
            current_date = pd.Timestamp(start_date)
            end_date = pd.Timestamp(end_date)
            
            while current_date < end_date:
                train_start = current_date
                train_end = train_start + pd.Timedelta(days=self.rolling_window)
                valid_start = train_end
                valid_end = valid_start + pd.Timedelta(days=63)  # 3ä¸ªæœˆéªŒè¯
                
                if valid_end <= end_date:
                    splits.append({
                        'train': (train_start, train_end),
                        'valid': (valid_start, valid_end),
                        'test': (valid_end, valid_end + pd.Timedelta(days=63))
                    })
                
                current_date += pd.Timedelta(days=self.retraining_freq)
                
            return splits
    ```
    
    ### 4.2 è®­ç»ƒæµç¨‹ä¸Žæ—©åœæœºåˆ¶
    
    ```python
    class RLTrainer:
        """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
        
        def __init__(self, agent, env, config):
            self.agent = agent
            self.env = env
            self.config = config
            
            # ä¼˜åŒ–å™¨
            self.actor_optimizer = torch.optim.Adam(
                agent.actor.parameters(), lr=config['actor_lr']
            )
            self.critic_optimizer = torch.optim.Adam(
                list(agent.critic_1.parameters()) + list(agent.critic_2.parameters()),
                lr=config['critic_lr']
            )
            self.alpha_optimizer = torch.optim.Adam(
                [agent.log_alpha], lr=config['alpha_lr']
            )
            
            # ç»éªŒå›žæ”¾ç¼“å†²åŒº
            self.replay_buffer = ReplayBuffer(config['buffer_size'])
            
            # æ—©åœæœºåˆ¶
            self.early_stopping = EarlyStopping(
                patience=config['patience'],
                min_delta=config['min_delta']
            )
            
            # æ£€æŸ¥ç‚¹ç®¡ç†
            self.checkpoint_manager = CheckpointManager(config['checkpoint_dir'])
            
        def train(self, n_episodes, validation_env):
            """è®­ç»ƒä¸»å¾ªçŽ¯"""
            best_validation_score = -np.inf
            
            for episode in range(n_episodes):
                # æ”¶é›†ç»éªŒ
                episode_reward = self._collect_experience()
                
                # æ›´æ–°ç½‘ç»œ
                if len(self.replay_buffer) > self.config['batch_size']:
                    losses = self._update_networks()
                
                # éªŒè¯é›†è¯„ä¼°
                if episode % self.config['eval_freq'] == 0:
                    validation_score = self._evaluate(validation_env)
                    
                    # æ—©åœæ£€æŸ¥
                    if self.early_stopping(validation_score):
                        print(f"Early stopping triggered at episode {episode}")
                        break
                    
                    # ä¿å­˜æœ€ä½³æ¨¡åž‹
                    if validation_score > best_validation_score:
                        best_validation_score = validation_score
                        self.checkpoint_manager.save_checkpoint(
                            self.agent, episode, validation_score
                        )
                    
                # è®°å½•è®­ç»ƒæŒ‡æ ‡
                self._log_metrics(episode, episode_reward, losses, validation_score)
        
        def _update_networks(self):
            """SACç®—æ³•æ›´æ–°"""
            batch = self.replay_buffer.sample(self.config['batch_size'])
            
            with torch.no_grad():
                # è®¡ç®—ç›®æ ‡Qå€¼
                next_action, next_log_prob = self.agent.get_action(batch.next_state)
                target_q1 = self.agent.target_critic_1(batch.next_state, next_action)
                target_q2 = self.agent.target_critic_2(batch.next_state, next_action)
                target_q = torch.min(target_q1, target_q2) - self.agent.log_alpha.exp() * next_log_prob
                target_value = batch.reward + self.config['gamma'] * (1 - batch.done) * target_q
            
            # æ›´æ–°Critic
            q1 = self.agent.critic_1(batch.state, batch.action)
            q2 = self.agent.critic_2(batch.state, batch.action)
            critic_loss = F.mse_loss(q1, target_value) + F.mse_loss(q2, target_value)
            
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()
            
            # æ›´æ–°Actor
            action, log_prob = self.agent.get_action(batch.state)
            q1_new = self.agent.critic_1(batch.state, action)
            q2_new = self.agent.critic_2(batch.state, action)
            q_new = torch.min(q1_new, q2_new)
            
            actor_loss = (self.agent.log_alpha.exp() * log_prob - q_new).mean()
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()
            
            # æ›´æ–°æ¸©åº¦å‚æ•°
            alpha_loss = -(self.agent.log_alpha * (log_prob + self.config['target_entropy']).detach()).mean()
            
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self._soft_update_target_networks()
            
            return {
                'critic_loss': critic_loss.item(),
                'actor_loss': actor_loss.item(),
                'alpha_loss': alpha_loss.item()
            }
    ```
    
    ## äº”ã€ç»†ç²’åº¦å›žæµ‹ä¸Žè¯„ä¼°
    
    ### 5.1 å¤šé¢‘çŽ‡å›žæµ‹å¼•æ“Ž
    
    ```python
    class MultiFrequencyBacktest:
        """æ”¯æŒæ—¥é¢‘å’Œåˆ†é’Ÿé¢‘çš„å›žæµ‹å¼•æ“Ž"""
        
        def __init__(self, config):
            self.freq = config['freq']  # '1d' or '1min'
            self.rebalance_freq = config['rebalance_freq']  # è°ƒä»“é¢‘çŽ‡
            
            # åˆå§‹åŒ–Qlibå›žæµ‹å¼•æ“Ž
            if self.freq == '1min':
                self.engine = qlib.backtest.HighFreqTradeSim(
                    trade_calendar=config['calendar'],
                    deal_price='vwap',  # ä½¿ç”¨VWAPæˆäº¤
                    min_cost=5,  # æœ€å°æ‰‹ç»­è´¹5å…ƒ
                )
            else:
                self.engine = qlib.backtest.Backtest(
                    trade_calendar=config['calendar'],
                    benchmark=config['benchmark']
                )
        
        def run_backtest(self, agent, start_date, end_date):
            """è¿è¡Œå›žæµ‹"""
            results = {
                'dates': [],
                'portfolio_values': [],
                'positions': [],
                'transactions': [],
                'metrics': {}
            }
            
            # åˆå§‹åŒ–çŽ¯å¢ƒ
            env = PortfolioEnv(self._get_env_config())
            state = env.reset(start_date)
            
            current_date = start_date
            while current_date <= end_date:
                # æ£€æŸ¥æ˜¯å¦éœ€è¦è°ƒä»“
                if self._should_rebalance(current_date):
                    # èŽ·å–åŠ¨ä½œ
                    action, _ = agent.get_action(state, deterministic=True)
                    
                    # æ‰§è¡Œäº¤æ˜“
                    next_state, reward, done, info = env.step(action)
                    
                    # è®°å½•äº¤æ˜“ä¿¡æ¯
                    results['transactions'].append({
                        'date': current_date,
                        'action': action,
                        'cost': info['transaction_cost'],
                        'executed_weights': info['executed_weights']
                    })
                
                # æ›´æ–°æŠ•èµ„ç»„åˆä»·å€¼
                portfolio_value = self._calculate_portfolio_value(env)
                results['portfolio_values'].append(portfolio_value)
                results['dates'].append(current_date)
                results['positions'].append(env.positions.copy())
                
                # ä¸‹ä¸€æ­¥
                current_date += pd.Timedelta(days=1 if self.freq == '1d' else minutes=1)
                state = next_state
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            results['metrics'] = self._calculate_metrics(results)
            
            return results
        
        def _calculate_metrics(self, results):
            """è®¡ç®—è¯¦ç»†çš„è¯„ä¼°æŒ‡æ ‡"""
            returns = pd.Series(results['portfolio_values']).pct_change().dropna()
            
            metrics = {
                # æ”¶ç›ŠæŒ‡æ ‡
                'total_return': (results['portfolio_values'][-1] / results['portfolio_values'][0] - 1),
                'annual_return': self._annualized_return(returns),
                'monthly_returns': returns.resample('M').apply(lambda x: (1 + x).prod() - 1),
                
                # é£Žé™©æŒ‡æ ‡
                'volatility': returns.std() * np.sqrt(252),
                'max_drawdown': self._calculate_max_drawdown(results['portfolio_values']),
                'var_95': np.percentile(returns, 5),
                'cvar_95': returns[returns <= np.percentile(returns, 5)].mean(),
                
                # é£Žé™©è°ƒæ•´æŒ‡æ ‡
                'sharpe_ratio': self._calculate_sharpe_ratio(returns),
                'sortino_ratio': self._calculate_sortino_ratio(returns),
                'calmar_ratio': self._calculate_calmar_ratio(returns, metrics['max_drawdown']),
                
                # äº¤æ˜“æŒ‡æ ‡
                'turnover_rate': self._calculate_turnover(results['transactions']),
                'avg_transaction_cost': np.mean([t['cost'] for t in results['transactions']]),
                'trade_count': len(results['transactions']),
                
                # å› å­æš´éœ²åˆ†æž
                'factor_exposures': self._analyze_factor_exposures(results['positions'])
            }
            
            return metrics
    ```
    
    ## å…­ã€å®žæ—¶ç›‘æŽ§ä¸Žè‡ªåŠ¨åŒ–è¿ç»´
    
    ### 6.1 Prometheusç›‘æŽ§é›†æˆ
    
    ```python
    from prometheus_client import Counter, Gauge, Histogram, start_http_server
    import time
    
    class TradingSystemMonitor:
        """äº¤æ˜“ç³»ç»Ÿç›‘æŽ§"""
        
        def __init__(self):
            # å®šä¹‰PrometheusæŒ‡æ ‡
            self.metrics = {
                # æ€§èƒ½æŒ‡æ ‡
                'daily_return': Gauge('trading_daily_return', 'Daily portfolio return'),
                'sharpe_ratio': Gauge('trading_sharpe_ratio', 'Rolling 30-day Sharpe ratio'),
                'ic': Gauge('trading_ic', 'Information coefficient'),
                'max_drawdown': Gauge('trading_max_drawdown', 'Current maximum drawdown'),
                
                # é£Žé™©æŒ‡æ ‡
                'var_95': Gauge('trading_var_95', '95% Value at Risk'),
                'position_concentration': Gauge('trading_position_concentration', 'Herfindahl index'),
                'sector_exposure': Gauge('trading_sector_exposure', 'Maximum sector exposure', ['sector']),
                
                # ç³»ç»ŸæŒ‡æ ‡
                'model_inference_time': Histogram('model_inference_seconds', 'Model inference time'),
                'data_latency': Histogram('data_latency_seconds', 'Data update latency'),
                'error_count': Counter('trading_errors_total', 'Total error count', ['error_type']),
                
                # äº¤æ˜“æŒ‡æ ‡
                'transaction_cost': Gauge('trading_transaction_cost', 'Daily transaction cost'),
                'turnover_rate': Gauge('trading_turnover_rate', 'Portfolio turnover rate'),
                'execution_slippage': Gauge('trading_execution_slippage', 'Execution slippage')
            }
            
            # å¯åŠ¨HTTPæœåŠ¡å™¨
            start_http_server(8000)
            
            # åŠ¨æ€é˜ˆå€¼ç®¡ç†
            self.threshold_manager = DynamicThresholdManager()
            
        def update_metrics(self, trading_data):
            """æ›´æ–°ç›‘æŽ§æŒ‡æ ‡"""
            # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
            self.metrics['daily_return'].set(trading_data['daily_return'])
            self.metrics['sharpe_ratio'].set(trading_data['sharpe_ratio'])
            self.metrics['ic'].set(trading_data['ic'])
            self.metrics['max_drawdown'].set(trading_data['max_drawdown'])
            
            # æ›´æ–°é£Žé™©æŒ‡æ ‡
            self.metrics['var_95'].set(trading_data['var_95'])
            self.metrics['position_concentration'].set(trading_data['herfindahl_index'])
            
            # æ›´æ–°è¡Œä¸šæš´éœ²
            for sector, exposure in trading_data['sector_exposures'].items():
                self.metrics['sector_exposure'].labels(sector=sector).set(exposure)
            
            # æ£€æŸ¥å‘Šè­¦
            self._check_alerts(trading_data)
        
        def _check_alerts(self, trading_data):
            """åŸºäºŽåŠ¨æ€é˜ˆå€¼çš„å‘Šè­¦æ£€æŸ¥"""
            alerts = []
            
            # èŽ·å–åŠ¨æ€é˜ˆå€¼
            thresholds = self.threshold_manager.get_thresholds(trading_data)
            
            # æ£€æŸ¥å„é¡¹æŒ‡æ ‡
            if trading_data['max_drawdown'] > thresholds['max_drawdown_threshold']:
                alerts.append({
                    'level': 'critical',
                    'metric': 'max_drawdown',
                    'value': trading_data['max_drawdown'],
                    'threshold': thresholds['max_drawdown_threshold'],
                    'message': f"Maximum drawdown {trading_data['max_drawdown']:.2%} exceeds threshold"
                })
            
            if trading_data['sharpe_ratio'] < thresholds['min_sharpe_threshold']:
                alerts.append({
                    'level': 'warning',
                    'metric': 'sharpe_ratio',
                    'value': trading_data['sharpe_ratio'],
                    'threshold': thresholds['min_sharpe_threshold'],
                    'message': f"Sharpe ratio {trading_data['sharpe_ratio']:.2f} below threshold"
                })
            
            # å‘é€å‘Šè­¦
            if alerts:
                self._send_alerts(alerts)
    
    class DynamicThresholdManager:
        """åŠ¨æ€é˜ˆå€¼ç®¡ç†å™¨"""
        
        def __init__(self, lookback_days=90):
            self.lookback_days = lookback_days
            self.historical_data = []
            
        def get_thresholds(self, current_data):
            """åŸºäºŽåŽ†å²åˆ†ä½æ•°è®¡ç®—åŠ¨æ€é˜ˆå€¼"""
            self.historical_data.append(current_data)
            
            if len(self.historical_data) < self.lookback_days:
                # ä½¿ç”¨é»˜è®¤é˜ˆå€¼
                return {
                    'max_drawdown_threshold': 0.15,
                    'min_sharpe_threshold': 0.5,
                    'max_var_threshold': 0.03
                }
            
            # è®¡ç®—åŽ†å²åˆ†ä½æ•°
            historical_df = pd.DataFrame(self.historical_data[-self.lookback_days:])
            
            return {
                'max_drawdown_threshold': historical_df['max_drawdown'].quantile(0.95),
                'min_sharpe_threshold': historical_df['sharpe_ratio'].quantile(0.05),
                'max_var_threshold': historical_df['var_95'].quantile(0.95)
            }
    ```
    
    ### 6.2 Canaryéƒ¨ç½²ä¸Žç°åº¦å‘å¸ƒ
    
    ```python
    class CanaryDeployment:
        """é‡‘ä¸é›€éƒ¨ç½²ç®¡ç†"""
        
        def __init__(self, config):
            self.canary_ratio = config['initial_canary_ratio']  # åˆå§‹5%èµ„é‡‘
            self.evaluation_period = config['evaluation_period']  # è¯„ä¼°æœŸ14å¤©
            self.promotion_criteria = config['promotion_criteria']
            
        def deploy_new_model(self, new_model, production_model):
            """éƒ¨ç½²æ–°æ¨¡åž‹"""
            deployment = {
                'model_version': new_model.version,
                'start_date': datetime.now(),
                'status': 'canary',
                'metrics': [],
                'canary_portfolio': Portfolio(self.canary_ratio),
                'production_portfolio': Portfolio(1 - self.canary_ratio)
            }
            
            # å¯åŠ¨å¹¶è¡Œè¿è¡Œ
            self._run_parallel_evaluation(new_model, production_model, deployment)
            
            return deployment
        
        def _run_parallel_evaluation(self, new_model, production_model, deployment):
            """å¹¶è¡Œè¯„ä¼°æ–°æ—§æ¨¡åž‹"""
            for day in range(self.evaluation_period):
                # èŽ·å–å½“æ—¥æ•°æ®
                market_data = self._get_market_data()
                
                # è¿è¡Œä¸¤ä¸ªæ¨¡åž‹
                canary_action = new_model.predict(market_data)
                production_action = production_model.predict(market_data)
                
                # æ‰§è¡Œäº¤æ˜“
                canary_return = deployment['canary_portfolio'].execute(canary_action)
                production_return = deployment['production_portfolio'].execute(production_action)
                
                # è®°å½•æŒ‡æ ‡
                daily_metrics = {
                    'date': datetime.now(),
                    'canary_return': canary_return,
                    'production_return': production_return,
                    'canary_sharpe': self._calculate_sharpe(deployment['canary_portfolio']),
                    'production_sharpe': self._calculate_sharpe(deployment['production_portfolio'])
                }
                deployment['metrics'].append(daily_metrics)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦å›žæ»š
                if self._should_rollback(deployment['metrics']):
                    self._rollback_deployment(deployment)
                    return
            
            # è¯„ä¼°æ˜¯å¦æŽ¨å¹¿
            if self._should_promote(deployment['metrics']):
                self._promote_to_production(new_model, deployment)
        
        def _should_promote(self, metrics):
            """åˆ¤æ–­æ˜¯å¦æŽ¨å¹¿åˆ°ç”Ÿäº§çŽ¯å¢ƒ"""
            metrics_df = pd.DataFrame(metrics)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            canary_avg_return = metrics_df['canary_return'].mean()
            production_avg_return = metrics_df['production_return'].mean()
            canary_sharpe = metrics_df['canary_sharpe'].mean()
            production_sharpe = metrics_df['production_sharpe'].mean()
            
            # æŽ¨å¹¿æ¡ä»¶
            return (
                canary_avg_return > production_avg_return * 0.95 and  # æ”¶ç›Šä¸ä½ŽäºŽ95%
                canary_sharpe > production_sharpe * 0.9 and  # Sharpeä¸ä½ŽäºŽ90%
                metrics_df['canary_return'].min() > -0.02  # å•æ—¥æœ€å¤§äºæŸä¸è¶…è¿‡2%
            )
        
        def _promote_to_production(self, new_model, deployment):
            """é€æ­¥æŽ¨å¹¿åˆ°ç”Ÿäº§çŽ¯å¢ƒ"""
            promotion_schedule = [0.1, 0.25, 0.5, 0.75, 1.0]  # é€æ­¥å¢žåŠ æ¯”ä¾‹
            
            for ratio in promotion_schedule:
                deployment['canary_portfolio'].resize(ratio)
                deployment['production_portfolio'].resize(1 - ratio)
                
                # ç»§ç»­ç›‘æŽ§7å¤©
                additional_metrics = self._monitor_for_days(7)
                
                if not self._is_stable(additional_metrics):
                    self._rollback_deployment(deployment)
                    return
            
            deployment['status'] = 'production'
            print(f"Model {deployment['model_version']} successfully promoted to production")
    ```
    
    ## ä¸ƒã€åˆè§„å®¡è®¡ä¸Žå¯è§£é‡Šæ€§
    
    ### 7.1 å†³ç­–å®¡è®¡æ—¥å¿—
    
    ```python
    class AuditLogger:
        """å®¡è®¡æ—¥å¿—ç³»ç»Ÿ"""
        
        def __init__(self, retention_years=5):
            self.retention_years = retention_years
            self.log_storage = TimeSeries Database()  # ä½¿ç”¨æ—¶åºæ•°æ®åº“
            
        def log_trading_decision(self, decision_data):
            """è®°å½•äº¤æ˜“å†³ç­–"""
            audit_record = {
                'timestamp': datetime.now().isoformat(),
                'model_version': decision_data['model_version'],
                'market_snapshot': {
                    'index_level': decision_data['index_level'],
                    'vix': decision_data['vix'],
                    'market_turnover': decision_data['market_turnover']
                },
                'features': decision_data['features'].to_dict(),
                'model_output': {
                    'raw_scores': decision_data['raw_scores'],
                    'final_weights': decision_data['final_weights'],
                    'confidence': decision_data['confidence']
                },
                'risk_metrics': {
                    'portfolio_var': decision_data['var'],
                    'max_drawdown': decision_data['max_drawdown'],
                    'concentration': decision_data['concentration']
                },
                'execution': {
                    'target_weights': decision_data['target_weights'],
                    'executed_weights': decision_data['executed_weights'],
                    'transaction_cost': decision_data['transaction_cost'],
                    'slippage': decision_data['slippage']
                },
                'compliance_checks': decision_data['compliance_checks']
            }
            
            # å­˜å‚¨åˆ°æ—¶åºæ•°æ®åº“
            self.log_storage.insert(audit_record)
            
            # ç”Ÿæˆå®¡è®¡æŠ¥å‘Š
            if decision_data.get('generate_report', False):
                self._generate_audit_report(audit_record)
        
        def query_historical_decisions(self, start_date, end_date, filters=None):
            """æŸ¥è¯¢åŽ†å²å†³ç­–è®°å½•"""
            query = {
                'time_range': (start_date, end_date),
                'filters': filters or {}
            }
            
            return self.log_storage.query(query)
    ```
    
    ### 7.2 æ¨¡åž‹å¯è§£é‡Šæ€§
    
    ```python
    import shap
    from lime import lime_tabular
    
    class ModelExplainer:
        """æ¨¡åž‹å†³ç­–è§£é‡Šå™¨"""
        
        def __init__(self, model, feature_names):
            self.model = model
            self.feature_names = feature_names
            
            # åˆå§‹åŒ–SHAPè§£é‡Šå™¨
            self.shap_explainer = shap.DeepExplainer(
                model, 
                background_data=self._get_background_data()
            )
            
            # åˆå§‹åŒ–LIMEè§£é‡Šå™¨
            self.lime_explainer = lime_tabular.LimeTabularExplainer(
                training_data=self._get_training_data(),
                feature_names=feature_names,
                mode='regression'
            )
        
        def explain_prediction(self, input_data, stock_id):
            """è§£é‡Šå•ä¸ªé¢„æµ‹"""
            explanation = {
                'stock_id': stock_id,
                'prediction': self.model.predict(input_data),
                'shap_values': self._get_shap_explanation(input_data),
                'lime_explanation': self._get_lime_explanation(input_data),
                'feature_importance': self._get_feature_importance(input_data)
            }
            
            # ç”Ÿæˆå¯è§†åŒ–æŠ¥å‘Š
            self._generate_explanation_report(explanation)
            
            return explanation
        
        def _get_shap_explanation(self, input_data):
            """SHAPè§£é‡Š"""
            shap_values = self.shap_explainer.shap_values(input_data)
            
            # èŽ·å–å‰10ä¸ªæœ€é‡è¦çš„ç‰¹å¾
            feature_importance = pd.DataFrame({
                'feature': self.feature_names,
                'shap_value': shap_values[0],
                'abs_shap_value': np.abs(shap_values[0])
            }).sort_values('abs_shap_value', ascending=False).head(10)
            
            return {
                'shap_values': shap_values,
                'top_features': feature_importance.to_dict('records')
            }
        
        def _get_feature_importance(self, input_data):
            """ç‰¹å¾é‡è¦æ€§åˆ†æž"""
            # ä½¿ç”¨æ³¨æ„åŠ›æƒé‡ï¼ˆå¦‚æžœæ˜¯Transformeræ¨¡åž‹ï¼‰
            if hasattr(self.model, 'get_attention_weights'):
                attention_weights = self.model.get_attention_weights(input_data)
                
                # èšåˆæ—¶é—´ç»´åº¦çš„æ³¨æ„åŠ›
                temporal_importance = attention_weights.mean(axis=0)
                
                # èšåˆç‰¹å¾ç»´åº¦çš„æ³¨æ„åŠ›
                feature_importance = attention_weights.mean(axis=1)
                
                return {
                    'temporal_importance': temporal_importance,
                    'feature_importance': feature_importance,
                    'attention_heatmap': attention_weights
                }
            
            return {}
        
        def generate_compliance_report(self, decisions, period):
            """ç”Ÿæˆåˆè§„æŠ¥å‘Š"""
            report = {
                'period': period,
                'total_decisions': len(decisions),
                'model_versions': list(set(d['model_version'] for d in decisions)),
                'risk_violations': self._check_risk_violations(decisions),
                'concentration_analysis': self._analyze_concentration(decisions),
                'turnover_analysis': self._analyze_turnover(decisions),
                'cost_analysis': self._analyze_costs(decisions),
                'performance_attribution': self._attribute_performance(decisions)
            }
            
            # ä¿å­˜æŠ¥å‘Š
            self._save_compliance_report(report)
            
            return report
    ```
    
    ## å…«ã€é¡¹ç›®å®žæ–½è®¡åˆ’
    
    ### 8.1 é¡¹ç›®ç»“æž„
    
    ```
    rl_trading_system/
    â”œâ”€â”€ config/
    â”‚   â”œâ”€â”€ model_config.yaml
    â”‚   â”œâ”€â”€ trading_config.yaml
    â”‚   â”œâ”€â”€ monitoring_config.yaml
    â”‚   â””â”€â”€ compliance_config.yaml
    â”œâ”€â”€ data/
    â”‚   â”œâ”€â”€ collectors/
    â”‚   â”‚   â”œâ”€â”€ qlib_collector.py
    â”‚   â”‚   â””â”€â”€ akshare_collector.py
    â”‚   â”œâ”€â”€ processors/
    â”‚   â”‚   â”œâ”€â”€ feature_engineering.py
    â”‚   â”‚   â””â”€â”€ data_validation.py
    â”‚   â””â”€â”€ cache/
    â”œâ”€â”€ models/
    â”‚   â”œâ”€â”€ transformer/
    â”‚   â”‚   â”œâ”€â”€ encoder.py
    â”‚   â”‚   â””â”€â”€ attention.py
    â”‚   â”œâ”€â”€ rl_agents/
    â”‚   â”‚   â”œâ”€â”€ sac_agent.py
    â”‚   â”‚   â””â”€â”€ ppo_agent.py
    â”‚   â””â”€â”€ utils/
    â”œâ”€â”€ trading/
    â”‚   â”œâ”€â”€ environment/
    â”‚   â”‚   â”œâ”€â”€ portfolio_env.py
    â”‚   â”‚   â””â”€â”€ market_simulator.py
    â”‚   â”œâ”€â”€ execution/
    â”‚   â”‚   â”œâ”€â”€ order_manager.py
    â”‚   â”‚   â””â”€â”€ cost_model.py
    â”‚   â””â”€â”€ risk/
    â”œâ”€â”€ backtest/
    â”‚   â”œâ”€â”€ engine/
    â”‚   â”œâ”€â”€ analysis/
    â”‚   â””â”€â”€ visualization/
    â”œâ”€â”€ monitoring/
    â”‚   â”œâ”€â”€ metrics/
    â”‚   â”œâ”€â”€ alerts/
    â”‚   â””â”€â”€ dashboards/
    â”œâ”€â”€ deployment/
    â”‚   â”œâ”€â”€ canary/
    â”‚   â”œâ”€â”€ rollback/
    â”‚   â””â”€â”€ versioning/
    â”œâ”€â”€ audit/
    â”‚   â”œâ”€â”€ logger/
    â”‚   â”œâ”€â”€ explainer/
    â”‚   â””â”€â”€ reports/
    â”œâ”€â”€ tests/
    â”‚   â”œâ”€â”€ unit/
    â”‚   â”œâ”€â”€ integration/
    â”‚   â””â”€â”€ e2e/
    â””â”€â”€ scripts/
        â”œâ”€â”€ train.py
        â”œâ”€â”€ evaluate.py
        â”œâ”€â”€ deploy.py
        â””â”€â”€ monitor.py
    ```
    
    ## ä¹ã€æ€»ç»“
    
    æœ¬æ–¹æ¡ˆé€šè¿‡ç»“åˆå¼ºåŒ–å­¦ä¹ å’ŒTransformeræž¶æž„ï¼Œæž„å»ºäº†ä¸€ä¸ªå®Œæ•´çš„Aè‚¡é‡åŒ–äº¤æ˜“ç³»ç»Ÿã€‚å…³é”®åˆ›æ–°ç‚¹åŒ…æ‹¬ï¼š
    
    1. **å¼ºåŒ–å­¦ä¹ å†³ç­–**ï¼šä½¿ç”¨SAC/PPOç®—æ³•è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜äº¤æ˜“ç­–ç•¥
    2. **Transformeræ—¶åºå»ºæ¨¡**ï¼šæ•æ‰é•¿æœŸå¸‚åœºæ¨¡å¼å’Œå¤æ‚ä¾èµ–å…³ç³»
    3. **ç²¾ç¡®æˆæœ¬å»ºæ¨¡**ï¼šè€ƒè™‘æ‰‹ç»­è´¹ã€å°èŠ±ç¨Žå’Œå¸‚åœºå†²å‡»
    4. **ä¸¥æ ¼çš„è®­ç»ƒæµç¨‹**ï¼šä¸‰æ®µå¼æ•°æ®åˆ’åˆ†å’Œæ»šåŠ¨çª—å£éªŒè¯
    5. **ç»†ç²’åº¦ç›‘æŽ§**ï¼šå®žæ—¶æŒ‡æ ‡ç›‘æŽ§å’ŒåŠ¨æ€é˜ˆå€¼å‘Šè­¦
    6. **å®Œæ•´çš„åˆè§„ä½“ç³»**ï¼šå®¡è®¡æ—¥å¿—å’Œæ¨¡åž‹å¯è§£é‡Šæ€§
    
    ç³»ç»Ÿé¢„æœŸåœ¨ä¸¥æ ¼æŽ§åˆ¶é£Žé™©çš„å‰æä¸‹ï¼Œå®žçŽ°8%-12%çš„å¹´åŒ–æ”¶ç›Šç›®æ ‡ï¼Œä¸ºæŠ•èµ„è€…æä¾›ç¨³å¥å¯é çš„é‡åŒ–æŠ•èµ„å·¥å…·ã€‚
    ]]></file>
  <file path=".claude/settings.local.json"><![CDATA[
    {
      "permissions": {
        "allow": [
          "Bash(mkdir:*)",
          "Bash(python test:*)",
          "Bash(python:*)",
          "Bash(ls:*)",
          "Bash(rm:*)",
          "Bash(find:*)",
          "Bash(grep:*)",
          "Bash(true)",
          "Bash(touch:*)",
          "Bash(tree:*)",
          "Bash(timeout 30s python -m pytest:*)",
          "Bash(timeout:*)",
          "Bash(sed:*)",
          "Bash(pip install prometheus-client psutil)",
          "Bash(pip install:*)"
        ],
        "deny": []
      }
    }
    ]]></file>
  <file path="tests/integration/test_sac_integration.py"><![CDATA[
    """
    SACæ™ºèƒ½ä½“é›†æˆæµ‹è¯•
    """
    import pytest
    import torch
    import numpy as np
    
    from src.rl_trading_system.models import (
        SACAgent, SACConfig, Experience
    )
    
    
    class TestSACIntegration:
        """SACæ™ºèƒ½ä½“é›†æˆæµ‹è¯•"""
        
        def test_complete_sac_workflow(self):
            """æµ‹è¯•å®Œæ•´çš„SACå·¥ä½œæµç¨‹"""
            # åˆ›å»ºé…ç½®
            config = SACConfig(
                state_dim=32,
                action_dim=5,
                hidden_dim=64,
                batch_size=16,
                buffer_capacity=500,
                learning_starts=20
            )
            
            # åˆ›å»ºæ™ºèƒ½ä½“
            agent = SACAgent(config)
            
            # æ¨¡æ‹ŸçŽ¯å¢ƒäº¤äº’
            for episode in range(10):
                state = torch.randn(config.state_dim)
                
                for step in range(20):
                    # èŽ·å–åŠ¨ä½œ
                    action = agent.get_action(state, deterministic=False)
                    
                    # æ¨¡æ‹ŸçŽ¯å¢ƒåé¦ˆ
                    reward = np.random.uniform(-1, 1)
                    next_state = torch.randn(config.state_dim)
                    done = step == 19
                    
                    # æ·»åŠ ç»éªŒ
                    experience = Experience(
                        state=state,
                        action=action,
                        reward=reward,
                        next_state=next_state,
                        done=done
                    )
                    agent.add_experience(experience)
                    
                    # æ›´æ–°ç½‘ç»œ
                    if agent.can_update():
                        losses = agent.update()
                        
                        # éªŒè¯æŸå¤±å€¼
                        if losses:
                            assert 'critic_loss' in losses
                            assert 'actor_loss' in losses
                            assert isinstance(losses['critic_loss'], float)
                            assert isinstance(losses['actor_loss'], float)
                    
                    state = next_state
            
            # éªŒè¯æ™ºèƒ½ä½“çŠ¶æ€
            assert agent.training_step > 0
            assert agent.total_env_steps > 0
            assert agent.replay_buffer.size > 0
            
            # æµ‹è¯•è¯„ä¼°æ¨¡å¼
            agent.eval()
            test_state = torch.randn(config.state_dim)
            action1 = agent.get_action(test_state, deterministic=True)
            action2 = agent.get_action(test_state, deterministic=True)
            
            # ç¡®å®šæ€§åŠ¨ä½œåº”è¯¥ç›¸åŒ
            assert torch.allclose(action1, action2, atol=1e-6)
            
            print("âœ… SACæ™ºèƒ½ä½“é›†æˆæµ‹è¯•é€šè¿‡")
    
    
    if __name__ == "__main__":
        test = TestSACIntegration()
        test.test_complete_sac_workflow()
    ]]></file>
  <file path="tests/integration/test_canary_deployment_integration.py"><![CDATA[
    """
    é‡‘ä¸é›€éƒ¨ç½²ç³»ç»Ÿé›†æˆæµ‹è¯•
    æµ‹è¯•å®Œæ•´çš„é‡‘ä¸é›€éƒ¨ç½²æµç¨‹ï¼ŒåŒ…æ‹¬æ¨¡åž‹éƒ¨ç½²ã€A/Bæµ‹è¯•ã€æ€§èƒ½ç›‘æŽ§å’Œè‡ªåŠ¨å›žæ»š
    """
    import pytest
    import numpy as np
    from datetime import datetime, timedelta
    from unittest.mock import Mock
    
    from src.rl_trading_system.deployment.canary_deployment import (
        CanaryDeployment,
        ABTestFramework,
        ModelPerformanceComparator,
        DeploymentSafetyController,
        RollbackManager,
        PerformanceMetrics,
        DeploymentConfig,
        DeploymentStatus
    )
    from src.rl_trading_system.deployment.model_version_manager import (
        ModelVersionManager,
        ModelMetadata,
        ModelStatus
    )
    
    
    class DummyModel:
        """å¯åºåˆ—åŒ–çš„è™šæ‹Ÿæ¨¡åž‹ç±»"""
        def __init__(self, version, weights=None):
            self.version = version
            self.weights = weights or np.random.random((10, 10))
        
        def predict(self, x):
            return np.random.random(3)
    
    
    class TestCanaryDeploymentIntegration:
        """é‡‘ä¸é›€éƒ¨ç½²ç³»ç»Ÿé›†æˆæµ‹è¯•"""
        
        @pytest.fixture
        def mock_models(self):
            """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡åž‹"""
            baseline_model = Mock()
            baseline_model.predict = Mock(return_value=np.array([0.2, 0.3, 0.5]))
            baseline_model.version = "v1.0.0"
            
            canary_model = Mock()
            canary_model.predict = Mock(return_value=np.array([0.1, 0.2, 0.7]))
            canary_model.version = "v1.1.0"
            
            return baseline_model, canary_model
        
        @pytest.fixture
        def serializable_models(self):
            """åˆ›å»ºå¯åºåˆ—åŒ–çš„æ¨¡æ‹Ÿæ¨¡åž‹"""
            baseline_model = DummyModel("v1.0.0")
            canary_model = DummyModel("v1.1.0")
            
            return baseline_model, canary_model
        
        @pytest.fixture
        def deployment_config(self):
            """åˆ›å»ºéƒ¨ç½²é…ç½®"""
            return DeploymentConfig(
                canary_percentage=10.0,
                evaluation_period=300,  # 5åˆ†é’Ÿ
                success_threshold=0.90,
                error_threshold=0.10,
                performance_threshold=0.05,
                rollback_threshold=0.15,
                max_canary_duration=1800  # 30åˆ†é’Ÿ
            )
        
        def test_successful_canary_deployment_flow(self, mock_models, deployment_config):
            """æµ‹è¯•æˆåŠŸçš„é‡‘ä¸é›€éƒ¨ç½²æµç¨‹"""
            baseline_model, canary_model = mock_models
            
            # 1. åˆ›å»ºé‡‘ä¸é›€éƒ¨ç½²
            deployment = CanaryDeployment(
                canary_model=canary_model,
                baseline_model=baseline_model,
                config=deployment_config
            )
            
            # 2. å¯åŠ¨éƒ¨ç½²
            deployment.start_deployment()
            assert deployment.status == DeploymentStatus.ACTIVE
            assert deployment.traffic_percentage == 10.0
            
            # 3. æ¨¡æ‹Ÿè‰¯å¥½çš„æ€§èƒ½æŒ‡æ ‡
            for i in range(5):
                metrics = PerformanceMetrics(
                    success_rate=0.95 + i * 0.001,
                    error_rate=0.02 - i * 0.001,
                    avg_response_time=0.1 + i * 0.001,
                    throughput=1000 + i * 10,
                    accuracy=0.92 + i * 0.001,
                    precision=0.90 + i * 0.001,
                    recall=0.88 + i * 0.001,
                    f1_score=0.89 + i * 0.001
                )
                deployment.update_canary_metrics(metrics)
            
            # 4. éªŒè¯æˆåŠŸæ ‡å‡†
            assert deployment.evaluate_success_criteria() == True
            assert deployment.should_trigger_rollback() == False
            
            # 5. é€æ­¥å¢žåŠ æµé‡
            deployment.increase_traffic(20.0)
            assert deployment.traffic_percentage == 30.0
            
            deployment.increase_traffic(30.0)
            assert deployment.traffic_percentage == 60.0
            
            deployment.increase_traffic(40.0)
            assert deployment.traffic_percentage == 100.0
            
            # 6. å®Œæˆéƒ¨ç½²
            deployment.complete_deployment()
            assert deployment.status == DeploymentStatus.COMPLETED
            assert deployment.traffic_percentage == 100.0
        
        def test_failed_canary_deployment_with_rollback(self, mock_models, deployment_config):
            """æµ‹è¯•å¤±è´¥çš„é‡‘ä¸é›€éƒ¨ç½²å’Œè‡ªåŠ¨å›žæ»š"""
            baseline_model, canary_model = mock_models
            
            # 1. åˆ›å»ºé‡‘ä¸é›€éƒ¨ç½²
            deployment = CanaryDeployment(
                canary_model=canary_model,
                baseline_model=baseline_model,
                config=deployment_config
            )
            
            # 2. å¯åŠ¨éƒ¨ç½²
            deployment.start_deployment()
            
            # 3. æ¨¡æ‹ŸåˆæœŸè‰¯å¥½çš„æ€§èƒ½
            for i in range(3):
                metrics = PerformanceMetrics(
                    success_rate=0.92,
                    error_rate=0.05,
                    avg_response_time=0.12,
                    throughput=950,
                    accuracy=0.90,
                    precision=0.88,
                    recall=0.86,
                    f1_score=0.87
                )
                deployment.update_canary_metrics(metrics)
            
            # 4. å¢žåŠ æµé‡
            deployment.increase_traffic(20.0)
            assert deployment.traffic_percentage == 30.0
            
            # 5. æ¨¡æ‹Ÿæ€§èƒ½ä¸¥é‡ä¸‹é™
            deployment.performance_comparator.latest_comparison = {
                'success_rate_diff': 0.20,  # é‡‘ä¸é›€æ¨¡åž‹æ›´å·®
                'error_rate_diff': 0.15,
                'overall_performance_score': -0.25,  # ä¸¥é‡æ€§èƒ½ä¸‹é™
                'statistical_significance': True
            }
            
            # 6. éªŒè¯è§¦å‘å›žæ»š
            assert deployment.should_trigger_rollback() == True
            
            # 7. æ‰§è¡Œå›žæ»š
            deployment.rollback_deployment("æ€§èƒ½ä¸¥é‡ä¸‹é™")
            assert deployment.status == DeploymentStatus.ROLLED_BACK
            assert deployment.traffic_percentage == 0.0
        
        def test_ab_test_integration(self, mock_models):
            """æµ‹è¯•A/Bæµ‹è¯•é›†æˆ"""
            baseline_model, canary_model = mock_models
            
            # 1. åˆ›å»ºA/Bæµ‹è¯•æ¡†æž¶
            ab_test = ABTestFramework(
                model_a=baseline_model,
                model_b=canary_model,
                traffic_split=0.5,
                minimum_sample_size=50,
                confidence_level=0.95,
                test_duration=3600
            )
            
            # 2. å¼€å§‹æµ‹è¯•
            ab_test.start_test()
            
            # 3. æ¨¡æ‹Ÿç”¨æˆ·æµé‡å’Œç»“æžœ
            np.random.seed(42)
            
            # æ¨¡åž‹Aæ•°æ®ï¼ˆåŸºçº¿æ€§èƒ½ï¼‰
            for i in range(60):
                user_id = f"user_a_{i}"
                model = ab_test.route_traffic(user_id)
                success = 1.0 if np.random.random() > 0.25 else 0.0  # 75%æˆåŠŸçŽ‡
                ab_test.record_result(user_id, model, 0.75, success)
            
            # æ¨¡åž‹Bæ•°æ®ï¼ˆæ›´å¥½æ€§èƒ½ï¼‰
            for i in range(60):
                user_id = f"user_b_{i}"
                model = ab_test.route_traffic(user_id)
                success = 1.0 if np.random.random() > 0.15 else 0.0  # 85%æˆåŠŸçŽ‡
                ab_test.record_result(user_id, model, 0.85, success)
            
            # 4. éªŒè¯æ ·æœ¬é‡å……è¶³
            assert ab_test.has_sufficient_sample_size() == True
            
            # 5. è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§
            significance_result = ab_test.calculate_statistical_significance()
            assert isinstance(significance_result, dict)
            assert 'p_value' in significance_result
            assert 'is_significant' in significance_result
            
            # 6. æ¨¡æ‹Ÿæµ‹è¯•å®Œæˆ
            ab_test.start_time = datetime.now() - timedelta(hours=2)
            assert ab_test.is_test_complete() == True
            
            # 7. ç¡®å®šèŽ·èƒœè€…
            winner = ab_test.determine_winner()
            assert winner is not None
            assert winner in [baseline_model, canary_model]
        
        def test_model_version_manager_integration(self, serializable_models):
            """æµ‹è¯•æ¨¡åž‹ç‰ˆæœ¬ç®¡ç†å™¨é›†æˆ"""
            baseline_model, canary_model = serializable_models
            
            # 1. åˆ›å»ºç‰ˆæœ¬ç®¡ç†å™¨ï¼ˆä½¿ç”¨å”¯ä¸€è·¯å¾„ï¼‰
            import tempfile
            import shutil
            temp_dir = tempfile.mkdtemp()
            version_manager = ModelVersionManager(storage_path=temp_dir)
            
            # 2. æ³¨å†ŒåŸºçº¿æ¨¡åž‹
            baseline_metadata = ModelMetadata(
                model_id="trading_model",
                version="v1.0.0",
                name="åŸºçº¿äº¤æ˜“æ¨¡åž‹",
                description="ç¨³å®šçš„åŸºçº¿æ¨¡åž‹",
                created_at=datetime.now() - timedelta(days=30),
                created_by="system",
                model_type="rl_agent",
                framework="pytorch",
                metrics={
                    "accuracy": 0.85,
                    "precision": 0.83,
                    "recall": 0.80,
                    "f1_score": 0.81
                }
            )
            
            baseline_path = version_manager.register_model(baseline_model, baseline_metadata)
            assert baseline_path is not None
            
            # 3. æ³¨å†Œé‡‘ä¸é›€æ¨¡åž‹
            canary_metadata = ModelMetadata(
                model_id="trading_model",
                version="v1.1.0",
                name="æ”¹è¿›çš„äº¤æ˜“æ¨¡åž‹",
                description="æ–°çš„æ”¹è¿›ç‰ˆæœ¬",
                created_at=datetime.now(),
                created_by="developer",
                model_type="rl_agent",
                framework="pytorch",
                metrics={
                    "accuracy": 0.90,
                    "precision": 0.88,
                    "recall": 0.85,
                    "f1_score": 0.86
                }
            )
            
            canary_path = version_manager.register_model(canary_model, canary_metadata)
            assert canary_path is not None
            
            # 4. éªŒè¯ç‰ˆæœ¬åˆ—è¡¨
            versions = version_manager.list_versions("trading_model")
            assert len(versions) == 2
            assert versions[0].version == "v1.1.0"  # æœ€æ–°ç‰ˆæœ¬åœ¨å‰
            assert versions[1].version == "v1.0.0"
            
            # 5. æ¯”è¾ƒæ¨¡åž‹ç‰ˆæœ¬
            comparison = version_manager.compare_models(
                "trading_model", "trading_model",
                "v1.0.0", "v1.1.0"
            )
            assert isinstance(comparison, object)
            assert comparison.performance_diff['accuracy'] > 0  # æ–°ç‰ˆæœ¬æ›´å¥½
            
            # 6. æå‡æ–°ç‰ˆæœ¬ä¸ºæ´»è·ƒç‰ˆæœ¬
            success = version_manager.promote_model("trading_model", "v1.1.0")
            assert success == True
            
            # 7. èŽ·å–æ´»è·ƒæ¨¡åž‹
            active_metadata = version_manager.get_model_metadata("trading_model")
            assert active_metadata.version == "v1.1.0"
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_dir)
        
        def test_end_to_end_deployment_scenario(self, serializable_models, deployment_config):
            """æµ‹è¯•ç«¯åˆ°ç«¯éƒ¨ç½²åœºæ™¯"""
            baseline_model, canary_model = serializable_models
            
            # 1. ç‰ˆæœ¬ç®¡ç† - æ³¨å†Œæ¨¡åž‹ï¼ˆä½¿ç”¨å”¯ä¸€è·¯å¾„ï¼‰
            import tempfile
            import shutil
            temp_dir = tempfile.mkdtemp()
            version_manager = ModelVersionManager(storage_path=temp_dir)
            
            baseline_metadata = ModelMetadata(
                model_id="e2e_model_unique",
                version="v1.0.0",
                name="E2EåŸºçº¿æ¨¡åž‹",
                description="ç«¯åˆ°ç«¯æµ‹è¯•åŸºçº¿æ¨¡åž‹",
                created_at=datetime.now() - timedelta(days=7),
                created_by="system",
                model_type="rl_agent",
                framework="pytorch"
            )
            version_manager.register_model(baseline_model, baseline_metadata)
            
            canary_metadata = ModelMetadata(
                model_id="e2e_model_unique",
                version="v1.1.0",
                name="E2Eé‡‘ä¸é›€æ¨¡åž‹",
                description="ç«¯åˆ°ç«¯æµ‹è¯•é‡‘ä¸é›€æ¨¡åž‹",
                created_at=datetime.now(),
                created_by="developer",
                model_type="rl_agent",
                framework="pytorch"
            )
            version_manager.register_model(canary_model, canary_metadata)
            
            # 2. é‡‘ä¸é›€éƒ¨ç½² - å¯åŠ¨éƒ¨ç½²
            deployment = CanaryDeployment(
                canary_model=canary_model,
                baseline_model=baseline_model,
                config=deployment_config
            )
            deployment.start_deployment()
            
            # 3. A/Bæµ‹è¯• - å¹¶è¡Œè¿è¡Œ
            ab_test = ABTestFramework(
                model_a=baseline_model,
                model_b=canary_model,
                traffic_split=0.5,
                minimum_sample_size=30,
                confidence_level=0.95,
                test_duration=1800
            )
            ab_test.start_test()
            
            # 4. æ¨¡æ‹Ÿè¿è¡ŒæœŸé—´çš„æŒ‡æ ‡æ”¶é›†
            for i in range(10):
                # é‡‘ä¸é›€éƒ¨ç½²æŒ‡æ ‡
                canary_metrics = PerformanceMetrics(
                    success_rate=0.92 + i * 0.002,
                    error_rate=0.05 - i * 0.002,
                    avg_response_time=0.11 + i * 0.001,
                    throughput=980 + i * 5,
                    accuracy=0.91 + i * 0.001,
                    precision=0.89 + i * 0.001,
                    recall=0.87 + i * 0.001,
                    f1_score=0.88 + i * 0.001
                )
                deployment.update_canary_metrics(canary_metrics)
                
                # A/Bæµ‹è¯•æ•°æ® - ç¡®ä¿æ¯ä¸ªæ¨¡åž‹éƒ½æœ‰è¶³å¤Ÿçš„æ ·æœ¬
                for j in range(10):  # å¢žåŠ æ ·æœ¬æ•°é‡
                    user_id = f"user_{i}_{j}"
                    model = ab_test.route_traffic(user_id)
                    success = 1.0 if np.random.random() > 0.2 else 0.0
                    ab_test.record_result(user_id, model, 0.8, success)
            
            # 5. éªŒè¯ç³»ç»ŸçŠ¶æ€
            assert deployment.status == DeploymentStatus.ACTIVE
            assert deployment.evaluate_success_criteria() == True
            assert ab_test.has_sufficient_sample_size() == True
            
            # 6. æ¨¡æ‹Ÿæµ‹è¯•å®Œæˆå’Œéƒ¨ç½²æŽ¨è¿›
            ab_test.start_time = datetime.now() - timedelta(minutes=35)
            if ab_test.is_test_complete():
                winner = ab_test.determine_winner()
                if winner == canary_model:
                    # é‡‘ä¸é›€æ¨¡åž‹èŽ·èƒœï¼Œç»§ç»­éƒ¨ç½²
                    deployment.increase_traffic(40.0)
                    deployment.complete_deployment()
                    
                    # æå‡ä¸ºæ´»è·ƒç‰ˆæœ¬
                    version_manager.promote_model("e2e_model_unique", "v1.1.0")
                    
                    assert deployment.status == DeploymentStatus.COMPLETED
                    assert version_manager.get_model_metadata("e2e_model_unique").version == "v1.1.0"
            
            # 7. éªŒè¯éƒ¨ç½²åŽ†å²
            deployment_status = deployment.get_deployment_status()
            assert deployment_status['deployment_id'] is not None
            assert deployment_status['canary_model_version'] == "v1.1.0"
            assert deployment_status['baseline_model_version'] == "v1.0.0"
            
            # æ¸…ç†ä¸´æ—¶ç›®å½•
            shutil.rmtree(temp_dir)
    ]]></file>
  <file path="tests/integration/__init__.py"><![CDATA[
    # é›†æˆæµ‹è¯•
    ]]></file>
  <file path="tests/e2e/__init__.py"><![CDATA[
    # ç«¯åˆ°ç«¯æµ‹è¯•
    ]]></file>
  <file path="tests/unit/test_transformer.py"><![CDATA[
    """
    æµ‹è¯•Transformerç¼–ç å™¨çš„å•å…ƒæµ‹è¯•
    """
    
    import pytest
    import torch
    import torch.nn as nn
    import numpy as np
    from typing import Tuple, Optional
    
    from src.rl_trading_system.models.transformer import (
        TimeSeriesTransformer,
        TransformerConfig,
        TransformerEncoderLayer,
        FeedForwardNetwork
    )
    
    
    class TestTransformerConfig:
        """æµ‹è¯•Transformeré…ç½®"""
        
        def test_default_config(self):
            """æµ‹è¯•é»˜è®¤é…ç½®"""
            config = TransformerConfig()
            
            assert config.d_model == 256
            assert config.n_heads == 8
            assert config.n_layers == 6
            assert config.d_ff == 1024
            assert config.dropout == 0.1
            assert config.max_seq_len == 252
            assert config.n_features == 50
            assert config.activation == 'gelu'
        
        def test_custom_config(self):
            """æµ‹è¯•è‡ªå®šä¹‰é…ç½®"""
            config = TransformerConfig(
                d_model=512,
                n_heads=16,
                n_layers=12,
                d_ff=2048,
                dropout=0.2,
                max_seq_len=500,
                n_features=100,
                activation='relu'
            )
            
            assert config.d_model == 512
            assert config.n_heads == 16
            assert config.n_layers == 12
            assert config.d_ff == 2048
            assert config.dropout == 0.2
            assert config.max_seq_len == 500
            assert config.n_features == 100
            assert config.activation == 'relu'
        
        def test_config_validation(self):
            """æµ‹è¯•é…ç½®éªŒè¯"""
            # d_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤
            with pytest.raises(AssertionError):
                TransformerConfig(d_model=256, n_heads=7)
            
            # æ­£å¸¸æƒ…å†µåº”è¯¥ä¸æŠ¥é”™
            config = TransformerConfig(d_model=256, n_heads=8)
            assert config.d_model == 256
            assert config.n_heads == 8
    
    
    class TestFeedForwardNetwork:
        """æµ‹è¯•å‰é¦ˆç½‘ç»œ"""
        
        @pytest.fixture
        def ffn(self):
            """åˆ›å»ºå‰é¦ˆç½‘ç»œå®žä¾‹"""
            return FeedForwardNetwork(d_model=256, d_ff=1024, dropout=0.1, activation='gelu')
        
        def test_initialization(self, ffn):
            """æµ‹è¯•å‰é¦ˆç½‘ç»œåˆå§‹åŒ–"""
            assert isinstance(ffn.linear1, nn.Linear)
            assert isinstance(ffn.linear2, nn.Linear)
            assert isinstance(ffn.dropout, nn.Dropout)
            assert ffn.linear1.in_features == 256
            assert ffn.linear1.out_features == 1024
            assert ffn.linear2.in_features == 1024
            assert ffn.linear2.out_features == 256
        
        def test_forward_pass(self, ffn):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_model = 2, 10, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            output = ffn(x)
            
            assert output.shape == (batch_size, seq_len, d_model)
        
        def test_different_activations(self):
            """æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°"""
            d_model, d_ff = 256, 1024
            activations = ['relu', 'gelu', 'swish']
            
            for activation in activations:
                ffn = FeedForwardNetwork(d_model, d_ff, activation=activation)
                x = torch.randn(2, 10, d_model)
                output = ffn(x)
                assert output.shape == (2, 10, d_model)
        
        def test_gradient_flow(self, ffn):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            x = torch.randn(2, 10, 256, requires_grad=True)
            output = ffn(x)
            loss = output.sum()
            loss.backward()
            
            assert x.grad is not None
            for param in ffn.parameters():
                assert param.grad is not None
    
    
    class TestTransformerEncoderLayer:
        """æµ‹è¯•Transformerç¼–ç å™¨å±‚"""
        
        @pytest.fixture
        def encoder_layer(self):
            """åˆ›å»ºç¼–ç å™¨å±‚å®žä¾‹"""
            config = TransformerConfig(d_model=256, n_heads=8, d_ff=1024, dropout=0.1)
            return TransformerEncoderLayer(config)
        
        def test_initialization(self, encoder_layer):
            """æµ‹è¯•ç¼–ç å™¨å±‚åˆå§‹åŒ–"""
            assert hasattr(encoder_layer, 'self_attention')
            assert hasattr(encoder_layer, 'feed_forward')
            assert hasattr(encoder_layer, 'norm1')
            assert hasattr(encoder_layer, 'norm2')
            assert hasattr(encoder_layer, 'dropout1')
            assert hasattr(encoder_layer, 'dropout2')
        
        def test_forward_pass(self, encoder_layer):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            output = encoder_layer(x)
            
            assert output.shape == (batch_size, seq_len, d_model)
        
        def test_with_mask(self, encoder_layer):
            """æµ‹è¯•å¸¦æŽ©ç çš„ç¼–ç å™¨å±‚"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            # åˆ›å»ºæ³¨æ„åŠ›æŽ©ç 
            mask = torch.zeros(batch_size, seq_len, seq_len)
            mask[:, :, 10:] = float('-inf')  # æŽ©ç›–åŽ10ä¸ªä½ç½®
            
            output = encoder_layer(x, mask=mask)
            
            assert output.shape == (batch_size, seq_len, d_model)
        
        def test_residual_connections(self, encoder_layer):
            """æµ‹è¯•æ®‹å·®è¿žæŽ¥"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            encoder_layer.eval()
            
            # èŽ·å–ä¸­é—´ç»“æžœæ¥éªŒè¯æ®‹å·®è¿žæŽ¥
            # è¿™éœ€è¦ä¿®æ”¹forwardæ–¹æ³•æˆ–æ·»åŠ hookï¼Œè¿™é‡Œç®€åŒ–æµ‹è¯•
            output = encoder_layer(x)
            
            # è¾“å‡ºä¸åº”è¯¥ç­‰äºŽè¾“å…¥ï¼ˆå› ä¸ºæœ‰å˜æ¢ï¼‰
            assert not torch.allclose(output, x, atol=1e-3)
            
            # ä½†åº”è¯¥ä¿æŒç›¸åŒçš„å½¢çŠ¶
            assert output.shape == x.shape
        
        def test_gradient_flow(self, encoder_layer):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            x = torch.randn(2, 20, 256, requires_grad=True)
            output = encoder_layer(x)
            loss = output.sum()
            loss.backward()
            
            assert x.grad is not None
            for param in encoder_layer.parameters():
                assert param.grad is not None
        
        def test_layer_norm_placement(self, encoder_layer):
            """æµ‹è¯•å±‚å½’ä¸€åŒ–çš„ä½ç½®"""
            # éªŒè¯å±‚å½’ä¸€åŒ–ç¡®å®žè¢«åº”ç”¨
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            encoder_layer.eval()
            output = encoder_layer(x)
            
            # è¾“å‡ºåº”è¯¥ç»è¿‡å½’ä¸€åŒ–ï¼Œå…·æœ‰åˆç†çš„ç»Ÿè®¡ç‰¹æ€§
            output_mean = output.mean(dim=-1)
            output_std = output.std(dim=-1)
            
            # å±‚å½’ä¸€åŒ–åŽï¼Œæœ€åŽä¸€ä¸ªç»´åº¦çš„å‡å€¼åº”è¯¥æŽ¥è¿‘0ï¼Œæ ‡å‡†å·®æŽ¥è¿‘1
            # ä½†ç”±äºŽæ®‹å·®è¿žæŽ¥ï¼Œè¿™ä¸ªæµ‹è¯•å¯èƒ½ä¸ä¸¥æ ¼æˆç«‹
            assert output.shape == (batch_size, seq_len, d_model)
    
    
    class TestTimeSeriesTransformer:
        """æµ‹è¯•æ—¶åºTransformer"""
        
        @pytest.fixture
        def transformer_config(self):
            """åˆ›å»ºTransformeré…ç½®"""
            return TransformerConfig(
                d_model=256,
                n_heads=8,
                n_layers=6,
                d_ff=1024,
                dropout=0.1,
                max_seq_len=252,
                n_features=50
            )
        
        @pytest.fixture
        def transformer(self, transformer_config):
            """åˆ›å»ºTransformerå®žä¾‹"""
            return TimeSeriesTransformer(transformer_config)
        
        def test_initialization(self, transformer, transformer_config):
            """æµ‹è¯•Transformeråˆå§‹åŒ–"""
            assert transformer.config == transformer_config
            assert hasattr(transformer, 'input_projection')
            assert hasattr(transformer, 'pos_encoding')
            assert hasattr(transformer, 'encoder_layers')
            assert hasattr(transformer, 'temporal_attention')
            assert hasattr(transformer, 'output_projection')
            assert len(transformer.encoder_layers) == transformer_config.n_layers
        
        def test_forward_pass(self, transformer):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, n_stocks, n_features = 2, 60, 10, 50
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            output = transformer(x)
            
            assert output.shape == (batch_size, n_stocks, 256)  # d_model
        
        def test_different_input_dimensions(self, transformer):
            """æµ‹è¯•ä¸åŒè¾“å…¥ç»´åº¦ä¸‹çš„è¡¨çŽ°"""
            n_features = 50
            d_model = 256
            
            test_cases = [
                (1, 30, 5),   # å°æ‰¹æ¬¡ï¼ŒçŸ­åºåˆ—ï¼Œå°‘è‚¡ç¥¨
                (2, 60, 10),  # ä¸­ç­‰æ‰¹æ¬¡ï¼Œä¸­ç­‰åºåˆ—ï¼Œä¸­ç­‰è‚¡ç¥¨
                (4, 120, 20), # å¤§æ‰¹æ¬¡ï¼Œé•¿åºåˆ—ï¼Œå¤šè‚¡ç¥¨
            ]
            
            for batch_size, seq_len, n_stocks in test_cases:
                x = torch.randn(batch_size, seq_len, n_stocks, n_features)
                output = transformer(x)
                assert output.shape == (batch_size, n_stocks, d_model)
        
        def test_with_mask(self, transformer):
            """æµ‹è¯•å¸¦æŽ©ç çš„Transformer"""
            batch_size, seq_len, n_stocks, n_features = 2, 60, 10, 50
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            # åˆ›å»ºåºåˆ—æŽ©ç 
            mask = torch.zeros(batch_size, seq_len)
            mask[:, 40:] = float('-inf')  # æŽ©ç›–åŽ20ä¸ªæ—¶é—´æ­¥
            
            output = transformer(x, mask=mask)
            
            assert output.shape == (batch_size, n_stocks, 256)
        
        def test_gradient_flow(self, transformer):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            batch_size, seq_len, n_stocks, n_features = 2, 30, 5, 50
            x = torch.randn(batch_size, seq_len, n_stocks, n_features, requires_grad=True)
            
            output = transformer(x)
            loss = output.sum()
            loss.backward()
            
            assert x.grad is not None
            for param in transformer.parameters():
                assert param.grad is not None
        
        def test_sequence_length_handling(self, transformer):
            """æµ‹è¯•åºåˆ—é•¿åº¦å¤„ç†"""
            batch_size, n_stocks, n_features = 2, 10, 50
            max_seq_len = transformer.config.max_seq_len
            
            # æµ‹è¯•ä¸åŒé•¿åº¦çš„åºåˆ—
            for seq_len in [10, 50, 100, max_seq_len]:
                x = torch.randn(batch_size, seq_len, n_stocks, n_features)
                output = transformer(x)
                assert output.shape == (batch_size, n_stocks, 256)
            
            # æµ‹è¯•è¶…è¿‡æœ€å¤§é•¿åº¦çš„åºåˆ—
            if max_seq_len < 500:  # é¿å…å†…å­˜é—®é¢˜
                x = torch.randn(batch_size, max_seq_len + 10, n_stocks, n_features)
                with pytest.raises(IndexError):
                    transformer(x)
        
        def test_batch_processing(self, transformer):
            """æµ‹è¯•æ‰¹å¤„ç†"""
            seq_len, n_stocks, n_features = 60, 10, 50
            
            # æµ‹è¯•ä¸åŒæ‰¹æ¬¡å¤§å°
            for batch_size in [1, 2, 4, 8]:
                x = torch.randn(batch_size, seq_len, n_stocks, n_features)
                output = transformer(x)
                assert output.shape == (batch_size, n_stocks, 256)
        
        def test_feature_dimension_handling(self, transformer_config):
            """æµ‹è¯•ç‰¹å¾ç»´åº¦å¤„ç†"""
            # æµ‹è¯•ä¸åŒç‰¹å¾ç»´åº¦
            for n_features in [20, 50, 100]:
                config = TransformerConfig(
                    d_model=256,
                    n_heads=8,
                    n_layers=3,  # å‡å°‘å±‚æ•°ä»¥åŠ å¿«æµ‹è¯•
                    n_features=n_features
                )
                transformer = TimeSeriesTransformer(config)
                
                batch_size, seq_len, n_stocks = 2, 30, 5
                x = torch.randn(batch_size, seq_len, n_stocks, n_features)
                output = transformer(x)
                assert output.shape == (batch_size, n_stocks, 256)
        
        def test_model_parameters(self, transformer):
            """æµ‹è¯•æ¨¡åž‹å‚æ•°"""
            # æ£€æŸ¥æ¨¡åž‹æœ‰å‚æ•°
            total_params = sum(p.numel() for p in transformer.parameters())
            assert total_params > 0
            
            # æ£€æŸ¥å¯è®­ç»ƒå‚æ•°
            trainable_params = sum(p.numel() for p in transformer.parameters() if p.requires_grad)
            assert trainable_params > 0
            assert trainable_params == total_params  # æ‰€æœ‰å‚æ•°éƒ½åº”è¯¥å¯è®­ç»ƒ
        
        def test_model_modes(self, transformer):
            """æµ‹è¯•æ¨¡åž‹æ¨¡å¼ï¼ˆè®­ç»ƒ/è¯„ä¼°ï¼‰"""
            batch_size, seq_len, n_stocks, n_features = 2, 30, 5, 50
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            # è®­ç»ƒæ¨¡å¼
            transformer.train()
            output_train = transformer(x)
            
            # è¯„ä¼°æ¨¡å¼
            transformer.eval()
            output_eval = transformer(x)
            
            # å½¢çŠ¶åº”è¯¥ç›¸åŒ
            assert output_train.shape == output_eval.shape
            
            # ç”±äºŽdropoutï¼Œè¾“å‡ºå¯èƒ½ä¸åŒï¼ˆä½†è¿™ä¸ªæµ‹è¯•å¯èƒ½ä¸ç¨³å®šï¼‰
            # assert not torch.allclose(output_train, output_eval, atol=1e-6)
        
        def test_memory_efficiency(self, transformer):
            """æµ‹è¯•å†…å­˜æ•ˆçŽ‡"""
            # æµ‹è¯•è¾ƒå¤§çš„è¾“å…¥
            batch_size, seq_len, n_stocks, n_features = 4, 100, 20, 50
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            # åº”è¯¥èƒ½å¤Ÿå¤„ç†è¾ƒå¤§çš„è¾“å…¥è€Œä¸å‡ºçŽ°å†…å­˜é”™è¯¯
            output = transformer(x)
            assert output.shape == (batch_size, n_stocks, 256)
        
        def test_deterministic_output(self, transformer):
            """æµ‹è¯•ç¡®å®šæ€§è¾“å‡º"""
            batch_size, seq_len, n_stocks, n_features = 2, 30, 5, 50
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ä»¥ç¦ç”¨dropout
            transformer.eval()
            
            # å¤šæ¬¡å‰å‘ä¼ æ’­åº”è¯¥äº§ç”Ÿç›¸åŒç»“æžœ
            output1 = transformer(x)
            output2 = transformer(x)
            
            torch.testing.assert_close(output1, output2, rtol=1e-5, atol=1e-6)
    
    
    class TestTransformerIntegration:
        """æµ‹è¯•Transformeré›†æˆ"""
        
        def test_end_to_end_pipeline(self):
            """æµ‹è¯•ç«¯åˆ°ç«¯æµæ°´çº¿"""
            # åˆ›å»ºé…ç½®
            config = TransformerConfig(
                d_model=128,  # è¾ƒå°çš„æ¨¡åž‹ä»¥åŠ å¿«æµ‹è¯•
                n_heads=4,
                n_layers=2,
                d_ff=256,
                n_features=20
            )
            
            # åˆ›å»ºæ¨¡åž‹
            transformer = TimeSeriesTransformer(config)
            
            # åˆ›å»ºè¾“å…¥æ•°æ®
            batch_size, seq_len, n_stocks, n_features = 2, 30, 5, 20
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            # å‰å‘ä¼ æ’­
            output = transformer(x)
            
            # éªŒè¯è¾“å‡º
            assert output.shape == (batch_size, n_stocks, 128)
            assert not torch.isnan(output).any()
            assert not torch.isinf(output).any()
        
        def test_training_step_simulation(self):
            """æµ‹è¯•è®­ç»ƒæ­¥éª¤æ¨¡æ‹Ÿ"""
            config = TransformerConfig(
                d_model=128,
                n_heads=4,
                n_layers=2,
                n_features=20
            )
            
            transformer = TimeSeriesTransformer(config)
            optimizer = torch.optim.Adam(transformer.parameters(), lr=0.001)
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
            batch_size, seq_len, n_stocks, n_features = 2, 30, 5, 20
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            target = torch.randn(batch_size, n_stocks, 128)
            
            # å‰å‘ä¼ æ’­
            output = transformer(x)
            
            # è®¡ç®—æŸå¤±
            loss = nn.MSELoss()(output, target)
            
            # åå‘ä¼ æ’­
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # éªŒè¯æ¢¯åº¦è¢«è®¡ç®—
            for param in transformer.parameters():
                assert param.grad is not None
        
        def test_model_saving_loading(self, tmp_path):
            """æµ‹è¯•æ¨¡åž‹ä¿å­˜å’ŒåŠ è½½"""
            config = TransformerConfig(d_model=128, n_heads=4, n_layers=2, n_features=20)
            transformer = TimeSeriesTransformer(config)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ä»¥ç¦ç”¨dropout
            transformer.eval()
            
            # åˆ›å»ºæµ‹è¯•è¾“å…¥
            x = torch.randn(1, 30, 5, 20)
            original_output = transformer(x)
            
            # ä¿å­˜æ¨¡åž‹
            model_path = tmp_path / "transformer.pth"
            torch.save(transformer.state_dict(), model_path)
            
            # åˆ›å»ºæ–°æ¨¡åž‹å¹¶åŠ è½½æƒé‡
            new_transformer = TimeSeriesTransformer(config)
            new_transformer.load_state_dict(torch.load(model_path))
            new_transformer.eval()  # ä¹Ÿè®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼
            
            # éªŒè¯è¾“å‡ºç›¸åŒ
            new_output = new_transformer(x)
            torch.testing.assert_close(original_output, new_output, rtol=1e-5, atol=1e-6)
        
        @pytest.mark.parametrize("batch_size,seq_len,n_stocks", [
            (1, 20, 3),
            (2, 40, 5),
            (4, 60, 10),
        ])
        def test_scalability(self, batch_size, seq_len, n_stocks):
            """æµ‹è¯•å¯æ‰©å±•æ€§"""
            config = TransformerConfig(d_model=128, n_heads=4, n_layers=2, n_features=20)
            transformer = TimeSeriesTransformer(config)
            
            x = torch.randn(batch_size, seq_len, n_stocks, 20)
            output = transformer(x)
            
            assert output.shape == (batch_size, n_stocks, 128)
            assert not torch.isnan(output).any()
        
        def test_performance_benchmark(self):
            """æµ‹è¯•æ€§èƒ½åŸºå‡†"""
            config = TransformerConfig(d_model=256, n_heads=8, n_layers=6, n_features=50)
            transformer = TimeSeriesTransformer(config)
            transformer.eval()
            
            # æµ‹è¯•æŽ¨ç†æ—¶é—´
            batch_size, seq_len, n_stocks, n_features = 2, 60, 10, 50
            x = torch.randn(batch_size, seq_len, n_stocks, n_features)
            
            import time
            start_time = time.time()
            
            with torch.no_grad():
                for _ in range(10):
                    output = transformer(x)
            
            end_time = time.time()
            avg_time = (end_time - start_time) / 10
            
            # æŽ¨ç†æ—¶é—´åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
            assert avg_time < 1.0  # 1ç§’å†…å®ŒæˆæŽ¨ç†
            assert output.shape == (batch_size, n_stocks, 256)
    ]]></file>
  <file path="tests/unit/test_transaction_cost_model.py"><![CDATA[
    """
    æµ‹è¯•äº¤æ˜“æˆæœ¬è®¡ç®—æ¨¡å—
    æµ‹è¯•æ‰‹ç»­è´¹ã€å°èŠ±ç¨Žå’Œæ»‘ç‚¹æˆæœ¬è®¡ç®—ï¼Œä»¥åŠAè‚¡ç‰¹æœ‰çš„äº¤æ˜“è§„åˆ™å’Œæˆæœ¬ç»“æž„
    """
    
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, timezone
    from typing import Dict, Any, List
    
    from src.rl_trading_system.trading.transaction_cost_model import (
        TransactionCostModel,
        CostParameters,
        CostBreakdown,
        TradeInfo
    )
    from src.rl_trading_system.trading.almgren_chriss_model import (
        AlmgrenChrissModel,
        MarketImpactParameters
    )
    
    
    class TestCostParameters:
        """æµ‹è¯•æˆæœ¬å‚æ•°ç±»"""
        
        def test_parameters_creation(self):
            """æµ‹è¯•å‚æ•°æ­£å¸¸åˆ›å»º"""
            params = CostParameters(
                commission_rate=0.001,
                stamp_tax_rate=0.001,
                min_commission=5.0,
                transfer_fee_rate=0.00002,
                market_impact_model=None
            )
            
            assert params.commission_rate == 0.001
            assert params.stamp_tax_rate == 0.001
            assert params.min_commission == 5.0
            assert params.transfer_fee_rate == 0.00002
            assert params.market_impact_model is None
        
        def test_parameters_validation_negative_rates(self):
            """æµ‹è¯•è´Ÿè´¹çŽ‡éªŒè¯"""
            with pytest.raises(ValueError, match="æ‰‹ç»­è´¹çŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°"):
                CostParameters(
                    commission_rate=-0.001,
                    stamp_tax_rate=0.001,
                    min_commission=5.0,
                    transfer_fee_rate=0.00002
                )
            
            with pytest.raises(ValueError, match="å°èŠ±ç¨ŽçŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°"):
                CostParameters(
                    commission_rate=0.001,
                    stamp_tax_rate=-0.001,
                    min_commission=5.0,
                    transfer_fee_rate=0.00002
                )
        
        def test_parameters_validation_negative_min_commission(self):
            """æµ‹è¯•è´Ÿæœ€å°æ‰‹ç»­è´¹éªŒè¯"""
            with pytest.raises(ValueError, match="æœ€å°æ‰‹ç»­è´¹ä¸èƒ½ä¸ºè´Ÿæ•°"):
                CostParameters(
                    commission_rate=0.001,
                    stamp_tax_rate=0.001,
                    min_commission=-5.0,
                    transfer_fee_rate=0.00002
                )
        
        def test_parameters_validation_excessive_rates(self):
            """æµ‹è¯•è¿‡é«˜è´¹çŽ‡éªŒè¯"""
            with pytest.raises(ValueError, match="æ‰‹ç»­è´¹çŽ‡ä¸èƒ½è¶…è¿‡10%"):
                CostParameters(
                    commission_rate=0.15,  # 15%
                    stamp_tax_rate=0.001,
                    min_commission=5.0,
                    transfer_fee_rate=0.00002
                )
    
    
    class TestTradeInfo:
        """æµ‹è¯•äº¤æ˜“ä¿¡æ¯ç±»"""
        
        def test_trade_info_creation(self):
            """æµ‹è¯•äº¤æ˜“ä¿¡æ¯æ­£å¸¸åˆ›å»º"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp,
                market_volume=1000000,
                volatility=0.02
            )
            
            assert trade.symbol == "000001.SZ"
            assert trade.side == "buy"
            assert trade.quantity == 1000
            assert trade.price == 10.5
            assert trade.timestamp == timestamp
            assert trade.market_volume == 1000000
            assert trade.volatility == 0.02
        
        def test_trade_info_validation_invalid_side(self):
            """æµ‹è¯•æ— æ•ˆäº¤æ˜“æ–¹å‘éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="äº¤æ˜“æ–¹å‘å¿…é¡»æ˜¯'buy'æˆ–'sell'"):
                TradeInfo(
                    symbol="000001.SZ",
                    side="invalid",
                    quantity=1000,
                    price=10.5,
                    timestamp=timestamp
                )
        
        def test_trade_info_validation_negative_quantity(self):
            """æµ‹è¯•è´Ÿæ•°é‡éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="äº¤æ˜“æ•°é‡ä¸èƒ½ä¸ºè´Ÿæ•°"):
                TradeInfo(
                    symbol="000001.SZ",
                    side="buy",
                    quantity=-1000,
                    price=10.5,
                    timestamp=timestamp
                )
        
        def test_trade_info_validation_negative_price(self):
            """æµ‹è¯•è´Ÿä»·æ ¼éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿæ•°"):
                TradeInfo(
                    symbol="000001.SZ",
                    side="buy",
                    quantity=1000,
                    price=-10.5,
                    timestamp=timestamp
                )
        
        def test_trade_info_get_trade_value(self):
            """æµ‹è¯•èŽ·å–äº¤æ˜“ä»·å€¼æ–¹æ³•"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            assert trade.get_trade_value() == 10500.0
        
        def test_trade_info_is_buy_sell(self):
            """æµ‹è¯•ä¹°å–åˆ¤æ–­æ–¹æ³•"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            buy_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            sell_trade = TradeInfo(
                symbol="000001.SZ",
                side="sell",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            assert buy_trade.is_buy()
            assert not buy_trade.is_sell()
            assert sell_trade.is_sell()
            assert not sell_trade.is_buy()
    
    
    class TestCostBreakdown:
        """æµ‹è¯•æˆæœ¬åˆ†è§£ç±»"""
        
        def test_cost_breakdown_creation(self):
            """æµ‹è¯•æˆæœ¬åˆ†è§£æ­£å¸¸åˆ›å»º"""
            breakdown = CostBreakdown(
                commission=10.5,
                stamp_tax=10.5,
                transfer_fee=2.1,
                market_impact=5.2,
                total_cost=28.3
            )
            
            assert breakdown.commission == 10.5
            assert breakdown.stamp_tax == 10.5
            assert breakdown.transfer_fee == 2.1
            assert breakdown.market_impact == 5.2
            assert breakdown.total_cost == 28.3
        
        def test_cost_breakdown_validation_negative_costs(self):
            """æµ‹è¯•è´Ÿæˆæœ¬éªŒè¯"""
            with pytest.raises(ValueError, match="æ‰‹ç»­è´¹ä¸èƒ½ä¸ºè´Ÿæ•°"):
                CostBreakdown(
                    commission=-10.5,
                    stamp_tax=10.5,
                    transfer_fee=2.1,
                    market_impact=5.2,
                    total_cost=28.3
                )
        
        def test_cost_breakdown_validation_inconsistent_total(self):
            """æµ‹è¯•æ€»æˆæœ¬ä¸€è‡´æ€§éªŒè¯"""
            with pytest.raises(ValueError, match="æ€»æˆæœ¬åº”ç­‰äºŽå„é¡¹æˆæœ¬ä¹‹å’Œ"):
                CostBreakdown(
                    commission=10.5,
                    stamp_tax=10.5,
                    transfer_fee=2.1,
                    market_impact=5.2,
                    total_cost=50.0  # ä¸ç­‰äºŽå„é¡¹ä¹‹å’Œ
                )
        
        def test_cost_breakdown_get_cost_ratio(self):
            """æµ‹è¯•èŽ·å–æˆæœ¬æ¯”çŽ‡æ–¹æ³•"""
            breakdown = CostBreakdown(
                commission=10.5,
                stamp_tax=10.5,
                transfer_fee=2.1,
                market_impact=5.2,
                total_cost=28.3
            )
            
            trade_value = 10500.0
            ratio = breakdown.get_cost_ratio(trade_value)
            expected_ratio = 28.3 / 10500.0
            assert abs(ratio - expected_ratio) < 1e-8
        
        def test_cost_breakdown_get_cost_basis_points(self):
            """æµ‹è¯•èŽ·å–åŸºç‚¹æˆæœ¬æ–¹æ³•"""
            breakdown = CostBreakdown(
                commission=10.5,
                stamp_tax=10.5,
                transfer_fee=2.1,
                market_impact=5.2,
                total_cost=28.3
            )
            
            trade_value = 10500.0
            bp = breakdown.get_cost_basis_points(trade_value)
            expected_bp = (28.3 / 10500.0) * 10000
            assert abs(bp - expected_bp) < 1e-6
    
    
    class TestTransactionCostModel:
        """æµ‹è¯•äº¤æ˜“æˆæœ¬æ¨¡åž‹"""
        
        @pytest.fixture
        def default_cost_parameters(self):
            """é»˜è®¤æˆæœ¬å‚æ•°"""
            return CostParameters(
                commission_rate=0.001,
                stamp_tax_rate=0.001,
                min_commission=5.0,
                transfer_fee_rate=0.00002
            )
        
        @pytest.fixture
        def almgren_chriss_model(self):
            """Almgren-Chrissæ¨¡åž‹"""
            params = MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=1000000,
                participation_rate=0.1
            )
            return AlmgrenChrissModel(params)
        
        @pytest.fixture
        def cost_model(self, default_cost_parameters):
            """é»˜è®¤æˆæœ¬æ¨¡åž‹"""
            return TransactionCostModel(default_cost_parameters)
        
        @pytest.fixture
        def cost_model_with_impact(self, default_cost_parameters, almgren_chriss_model):
            """å¸¦å¸‚åœºå†²å‡»çš„æˆæœ¬æ¨¡åž‹"""
            default_cost_parameters.market_impact_model = almgren_chriss_model
            return TransactionCostModel(default_cost_parameters)
        
        def test_model_creation(self, default_cost_parameters):
            """æµ‹è¯•æ¨¡åž‹æ­£å¸¸åˆ›å»º"""
            model = TransactionCostModel(default_cost_parameters)
            assert model.parameters == default_cost_parameters
        
        def test_calculate_commission_basic(self, cost_model):
            """æµ‹è¯•åŸºæœ¬æ‰‹ç»­è´¹è®¡ç®—"""
            trade_value = 10500.0
            commission = cost_model._calculate_commission(trade_value)
            
            # æ‰‹ç»­è´¹ = max(trade_value * rate, min_commission)
            expected = max(10500.0 * 0.001, 5.0)
            assert abs(commission - expected) < 1e-8
        
        def test_calculate_commission_minimum(self, cost_model):
            """æµ‹è¯•æœ€å°æ‰‹ç»­è´¹"""
            trade_value = 1000.0  # å°é¢äº¤æ˜“
            commission = cost_model._calculate_commission(trade_value)
            
            # åº”è¯¥ä½¿ç”¨æœ€å°æ‰‹ç»­è´¹
            assert commission == 5.0
        
        def test_calculate_stamp_tax_buy(self, cost_model):
            """æµ‹è¯•ä¹°å…¥å°èŠ±ç¨Žï¼ˆåº”ä¸º0ï¼‰"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            buy_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            stamp_tax = cost_model._calculate_stamp_tax(buy_trade)
            assert stamp_tax == 0.0
        
        def test_calculate_stamp_tax_sell(self, cost_model):
            """æµ‹è¯•å–å‡ºå°èŠ±ç¨Ž"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            sell_trade = TradeInfo(
                symbol="000001.SZ",
                side="sell",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            stamp_tax = cost_model._calculate_stamp_tax(sell_trade)
            expected = 10500.0 * 0.001
            assert abs(stamp_tax - expected) < 1e-8
        
        def test_calculate_transfer_fee(self, cost_model):
            """æµ‹è¯•è¿‡æˆ·è´¹è®¡ç®—"""
            trade_value = 10500.0
            transfer_fee = cost_model._calculate_transfer_fee(trade_value)
            
            expected = 10500.0 * 0.00002
            assert abs(transfer_fee - expected) < 1e-8
        
        def test_calculate_market_impact_without_model(self, cost_model):
            """æµ‹è¯•æ— å¸‚åœºå†²å‡»æ¨¡åž‹æ—¶çš„è®¡ç®—"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            impact = cost_model._calculate_market_impact(trade)
            assert impact == 0.0
        
        def test_calculate_market_impact_with_model(self, cost_model_with_impact):
            """æµ‹è¯•æœ‰å¸‚åœºå†²å‡»æ¨¡åž‹æ—¶çš„è®¡ç®—"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=100000,
                price=10.5,
                timestamp=timestamp,
                market_volume=1000000,
                volatility=0.02
            )
            
            impact = cost_model_with_impact._calculate_market_impact(trade)
            assert impact > 0.0
        
        def test_calculate_cost_buy_trade(self, cost_model):
            """æµ‹è¯•ä¹°å…¥äº¤æ˜“æˆæœ¬è®¡ç®—"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            buy_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            breakdown = cost_model.calculate_cost(buy_trade)
            
            # éªŒè¯å„é¡¹æˆæœ¬
            assert breakdown.commission == 10.5  # max(10500 * 0.001, 5.0)
            assert breakdown.stamp_tax == 0.0    # ä¹°å…¥æ— å°èŠ±ç¨Ž
            assert abs(breakdown.transfer_fee - 0.21) < 1e-8  # 10500 * 0.00002
            assert breakdown.market_impact == 0.0  # æ— å¸‚åœºå†²å‡»æ¨¡åž‹
            
            expected_total = 10.5 + 0.0 + 0.21 + 0.0
            assert abs(breakdown.total_cost - expected_total) < 1e-8
        
        def test_calculate_cost_sell_trade(self, cost_model):
            """æµ‹è¯•å–å‡ºäº¤æ˜“æˆæœ¬è®¡ç®—"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            sell_trade = TradeInfo(
                symbol="000001.SZ",
                side="sell",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            breakdown = cost_model.calculate_cost(sell_trade)
            
            # éªŒè¯å„é¡¹æˆæœ¬
            assert breakdown.commission == 10.5   # max(10500 * 0.001, 5.0)
            assert breakdown.stamp_tax == 10.5    # 10500 * 0.001
            assert abs(breakdown.transfer_fee - 0.21) < 1e-8 # 10500 * 0.00002
            assert breakdown.market_impact == 0.0 # æ— å¸‚åœºå†²å‡»æ¨¡åž‹
            
            expected_total = 10.5 + 10.5 + 0.21 + 0.0
            assert abs(breakdown.total_cost - expected_total) < 1e-8
        
        def test_calculate_cost_with_market_impact(self, cost_model_with_impact):
            """æµ‹è¯•åŒ…å«å¸‚åœºå†²å‡»çš„æˆæœ¬è®¡ç®—"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=100000,
                price=10.5,
                timestamp=timestamp,
                market_volume=1000000,
                volatility=0.02
            )
            
            breakdown = cost_model_with_impact.calculate_cost(trade)
            
            # éªŒè¯å¸‚åœºå†²å‡»å¤§äºŽ0
            assert breakdown.market_impact > 0.0
            
            # éªŒè¯æ€»æˆæœ¬åŒ…å«å¸‚åœºå†²å‡»
            expected_total = breakdown.commission + breakdown.stamp_tax + breakdown.transfer_fee + breakdown.market_impact
            assert abs(breakdown.total_cost - expected_total) < 1e-8
        
        def test_calculate_batch_costs(self, cost_model):
            """æµ‹è¯•æ‰¹é‡æˆæœ¬è®¡ç®—"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trades = [
                TradeInfo("000001.SZ", "buy", 1000, 10.5, timestamp),
                TradeInfo("000002.SZ", "sell", 2000, 15.2, timestamp),
                TradeInfo("000003.SZ", "buy", 500, 8.8, timestamp)
            ]
            
            breakdowns = cost_model.calculate_batch_costs(trades)
            
            assert len(breakdowns) == 3
            
            # éªŒè¯æ¯ä¸ªç»“æžœéƒ½æ˜¯æœ‰æ•ˆçš„
            for breakdown in breakdowns:
                assert isinstance(breakdown, CostBreakdown)
                assert breakdown.total_cost > 0
        
        def test_a_share_trading_rules_validation(self, cost_model):
            """æµ‹è¯•Aè‚¡äº¤æ˜“è§„åˆ™éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            # æµ‹è¯•æ­£å¸¸äº¤æ˜“
            normal_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=100,  # 100è‚¡ï¼Œç¬¦åˆAè‚¡æœ€å°äº¤æ˜“å•ä½
                price=10.5,
                timestamp=timestamp
            )
            
            breakdown = cost_model.calculate_cost(normal_trade)
            assert breakdown.total_cost > 0
            
            # æµ‹è¯•ä¸ç¬¦åˆæœ€å°äº¤æ˜“å•ä½çš„äº¤æ˜“ï¼ˆåº”è¯¥è¢«å¤„ç†ï¼‰
            odd_lot_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=150,  # 150è‚¡ï¼Œä¸æ˜¯100çš„å€æ•°
                price=10.5,
                timestamp=timestamp
            )
            
            # æ¨¡åž‹åº”è¯¥èƒ½å¤„ç†è¿™ç§æƒ…å†µ
            breakdown = cost_model.calculate_cost(odd_lot_trade)
            assert breakdown.total_cost > 0
        
        def test_cost_model_different_scenarios(self, cost_model):
            """æµ‹è¯•æˆæœ¬æ¨¡åž‹åœ¨å„ç§äº¤æ˜“åœºæ™¯ä¸‹çš„è¡¨çŽ°"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            scenarios = [
                # å°é¢äº¤æ˜“
                TradeInfo("000001.SZ", "buy", 100, 5.0, timestamp),
                # å¤§é¢äº¤æ˜“
                TradeInfo("000002.SZ", "sell", 100000, 50.0, timestamp),
                # é«˜ä»·è‚¡
                TradeInfo("000003.SZ", "buy", 100, 200.0, timestamp),
                # ä½Žä»·è‚¡
                TradeInfo("000004.SZ", "sell", 10000, 2.0, timestamp)
            ]
            
            for trade in scenarios:
                breakdown = cost_model.calculate_cost(trade)
                
                # éªŒè¯æˆæœ¬åˆç†æ€§
                assert breakdown.total_cost > 0
                assert breakdown.commission >= 5.0  # æœ€å°æ‰‹ç»­è´¹
                
                # éªŒè¯å°èŠ±ç¨Žè§„åˆ™
                if trade.is_sell():
                    assert breakdown.stamp_tax > 0
                else:
                    assert breakdown.stamp_tax == 0
                
                # éªŒè¯æˆæœ¬æ¯”çŽ‡åœ¨åˆç†èŒƒå›´å†…
                cost_ratio = breakdown.get_cost_ratio(trade.get_trade_value())
                assert 0 < cost_ratio < 0.1  # æˆæœ¬æ¯”çŽ‡åº”åœ¨0-10%ä¹‹é—´
    
    
    class TestTransactionCostModelBoundaryConditions:
        """æµ‹è¯•äº¤æ˜“æˆæœ¬æ¨¡åž‹è¾¹ç•Œæ¡ä»¶"""
        
        def test_zero_quantity_trade(self):
            """æµ‹è¯•é›¶æ•°é‡äº¤æ˜“"""
            params = CostParameters(
                commission_rate=0.001,
                stamp_tax_rate=0.001,
                min_commission=5.0,
                transfer_fee_rate=0.00002
            )
            model = TransactionCostModel(params)
            
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            zero_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=0,
                price=10.5,
                timestamp=timestamp
            )
            
            breakdown = model.calculate_cost(zero_trade)
            
            # é›¶æ•°é‡äº¤æ˜“ä»åº”æœ‰æœ€å°æ‰‹ç»­è´¹
            assert breakdown.commission == 5.0
            assert breakdown.stamp_tax == 0.0
            assert breakdown.transfer_fee == 0.0
        
        def test_very_high_price_trade(self):
            """æµ‹è¯•æžé«˜ä»·æ ¼äº¤æ˜“"""
            params = CostParameters(
                commission_rate=0.001,
                stamp_tax_rate=0.001,
                min_commission=5.0,
                transfer_fee_rate=0.00002
            )
            model = TransactionCostModel(params)
            
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            high_price_trade = TradeInfo(
                symbol="000001.SZ",
                side="sell",
                quantity=100,
                price=10000.0,  # æžé«˜ä»·æ ¼
                timestamp=timestamp
            )
            
            breakdown = model.calculate_cost(high_price_trade)
            
            # éªŒè¯æˆæœ¬è®¡ç®—æ­£ç¡®
            trade_value = 100 * 10000.0
            expected_commission = trade_value * 0.001
            expected_stamp_tax = trade_value * 0.001
            expected_transfer_fee = trade_value * 0.00002
            
            assert abs(breakdown.commission - expected_commission) < 1e-6
            assert abs(breakdown.stamp_tax - expected_stamp_tax) < 1e-6
            assert abs(breakdown.transfer_fee - expected_transfer_fee) < 1e-6
        
        def test_extreme_parameters(self):
            """æµ‹è¯•æžç«¯å‚æ•°"""
            # æžä½Žè´¹çŽ‡
            low_params = CostParameters(
                commission_rate=1e-6,
                stamp_tax_rate=1e-6,
                min_commission=0.01,
                transfer_fee_rate=1e-8
            )
            
            model = TransactionCostModel(low_params)
            
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp
            )
            
            breakdown = model.calculate_cost(trade)
            
            # éªŒè¯è®¡ç®—ç»“æžœæœ‰æ•ˆ
            assert breakdown.total_cost > 0
            assert not np.isnan(breakdown.total_cost)
            assert not np.isinf(breakdown.total_cost)
    
    
    class TestTransactionCostModelPerformance:
        """æµ‹è¯•äº¤æ˜“æˆæœ¬æ¨¡åž‹æ€§èƒ½"""
        
        def test_batch_calculation_performance(self):
            """æµ‹è¯•æ‰¹é‡è®¡ç®—æ€§èƒ½"""
            params = CostParameters(
                commission_rate=0.001,
                stamp_tax_rate=0.001,
                min_commission=5.0,
                transfer_fee_rate=0.00002
            )
            model = TransactionCostModel(params)
            
            # ç”Ÿæˆå¤§é‡äº¤æ˜“æ•°æ®
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            trades = []
            
            for i in range(1000):
                trade = TradeInfo(
                    symbol=f"00000{i % 100}.SZ",
                    side="buy" if i % 2 == 0 else "sell",
                    quantity=np.random.randint(100, 10000),
                    price=np.random.uniform(5.0, 50.0),
                    timestamp=timestamp
                )
                trades.append(trade)
            
            import time
            start_time = time.time()
            
            breakdowns = model.calculate_batch_costs(trades)
            
            calculation_time = time.time() - start_time
            
            # è®¡ç®—æ—¶é—´åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
            assert calculation_time < 2.0
            assert len(breakdowns) == 1000
            
            # éªŒè¯æ‰€æœ‰ç»“æžœéƒ½æœ‰æ•ˆ
            for breakdown in breakdowns:
                assert breakdown.total_cost > 0
                assert not np.isnan(breakdown.total_cost)
    
    
    class TestTransactionCostModelIntegration:
        """æµ‹è¯•äº¤æ˜“æˆæœ¬æ¨¡åž‹é›†æˆ"""
        
        def test_integration_with_almgren_chriss(self):
            """æµ‹è¯•ä¸ŽAlmgren-Chrissæ¨¡åž‹çš„é›†æˆ"""
            # åˆ›å»ºå¸‚åœºå†²å‡»æ¨¡åž‹
            impact_params = MarketImpactParameters(
                permanent_impact_coeff=0.08,
                temporary_impact_coeff=0.4,
                volatility=0.025,
                daily_volume=5000000,
                participation_rate=0.05
            )
            impact_model = AlmgrenChrissModel(impact_params)
            
            # åˆ›å»ºäº¤æ˜“æˆæœ¬æ¨¡åž‹
            cost_params = CostParameters(
                commission_rate=0.001,
                stamp_tax_rate=0.001,
                min_commission=5.0,
                transfer_fee_rate=0.00002,
                market_impact_model=impact_model
            )
            cost_model = TransactionCostModel(cost_params)
            
            # æµ‹è¯•ä¸åŒè§„æ¨¡çš„äº¤æ˜“
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            small_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.5,
                timestamp=timestamp,
                market_volume=5000000,
                volatility=0.025
            )
            
            large_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=100000,
                price=10.5,
                timestamp=timestamp,
                market_volume=5000000,
                volatility=0.025
            )
            
            small_breakdown = cost_model.calculate_cost(small_trade)
            large_breakdown = cost_model.calculate_cost(large_trade)
            
            # éªŒè¯å¤§é¢äº¤æ˜“æœ‰æ›´é«˜çš„å¸‚åœºå†²å‡»
            assert large_breakdown.market_impact > small_breakdown.market_impact
            
            # éªŒè¯æ€»æˆæœ¬éšäº¤æ˜“è§„æ¨¡å¢žé•¿
            small_ratio = small_breakdown.get_cost_ratio(small_trade.get_trade_value())
            large_ratio = large_breakdown.get_cost_ratio(large_trade.get_trade_value())
            assert large_ratio > small_ratio
        
        def test_real_world_cost_estimation(self):
            """æµ‹è¯•çœŸå®žä¸–ç•Œæˆæœ¬ä¼°ç®—"""
            # ä½¿ç”¨çœŸå®žçš„Aè‚¡æˆæœ¬å‚æ•°
            real_params = CostParameters(
                commission_rate=0.0003,  # ä¸‡ä¸‰æ‰‹ç»­è´¹
                stamp_tax_rate=0.001,   # åƒä¸€å°èŠ±ç¨Ž
                min_commission=5.0,     # 5å…ƒæœ€å°æ‰‹ç»­è´¹
                transfer_fee_rate=0.00002  # ä¸‡0.2è¿‡æˆ·è´¹
            )
            
            model = TransactionCostModel(real_params)
            
            # æµ‹è¯•å…¸åž‹çš„Aè‚¡äº¤æ˜“
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            # å°é¢äº¤æ˜“ï¼ˆ1ä¸‡å…ƒï¼‰
            small_trade = TradeInfo(
                symbol="000001.SZ",
                side="buy",
                quantity=1000,
                price=10.0,
                timestamp=timestamp
            )
            
            # ä¸­ç­‰äº¤æ˜“ï¼ˆ10ä¸‡å…ƒï¼‰
            medium_trade = TradeInfo(
                symbol="000002.SZ",
                side="sell",
                quantity=5000,
                price=20.0,
                timestamp=timestamp
            )
            
            # å¤§é¢äº¤æ˜“ï¼ˆ100ä¸‡å…ƒï¼‰
            large_trade = TradeInfo(
                symbol="000003.SZ",
                side="buy",
                quantity=20000,
                price=50.0,
                timestamp=timestamp
            )
            
            small_breakdown = model.calculate_cost(small_trade)
            medium_breakdown = model.calculate_cost(medium_trade)
            large_breakdown = model.calculate_cost(large_trade)
            
            # éªŒè¯æˆæœ¬åœ¨åˆç†èŒƒå›´å†…
            small_bp = small_breakdown.get_cost_basis_points(small_trade.get_trade_value())
            medium_bp = medium_breakdown.get_cost_basis_points(medium_trade.get_trade_value())
            large_bp = large_breakdown.get_cost_basis_points(large_trade.get_trade_value())
            
            # å°é¢äº¤æ˜“æˆæœ¬è¾ƒé«˜ï¼ˆç”±äºŽæœ€å°æ‰‹ç»­è´¹ï¼‰
            assert 5 <= small_bp <= 100  # 0.5-10bp
            
            # ä¸­ç­‰äº¤æ˜“æˆæœ¬é€‚ä¸­
            assert 10 <= medium_bp <= 50   # 1-5bp
            
            # å¤§é¢äº¤æ˜“æˆæœ¬ç›¸å¯¹è¾ƒä½Ž
            assert 3 <= large_bp <= 30     # 0.3-3bp
            
            # éªŒè¯å–å‡ºäº¤æ˜“æœ‰å°èŠ±ç¨Ž
            assert medium_breakdown.stamp_tax > 0  # å–å‡ºäº¤æ˜“
            assert small_breakdown.stamp_tax == 0  # ä¹°å…¥äº¤æ˜“
            assert large_breakdown.stamp_tax == 0  # ä¹°å…¥äº¤æ˜“
    ]]></file>
  <file path="tests/unit/test_trainer.py"><![CDATA[
    """
    RLTrainerçš„å•å…ƒæµ‹è¯•
    æµ‹è¯•è®­ç»ƒå¾ªçŽ¯ã€æ—©åœæœºåˆ¶ã€è®­ç»ƒè¿‡ç¨‹ç¨³å®šæ€§å’Œæ”¶æ•›æ€§
    """
    import pytest
    import numpy as np
    import pandas as pd
    import torch
    import torch.nn as nn
    from datetime import datetime, timedelta
    from unittest.mock import Mock, patch, MagicMock
    from typing import Dict, List, Tuple, Any, Optional
    import gym
    from gym import spaces
    
    from src.rl_trading_system.training.trainer import (
        RLTrainer, 
        TrainingConfig,
        EarlyStopping,
        TrainingMetrics
    )
    from src.rl_trading_system.training.data_split_strategy import (
        DataSplitStrategy,
        TimeSeriesSplitStrategy,
        SplitConfig,
        SplitResult
    )
    # SAC agent will be implemented later
    # from src.rl_trading_system.rl_agent.sac_agent import SACAgent
    # Portfolio environment imports - using mock environment instead
    # from src.rl_trading_system.trading.portfolio_environment import PortfolioEnvironment, PortfolioConfig
    
    
    class MockEnvironment(gym.Env):
        """æ¨¡æ‹Ÿäº¤æ˜“çŽ¯å¢ƒ"""
        
        def __init__(self, n_stocks=4, lookback_window=30):
            self.n_stocks = n_stocks
            self.lookback_window = lookback_window
            self.n_features = 10
            self.n_market_features = 5
            
            # å®šä¹‰è§‚å¯Ÿç©ºé—´å’ŒåŠ¨ä½œç©ºé—´
            self.observation_space = spaces.Dict({
                'features': spaces.Box(
                    low=-np.inf, 
                    high=np.inf, 
                    shape=(lookback_window, n_stocks, self.n_features),
                    dtype=np.float32
                ),
                'positions': spaces.Box(
                    low=0, 
                    high=1, 
                    shape=(n_stocks,),
                    dtype=np.float32
                ),
                'market_state': spaces.Box(
                    low=-np.inf, 
                    high=np.inf, 
                    shape=(self.n_market_features,),
                    dtype=np.float32
                )
            })
            
            self.action_space = spaces.Box(
                low=0, 
                high=1, 
                shape=(n_stocks,),
                dtype=np.float32
            )
            
            self.current_step = 0
            self.max_steps = 100
            self.episode_returns = []
            
        def reset(self):
            """é‡ç½®çŽ¯å¢ƒ"""
            self.current_step = 0
            self.episode_returns = []
            
            return {
                'features': np.random.randn(self.lookback_window, self.n_stocks, self.n_features).astype(np.float32),
                'positions': np.ones(self.n_stocks, dtype=np.float32) / self.n_stocks,
                'market_state': np.random.randn(self.n_market_features).astype(np.float32)
            }
        
        def step(self, action):
            """æ‰§è¡Œä¸€æ­¥"""
            self.current_step += 1
            
            # ç”Ÿæˆéšæœºå¥–åŠ±
            reward = np.random.randn() * 0.01  # å°çš„éšæœºå¥–åŠ±
            self.episode_returns.append(reward)
            
            # æ£€æŸ¥æ˜¯å¦ç»“æŸ
            done = self.current_step >= self.max_steps
            
            # ç”Ÿæˆä¸‹ä¸€ä¸ªè§‚å¯Ÿ
            obs = {
                'features': np.random.randn(self.lookback_window, self.n_stocks, self.n_features).astype(np.float32),
                'positions': action.astype(np.float32),
                'market_state': np.random.randn(self.n_market_features).astype(np.float32)
            }
            
            # ç”Ÿæˆä¿¡æ¯
            info = {
                'portfolio_return': reward,
                'transaction_cost': abs(reward) * 0.1,
                'positions': action
            }
            
            return obs, reward, done, info
    
    
    class MockSACAgent:
        """æ¨¡æ‹ŸSACæ™ºèƒ½ä½“"""
        
        def __init__(self, observation_space, action_space):
            self.observation_space = observation_space
            self.action_space = action_space
            self.training_mode = True
            self.total_updates = 0
            
        def act(self, obs, deterministic=False):
            """é€‰æ‹©åŠ¨ä½œ"""
            action_shape = self.action_space.shape
            action = np.random.rand(*action_shape).astype(np.float32)
            action = action / action.sum()  # æ ‡å‡†åŒ–
            return action
        
        def update(self, replay_buffer, batch_size=256):
            """æ›´æ–°æ™ºèƒ½ä½“å‚æ•°"""
            self.total_updates += 1
            
            # æ¨¡æ‹Ÿè®­ç»ƒæŒ‡æ ‡
            actor_loss = np.random.randn() * 0.1
            critic_loss = np.random.randn() * 0.1
            temperature_loss = np.random.randn() * 0.01
            
            return {
                'actor_loss': actor_loss,
                'critic_loss': critic_loss,
                'temperature_loss': temperature_loss,
                'temperature': 0.2,
                'q_values': np.random.randn(batch_size).mean()
            }
        
        def train(self):
            """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
            self.training_mode = True
        
        def eval(self):
            """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
            self.training_mode = False
        
        def save(self, filepath):
            """ä¿å­˜æ¨¡åž‹"""
            pass
        
        def load(self, filepath):
            """åŠ è½½æ¨¡åž‹"""
            pass
    
    
    class TestTrainingConfig:
        """è®­ç»ƒé…ç½®æµ‹è¯•ç±»"""
        
        def test_training_config_creation(self):
            """æµ‹è¯•è®­ç»ƒé…ç½®åˆ›å»º"""
            config = TrainingConfig(
                n_episodes=1000,
                max_steps_per_episode=200,
                batch_size=256,
                learning_rate=3e-4,
                buffer_size=100000,
                validation_frequency=50,
                save_frequency=100
            )
            
            assert config.n_episodes == 1000
            assert config.max_steps_per_episode == 200
            assert config.batch_size == 256
            assert config.learning_rate == 3e-4
            assert config.buffer_size == 100000
            assert config.validation_frequency == 50
            assert config.save_frequency == 100
        
        def test_training_config_defaults(self):
            """æµ‹è¯•è®­ç»ƒé…ç½®é»˜è®¤å€¼"""
            config = TrainingConfig()
            
            assert config.n_episodes == 5000
            assert config.max_steps_per_episode == 252
            assert config.batch_size == 256
            assert config.learning_rate == 3e-4
            assert config.buffer_size == 1000000
            assert config.gamma == 0.99
            assert config.tau == 0.005
        
        def test_training_config_validation(self):
            """æµ‹è¯•è®­ç»ƒé…ç½®éªŒè¯"""
            # æµ‹è¯•æ— æ•ˆçš„episodeæ•°é‡
            with pytest.raises(ValueError, match="n_episodeså¿…é¡»ä¸ºæ­£æ•°"):
                TrainingConfig(n_episodes=0)
            
            # æµ‹è¯•æ— æ•ˆçš„å­¦ä¹ çŽ‡
            with pytest.raises(ValueError, match="learning_rateå¿…é¡»ä¸ºæ­£æ•°"):
                TrainingConfig(learning_rate=-0.1)
            
            # æµ‹è¯•æ— æ•ˆçš„batch size
            with pytest.raises(ValueError, match="batch_sizeå¿…é¡»ä¸ºæ­£æ•°"):
                TrainingConfig(batch_size=0)
    
    
    class TestEarlyStopping:
        """æ—©åœæœºåˆ¶æµ‹è¯•ç±»"""
        
        def test_early_stopping_creation(self):
            """æµ‹è¯•æ—©åœæœºåˆ¶åˆ›å»º"""
            early_stopping = EarlyStopping(
                patience=10,
                min_delta=0.001,
                mode='max'
            )
            
            assert early_stopping.patience == 10
            assert early_stopping.min_delta == 0.001
            assert early_stopping.mode == 'max'
            assert early_stopping.best_score is None
            assert early_stopping.counter == 0
            assert not early_stopping.early_stop
        
        def test_early_stopping_improvement_detection(self):
            """æµ‹è¯•æ—©åœæœºåˆ¶çš„æ”¹è¿›æ£€æµ‹"""
            # æµ‹è¯•æœ€å¤§åŒ–æ¨¡å¼
            early_stopping = EarlyStopping(patience=3, min_delta=0.01, mode='max')
            
            # ç¬¬ä¸€æ¬¡æ›´æ–°ï¼Œåº”è¯¥æ˜¯æ”¹è¿›
            assert early_stopping.step(0.8) == False
            assert early_stopping.best_score == 0.8
            assert early_stopping.counter == 0
            
            # ç¬¬äºŒæ¬¡æ›´æ–°ï¼Œæœ‰æ˜¾è‘—æ”¹è¿›
            assert early_stopping.step(0.85) == False
            assert early_stopping.best_score == 0.85
            assert early_stopping.counter == 0
            
            # ç¬¬ä¸‰æ¬¡æ›´æ–°ï¼Œæ²¡æœ‰æ˜¾è‘—æ”¹è¿›
            assert early_stopping.step(0.851) == False
            assert early_stopping.counter == 1
            
            # è¿žç»­æ²¡æœ‰æ”¹è¿›
            assert early_stopping.step(0.84) == False
            assert early_stopping.counter == 2
            
            assert early_stopping.step(0.83) == False
            assert early_stopping.counter == 3
            
            # è¾¾åˆ°patienceï¼Œè§¦å‘æ—©åœ
            assert early_stopping.step(0.82) == True
            assert early_stopping.early_stop == True
        
        def test_early_stopping_minimization_mode(self):
            """æµ‹è¯•æ—©åœæœºåˆ¶çš„æœ€å°åŒ–æ¨¡å¼"""
            early_stopping = EarlyStopping(patience=2, min_delta=0.01, mode='min')
            
            # æŸå¤±é€æ¸å‡å°
            assert early_stopping.step(1.0) == False
            assert early_stopping.step(0.8) == False  # æ”¹è¿›
            assert early_stopping.step(0.85) == False  # æ²¡æœ‰æ”¹è¿›
            assert early_stopping.step(0.86) == False  # æ²¡æœ‰æ”¹è¿›
            assert early_stopping.step(0.87) == True   # è§¦å‘æ—©åœ
        
        def test_early_stopping_reset(self):
            """æµ‹è¯•æ—©åœæœºåˆ¶é‡ç½®"""
            early_stopping = EarlyStopping(patience=2, min_delta=0.01, mode='max')
            
            # è¿è¡Œåˆ°æŽ¥è¿‘æ—©åœ
            early_stopping.step(0.8)
            early_stopping.step(0.75)
            early_stopping.step(0.74)
            
            assert early_stopping.counter > 0
            
            # é‡ç½®
            early_stopping.reset()
            
            assert early_stopping.best_score is None
            assert early_stopping.counter == 0
            assert early_stopping.early_stop == False
    
    
    class TestTrainingMetrics:
        """è®­ç»ƒæŒ‡æ ‡æµ‹è¯•ç±»"""
        
        def test_training_metrics_creation(self):
            """æµ‹è¯•è®­ç»ƒæŒ‡æ ‡åˆ›å»º"""
            metrics = TrainingMetrics()
            
            assert len(metrics.episode_rewards) == 0
            assert len(metrics.episode_lengths) == 0
            assert len(metrics.actor_losses) == 0
            assert len(metrics.critic_losses) == 0
            assert len(metrics.validation_scores) == 0
        
        def test_training_metrics_update(self):
            """æµ‹è¯•è®­ç»ƒæŒ‡æ ‡æ›´æ–°"""
            metrics = TrainingMetrics()
            
            # æ·»åŠ episodeæŒ‡æ ‡
            metrics.add_episode_metrics(reward=100.0, length=200, actor_loss=0.1, critic_loss=0.2)
            
            assert len(metrics.episode_rewards) == 1
            assert metrics.episode_rewards[0] == 100.0
            assert metrics.episode_lengths[0] == 200
            assert metrics.actor_losses[0] == 0.1
            assert metrics.critic_losses[0] == 0.2
            
            # æ·»åŠ éªŒè¯æŒ‡æ ‡
            metrics.add_validation_score(0.85)
            assert len(metrics.validation_scores) == 1
            assert metrics.validation_scores[0] == 0.85
        
        def test_training_metrics_statistics(self):
            """æµ‹è¯•è®­ç»ƒæŒ‡æ ‡ç»Ÿè®¡"""
            metrics = TrainingMetrics()
            
            # æ·»åŠ å¤šä¸ªepisode
            rewards = [10, 20, 30, 40, 50]
            for reward in rewards:
                metrics.add_episode_metrics(reward=reward, length=100, actor_loss=0.1, critic_loss=0.1)
            
            stats = metrics.get_statistics()
            
            assert stats['mean_reward'] == 30.0
            assert stats['std_reward'] == pytest.approx(np.std(rewards), rel=1e-6)
            assert stats['mean_length'] == 100.0
            assert stats['mean_actor_loss'] == 0.1
            assert stats['mean_critic_loss'] == 0.1
        
        def test_training_metrics_recent_statistics(self):
            """æµ‹è¯•æœ€è¿‘è®­ç»ƒæŒ‡æ ‡ç»Ÿè®¡"""
            metrics = TrainingMetrics()
            
            # æ·»åŠ 10ä¸ªepisode
            for i in range(10):
                metrics.add_episode_metrics(reward=i, length=100, actor_loss=0.1, critic_loss=0.1)
            
            # èŽ·å–æœ€è¿‘5ä¸ªepisodeçš„ç»Ÿè®¡
            recent_stats = metrics.get_recent_statistics(window=5)
            
            assert recent_stats['mean_reward'] == 7.0  # (5+6+7+8+9)/5
            assert len(recent_stats) > 0
    
    
    class TestRLTrainer:
        """RLTraineræµ‹è¯•ç±»"""
        
        @pytest.fixture
        def training_config(self):
            """è®­ç»ƒé…ç½®fixture"""
            return TrainingConfig(
                n_episodes=100,
                max_steps_per_episode=50,
                batch_size=32,
                learning_rate=1e-3,
                validation_frequency=10,
                save_frequency=20
            )
        
        @pytest.fixture
        def mock_environment(self):
            """æ¨¡æ‹ŸçŽ¯å¢ƒfixture"""
            return MockEnvironment(n_stocks=4, lookback_window=20)
        
        @pytest.fixture
        def mock_agent(self, mock_environment):
            """æ¨¡æ‹Ÿæ™ºèƒ½ä½“fixture"""
            return MockSACAgent(
                observation_space=mock_environment.observation_space,
                action_space=mock_environment.action_space
            )
        
        @pytest.fixture
        def mock_data_split(self):
            """æ¨¡æ‹Ÿæ•°æ®åˆ’åˆ†fixture"""
            # åˆ›å»ºæ¨¡æ‹Ÿçš„åˆ’åˆ†ç»“æžœ
            total_samples = 1000
            train_size = int(total_samples * 0.7)
            val_size = int(total_samples * 0.2)
            
            train_indices = np.arange(train_size)
            val_indices = np.arange(train_size, train_size + val_size)
            test_indices = np.arange(train_size + val_size, total_samples)
            
            return SplitResult(
                train_indices=train_indices,
                validation_indices=val_indices,
                test_indices=test_indices,
                split_dates={
                    'train_start': '2020-01-01',
                    'train_end': '2022-12-31',
                    'val_start': '2023-01-01',
                    'val_end': '2023-08-31',
                    'test_start': '2023-09-01',
                    'test_end': '2023-12-31'
                }
            )
        
        @pytest.fixture
        def trainer(self, training_config, mock_environment, mock_agent, mock_data_split):
            """è®­ç»ƒå™¨fixture"""
            return RLTrainer(
                config=training_config,
                environment=mock_environment,
                agent=mock_agent,
                data_split=mock_data_split
            )
        
        def test_trainer_initialization(self, trainer, training_config):
            """æµ‹è¯•è®­ç»ƒå™¨åˆå§‹åŒ–"""
            assert trainer.config == training_config
            assert trainer.environment is not None
            assert trainer.agent is not None
            assert trainer.data_split is not None
            assert isinstance(trainer.metrics, TrainingMetrics)
            assert isinstance(trainer.early_stopping, EarlyStopping)
        
        def test_trainer_single_episode(self, trainer):
            """æµ‹è¯•å•ä¸ªepisodeè®­ç»ƒ"""
            episode_reward, episode_length = trainer._run_episode(episode_num=1, training=True)
            
            assert isinstance(episode_reward, (int, float))
            assert isinstance(episode_length, int)
            assert episode_length > 0
            assert episode_length <= trainer.config.max_steps_per_episode
        
        def test_trainer_validation_episode(self, trainer):
            """æµ‹è¯•éªŒè¯episode"""
            validation_score = trainer._validate()
            
            assert isinstance(validation_score, (int, float))
            assert np.isfinite(validation_score)
        
        def test_trainer_save_load_checkpoint(self, trainer, tmp_path):
            """æµ‹è¯•æ¨¡åž‹ä¿å­˜å’ŒåŠ è½½"""
            checkpoint_path = tmp_path / "test_checkpoint.pth"
            
            # ä¿å­˜æ£€æŸ¥ç‚¹
            trainer.save_checkpoint(str(checkpoint_path), episode=10)
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åˆ›å»º
            assert checkpoint_path.exists()
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            loaded_episode = trainer.load_checkpoint(str(checkpoint_path))
            assert loaded_episode == 10
        
        def test_trainer_early_stopping_integration(self, training_config):
            """æµ‹è¯•æ—©åœæœºåˆ¶é›†æˆ"""
            # åˆ›å»ºä¼šå¿«é€Ÿè§¦å‘æ—©åœçš„é…ç½®
            early_config = TrainingConfig(
                n_episodes=100,
                max_steps_per_episode=20,
                early_stopping_patience=3,
                early_stopping_min_delta=0.01,  # æ›´åˆç†çš„æœ€å°æ”¹è¿›å¹…åº¦
                validation_frequency=5  # æ¯5ä¸ªepisodeéªŒè¯ä¸€æ¬¡
            )
            
            mock_env = MockEnvironment(n_stocks=2, lookback_window=10)
            mock_agent = MockSACAgent(mock_env.observation_space, mock_env.action_space)
            
            # åˆ›å»ºç®€å•çš„æ•°æ®åˆ’åˆ†
            train_indices = np.arange(100)
            val_indices = np.arange(100, 120)
            test_indices = np.arange(120, 150)
            
            data_split = SplitResult(
                train_indices=train_indices,
                validation_indices=val_indices,
                test_indices=test_indices
            )
            
            trainer = RLTrainer(
                config=early_config,
                environment=mock_env,
                agent=mock_agent,
                data_split=data_split
            )
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­éªŒè¯åˆ†æ•°ä¸æ”¹è¿›çš„æƒ…å†µ
            # ç¬¬ä¸€æ¬¡éªŒè¯åˆ†æ•°è¾ƒé«˜ï¼Œç„¶åŽæ²¡æœ‰æ˜¾è‘—æ”¹è¿›
            with patch.object(trainer, '_validate', side_effect=[0.5, 0.52, 0.51, 0.50, 0.49, 0.48]):
                # è¿è¡Œè®­ç»ƒï¼Œåº”è¯¥ä¼šå› ä¸ºæ—©åœè€Œæå‰ç»“æŸ
                trainer.train()
                
                # æ£€æŸ¥æ˜¯å¦è§¦å‘äº†æ—©åœ
                assert trainer.early_stopping.early_stop
        
        def test_trainer_metrics_collection(self, trainer):
            """æµ‹è¯•è®­ç»ƒæŒ‡æ ‡æ”¶é›†"""
            # è¿è¡Œå‡ ä¸ªepisode
            for episode in range(5):
                reward, length = trainer._run_episode(episode_num=episode, training=True)
                trainer.metrics.add_episode_metrics(
                    reward=reward,
                    length=length,
                    actor_loss=0.1,
                    critic_loss=0.1
                )
            
            # æ£€æŸ¥æŒ‡æ ‡æ”¶é›†
            assert len(trainer.metrics.episode_rewards) == 5
            assert len(trainer.metrics.episode_lengths) == 5
            
            # èŽ·å–ç»Ÿè®¡ä¿¡æ¯
            stats = trainer.metrics.get_statistics()
            assert 'mean_reward' in stats
            assert 'std_reward' in stats
            assert 'mean_length' in stats
        
        def test_trainer_learning_rate_scheduling(self, trainer):
            """æµ‹è¯•å­¦ä¹ çŽ‡è°ƒåº¦"""
            initial_lr = trainer.config.learning_rate
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹ä¸­çš„å­¦ä¹ çŽ‡è°ƒæ•´
            for episode in range(50):
                # åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™é‡Œä¼šè°ƒæ•´å­¦ä¹ çŽ‡
                current_lr = trainer._get_current_learning_rate(episode)
                assert current_lr > 0
                assert current_lr <= initial_lr
        
        def test_trainer_gradient_clipping(self, trainer):
            """æµ‹è¯•æ¢¯åº¦è£å‰ª"""
            # åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™é‡Œä¼šæµ‹è¯•æ¢¯åº¦è£å‰ªåŠŸèƒ½
            # ç”±äºŽä½¿ç”¨æ¨¡æ‹Ÿæ™ºèƒ½ä½“ï¼Œè¿™é‡Œåªæ˜¯ç¡®ä¿ç›¸å…³é…ç½®å­˜åœ¨
            assert hasattr(trainer.config, 'gradient_clip_norm')
            if trainer.config.gradient_clip_norm is not None:
                assert trainer.config.gradient_clip_norm > 0
        
        def test_trainer_replay_buffer_integration(self, trainer):
            """æµ‹è¯•ç»éªŒå›žæ”¾ç¼“å†²åŒºé›†æˆ"""
            # è¿è¡Œä¸€ä¸ªepisodeæ¥å¡«å……ç¼“å†²åŒº
            trainer._run_episode(episode_num=1, training=True)
            
            # æ£€æŸ¥æ™ºèƒ½ä½“æ˜¯å¦è¢«æ›´æ–°
            assert trainer.agent.total_updates >= 0
        
        def test_trainer_multi_episode_training(self, trainer):
            """æµ‹è¯•å¤šepisodeè®­ç»ƒ"""
            # è®¾ç½®è¾ƒå°çš„episodeæ•°é‡è¿›è¡Œæµ‹è¯•
            trainer.config.n_episodes = 10
            trainer.config.validation_frequency = 5
            
            # è¿è¡Œè®­ç»ƒ
            trainer.train()
            
            # æ£€æŸ¥è®­ç»ƒæ˜¯å¦å®Œæˆ
            assert len(trainer.metrics.episode_rewards) > 0
            assert len(trainer.metrics.episode_rewards) <= trainer.config.n_episodes
        
        def test_trainer_validation_frequency(self, trainer):
            """æµ‹è¯•éªŒè¯é¢‘çŽ‡"""
            trainer.config.n_episodes = 20
            trainer.config.validation_frequency = 5
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            validation_count = 0
            for episode in range(trainer.config.n_episodes):
                if (episode + 1) % trainer.config.validation_frequency == 0:
                    validation_count += 1
            
            # åº”è¯¥è¿›è¡Œ4æ¬¡éªŒè¯ (episode 5, 10, 15, 20)
            assert validation_count == 4
        
        def test_trainer_save_frequency(self, trainer, tmp_path):
            """æµ‹è¯•ä¿å­˜é¢‘çŽ‡"""
            trainer.config.n_episodes = 15
            trainer.config.save_frequency = 5
            trainer.config.save_dir = str(tmp_path)
            
            # æ¨¡æ‹Ÿä¿å­˜é€»è¾‘
            save_count = 0
            for episode in range(trainer.config.n_episodes):
                if (episode + 1) % trainer.config.save_frequency == 0:
                    save_count += 1
            
            # åº”è¯¥ä¿å­˜3æ¬¡ (episode 5, 10, 15)
            assert save_count == 3
        
        @pytest.mark.parametrize("n_episodes,max_steps,batch_size", [
            (10, 20, 16),
            (50, 100, 32),
            (100, 200, 64)
        ])
        def test_trainer_different_configurations(self, n_episodes, max_steps, batch_size):
            """æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„è®­ç»ƒå™¨"""
            config = TrainingConfig(
                n_episodes=n_episodes,
                max_steps_per_episode=max_steps,
                batch_size=batch_size
            )
            
            mock_env = MockEnvironment()
            mock_agent = MockSACAgent(mock_env.observation_space, mock_env.action_space)
            
            # ç®€å•çš„æ•°æ®åˆ’åˆ†
            data_split = SplitResult(
                train_indices=np.arange(100),
                validation_indices=np.arange(100, 120),
                test_indices=np.arange(120, 150)
            )
            
            trainer = RLTrainer(
                config=config,
                environment=mock_env,
                agent=mock_agent,
                data_split=data_split
            )
            
            # è¿è¡Œå°‘é‡episodeéªŒè¯åŸºæœ¬åŠŸèƒ½
            trainer.config.n_episodes = 3  # å‡å°‘æµ‹è¯•æ—¶é—´
            trainer.train()
            
            assert len(trainer.metrics.episode_rewards) <= 3
        
        def test_trainer_error_handling(self, trainer):
            """æµ‹è¯•è®­ç»ƒå™¨é”™è¯¯å¤„ç†"""
            # æµ‹è¯•çŽ¯å¢ƒé”™è¯¯
            with patch.object(trainer.environment, 'step', side_effect=Exception("Environment error")):
                with pytest.raises(Exception):
                    trainer._run_episode(episode_num=1, training=True)
            
            # æµ‹è¯•æ™ºèƒ½ä½“é”™è¯¯
            with patch.object(trainer.agent, 'act', side_effect=Exception("Agent error")):
                with pytest.raises(Exception):
                    trainer._run_episode(episode_num=1, training=True)
        
        def test_trainer_memory_efficiency(self, trainer):
            """æµ‹è¯•è®­ç»ƒå™¨å†…å­˜æ•ˆçŽ‡"""
            # è¿è¡Œè®­ç»ƒå¹¶æ£€æŸ¥å†…å­˜ä½¿ç”¨ä¸ä¼šæ— é™å¢žé•¿
            initial_metrics_length = len(trainer.metrics.episode_rewards)
            
            # æ¨¡æ‹Ÿé•¿æœŸè®­ç»ƒ
            for _ in range(10):
                reward, length = trainer._run_episode(episode_num=1, training=True)
                trainer.metrics.add_episode_metrics(
                    reward=reward,
                    length=length,
                    actor_loss=0.1,
                    critic_loss=0.1
                )
            
            # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦æ­£ç¡®ç´¯ç§¯
            assert len(trainer.metrics.episode_rewards) == initial_metrics_length + 10
        
        def test_trainer_deterministic_behavior(self):
            """æµ‹è¯•è®­ç»ƒå™¨çš„ç¡®å®šæ€§è¡Œä¸º"""
            # ä½¿ç”¨å›ºå®šéšæœºç§å­
            config = TrainingConfig(n_episodes=5, random_seed=42)
            
            # åˆ›å»ºä¸¤ä¸ªç›¸åŒçš„è®­ç»ƒå™¨
            mock_env1 = MockEnvironment()
            mock_agent1 = MockSACAgent(mock_env1.observation_space, mock_env1.action_space)
            data_split1 = SplitResult(
                train_indices=np.arange(50),
                validation_indices=np.arange(50, 60),
                test_indices=np.arange(60, 70)
            )
            
            mock_env2 = MockEnvironment()
            mock_agent2 = MockSACAgent(mock_env2.observation_space, mock_env2.action_space)
            data_split2 = SplitResult(
                train_indices=np.arange(50),
                validation_indices=np.arange(50, 60),
                test_indices=np.arange(60, 70)
            )
            
            trainer1 = RLTrainer(config, mock_env1, mock_agent1, data_split1)
            trainer2 = RLTrainer(config, mock_env2, mock_agent2, data_split2)
            
            # ç”±äºŽçŽ¯å¢ƒæ˜¯éšæœºçš„ï¼Œè¿™é‡Œä¸»è¦æµ‹è¯•è®­ç»ƒå™¨çš„ç»“æž„ä¸€è‡´æ€§
            assert trainer1.config.random_seed == trainer2.config.random_seed
            assert trainer1.config.n_episodes == trainer2.config.n_episodes
    ]]></file>
  <file path="tests/unit/test_trading_system_monitor.py"><![CDATA[
    """
    äº¤æ˜“ç³»ç»Ÿç›‘æŽ§æ¨¡å—çš„å•å…ƒæµ‹è¯•
    æµ‹è¯•ç›‘æŽ§æŒ‡æ ‡çš„å®šä¹‰å’Œæ”¶é›†åŠŸèƒ½ï¼ŒæŒ‡æ ‡å¯¼å‡ºå’ŒGrafanaä»ªè¡¨æ¿é›†æˆï¼Œç›‘æŽ§ç³»ç»Ÿçš„å®žæ—¶æ€§å’Œå‡†ç¡®æ€§
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import pytest
    import time
    import threading
    from unittest.mock import Mock, patch, MagicMock
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional, Any
    from decimal import Decimal
    
    from src.rl_trading_system.monitoring.trading_system_monitor import (
        TradingSystemMonitor,
        MetricsCollector,
        PrometheusExporter,
        GrafanaDashboardManager
    )
    from src.rl_trading_system.backtest.multi_frequency_backtest import Trade, OrderType
    
    
    class TestMetricsCollector:
        """æŒ‡æ ‡æ”¶é›†å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def metrics_collector(self):
            """åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨"""
            return MetricsCollector()
    
        def test_metrics_collector_initialization(self, metrics_collector):
            """æµ‹è¯•æŒ‡æ ‡æ”¶é›†å™¨åˆå§‹åŒ–"""
            assert metrics_collector.metrics_registry is not None
            assert isinstance(metrics_collector.performance_metrics, dict)
            assert isinstance(metrics_collector.risk_metrics, dict)
            assert isinstance(metrics_collector.system_metrics, dict)
            assert isinstance(metrics_collector.trading_metrics, dict)
    
        def test_performance_metrics_collection(self, metrics_collector):
            """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡æ”¶é›†"""
            # æ”¶é›†æ€§èƒ½æŒ‡æ ‡
            portfolio_value = 1050000.0
            daily_return = 0.02
            total_return = 0.05
            sharpe_ratio = 1.5
    
            metrics_collector.collect_performance_metrics(
                portfolio_value=portfolio_value,
                daily_return=daily_return,
                total_return=total_return,
                sharpe_ratio=sharpe_ratio
            )
    
            # éªŒè¯æŒ‡æ ‡è¢«æ­£ç¡®æ”¶é›†
            assert metrics_collector.performance_metrics['portfolio_value'] == portfolio_value
            assert metrics_collector.performance_metrics['daily_return'] == daily_return
            assert metrics_collector.performance_metrics['total_return'] == total_return
            assert metrics_collector.performance_metrics['sharpe_ratio'] == sharpe_ratio
            assert 'timestamp' in metrics_collector.performance_metrics
    
        def test_risk_metrics_collection(self, metrics_collector):
            """æµ‹è¯•é£Žé™©æŒ‡æ ‡æ”¶é›†"""
            # æ”¶é›†é£Žé™©æŒ‡æ ‡
            volatility = 0.15
            max_drawdown = 0.08
            var_95 = 0.03
            beta = 1.2
    
            metrics_collector.collect_risk_metrics(
                volatility=volatility,
                max_drawdown=max_drawdown,
                var_95=var_95,
                beta=beta
            )
    
            # éªŒè¯æŒ‡æ ‡è¢«æ­£ç¡®æ”¶é›†
            assert metrics_collector.risk_metrics['volatility'] == volatility
            assert metrics_collector.risk_metrics['max_drawdown'] == max_drawdown
            assert metrics_collector.risk_metrics['var_95'] == var_95
            assert metrics_collector.risk_metrics['beta'] == beta
            assert 'timestamp' in metrics_collector.risk_metrics
    
        def test_system_metrics_collection(self, metrics_collector):
            """æµ‹è¯•ç³»ç»ŸæŒ‡æ ‡æ”¶é›†"""
            # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
            cpu_usage = 45.5
            memory_usage = 78.2
            disk_usage = 60.0
            model_inference_time = 0.125
    
            metrics_collector.collect_system_metrics(
                cpu_usage=cpu_usage,
                memory_usage=memory_usage,
                disk_usage=disk_usage,
                model_inference_time=model_inference_time
            )
    
            # éªŒè¯æŒ‡æ ‡è¢«æ­£ç¡®æ”¶é›†
            assert metrics_collector.system_metrics['cpu_usage'] == cpu_usage
            assert metrics_collector.system_metrics['memory_usage'] == memory_usage
            assert metrics_collector.system_metrics['disk_usage'] == disk_usage
            assert metrics_collector.system_metrics['model_inference_time'] == model_inference_time
            assert 'timestamp' in metrics_collector.system_metrics
    
        def test_trading_metrics_collection(self, metrics_collector):
            """æµ‹è¯•äº¤æ˜“æŒ‡æ ‡æ”¶é›†"""
            # æ”¶é›†äº¤æ˜“æŒ‡æ ‡
            total_trades = 150
            successful_trades = 135
            win_rate = 0.72
            average_trade_size = 10000.0
            turnover_rate = 2.5
    
            metrics_collector.collect_trading_metrics(
                total_trades=total_trades,
                successful_trades=successful_trades,
                win_rate=win_rate,
                average_trade_size=average_trade_size,
                turnover_rate=turnover_rate
            )
    
            # éªŒè¯æŒ‡æ ‡è¢«æ­£ç¡®æ”¶é›†
            assert metrics_collector.trading_metrics['total_trades'] == total_trades
            assert metrics_collector.trading_metrics['successful_trades'] == successful_trades
            assert metrics_collector.trading_metrics['win_rate'] == win_rate
            assert metrics_collector.trading_metrics['average_trade_size'] == average_trade_size
            assert metrics_collector.trading_metrics['turnover_rate'] == turnover_rate
            assert 'timestamp' in metrics_collector.trading_metrics
    
        def test_metrics_registry_management(self, metrics_collector):
            """æµ‹è¯•æŒ‡æ ‡æ³¨å†Œè¡¨ç®¡ç†"""
            # æ³¨å†Œè‡ªå®šä¹‰æŒ‡æ ‡
            metric_name = "custom_metric"
            metric_description = "A custom monitoring metric"
            metric_type = "gauge"
    
            metrics_collector.register_metric(
                name=metric_name,
                description=metric_description,
                metric_type=metric_type
            )
    
            # éªŒè¯æŒ‡æ ‡è¢«æ³¨å†Œ
            assert metric_name in metrics_collector.metrics_registry
            assert metrics_collector.metrics_registry[metric_name]['description'] == metric_description
            assert metrics_collector.metrics_registry[metric_name]['type'] == metric_type
    
            # æ›´æ–°æŒ‡æ ‡å€¼
            metric_value = 42.0
            metrics_collector.update_metric(metric_name, metric_value)
    
            # éªŒè¯æŒ‡æ ‡å€¼è¢«æ›´æ–°
            assert metrics_collector.metrics_registry[metric_name]['value'] == metric_value
    
        def test_metrics_reset(self, metrics_collector):
            """æµ‹è¯•æŒ‡æ ‡é‡ç½®"""
            # å…ˆæ”¶é›†ä¸€äº›æŒ‡æ ‡
            metrics_collector.collect_performance_metrics(
                portfolio_value=1000000.0,
                daily_return=0.01,
                total_return=0.1,
                sharpe_ratio=1.0
            )
    
            # éªŒè¯æŒ‡æ ‡å­˜åœ¨
            assert len(metrics_collector.performance_metrics) > 1
    
            # é‡ç½®æŒ‡æ ‡
            metrics_collector.reset_metrics()
    
            # éªŒè¯æŒ‡æ ‡è¢«é‡ç½®
            assert len(metrics_collector.performance_metrics) == 0
            assert len(metrics_collector.risk_metrics) == 0
            assert len(metrics_collector.system_metrics) == 0
            assert len(metrics_collector.trading_metrics) == 0
    
        def test_metrics_export_format(self, metrics_collector):
            """æµ‹è¯•æŒ‡æ ‡å¯¼å‡ºæ ¼å¼"""
            # æ”¶é›†å„ç±»æŒ‡æ ‡
            metrics_collector.collect_performance_metrics(
                portfolio_value=1000000.0,
                daily_return=0.01,
                total_return=0.1,
                sharpe_ratio=1.0
            )
            
            metrics_collector.collect_risk_metrics(
                volatility=0.15,
                max_drawdown=0.05,
                var_95=0.02,
                beta=1.1
            )
    
            # å¯¼å‡ºæŒ‡æ ‡
            exported_metrics = metrics_collector.export_metrics()
    
            # éªŒè¯å¯¼å‡ºæ ¼å¼
            assert isinstance(exported_metrics, dict)
            assert 'performance_metrics' in exported_metrics
            assert 'risk_metrics' in exported_metrics
            assert 'system_metrics' in exported_metrics
            assert 'trading_metrics' in exported_metrics
    
            # éªŒè¯æ¯ä¸ªç±»åˆ«åŒ…å«é¢„æœŸçš„æŒ‡æ ‡
            performance = exported_metrics['performance_metrics']
            assert 'portfolio_value' in performance
            assert 'daily_return' in performance
            assert 'timestamp' in performance
    
            risk = exported_metrics['risk_metrics']
            assert 'volatility' in risk
            assert 'max_drawdown' in risk
    
        def test_invalid_metric_name_error(self, metrics_collector):
            """æµ‹è¯•æ— æ•ˆæŒ‡æ ‡åç§°é”™è¯¯"""
            # æµ‹è¯•ç©ºæŒ‡æ ‡åç§°
            with pytest.raises(ValueError, match="æŒ‡æ ‡åç§°ä¸èƒ½ä¸ºç©º"):
                metrics_collector.register_metric("", "description", "gauge")
    
            # æµ‹è¯•æ›´æ–°ä¸å­˜åœ¨çš„æŒ‡æ ‡
            with pytest.raises(ValueError, match="æŒ‡æ ‡.*ä¸å­˜åœ¨"):
                metrics_collector.update_metric("nonexistent_metric", 42.0)
    
        def test_invalid_metric_type_error(self, metrics_collector):
            """æµ‹è¯•æ— æ•ˆæŒ‡æ ‡ç±»åž‹é”™è¯¯"""
            # æµ‹è¯•ä¸æ”¯æŒçš„æŒ‡æ ‡ç±»åž‹
            with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„æŒ‡æ ‡ç±»åž‹"):
                metrics_collector.register_metric("test_metric", "description", "invalid_type")
    
    
    class TestPrometheusExporter:
        """Prometheuså¯¼å‡ºå™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def metrics_collector(self):
            """åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨"""
            collector = MetricsCollector()
            # é¢„å¡«å……ä¸€äº›æµ‹è¯•æ•°æ®
            collector.collect_performance_metrics(
                portfolio_value=1000000.0,
                daily_return=0.01,
                total_return=0.1,
                sharpe_ratio=1.5
            )
            collector.collect_risk_metrics(
                volatility=0.15,
                max_drawdown=0.05,
                var_95=0.02,
                beta=1.1
            )
            return collector
    
        @pytest.fixture
        def prometheus_exporter(self, metrics_collector):
            """åˆ›å»ºPrometheuså¯¼å‡ºå™¨"""
            return PrometheusExporter(metrics_collector, port=8001)
    
        def test_prometheus_exporter_initialization(self, prometheus_exporter):
            """æµ‹è¯•Prometheuså¯¼å‡ºå™¨åˆå§‹åŒ–"""
            assert prometheus_exporter.metrics_collector is not None
            assert prometheus_exporter.port == 8001
            assert prometheus_exporter.registry is not None
            assert prometheus_exporter.is_running is False
    
        def test_prometheus_metrics_registration(self, prometheus_exporter):
            """æµ‹è¯•PrometheusæŒ‡æ ‡æ³¨å†Œ"""
            # æ³¨å†ŒæŒ‡æ ‡åˆ°Prometheus
            prometheus_exporter.register_prometheus_metrics()
    
            # éªŒè¯æŒ‡æ ‡è¢«æ³¨å†Œåˆ°Prometheusæ³¨å†Œè¡¨
            registered_metrics = prometheus_exporter.get_registered_metrics()
            
            # æ£€æŸ¥åŸºæœ¬æŒ‡æ ‡ç±»åž‹
            assert 'portfolio_value' in registered_metrics
            assert 'daily_return' in registered_metrics
            assert 'volatility' in registered_metrics
            assert 'max_drawdown' in registered_metrics
    
        def test_metrics_export_format(self, prometheus_exporter):
            """æµ‹è¯•æŒ‡æ ‡å¯¼å‡ºæ ¼å¼"""
            # æ³¨å†Œå¹¶å¯¼å‡ºæŒ‡æ ‡
            prometheus_exporter.register_prometheus_metrics()
            exported_data = prometheus_exporter.generate_metrics_output()
    
            # éªŒè¯Prometheusæ ¼å¼
            assert isinstance(exported_data, str)
            assert "portfolio_value" in exported_data
            assert "daily_return" in exported_data
            assert "volatility" in exported_data
            
            # éªŒè¯Prometheusæ ¼å¼è§„èŒƒ
            lines = exported_data.strip().split('\n')
            for line in lines:
                if line.startswith('#'):
                    # æ³¨é‡Šè¡Œåº”è¯¥åŒ…å«HELPæˆ–TYPE
                    assert 'HELP' in line or 'TYPE' in line
                elif line:
                    # æŒ‡æ ‡è¡Œåº”è¯¥åŒ…å«æŒ‡æ ‡åå’Œå€¼
                    assert ' ' in line
                    parts = line.split(' ')
                    assert len(parts) >= 2
    
        def test_exporter_start_stop(self, prometheus_exporter):
            """æµ‹è¯•å¯¼å‡ºå™¨å¯åŠ¨åœæ­¢"""
            # å¯åŠ¨å¯¼å‡ºå™¨
            prometheus_exporter.start()
            assert prometheus_exporter.is_running is True
            
            # ç­‰å¾…ä¸€å°æ®µæ—¶é—´ç¡®ä¿æœåŠ¡å™¨å¯åŠ¨
            time.sleep(0.1)
            
            # åœæ­¢å¯¼å‡ºå™¨
            prometheus_exporter.stop()
            assert prometheus_exporter.is_running is False
    
        def test_concurrent_metrics_update(self, prometheus_exporter):
            """æµ‹è¯•å¹¶å‘æŒ‡æ ‡æ›´æ–°"""
            prometheus_exporter.register_prometheus_metrics()
            
            def update_metrics():
                for i in range(10):
                    prometheus_exporter.metrics_collector.collect_performance_metrics(
                        portfolio_value=1000000.0 + i * 1000,
                        daily_return=0.01 + i * 0.001,
                        total_return=0.1 + i * 0.01,
                        sharpe_ratio=1.5 + i * 0.1
                    )
                    time.sleep(0.01)
    
            # å¯åŠ¨å¤šä¸ªçº¿ç¨‹åŒæ—¶æ›´æ–°æŒ‡æ ‡
            threads = []
            for _ in range(3):
                thread = threading.Thread(target=update_metrics)
                threads.append(thread)
                thread.start()
    
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()
    
            # éªŒè¯æŒ‡æ ‡ä»ç„¶å¯ä»¥æ­£å¸¸å¯¼å‡º
            exported_data = prometheus_exporter.generate_metrics_output()
            assert "portfolio_value" in exported_data
    
        def test_custom_metrics_export(self, prometheus_exporter):
            """æµ‹è¯•è‡ªå®šä¹‰æŒ‡æ ‡å¯¼å‡º"""
            # æ³¨å†Œè‡ªå®šä¹‰æŒ‡æ ‡
            prometheus_exporter.metrics_collector.register_metric(
                "custom_strategy_score",
                "Custom strategy performance score",
                "gauge"
            )
            
            # æ›´æ–°è‡ªå®šä¹‰æŒ‡æ ‡
            prometheus_exporter.metrics_collector.update_metric("custom_strategy_score", 85.5)
            
            # é‡æ–°æ³¨å†ŒPrometheusæŒ‡æ ‡ä»¥åŒ…å«æ–°çš„è‡ªå®šä¹‰æŒ‡æ ‡
            prometheus_exporter.register_prometheus_metrics()
            
            # éªŒè¯è‡ªå®šä¹‰æŒ‡æ ‡è¢«å¯¼å‡º
            exported_data = prometheus_exporter.generate_metrics_output()
            assert "custom_strategy_score" in exported_data
            assert "85.5" in exported_data
    
        def test_invalid_port_error(self, metrics_collector):
            """æµ‹è¯•æ— æ•ˆç«¯å£é”™è¯¯"""
            # æµ‹è¯•æ— æ•ˆç«¯å£èŒƒå›´
            with pytest.raises(ValueError, match="ç«¯å£å·å¿…é¡»åœ¨1024-65535èŒƒå›´å†…"):
                PrometheusExporter(metrics_collector, port=999)
    
            with pytest.raises(ValueError, match="ç«¯å£å·å¿…é¡»åœ¨1024-65535èŒƒå›´å†…"):
                PrometheusExporter(metrics_collector, port=70000)
    
        def test_exporter_health_check(self, prometheus_exporter):
            """æµ‹è¯•å¯¼å‡ºå™¨å¥åº·æ£€æŸ¥"""
            # å¥åº·æ£€æŸ¥åº”è¯¥åœ¨å¯åŠ¨å‰å¤±è´¥
            assert prometheus_exporter.health_check() is False
            
            # å¯åŠ¨å¯¼å‡ºå™¨
            prometheus_exporter.start()
            
            # ç­‰å¾…å¯åŠ¨
            time.sleep(0.1)
            
            # å¥åº·æ£€æŸ¥åº”è¯¥æˆåŠŸ
            assert prometheus_exporter.health_check() is True
            
            # åœæ­¢å¯¼å‡ºå™¨
            prometheus_exporter.stop()
            
            # å¥åº·æ£€æŸ¥åº”è¯¥å†æ¬¡å¤±è´¥
            assert prometheus_exporter.health_check() is False
    
    
    class TestGrafanaDashboardManager:
        """Grafanaä»ªè¡¨æ¿ç®¡ç†å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def dashboard_manager(self):
            """åˆ›å»ºGrafanaä»ªè¡¨æ¿ç®¡ç†å™¨"""
            return GrafanaDashboardManager(
                grafana_url="http://localhost:3000",
                api_key="test_api_key"
            )
    
        def test_dashboard_manager_initialization(self, dashboard_manager):
            """æµ‹è¯•ä»ªè¡¨æ¿ç®¡ç†å™¨åˆå§‹åŒ–"""
            assert dashboard_manager.grafana_url == "http://localhost:3000"
            assert dashboard_manager.api_key == "test_api_key"
            assert dashboard_manager.session is not None
    
        def test_dashboard_config_generation(self, dashboard_manager):
            """æµ‹è¯•ä»ªè¡¨æ¿é…ç½®ç”Ÿæˆ"""
            # ç”Ÿæˆä»ªè¡¨æ¿é…ç½®
            dashboard_config = dashboard_manager.generate_dashboard_config()
    
            # éªŒè¯é…ç½®ç»“æž„
            assert isinstance(dashboard_config, dict)
            assert 'dashboard' in dashboard_config
            assert 'title' in dashboard_config['dashboard']
            assert 'panels' in dashboard_config['dashboard']
    
            # éªŒè¯é¢æ¿é…ç½®
            panels = dashboard_config['dashboard']['panels']
            assert len(panels) > 0
    
            # æ£€æŸ¥åŸºæœ¬é¢æ¿
            panel_titles = [panel['title'] for panel in panels]
            assert 'æŠ•èµ„ç»„åˆä»·å€¼' in panel_titles
            assert 'æ—¥æ”¶ç›ŠçŽ‡' in panel_titles
            assert 'é£Žé™©æŒ‡æ ‡' in panel_titles
            assert 'ç³»ç»Ÿæ€§èƒ½' in panel_titles
    
        def test_dashboard_panel_creation(self, dashboard_manager):
            """æµ‹è¯•ä»ªè¡¨æ¿é¢æ¿åˆ›å»º"""
            # åˆ›å»ºå•ä¸ªé¢æ¿
            panel_config = dashboard_manager.create_panel(
                title="æµ‹è¯•é¢æ¿",
                metric_query="portfolio_value",
                panel_type="graph",
                x_pos=0,
                y_pos=0,
                width=12,
                height=8
            )
    
            # éªŒè¯é¢æ¿é…ç½®
            assert panel_config['title'] == "æµ‹è¯•é¢æ¿"
            assert panel_config['type'] == "graph"
            assert panel_config['gridPos']['x'] == 0
            assert panel_config['gridPos']['y'] == 0
            assert panel_config['gridPos']['w'] == 12
            assert panel_config['gridPos']['h'] == 8
    
            # éªŒè¯æŸ¥è¯¢é…ç½®
            assert 'targets' in panel_config
            assert len(panel_config['targets']) > 0
            assert panel_config['targets'][0]['expr'] == "portfolio_value"
    
        def test_dashboard_deployment(self, dashboard_manager):
            """æµ‹è¯•ä»ªè¡¨æ¿éƒ¨ç½²"""
            with patch.object(dashboard_manager.session, 'post') as mock_post:
                # æ¨¡æ‹ŸæˆåŠŸçš„APIå“åº”
                mock_response = Mock()
                mock_response.status_code = 200
                mock_response.json.return_value = {
                    'status': 'success',
                    'id': 1,
                    'uid': 'test-dashboard',
                    'url': '/d/test-dashboard/trading-system-monitor'
                }
                mock_post.return_value = mock_response
    
                # éƒ¨ç½²ä»ªè¡¨æ¿
                result = dashboard_manager.deploy_dashboard()
    
                # éªŒè¯éƒ¨ç½²ç»“æžœ
                assert result['status'] == 'success'
                assert result['uid'] == 'test-dashboard'
    
                # éªŒè¯APIè°ƒç”¨
                mock_post.assert_called_once()
                call_args = mock_post.call_args
                assert call_args[0][0].endswith('/api/dashboards/db')
    
        def test_dashboard_update(self, dashboard_manager):
            """æµ‹è¯•ä»ªè¡¨æ¿æ›´æ–°"""
            with patch.object(dashboard_manager.session, 'post') as mock_post:
                # æ¨¡æ‹Ÿæ›´æ–°å“åº”
                mock_response = Mock()
                mock_response.status_code = 200
                mock_response.json.return_value = {
                    'status': 'success',
                    'version': 2
                }
                mock_post.return_value = mock_response
    
                # æ›´æ–°ä»ªè¡¨æ¿
                result = dashboard_manager.update_dashboard(dashboard_uid="test-dashboard")
    
                # éªŒè¯æ›´æ–°ç»“æžœ
                assert result['status'] == 'success'
                assert result['version'] == 2
    
        def test_dashboard_deletion(self, dashboard_manager):
            """æµ‹è¯•ä»ªè¡¨æ¿åˆ é™¤"""
            with patch.object(dashboard_manager.session, 'delete') as mock_delete:
                # æ¨¡æ‹Ÿåˆ é™¤å“åº”
                mock_response = Mock()
                mock_response.status_code = 200
                mock_response.json.return_value = {'status': 'success'}
                mock_delete.return_value = mock_response
    
                # åˆ é™¤ä»ªè¡¨æ¿
                result = dashboard_manager.delete_dashboard(dashboard_uid="test-dashboard")
    
                # éªŒè¯åˆ é™¤ç»“æžœ
                assert result['status'] == 'success'
    
                # éªŒè¯APIè°ƒç”¨
                mock_delete.assert_called_once()
                call_args = mock_delete.call_args
                assert 'test-dashboard' in call_args[0][0]
    
        def test_alert_rules_creation(self, dashboard_manager):
            """æµ‹è¯•å‘Šè­¦è§„åˆ™åˆ›å»º"""
            # åˆ›å»ºå‘Šè­¦è§„åˆ™
            alert_rule = dashboard_manager.create_alert_rule(
                rule_name="é«˜é£Žé™©å‘Šè­¦",
                metric_query="max_drawdown",
                threshold=0.1,
                condition="gt",  # greater than
                evaluation_interval="1m"
            )
    
            # éªŒè¯å‘Šè­¦è§„åˆ™é…ç½®
            assert alert_rule['name'] == "é«˜é£Žé™©å‘Šè­¦"
            assert alert_rule['condition']['query'] == "max_drawdown"
            assert alert_rule['condition']['threshold'] == 0.1
            assert alert_rule['condition']['type'] == "gt"
            assert alert_rule['frequency'] == "1m"
    
        def test_datasource_configuration(self, dashboard_manager):
            """æµ‹è¯•æ•°æ®æºé…ç½®"""
            # é…ç½®Prometheusæ•°æ®æº
            datasource_config = dashboard_manager.configure_prometheus_datasource(
                prometheus_url="http://localhost:9090",
                datasource_name="TradingSystem"
            )
    
            # éªŒè¯æ•°æ®æºé…ç½®
            assert datasource_config['name'] == "TradingSystem"
            assert datasource_config['type'] == "prometheus"
            assert datasource_config['url'] == "http://localhost:9090"
            assert datasource_config['access'] == "proxy"
    
        def test_invalid_grafana_url_error(self):
            """æµ‹è¯•æ— æ•ˆGrafana URLé”™è¯¯"""
            with pytest.raises(ValueError, match="Grafana URLä¸èƒ½ä¸ºç©º"):
                GrafanaDashboardManager("", "api_key")
    
            with pytest.raises(ValueError, match="æ— æ•ˆçš„Grafana URLæ ¼å¼"):
                GrafanaDashboardManager("invalid_url", "api_key")
    
        def test_api_authentication_error(self, dashboard_manager):
            """æµ‹è¯•APIè®¤è¯é”™è¯¯"""
            with patch.object(dashboard_manager.session, 'post') as mock_post:
                # æ¨¡æ‹Ÿè®¤è¯å¤±è´¥å“åº”
                mock_response = Mock()
                mock_response.status_code = 401
                mock_response.json.return_value = {'error': 'Unauthorized'}
                mock_post.return_value = mock_response
    
                # éƒ¨ç½²ä»ªè¡¨æ¿åº”è¯¥å¼•å‘è®¤è¯é”™è¯¯
                with pytest.raises(Exception, match="Grafana APIè®¤è¯å¤±è´¥"):
                    dashboard_manager.deploy_dashboard()
    
    
    class TestTradingSystemMonitor:
        """äº¤æ˜“ç³»ç»Ÿç›‘æŽ§å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_trades(self):
            """åˆ›å»ºæ ·æœ¬äº¤æ˜“æ•°æ®"""
            return [
                Trade("000001.SZ", OrderType.BUY, 1000, Decimal("10.00"), datetime(2023, 1, 2), Decimal("10.00")),
                Trade("000001.SZ", OrderType.SELL, 500, Decimal("11.00"), datetime(2023, 1, 15), Decimal("5.50")),
                Trade("000002.SZ", OrderType.BUY, 2000, Decimal("5.00"), datetime(2023, 1, 20), Decimal("10.00")),
            ]
    
        @pytest.fixture
        def trading_monitor(self):
            """åˆ›å»ºäº¤æ˜“ç³»ç»Ÿç›‘æŽ§å™¨"""
            return TradingSystemMonitor(
                prometheus_port=8002,
                grafana_url="http://localhost:3000",
                grafana_api_key="test_key"
            )
    
        def test_trading_monitor_initialization(self, trading_monitor):
            """æµ‹è¯•äº¤æ˜“ç›‘æŽ§å™¨åˆå§‹åŒ–"""
            assert trading_monitor.metrics_collector is not None
            assert trading_monitor.prometheus_exporter is not None
            assert trading_monitor.dashboard_manager is not None
            assert trading_monitor.is_monitoring is False
    
        def test_monitoring_start_stop(self, trading_monitor):
            """æµ‹è¯•ç›‘æŽ§å¯åŠ¨åœæ­¢"""
            # å¯åŠ¨ç›‘æŽ§
            trading_monitor.start_monitoring()
            assert trading_monitor.is_monitoring is True
            assert trading_monitor.prometheus_exporter.is_running is True
    
            # åœæ­¢ç›‘æŽ§
            trading_monitor.stop_monitoring()
            assert trading_monitor.is_monitoring is False
            assert trading_monitor.prometheus_exporter.is_running is False
    
        def test_portfolio_monitoring(self, trading_monitor):
            """æµ‹è¯•æŠ•èµ„ç»„åˆç›‘æŽ§"""
            # æ¨¡æ‹ŸæŠ•èµ„ç»„åˆæ•°æ®
            portfolio_data = {
                'value': 1050000.0,
                'daily_return': 0.02,
                'total_return': 0.05,
                'sharpe_ratio': 1.5,
                'max_drawdown': 0.03
            }
    
            # æ›´æ–°æŠ•èµ„ç»„åˆæŒ‡æ ‡
            trading_monitor.update_portfolio_metrics(portfolio_data)
    
            # éªŒè¯æŒ‡æ ‡è¢«æ”¶é›†
            performance_metrics = trading_monitor.metrics_collector.performance_metrics
            assert performance_metrics['portfolio_value'] == 1050000.0
            assert performance_metrics['daily_return'] == 0.02
    
            risk_metrics = trading_monitor.metrics_collector.risk_metrics
            assert risk_metrics['max_drawdown'] == 0.03
    
        def test_trading_activity_monitoring(self, trading_monitor, sample_trades):
            """æµ‹è¯•äº¤æ˜“æ´»åŠ¨ç›‘æŽ§"""
            # æ›´æ–°äº¤æ˜“æŒ‡æ ‡
            trading_monitor.update_trading_metrics(sample_trades)
    
            # éªŒè¯äº¤æ˜“æŒ‡æ ‡è¢«æ”¶é›†
            trading_metrics = trading_monitor.metrics_collector.trading_metrics
            assert trading_metrics['total_trades'] == len(sample_trades)
            
            # éªŒè¯äº¤æ˜“ç»Ÿè®¡
            buy_trades = [t for t in sample_trades if t.trade_type == OrderType.BUY]
            sell_trades = [t for t in sample_trades if t.trade_type == OrderType.SELL]
            assert trading_metrics['buy_trades'] == len(buy_trades)
            assert trading_metrics['sell_trades'] == len(sell_trades)
    
        def test_system_resource_monitoring(self, trading_monitor):
            """æµ‹è¯•ç³»ç»Ÿèµ„æºç›‘æŽ§"""
            with patch('psutil.cpu_percent') as mock_cpu, \
                 patch('psutil.virtual_memory') as mock_memory:
                
                # æ¨¡æ‹Ÿç³»ç»Ÿèµ„æºæ•°æ®
                mock_cpu.return_value = 45.5
                mock_memory.return_value = Mock(percent=78.2)
    
                # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
                trading_monitor.update_system_metrics()
    
                # éªŒè¯ç³»ç»ŸæŒ‡æ ‡è¢«æ”¶é›†
                system_metrics = trading_monitor.metrics_collector.system_metrics
                assert system_metrics['cpu_usage'] == 45.5
                assert system_metrics['memory_usage'] == 78.2
    
        def test_real_time_monitoring(self, trading_monitor):
            """æµ‹è¯•å®žæ—¶ç›‘æŽ§"""
            # å¯åŠ¨å®žæ—¶ç›‘æŽ§
            trading_monitor.start_monitoring()
    
            # æ¨¡æ‹Ÿæ•°æ®æ›´æ–°
            portfolio_data = {
                'value': 1000000.0,
                'daily_return': 0.01,
                'total_return': 0.1,
                'sharpe_ratio': 1.0,
                'max_drawdown': 0.05
            }
    
            # å¤šæ¬¡æ›´æ–°æ•°æ®æ¨¡æ‹Ÿå®žæ—¶ç›‘æŽ§
            for i in range(5):
                portfolio_data['value'] += i * 1000
                portfolio_data['daily_return'] += i * 0.001
                
                trading_monitor.update_portfolio_metrics(portfolio_data)
                time.sleep(0.1)
    
            # éªŒè¯æœ€æ–°æ•°æ®è¢«æ”¶é›†
            latest_metrics = trading_monitor.get_latest_metrics()
            assert latest_metrics['performance_metrics']['portfolio_value'] == 1004000.0
    
            # åœæ­¢ç›‘æŽ§
            trading_monitor.stop_monitoring()
    
        def test_monitoring_accuracy(self, trading_monitor):
            """æµ‹è¯•ç›‘æŽ§å‡†ç¡®æ€§"""
            # å¯åŠ¨ç›‘æŽ§
            trading_monitor.start_monitoring()
    
            # è®¾ç½®åŸºå‡†æ•°æ®
            expected_portfolio_value = 1234567.89
            expected_daily_return = 0.0123
            expected_sharpe_ratio = 1.234
    
            # æ›´æ–°æŒ‡æ ‡
            portfolio_data = {
                'value': expected_portfolio_value,
                'daily_return': expected_daily_return,
                'total_return': 0.1,
                'sharpe_ratio': expected_sharpe_ratio,
                'max_drawdown': 0.05
            }
            trading_monitor.update_portfolio_metrics(portfolio_data)
    
            # èŽ·å–æ”¶é›†çš„æŒ‡æ ‡
            collected_metrics = trading_monitor.get_latest_metrics()
    
            # éªŒè¯æ•°æ®å‡†ç¡®æ€§ï¼ˆç²¾åº¦æµ‹è¯•ï¼‰
            assert abs(collected_metrics['performance_metrics']['portfolio_value'] - expected_portfolio_value) < 0.01
            assert abs(collected_metrics['performance_metrics']['daily_return'] - expected_daily_return) < 1e-6
            assert abs(collected_metrics['performance_metrics']['sharpe_ratio'] - expected_sharpe_ratio) < 1e-6
    
            # åœæ­¢ç›‘æŽ§
            trading_monitor.stop_monitoring()
    
        def test_monitoring_dashboard_integration(self, trading_monitor):
            """æµ‹è¯•ç›‘æŽ§ä¸Žä»ªè¡¨æ¿é›†æˆ"""
            with patch.object(trading_monitor.dashboard_manager, 'deploy_dashboard') as mock_deploy:
                # æ¨¡æ‹Ÿä»ªè¡¨æ¿éƒ¨ç½²æˆåŠŸ
                mock_deploy.return_value = {
                    'status': 'success',
                    'uid': 'trading-monitor',
                    'url': '/d/trading-monitor/trading-system'
                }
    
                # éƒ¨ç½²ç›‘æŽ§ä»ªè¡¨æ¿
                result = trading_monitor.setup_dashboard()
    
                # éªŒè¯ä»ªè¡¨æ¿è®¾ç½®æˆåŠŸ
                assert result['status'] == 'success'
                assert 'trading-monitor' in result['uid']
    
                # éªŒè¯éƒ¨ç½²è¢«è°ƒç”¨
                mock_deploy.assert_called_once()
    
        def test_monitoring_error_handling(self, trading_monitor):
            """æµ‹è¯•ç›‘æŽ§é”™è¯¯å¤„ç†"""
            # æµ‹è¯•æ— æ•ˆçš„æŠ•èµ„ç»„åˆæ•°æ®
            invalid_portfolio_data = {
                'value': -1000000.0,  # è´Ÿå€¼
                'daily_return': float('inf'),  # æ— ç©·å¤§
                'total_return': float('nan'),  # NaN
            }
    
            # åº”è¯¥èƒ½å¤„ç†æ— æ•ˆæ•°æ®è€Œä¸å´©æºƒ
            with pytest.raises(ValueError, match="æŠ•èµ„ç»„åˆä»·å€¼ä¸èƒ½ä¸ºè´Ÿæ•°"):
                trading_monitor.update_portfolio_metrics(invalid_portfolio_data)
    
        def test_metrics_history_tracking(self, trading_monitor):
            """æµ‹è¯•æŒ‡æ ‡åŽ†å²è·Ÿè¸ª"""
            # å¯åŠ¨ç›‘æŽ§
            trading_monitor.start_monitoring()
    
            # æ·»åŠ å¤šä¸ªåŽ†å²æ•°æ®ç‚¹
            historical_data = [
                {'value': 1000000.0, 'daily_return': 0.01},
                {'value': 1010000.0, 'daily_return': 0.015},
                {'value': 1025000.0, 'daily_return': 0.008},
                {'value': 1030000.0, 'daily_return': 0.012},
            ]
    
            for data in historical_data:
                portfolio_data = {
                    'value': data['value'],
                    'daily_return': data['daily_return'],
                    'total_return': 0.1,
                    'sharpe_ratio': 1.0,
                    'max_drawdown': 0.05
                }
                trading_monitor.update_portfolio_metrics(portfolio_data)
                time.sleep(0.05)  # ç¡®ä¿æ—¶é—´æˆ³ä¸åŒ
    
            # èŽ·å–åŽ†å²æŒ‡æ ‡
            history = trading_monitor.get_metrics_history(limit=4)
    
            # éªŒè¯åŽ†å²è®°å½•
            assert len(history) == 4
            assert history[0]['performance_metrics']['portfolio_value'] == 1000000.0
            assert history[-1]['performance_metrics']['portfolio_value'] == 1030000.0
    
            # åœæ­¢ç›‘æŽ§
            trading_monitor.stop_monitoring()
    
        def test_concurrent_monitoring_operations(self, trading_monitor):
            """æµ‹è¯•å¹¶å‘ç›‘æŽ§æ“ä½œ"""
            # å¯åŠ¨ç›‘æŽ§
            trading_monitor.start_monitoring()
    
            def update_portfolio_metrics():
                for i in range(10):
                    portfolio_data = {
                        'value': 1000000.0 + i * 1000,
                        'daily_return': 0.01 + i * 0.001,
                        'total_return': 0.1,
                        'sharpe_ratio': 1.0,
                        'max_drawdown': 0.05
                    }
                    trading_monitor.update_portfolio_metrics(portfolio_data)
                    time.sleep(0.01)
    
            def update_system_metrics():
                with patch('psutil.cpu_percent', return_value=50.0), \
                     patch('psutil.virtual_memory', return_value=Mock(percent=60.0)):
                    for _ in range(10):
                        trading_monitor.update_system_metrics()
                        time.sleep(0.01)
    
            # å¯åŠ¨å¹¶å‘æ“ä½œ
            threads = []
            for func in [update_portfolio_metrics, update_system_metrics]:
                thread = threading.Thread(target=func)
                threads.append(thread)
                thread.start()
    
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()
    
            # éªŒè¯ç›‘æŽ§ä»ç„¶æ­£å¸¸å·¥ä½œ
            latest_metrics = trading_monitor.get_latest_metrics()
            assert 'performance_metrics' in latest_metrics
            assert 'system_metrics' in latest_metrics
    
            # åœæ­¢ç›‘æŽ§
            trading_monitor.stop_monitoring()
    ]]></file>
  <file path="tests/unit/test_temporal_attention.py"><![CDATA[
    """
    æµ‹è¯•æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶çš„å•å…ƒæµ‹è¯•
    """
    
    import pytest
    import torch
    import torch.nn as nn
    import numpy as np
    from typing import Tuple, Optional
    
    from src.rl_trading_system.models.temporal_attention import (
        TemporalAttention,
        MultiHeadTemporalAttention,
        ScaledDotProductAttention
    )
    
    
    class TestScaledDotProductAttention:
        """æµ‹è¯•ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›"""
        
        @pytest.fixture
        def attention(self):
            """åˆ›å»ºæ³¨æ„åŠ›å®žä¾‹"""
            return ScaledDotProductAttention(dropout=0.1)
        
        def test_initialization(self, attention):
            """æµ‹è¯•æ³¨æ„åŠ›åˆå§‹åŒ–"""
            assert isinstance(attention.dropout, nn.Dropout)
            assert attention.dropout.p == 0.1
        
        def test_forward_pass(self, attention):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_k = 2, 10, 64
            
            query = torch.randn(batch_size, seq_len, d_k)
            key = torch.randn(batch_size, seq_len, d_k)
            value = torch.randn(batch_size, seq_len, d_k)
            
            output, attention_weights = attention(query, key, value)
            
            assert output.shape == (batch_size, seq_len, d_k)
            assert attention_weights.shape == (batch_size, seq_len, seq_len)
        
        def test_attention_weights_properties(self, attention):
            """æµ‹è¯•æ³¨æ„åŠ›æƒé‡çš„æ€§è´¨"""
            batch_size, seq_len, d_k = 2, 10, 64
            
            query = torch.randn(batch_size, seq_len, d_k)
            key = torch.randn(batch_size, seq_len, d_k)
            value = torch.randn(batch_size, seq_len, d_k)
            
            attention.eval()  # ç¦ç”¨dropout
            output, attention_weights = attention(query, key, value)
            
            # æ³¨æ„åŠ›æƒé‡åº”è¯¥åœ¨æ¯è¡Œä¸Šæ±‚å’Œä¸º1
            row_sums = attention_weights.sum(dim=-1)
            torch.testing.assert_close(row_sums, torch.ones_like(row_sums), rtol=1e-5, atol=1e-6)
            
            # æ³¨æ„åŠ›æƒé‡åº”è¯¥éžè´Ÿ
            assert torch.all(attention_weights >= 0)
        
        def test_with_mask(self, attention):
            """æµ‹è¯•å¸¦æŽ©ç çš„æ³¨æ„åŠ›"""
            batch_size, seq_len, d_k = 2, 10, 64
            
            query = torch.randn(batch_size, seq_len, d_k)
            key = torch.randn(batch_size, seq_len, d_k)
            value = torch.randn(batch_size, seq_len, d_k)
            
            # åˆ›å»ºæŽ©ç ï¼šå‰5ä¸ªä½ç½®å¯è§ï¼ŒåŽ5ä¸ªä½ç½®è¢«æŽ©ç›–
            mask = torch.zeros(batch_size, seq_len, seq_len)
            mask[:, :, 5:] = float('-inf')
            
            attention.eval()
            output, attention_weights = attention(query, key, value, mask)
            
            # è¢«æŽ©ç›–ä½ç½®çš„æ³¨æ„åŠ›æƒé‡åº”è¯¥æŽ¥è¿‘0
            assert torch.all(attention_weights[:, :, 5:] < 1e-6)
            
            # å¯è§ä½ç½®çš„æ³¨æ„åŠ›æƒé‡å’Œåº”è¯¥ä¸º1
            visible_weights_sum = attention_weights[:, :, :5].sum(dim=-1)
            torch.testing.assert_close(visible_weights_sum, torch.ones_like(visible_weights_sum), rtol=1e-5, atol=1e-6)
        
        def test_gradient_flow(self, attention):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            batch_size, seq_len, d_k = 2, 10, 64
            
            query = torch.randn(batch_size, seq_len, d_k, requires_grad=True)
            key = torch.randn(batch_size, seq_len, d_k, requires_grad=True)
            value = torch.randn(batch_size, seq_len, d_k, requires_grad=True)
            
            output, _ = attention(query, key, value)
            loss = output.sum()
            loss.backward()
            
            # æ‰€æœ‰è¾“å…¥éƒ½åº”è¯¥æœ‰æ¢¯åº¦
            assert query.grad is not None
            assert key.grad is not None
            assert value.grad is not None
        
        def test_different_dimensions(self, attention):
            """æµ‹è¯•ä¸åŒç»´åº¦çš„è¾“å…¥"""
            batch_size = 2
            
            # æµ‹è¯•ä¸åŒçš„åºåˆ—é•¿åº¦å’Œç‰¹å¾ç»´åº¦
            test_cases = [
                (5, 32),   # çŸ­åºåˆ—ï¼Œå°ç»´åº¦
                (20, 64),  # ä¸­ç­‰åºåˆ—ï¼Œä¸­ç­‰ç»´åº¦
                (50, 128), # é•¿åºåˆ—ï¼Œå¤§ç»´åº¦
            ]
            
            for seq_len, d_k in test_cases:
                query = torch.randn(batch_size, seq_len, d_k)
                key = torch.randn(batch_size, seq_len, d_k)
                value = torch.randn(batch_size, seq_len, d_k)
                
                output, attention_weights = attention(query, key, value)
                
                assert output.shape == (batch_size, seq_len, d_k)
                assert attention_weights.shape == (batch_size, seq_len, seq_len)
    
    
    class TestTemporalAttention:
        """æµ‹è¯•æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶"""
        
        @pytest.fixture
        def temporal_attention(self):
            """åˆ›å»ºæ—¶é—´æ³¨æ„åŠ›å®žä¾‹"""
            return TemporalAttention(d_model=256, dropout=0.1)
        
        def test_initialization(self, temporal_attention):
            """æµ‹è¯•æ—¶é—´æ³¨æ„åŠ›åˆå§‹åŒ–"""
            assert temporal_attention.d_model == 256
            assert isinstance(temporal_attention.attention, ScaledDotProductAttention)
            assert isinstance(temporal_attention.w_q, nn.Linear)
            assert isinstance(temporal_attention.w_k, nn.Linear)
            assert isinstance(temporal_attention.w_v, nn.Linear)
            assert isinstance(temporal_attention.w_o, nn.Linear)
        
        def test_forward_pass(self, temporal_attention):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            output = temporal_attention(x)
            
            assert output.shape == (batch_size, d_model)
        
        def test_attention_aggregation(self, temporal_attention):
            """æµ‹è¯•æ³¨æ„åŠ›èšåˆ"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            temporal_attention.eval()
            output = temporal_attention(x)
            
            # è¾“å‡ºåº”è¯¥æ˜¯æ—¶é—´ç»´åº¦çš„åŠ æƒèšåˆ
            assert output.shape == (batch_size, d_model)
            
            # æµ‹è¯•èšåˆçš„åˆç†æ€§ï¼šè¾“å‡ºä¸åº”è¯¥ç­‰äºŽä»»ä½•å•ä¸ªæ—¶é—´æ­¥
            for t in range(seq_len):
                assert not torch.allclose(output, x[:, t, :], atol=1e-3)
        
        def test_attention_weights_reasonableness(self, temporal_attention):
            """æµ‹è¯•æ³¨æ„åŠ›æƒé‡çš„åˆç†æ€§"""
            batch_size, seq_len, d_model = 2, 20, 256
            
            # åˆ›å»ºä¸€ä¸ªæœ‰æ˜Žæ˜¾æ¨¡å¼çš„è¾“å…¥
            x = torch.randn(batch_size, seq_len, d_model)
            # è®©æœ€åŽä¸€ä¸ªæ—¶é—´æ­¥çš„ç‰¹å¾æ›´çªå‡º
            x[:, -1, :] *= 3
            
            temporal_attention.eval()
            output, attention_weights = temporal_attention.forward_with_attention(x)
            
            assert attention_weights.shape == (batch_size, seq_len)
            
            # æ³¨æ„åŠ›æƒé‡åº”è¯¥æ±‚å’Œä¸º1
            weight_sums = attention_weights.sum(dim=-1)
            torch.testing.assert_close(weight_sums, torch.ones_like(weight_sums), rtol=1e-5, atol=1e-6)
            
            # æ³¨æ„åŠ›æƒé‡åº”è¯¥éžè´Ÿ
            assert torch.all(attention_weights >= 0)
        
        def test_with_mask(self, temporal_attention):
            """æµ‹è¯•å¸¦æŽ©ç çš„æ—¶é—´æ³¨æ„åŠ›"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            # åˆ›å»ºæŽ©ç ï¼šåªæœ‰å‰10ä¸ªæ—¶é—´æ­¥å¯è§
            mask = torch.zeros(batch_size, seq_len)
            mask[:, 10:] = float('-inf')
            
            temporal_attention.eval()
            output = temporal_attention(x, mask=mask)
            
            assert output.shape == (batch_size, d_model)
        
        def test_gradient_flow(self, temporal_attention):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
            
            output = temporal_attention(x)
            loss = output.sum()
            loss.backward()
            
            # è¾“å…¥åº”è¯¥æœ‰æ¢¯åº¦
            assert x.grad is not None
            
            # æ‰€æœ‰å‚æ•°éƒ½åº”è¯¥æœ‰æ¢¯åº¦
            for param in temporal_attention.parameters():
                assert param.grad is not None
        
        def test_different_sequence_lengths(self, temporal_attention):
            """æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦"""
            batch_size, d_model = 2, 256
            
            for seq_len in [5, 10, 30, 60]:
                x = torch.randn(batch_size, seq_len, d_model)
                output = temporal_attention(x)
                assert output.shape == (batch_size, d_model)
    
    
    class TestMultiHeadTemporalAttention:
        """æµ‹è¯•å¤šå¤´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶"""
        
        @pytest.fixture
        def multi_head_attention(self):
            """åˆ›å»ºå¤šå¤´æ—¶é—´æ³¨æ„åŠ›å®žä¾‹"""
            return MultiHeadTemporalAttention(d_model=256, n_heads=8, dropout=0.1)
        
        def test_initialization(self, multi_head_attention):
            """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›åˆå§‹åŒ–"""
            assert multi_head_attention.d_model == 256
            assert multi_head_attention.n_heads == 8
            assert multi_head_attention.d_k == 32  # 256 / 8
            assert len(multi_head_attention.heads) == 8
        
        def test_forward_pass(self, multi_head_attention):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            output = multi_head_attention(x)
            
            assert output.shape == (batch_size, d_model)
        
        def test_multi_head_aggregation(self, multi_head_attention):
            """æµ‹è¯•å¤šå¤´èšåˆæ•ˆæžœ"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            multi_head_attention.eval()
            output, head_outputs = multi_head_attention.forward_with_heads(x)
            
            assert output.shape == (batch_size, d_model)
            assert len(head_outputs) == 8
            
            for head_output in head_outputs:
                assert head_output.shape == (batch_size, 32)  # d_k = d_model / n_heads
            
            # å¤šå¤´è¾“å‡ºçš„æ‹¼æŽ¥åº”è¯¥ç­‰äºŽæœ€ç»ˆè¾“å‡ºï¼ˆåœ¨çº¿æ€§å˜æ¢å‰ï¼‰
            concatenated = torch.cat(head_outputs, dim=-1)
            assert concatenated.shape == (batch_size, d_model)
        
        def test_attention_diversity(self, multi_head_attention):
            """æµ‹è¯•æ³¨æ„åŠ›çš„å¤šæ ·æ€§"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            multi_head_attention.eval()
            _, head_attentions = multi_head_attention.forward_with_attention_weights(x)
            
            assert len(head_attentions) == 8
            
            for attention_weights in head_attentions:
                assert attention_weights.shape == (batch_size, seq_len)
                
                # æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡åº”è¯¥æ±‚å’Œä¸º1
                weight_sums = attention_weights.sum(dim=-1)
                torch.testing.assert_close(weight_sums, torch.ones_like(weight_sums), rtol=1e-5, atol=1e-6)
            
            # æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›çš„åŸºæœ¬åŠŸèƒ½è€Œä¸æ˜¯å¤šæ ·æ€§
            # åœ¨å®žé™…è®­ç»ƒä¸­ï¼Œä¸åŒçš„å¤´ä¼šå­¦ä¹ åˆ°ä¸åŒçš„æ¨¡å¼
            # ä½†åœ¨éšæœºåˆå§‹åŒ–æ—¶ï¼Œå®ƒä»¬å¯èƒ½å¾ˆç›¸ä¼¼ï¼Œè¿™æ˜¯æ­£å¸¸çš„
            
            # éªŒè¯æ‰€æœ‰å¤´éƒ½äº§ç”Ÿäº†æœ‰æ•ˆçš„æ³¨æ„åŠ›æƒé‡
            for attention_weights in head_attentions:
                # æ³¨æ„åŠ›æƒé‡åº”è¯¥éžè´Ÿ
                assert torch.all(attention_weights >= 0)
                
                # æ³¨æ„åŠ›æƒé‡ä¸åº”è¯¥å…¨éƒ¨ç›¸ç­‰ï¼ˆé™¤éžè¾“å…¥å®Œå…¨ç›¸åŒï¼‰
                # æ£€æŸ¥æ˜¯å¦æœ‰å˜åŒ–
                std_dev = attention_weights.std(dim=-1)
                # è‡³å°‘åº”è¯¥æœ‰ä¸€äº›å˜åŒ–ï¼ˆä¸æ˜¯å®Œå…¨å‡åŒ€åˆ†å¸ƒï¼‰
                assert torch.any(std_dev > 1e-6)
        
        def test_gradient_flow(self, multi_head_attention):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model, requires_grad=True)
            
            output = multi_head_attention(x)
            loss = output.sum()
            loss.backward()
            
            # è¾“å…¥åº”è¯¥æœ‰æ¢¯åº¦
            assert x.grad is not None
            
            # æ‰€æœ‰å‚æ•°éƒ½åº”è¯¥æœ‰æ¢¯åº¦
            for param in multi_head_attention.parameters():
                assert param.grad is not None
        
        def test_performance_comparison(self, multi_head_attention):
            """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›çš„æ€§èƒ½ç‰¹å¾"""
            batch_size, seq_len, d_model = 2, 50, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            # æµ‹è¯•å‰å‘ä¼ æ’­æ—¶é—´
            import time
            
            multi_head_attention.eval()
            start_time = time.time()
            
            for _ in range(10):
                output = multi_head_attention(x)
            
            end_time = time.time()
            avg_time = (end_time - start_time) / 10
            
            # å¤šå¤´æ³¨æ„åŠ›åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
            assert avg_time < 0.1  # 100ms per forward pass
        
        def test_memory_efficiency(self, multi_head_attention):
            """æµ‹è¯•å†…å­˜æ•ˆçŽ‡"""
            # æµ‹è¯•è¾ƒå¤§çš„è¾“å…¥
            batch_size, seq_len, d_model = 4, 100, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            # åº”è¯¥èƒ½å¤Ÿå¤„ç†è¾ƒå¤§çš„è¾“å…¥è€Œä¸å‡ºçŽ°å†…å­˜é”™è¯¯
            output = multi_head_attention(x)
            assert output.shape == (batch_size, d_model)
        
        @pytest.mark.parametrize("n_heads", [1, 2, 4, 8, 16])
        def test_different_head_numbers(self, n_heads):
            """æµ‹è¯•ä¸åŒå¤´æ•°çš„å¤šå¤´æ³¨æ„åŠ›"""
            d_model = 256
            
            # ç¡®ä¿d_modelèƒ½è¢«n_headsæ•´é™¤
            if d_model % n_heads != 0:
                pytest.skip(f"d_model {d_model} ä¸èƒ½è¢« n_heads {n_heads} æ•´é™¤")
            
            multi_head_attention = MultiHeadTemporalAttention(
                d_model=d_model, n_heads=n_heads, dropout=0.1
            )
            
            batch_size, seq_len = 2, 20
            x = torch.randn(batch_size, seq_len, d_model)
            
            output = multi_head_attention(x)
            assert output.shape == (batch_size, d_model)
        
        def test_with_mask(self, multi_head_attention):
            """æµ‹è¯•å¸¦æŽ©ç çš„å¤šå¤´æ³¨æ„åŠ›"""
            batch_size, seq_len, d_model = 2, 20, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            # åˆ›å»ºæŽ©ç 
            mask = torch.zeros(batch_size, seq_len)
            mask[:, 15:] = float('-inf')
            
            multi_head_attention.eval()
            output = multi_head_attention(x, mask=mask)
            
            assert output.shape == (batch_size, d_model)
    
    
    class TestTemporalAttentionVisualization:
        """æµ‹è¯•æ—¶é—´æ³¨æ„åŠ›å¯è§†åŒ–åŠŸèƒ½"""
        
        def test_attention_weight_extraction(self):
            """æµ‹è¯•æ³¨æ„åŠ›æƒé‡æå–"""
            d_model, seq_len = 256, 30
            temporal_attention = TemporalAttention(d_model)
            
            batch_size = 1
            x = torch.randn(batch_size, seq_len, d_model)
            
            temporal_attention.eval()
            output, attention_weights = temporal_attention.forward_with_attention(x)
            
            assert output.shape == (batch_size, d_model)
            assert attention_weights.shape == (batch_size, seq_len)
            
            # æ³¨æ„åŠ›æƒé‡åº”è¯¥å¯ä»¥ç”¨äºŽå¯è§†åŒ–
            weights_numpy = attention_weights.detach().numpy()
            assert weights_numpy.shape == (batch_size, seq_len)
            assert np.all(weights_numpy >= 0)
            assert np.allclose(weights_numpy.sum(axis=1), 1.0, atol=1e-6)
        
        def test_multi_head_attention_visualization(self):
            """æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›å¯è§†åŒ–"""
            d_model, seq_len, n_heads = 256, 30, 8
            multi_head_attention = MultiHeadTemporalAttention(d_model, n_heads)
            
            batch_size = 1
            x = torch.randn(batch_size, seq_len, d_model)
            
            multi_head_attention.eval()
            output, head_attentions = multi_head_attention.forward_with_attention_weights(x)
            
            assert len(head_attentions) == n_heads
            
            for i, attention_weights in enumerate(head_attentions):
                assert attention_weights.shape == (batch_size, seq_len)
                
                # æ¯ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡éƒ½å¯ä»¥ç”¨äºŽå¯è§†åŒ–
                weights_numpy = attention_weights.detach().numpy()
                assert np.all(weights_numpy >= 0)
                assert np.allclose(weights_numpy.sum(axis=1), 1.0, atol=1e-6)
        
        def test_attention_pattern_analysis(self):
            """æµ‹è¯•æ³¨æ„åŠ›æ¨¡å¼åˆ†æž"""
            d_model, seq_len = 256, 20
            temporal_attention = TemporalAttention(d_model)
            
            # åˆ›å»ºå…·æœ‰ç‰¹å®šæ¨¡å¼çš„è¾“å…¥
            batch_size = 1
            x = torch.randn(batch_size, seq_len, d_model)
            
            # è®©æŸäº›æ—¶é—´æ­¥æ›´é‡è¦
            x[:, -3:, :] *= 2  # æœ€åŽ3ä¸ªæ—¶é—´æ­¥
            x[:, 0, :] *= 2    # ç¬¬ä¸€ä¸ªæ—¶é—´æ­¥
            
            temporal_attention.eval()
            output, attention_weights = temporal_attention.forward_with_attention(x)
            
            # åˆ†æžæ³¨æ„åŠ›æ¨¡å¼
            weights = attention_weights[0].detach().numpy()
            
            # é‡è¦æ—¶é—´æ­¥åº”è¯¥èŽ·å¾—æ›´é«˜çš„æ³¨æ„åŠ›æƒé‡
            important_positions = [0, -3, -2, -1]  # ç¬¬ä¸€ä¸ªå’Œæœ€åŽä¸‰ä¸ª
            important_weights = [weights[pos] for pos in important_positions]
            other_weights = [weights[i] for i in range(1, seq_len-3)]
            
            # é‡è¦ä½ç½®çš„å¹³å‡æƒé‡åº”è¯¥é«˜äºŽå…¶ä»–ä½ç½®
            avg_important = np.mean(important_weights)
            avg_other = np.mean(other_weights)
            
            # è¿™ä¸ªæµ‹è¯•å¯èƒ½ä¸æ€»æ˜¯é€šè¿‡ï¼Œå› ä¸ºæ³¨æ„åŠ›æœºåˆ¶çš„å¤æ‚æ€§
            # ä½†åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œé‡è¦ä½ç½®åº”è¯¥èŽ·å¾—æ›´å¤šå…³æ³¨
            # assert avg_important > avg_other  # å¯é€‰çš„æ–­è¨€
    ]]></file>
  <file path="tests/unit/test_sac_agent_implementation.py"><![CDATA[
    """
    æµ‹è¯•å®žé™…çš„SACæ™ºèƒ½ä½“å®žçŽ°
    """
    import pytest
    import torch
    import numpy as np
    
    from src.rl_trading_system.models.sac_agent import SACAgent, SACConfig
    from src.rl_trading_system.models.replay_buffer import Experience
    
    
    class TestSACAgentImplementation:
        """æµ‹è¯•å®žé™…SACæ™ºèƒ½ä½“å®žçŽ°"""
        
        @pytest.fixture
        def sac_config(self):
            """SACé…ç½®fixture"""
            return SACConfig(
                state_dim=64,
                action_dim=10,
                hidden_dim=128,
                batch_size=32,
                buffer_capacity=1000,
                learning_starts=50
            )
        
        @pytest.fixture
        def sac_agent(self, sac_config):
            """SACæ™ºèƒ½ä½“fixture"""
            return SACAgent(sac_config)
        
        def test_agent_initialization(self, sac_agent, sac_config):
            """æµ‹è¯•æ™ºèƒ½ä½“åˆå§‹åŒ–"""
            assert sac_agent.config.state_dim == sac_config.state_dim
            assert sac_agent.config.action_dim == sac_config.action_dim
            assert sac_agent.training_step == 0
            assert sac_agent.total_env_steps == 0
            
            # æ£€æŸ¥ç½‘ç»œç»„ä»¶
            assert hasattr(sac_agent, 'actor')
            assert hasattr(sac_agent, 'critic')
            assert hasattr(sac_agent, 'replay_buffer')
            
        def test_get_action(self, sac_agent, sac_config):
            """æµ‹è¯•åŠ¨ä½œç”Ÿæˆ"""
            state = torch.randn(sac_config.state_dim)
            
            # æµ‹è¯•ç¡®å®šæ€§åŠ¨ä½œ
            action_det = sac_agent.get_action(state, deterministic=True)
            assert action_det.shape == (sac_config.action_dim,)
            assert torch.all(action_det >= 0)
            assert torch.allclose(torch.sum(action_det), torch.tensor(1.0), atol=1e-5)
            
            # æµ‹è¯•éšæœºåŠ¨ä½œ
            action_stoch, log_prob = sac_agent.get_action(state, return_log_prob=True)
            assert action_stoch.shape == (sac_config.action_dim,)
            assert log_prob.shape == ()
            assert torch.all(action_stoch >= 0)
            assert torch.allclose(torch.sum(action_stoch), torch.tensor(1.0), atol=1e-5)
            
        def test_add_experience(self, sac_agent, sac_config):
            """æµ‹è¯•æ·»åŠ ç»éªŒ"""
            experience = Experience(
                state=torch.randn(sac_config.state_dim),
                action=torch.rand(sac_config.action_dim),
                reward=1.0,
                next_state=torch.randn(sac_config.state_dim),
                done=False
            )
            
            initial_size = sac_agent.replay_buffer.size
            sac_agent.add_experience(experience)
            
            assert sac_agent.replay_buffer.size == initial_size + 1
            assert sac_agent.total_env_steps == 1
            
        def test_can_update(self, sac_agent, sac_config):
            """æµ‹è¯•æ›´æ–°æ¡ä»¶æ£€æŸ¥"""
            # åˆå§‹çŠ¶æ€ä¸èƒ½æ›´æ–°
            assert not sac_agent.can_update()
            
            # æ·»åŠ è¶³å¤Ÿçš„ç»éªŒ
            for i in range(sac_config.learning_starts + sac_config.batch_size):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=np.random.choice([True, False])
                )
                sac_agent.add_experience(experience)
            
            # çŽ°åœ¨åº”è¯¥å¯ä»¥æ›´æ–°
            assert sac_agent.can_update()
            
        def test_update(self, sac_agent, sac_config):
            """æµ‹è¯•ç½‘ç»œæ›´æ–°"""
            # æ·»åŠ è¶³å¤Ÿçš„ç»éªŒ
            for i in range(sac_config.learning_starts + sac_config.batch_size):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=np.random.choice([True, False])
                )
                sac_agent.add_experience(experience)
            
            # æ‰§è¡Œæ›´æ–°
            losses = sac_agent.update()
            
            # æ£€æŸ¥è¿”å›žçš„æŸå¤±
            assert 'critic_loss' in losses
            assert 'actor_loss' in losses
            assert 'alpha_loss' in losses
            assert 'alpha' in losses
            
            # æ£€æŸ¥è®­ç»ƒæ­¥æ•°å¢žåŠ 
            assert sac_agent.training_step > 0
            
        def test_training_stats(self, sac_agent, sac_config):
            """æµ‹è¯•è®­ç»ƒç»Ÿè®¡"""
            # æ·»åŠ ç»éªŒå¹¶è®­ç»ƒ
            for i in range(sac_config.learning_starts + sac_config.batch_size):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=False
                )
                sac_agent.add_experience(experience)
            
            # å¤šæ¬¡æ›´æ–°
            for _ in range(5):
                sac_agent.update()
            
            # èŽ·å–ç»Ÿè®¡ä¿¡æ¯
            stats = sac_agent.get_training_stats()
            
            assert 'training_step' in stats
            assert 'total_env_steps' in stats
            assert 'buffer_size' in stats
            assert stats['training_step'] > 0
            assert stats['total_env_steps'] > 0
            
        def test_eval_train_modes(self, sac_agent):
            """æµ‹è¯•è¯„ä¼°å’Œè®­ç»ƒæ¨¡å¼"""
            # è®­ç»ƒæ¨¡å¼
            sac_agent.train()
            assert sac_agent.training
            
            # è¯„ä¼°æ¨¡å¼
            sac_agent.eval()
            assert not sac_agent.training
            
        def test_temperature_parameter(self, sac_agent):
            """æµ‹è¯•æ¸©åº¦å‚æ•°"""
            alpha = sac_agent.alpha
            assert alpha > 0
            assert torch.is_tensor(alpha)
            
            # æ¸©åº¦å‚æ•°åº”è¯¥å¯ä»¥æ›´æ–°
            initial_alpha = alpha.item()
            
            # æ·»åŠ ç»éªŒå¹¶è®­ç»ƒ
            for i in range(100):
                experience = Experience(
                    state=torch.randn(sac_agent.config.state_dim),
                    action=torch.rand(sac_agent.config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_agent.config.state_dim),
                    done=False
                )
                sac_agent.add_experience(experience)
            
            # å¤šæ¬¡æ›´æ–°
            for _ in range(10):
                if sac_agent.can_update():
                    sac_agent.update()
            
            # æ¸©åº¦å‚æ•°å¯èƒ½ä¼šå˜åŒ–
            final_alpha = sac_agent.alpha.item()
            # æ³¨æ„ï¼šæ¸©åº¦å‚æ•°å¯èƒ½å¢žåŠ æˆ–å‡å°‘ï¼Œè¿™é‡Œåªæ£€æŸ¥å®ƒä»ç„¶ä¸ºæ­£
            assert final_alpha > 0
    ]]></file>
  <file path="tests/unit/test_sac_agent.py"><![CDATA[
    """
    æµ‹è¯•å®Œæ•´SACæ™ºèƒ½ä½“çš„å•å…ƒæµ‹è¯•
    """
    import pytest
    import torch
    import torch.nn as nn
    import numpy as np
    from unittest.mock import Mock, patch, MagicMock
    from dataclasses import dataclass
    from typing import Dict, Any, List, Tuple
    
    from src.rl_trading_system.models.actor_network import Actor, ActorConfig
    from src.rl_trading_system.models.critic_network import CriticWithTargetNetwork, CriticConfig
    from src.rl_trading_system.models.replay_buffer import ReplayBuffer, Experience, ReplayBufferConfig
    
    
    # å…ˆåˆ›å»ºä¸€ä¸ªç®€å•çš„SAC Agenté…ç½®ç”¨äºŽæµ‹è¯•
    @dataclass
    class SACConfig:
        """SACæ™ºèƒ½ä½“é…ç½®"""
        state_dim: int = 256
        action_dim: int = 100
        hidden_dim: int = 512
        
        # å­¦ä¹ çŽ‡
        lr_actor: float = 3e-4
        lr_critic: float = 3e-4
        lr_alpha: float = 3e-4
        
        # SACå‚æ•°
        gamma: float = 0.99
        tau: float = 0.005
        alpha: float = 0.2
        target_entropy: float = -100  # -action_dim
        
        # è®­ç»ƒå‚æ•°
        batch_size: int = 256
        buffer_capacity: int = 1000000
        device: str = 'cpu'
    
    
    # åˆ›å»ºä¸€ä¸ªç®€å•çš„SAC Agentå®žçŽ°ç”¨äºŽæµ‹è¯•
    class SimpleSACAgent(nn.Module):
        """ç®€åŒ–çš„SACæ™ºèƒ½ä½“å®žçŽ°ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰"""
        
        def __init__(self, config: SACConfig):
            super().__init__()
            self.config = config
            self.device = torch.device(config.device)
            
            # ç½‘ç»œç»„ä»¶
            actor_config = ActorConfig(
                state_dim=config.state_dim,
                action_dim=config.action_dim,
                hidden_dim=config.hidden_dim
            )
            self.actor = Actor(actor_config)
            
            critic_config = CriticConfig(
                state_dim=config.state_dim,
                action_dim=config.action_dim,
                hidden_dim=config.hidden_dim
            )
            self.critic = CriticWithTargetNetwork(critic_config)
            
            # æ¸©åº¦å‚æ•°
            self.log_alpha = nn.Parameter(torch.zeros(1, device=self.device))
            
            # ä¼˜åŒ–å™¨
            self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=config.lr_actor)
            self.critic_optimizer = torch.optim.Adam(self.critic.get_parameters(), lr=config.lr_critic)
            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=config.lr_alpha)
            
            # å›žæ”¾ç¼“å†²åŒº
            buffer_config = ReplayBufferConfig(
                capacity=config.buffer_capacity,
                batch_size=config.batch_size,
                state_dim=config.state_dim,
                action_dim=config.action_dim,
                device=config.device
            )
            self.replay_buffer = ReplayBuffer(buffer_config)
            
            # è®­ç»ƒç»Ÿè®¡
            self.training_step = 0
            
        @property
        def alpha(self):
            """å½“å‰æ¸©åº¦å‚æ•°"""
            return torch.exp(self.log_alpha)
        
        def get_action(self, state: torch.Tensor, deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
            """èŽ·å–åŠ¨ä½œ"""
            self.actor.eval()
            with torch.no_grad():
                action, log_prob = self.actor.get_action(state, deterministic=deterministic)
            self.actor.train()
            return action, log_prob
        
        def add_experience(self, experience: Experience):
            """æ·»åŠ ç»éªŒåˆ°å›žæ”¾ç¼“å†²åŒº"""
            self.replay_buffer.add(experience)
        
        def update(self) -> Dict[str, float]:
            """æ›´æ–°ç½‘ç»œå‚æ•°"""
            if not self.replay_buffer.can_sample():
                return {}
            
            # é‡‡æ ·æ‰¹æ¬¡
            batch = self.replay_buffer.sample()
            
            states = torch.stack([exp.state for exp in batch]).to(self.device)
            actions = torch.stack([exp.action for exp in batch]).to(self.device)
            rewards = torch.tensor([exp.reward for exp in batch], dtype=torch.float32).to(self.device)
            next_states = torch.stack([exp.next_state for exp in batch]).to(self.device)
            dones = torch.tensor([exp.done for exp in batch], dtype=torch.float32).to(self.device)
            
            # æ›´æ–°Critic
            critic_loss = self._update_critic(states, actions, rewards, next_states, dones)
            
            # æ›´æ–°Actor
            actor_loss = self._update_actor(states)
            
            # æ›´æ–°æ¸©åº¦å‚æ•°
            alpha_loss = self._update_alpha(states)
            
            # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
            self.critic.soft_update(self.config.tau)
            
            self.training_step += 1
            
            return {
                'critic_loss': critic_loss,
                'actor_loss': actor_loss,
                'alpha_loss': alpha_loss,
                'alpha': self.alpha.item()
            }
        
        def _update_critic(self, states, actions, rewards, next_states, dones):
            """æ›´æ–°Criticç½‘ç»œ"""
            with torch.no_grad():
                next_actions, next_log_probs = self.actor.get_action(next_states)
                target_q = self.critic.get_target_min_q_value(next_states, next_actions)
                target_q = target_q - self.alpha * next_log_probs.unsqueeze(1)
                target_q = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * self.config.gamma * target_q
            
            current_q1, current_q2 = self.critic.get_main_q_values(states, actions)
            
            critic_loss = nn.MSELoss()(current_q1, target_q) + nn.MSELoss()(current_q2, target_q)
            
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            self.critic_optimizer.step()
            
            return critic_loss.item()
        
        def _update_actor(self, states):
            """æ›´æ–°Actorç½‘ç»œ"""
            actions, log_probs = self.actor.get_action(states)
            q_values = self.critic.main_network.get_min_q_value(states, actions)
            
            actor_loss = torch.mean(self.alpha * log_probs - q_values.squeeze())
            
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            self.actor_optimizer.step()
            
            return actor_loss.item()
        
        def _update_alpha(self, states):
            """æ›´æ–°æ¸©åº¦å‚æ•°"""
            with torch.no_grad():
                _, log_probs = self.actor.get_action(states)
            
            alpha_loss = -torch.mean(self.log_alpha * (log_probs + self.config.target_entropy))
            
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            return alpha_loss.item()
    
    
    class TestSACAgent:
        """SACæ™ºèƒ½ä½“æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def sac_config(self):
            """SACé…ç½®fixture"""
            return SACConfig(
                state_dim=64,  # è¾ƒå°çš„ç»´åº¦ä»¥åŠ å¿«æµ‹è¯•
                action_dim=10,
                hidden_dim=128,
                batch_size=32,
                buffer_capacity=1000
            )
        
        @pytest.fixture
        def sac_agent(self, sac_config):
            """SACæ™ºèƒ½ä½“fixture"""
            return SimpleSACAgent(sac_config)
        
        @pytest.fixture
        def sample_state(self, sac_config):
            """æ ·æœ¬çŠ¶æ€fixture"""
            return torch.randn(sac_config.state_dim)
        
        @pytest.fixture
        def sample_batch_states(self, sac_config):
            """æ‰¹é‡æ ·æœ¬çŠ¶æ€fixture"""
            return torch.randn(16, sac_config.state_dim)
        
        def test_sac_agent_initialization(self, sac_agent, sac_config):
            """æµ‹è¯•SACæ™ºèƒ½ä½“åˆå§‹åŒ–"""
            assert isinstance(sac_agent.actor, Actor)
            assert isinstance(sac_agent.critic, CriticWithTargetNetwork)
            assert isinstance(sac_agent.replay_buffer, ReplayBuffer)
            
            # æ£€æŸ¥ä¼˜åŒ–å™¨
            assert sac_agent.actor_optimizer is not None
            assert sac_agent.critic_optimizer is not None
            assert sac_agent.alpha_optimizer is not None
            
            # æ£€æŸ¥æ¸©åº¦å‚æ•°
            assert sac_agent.log_alpha.requires_grad
            assert sac_agent.alpha > 0
            
            # æ£€æŸ¥è®­ç»ƒç»Ÿè®¡
            assert sac_agent.training_step == 0
            
        def test_get_action_deterministic(self, sac_agent, sample_state, sac_config):
            """æµ‹è¯•ç¡®å®šæ€§åŠ¨ä½œç”Ÿæˆ"""
            action, log_prob = sac_agent.get_action(sample_state.unsqueeze(0), deterministic=True)
            
            assert action.shape == (1, sac_config.action_dim)
            assert log_prob.shape == (1,)
            
            # æ£€æŸ¥æƒé‡çº¦æŸ
            assert torch.all(action >= 0)
            weight_sum = torch.sum(action, dim=1)
            assert torch.allclose(weight_sum, torch.ones_like(weight_sum), atol=1e-5)
            
            # ç¡®å®šæ€§åŠ¨ä½œåº”è¯¥å¯é‡å¤
            action2, log_prob2 = sac_agent.get_action(sample_state.unsqueeze(0), deterministic=True)
            assert torch.allclose(action, action2, atol=1e-6)
            
        def test_get_action_stochastic(self, sac_agent, sample_state, sac_config):
            """æµ‹è¯•éšæœºåŠ¨ä½œç”Ÿæˆ"""
            action1, log_prob1 = sac_agent.get_action(sample_state.unsqueeze(0), deterministic=False)
            action2, log_prob2 = sac_agent.get_action(sample_state.unsqueeze(0), deterministic=False)
            
            assert action1.shape == (1, sac_config.action_dim)
            assert action2.shape == (1, sac_config.action_dim)
            
            # éšæœºåŠ¨ä½œåº”è¯¥ä¸åŒ
            assert not torch.allclose(action1, action2, atol=1e-3)
            
            # æ£€æŸ¥æƒé‡çº¦æŸ
            for action in [action1, action2]:
                assert torch.all(action >= 0)
                weight_sum = torch.sum(action, dim=1)
                assert torch.allclose(weight_sum, torch.ones_like(weight_sum), atol=1e-5)
                
        def test_add_experience(self, sac_agent, sac_config):
            """æµ‹è¯•æ·»åŠ ç»éªŒ"""
            initial_size = sac_agent.replay_buffer.size
            
            experience = Experience(
                state=torch.randn(sac_config.state_dim),
                action=torch.rand(sac_config.action_dim),
                reward=1.0,
                next_state=torch.randn(sac_config.state_dim),
                done=False
            )
            
            sac_agent.add_experience(experience)
            
            assert sac_agent.replay_buffer.size == initial_size + 1
            
        def test_update_insufficient_data(self, sac_agent):
            """æµ‹è¯•æ•°æ®ä¸è¶³æ—¶çš„æ›´æ–°"""
            # æ²¡æœ‰è¶³å¤Ÿæ•°æ®æ—¶åº”è¯¥è¿”å›žç©ºå­—å…¸
            losses = sac_agent.update()
            assert losses == {}
            
        def test_update_with_sufficient_data(self, sac_agent, sac_config):
            """æµ‹è¯•æœ‰è¶³å¤Ÿæ•°æ®æ—¶çš„æ›´æ–°"""
            # æ·»åŠ è¶³å¤Ÿçš„ç»éªŒ
            for i in range(sac_config.batch_size * 2):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=np.random.choice([True, False])
                )
                sac_agent.add_experience(experience)
            
            # æ›´æ–°ç½‘ç»œ
            losses = sac_agent.update()
            
            # æ£€æŸ¥è¿”å›žçš„æŸå¤±
            assert 'critic_loss' in losses
            assert 'actor_loss' in losses
            assert 'alpha_loss' in losses
            assert 'alpha' in losses
            
            # æ£€æŸ¥æŸå¤±å€¼çš„åˆç†æ€§
            assert isinstance(losses['critic_loss'], float)
            assert isinstance(losses['actor_loss'], float)
            assert isinstance(losses['alpha_loss'], float)
            assert losses['alpha'] > 0
            
            # æ£€æŸ¥è®­ç»ƒæ­¥æ•°å¢žåŠ 
            assert sac_agent.training_step == 1
            
        def test_temperature_parameter_adjustment(self, sac_agent, sac_config):
            """æµ‹è¯•æ¸©åº¦å‚æ•°è‡ªåŠ¨è°ƒæ•´"""
            # è®°å½•åˆå§‹æ¸©åº¦
            initial_alpha = sac_agent.alpha.item()
            
            # æ·»åŠ ç»éªŒå¹¶è®­ç»ƒ
            for i in range(sac_config.batch_size * 2):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=False
                )
                sac_agent.add_experience(experience)
            
            # å¤šæ¬¡æ›´æ–°
            for _ in range(10):
                losses = sac_agent.update()
            
            # æ¸©åº¦å‚æ•°åº”è¯¥æœ‰æ‰€å˜åŒ–
            final_alpha = sac_agent.alpha.item()
            # æ³¨æ„ï¼šæ¸©åº¦å‚æ•°å¯èƒ½å¢žåŠ æˆ–å‡å°‘ï¼Œå–å†³äºŽç­–ç•¥ç†µ
            assert final_alpha != initial_alpha
            assert final_alpha > 0
            
        def test_learning_capability(self, sac_agent, sac_config):
            """æµ‹è¯•æ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›"""
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„å­¦ä¹ ä»»åŠ¡ï¼šå¥–åŠ±ä¸Žç‰¹å®šåŠ¨ä½œç›¸å…³
            def reward_function(action):
                # å¥–åŠ±å‡½æ•°ï¼šåå¥½æŸäº›åŠ¨ä½œ
                target_action = torch.zeros_like(action)
                target_action[0] = 1.0  # åå¥½ç¬¬ä¸€ä¸ªåŠ¨ä½œ
                return -torch.sum((action - target_action) ** 2).item()
            
            # æ”¶é›†åˆå§‹æ€§èƒ½
            initial_rewards = []
            for _ in range(10):
                state = torch.randn(sac_config.state_dim)
                action, _ = sac_agent.get_action(state.unsqueeze(0), deterministic=True)
                reward = reward_function(action.squeeze())
                initial_rewards.append(reward)
            
            initial_avg_reward = np.mean(initial_rewards)
            
            # è®­ç»ƒæ™ºèƒ½ä½“
            for episode in range(50):
                state = torch.randn(sac_config.state_dim)
                action, _ = sac_agent.get_action(state.unsqueeze(0), deterministic=False)
                reward = reward_function(action.squeeze())
                next_state = torch.randn(sac_config.state_dim)
                
                experience = Experience(
                    state=state,
                    action=action.squeeze(),
                    reward=reward,
                    next_state=next_state,
                    done=False
                )
                sac_agent.add_experience(experience)
                
                # å®šæœŸæ›´æ–°
                if episode % 5 == 0:
                    sac_agent.update()
            
            # è¯„ä¼°è®­ç»ƒåŽæ€§èƒ½
            final_rewards = []
            for _ in range(10):
                state = torch.randn(sac_config.state_dim)
                action, _ = sac_agent.get_action(state.unsqueeze(0), deterministic=True)
                reward = reward_function(action.squeeze())
                final_rewards.append(reward)
            
            final_avg_reward = np.mean(final_rewards)
            
            # æ€§èƒ½åº”è¯¥æœ‰æ‰€æ”¹å–„
            assert final_avg_reward > initial_avg_reward - 0.1  # å…è®¸ä¸€äº›å˜å¼‚
            
        def test_policy_stability(self, sac_agent, sample_batch_states):
            """æµ‹è¯•ç­–ç•¥ç¨³å®šæ€§"""
            # åœ¨è¯„ä¼°æ¨¡å¼ä¸‹ï¼Œç­–ç•¥åº”è¯¥æ˜¯ç¨³å®šçš„
            sac_agent.eval()
            
            actions1, _ = sac_agent.get_action(sample_batch_states, deterministic=True)
            actions2, _ = sac_agent.get_action(sample_batch_states, deterministic=True)
            
            assert torch.allclose(actions1, actions2, atol=1e-6)
            
        def test_entropy_regularization(self, sac_agent, sac_config):
            """æµ‹è¯•ç†µæ­£åˆ™åŒ–"""
            # æ·»åŠ ä¸€äº›ç»éªŒ
            for i in range(sac_config.batch_size * 2):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=False
                )
                sac_agent.add_experience(experience)
            
            # è®¡ç®—ç­–ç•¥ç†µ
            states = torch.randn(32, sac_config.state_dim)
            actions, log_probs = sac_agent.get_action(states, deterministic=False)
            
            # ç†µåº”è¯¥ä¸ºæ­£ï¼ˆéšæœºç­–ç•¥ï¼‰
            entropy = -torch.mean(log_probs)
            assert entropy > 0
            
            # æ£€æŸ¥ç†µçš„åˆç†èŒƒå›´
            assert entropy < 10  # ä¸åº”è¯¥è¿‡å¤§
            
        def test_target_network_updates(self, sac_agent, sac_config):
            """æµ‹è¯•ç›®æ ‡ç½‘ç»œæ›´æ–°"""
            # èŽ·å–åˆå§‹ç›®æ ‡ç½‘ç»œå‚æ•°
            initial_target_params = []
            for param in sac_agent.critic.target_network.parameters():
                initial_target_params.append(param.clone())
            
            # æ·»åŠ ç»éªŒå¹¶è®­ç»ƒ
            for i in range(sac_config.batch_size * 2):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=False
                )
                sac_agent.add_experience(experience)
            
            # å¤šæ¬¡æ›´æ–°
            for _ in range(5):
                sac_agent.update()
            
            # æ£€æŸ¥ç›®æ ‡ç½‘ç»œå‚æ•°æ˜¯å¦æ›´æ–°
            final_target_params = []
            for param in sac_agent.critic.target_network.parameters():
                final_target_params.append(param.clone())
            
            # ç›®æ ‡ç½‘ç»œå‚æ•°åº”è¯¥æœ‰æ‰€å˜åŒ–ï¼ˆè½¯æ›´æ–°ï¼‰
            for initial, final in zip(initial_target_params, final_target_params):
                assert not torch.allclose(initial, final, atol=1e-6)
                
        def test_gradient_flow(self, sac_agent, sac_config):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            # æ·»åŠ ç»éªŒ
            for i in range(sac_config.batch_size * 2):
                experience = Experience(
                    state=torch.randn(sac_config.state_dim),
                    action=torch.rand(sac_config.action_dim),
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(sac_config.state_dim),
                    done=False
                )
                sac_agent.add_experience(experience)
            
            # æ›´æ–°å‰æ¸…é›¶æ¢¯åº¦
            sac_agent.actor_optimizer.zero_grad()
            sac_agent.critic_optimizer.zero_grad()
            sac_agent.alpha_optimizer.zero_grad()
            
            # æ‰§è¡Œæ›´æ–°
            losses = sac_agent.update()
            
            # æ£€æŸ¥æ‰€æœ‰ç½‘ç»œéƒ½æœ‰æ¢¯åº¦
            for name, param in sac_agent.actor.named_parameters():
                if param.requires_grad:
                    assert param.grad is not None, f"Actorå‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
                    
            for name, param in sac_agent.critic.main_network.named_parameters():
                if param.requires_grad:
                    assert param.grad is not None, f"Criticå‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
                    
            assert sac_agent.log_alpha.grad is not None, "æ¸©åº¦å‚æ•°æ²¡æœ‰æ¢¯åº¦"
            
        def test_batch_processing(self, sac_agent, sac_config):
            """æµ‹è¯•æ‰¹å¤„ç†èƒ½åŠ›"""
            batch_sizes = [1, 8, 16, 32]
            
            for batch_size in batch_sizes:
                states = torch.randn(batch_size, sac_config.state_dim)
                actions, log_probs = sac_agent.get_action(states, deterministic=False)
                
                assert actions.shape == (batch_size, sac_config.action_dim)
                assert log_probs.shape == (batch_size,)
                
                # æ£€æŸ¥æƒé‡çº¦æŸ
                assert torch.all(actions >= 0)
                weight_sums = torch.sum(actions, dim=1)
                assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5)
                
        def test_device_consistency(self, sac_config):
            """æµ‹è¯•è®¾å¤‡ä¸€è‡´æ€§"""
            if torch.cuda.is_available():
                # æµ‹è¯•CUDAè®¾å¤‡
                cuda_config = SACConfig(
                    state_dim=sac_config.state_dim,
                    action_dim=sac_config.action_dim,
                    device='cuda'
                )
                
                cuda_agent = SimpleSACAgent(cuda_config)
                
                # æ£€æŸ¥æ‰€æœ‰ç»„ä»¶éƒ½åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
                assert next(cuda_agent.actor.parameters()).device.type == 'cuda'
                assert next(cuda_agent.critic.parameters()).device.type == 'cuda'
                assert cuda_agent.log_alpha.device.type == 'cuda'
                
                # æµ‹è¯•å‰å‘ä¼ æ’­
                state = torch.randn(1, sac_config.state_dim).cuda()
                action, log_prob = cuda_agent.get_action(state)
                
                assert action.device.type == 'cuda'
                assert log_prob.device.type == 'cuda'
    ]]></file>
  <file path="tests/unit/test_report_generator.py"><![CDATA[
    """
    å›žæµ‹æŠ¥å‘Šç”Ÿæˆæ¨¡å—çš„å•å…ƒæµ‹è¯•
    æµ‹è¯•HTMLæŠ¥å‘Šç”Ÿæˆå’Œå¯è§†åŒ–å›¾è¡¨ï¼Œæ”¶ç›Šæ›²çº¿ã€æŒä»“åˆ†æžå’Œé£Žé™©åˆ†è§£æŠ¥å‘Šï¼ŒæŠ¥å‘Šçš„å®Œæ•´æ€§å’Œå¯è¯»æ€§
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, date
    from typing import Dict, List, Optional
    from decimal import Decimal
    import tempfile
    import os
    from pathlib import Path
    
    from src.rl_trading_system.evaluation.report_generator import (
        ReportGenerator,
        HTMLReportGenerator,
        ChartGenerator,
        ReportData
    )
    from src.rl_trading_system.backtest.multi_frequency_backtest import Trade, OrderType
    from src.rl_trading_system.evaluation.performance_metrics import PortfolioMetrics
    
    
    class TestReportData:
        """æŠ¥å‘Šæ•°æ®æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_report_data(self):
            """åˆ›å»ºæ ·æœ¬æŠ¥å‘Šæ•°æ®"""
            # åˆ›å»ºæ ·æœ¬æ•°æ®
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)
            
            # ç»„åˆä»·å€¼
            portfolio_values = [1000000]
            for ret in returns:
                portfolio_values.append(portfolio_values[-1] * (1 + ret))
            portfolio_values = pd.Series(portfolio_values[1:], index=dates)
            
            # åŸºå‡†æ”¶ç›ŠçŽ‡
            benchmark_returns = pd.Series(
                np.random.normal(0.0005, 0.015, 252),
                index=dates
            )
            
            # äº¤æ˜“è®°å½•
            trades = [
                Trade("000001.SZ", OrderType.BUY, 1000, Decimal("10.00"), datetime(2023, 1, 2), Decimal("10.00")),
                Trade("000001.SZ", OrderType.SELL, 500, Decimal("11.00"), datetime(2023, 6, 15), Decimal("5.50")),
                Trade("000002.SZ", OrderType.BUY, 2000, Decimal("5.00"), datetime(2023, 3, 20), Decimal("10.00")),
                Trade("000001.SZ", OrderType.SELL, 500, Decimal("10.50"), datetime(2023, 9, 1), Decimal("5.25")),
            ]
            
            # æŒä»“æ•°æ®
            positions_data = {
                '000001.SZ': {'quantity': 0, 'market_value': 0.0, 'weight': 0.0},
                '000002.SZ': {'quantity': 2000, 'market_value': 11000.0, 'weight': 0.01}
            }
            
            return ReportData(
                returns=pd.Series(returns, index=dates),
                portfolio_values=portfolio_values,
                benchmark_returns=benchmark_returns,
                trades=trades,
                positions=positions_data,
                start_date=date(2023, 1, 1),
                end_date=date(2023, 9, 9),
                initial_capital=1000000.0
            )
    
        def test_report_data_creation(self, sample_report_data):
            """æµ‹è¯•æŠ¥å‘Šæ•°æ®åˆ›å»º"""
            assert isinstance(sample_report_data.returns, pd.Series)
            assert isinstance(sample_report_data.portfolio_values, pd.Series)
            assert isinstance(sample_report_data.benchmark_returns, pd.Series)
            assert isinstance(sample_report_data.trades, list)
            assert isinstance(sample_report_data.positions, dict)
            assert sample_report_data.initial_capital == 1000000.0
    
        def test_report_data_validation(self):
            """æµ‹è¯•æŠ¥å‘Šæ•°æ®éªŒè¯"""
            # æµ‹è¯•ç©ºæ”¶ç›ŠçŽ‡åºåˆ—é”™è¯¯
            with pytest.raises(ValueError, match="æ”¶ç›ŠçŽ‡åºåˆ—ä¸èƒ½ä¸ºç©º"):
                ReportData(
                    returns=pd.Series([]),
                    portfolio_values=pd.Series([1000000]),
                    benchmark_returns=pd.Series([0.01]),
                    trades=[],
                    positions={},
                    start_date=date(2023, 1, 1),
                    end_date=date(2023, 12, 31),
                    initial_capital=1000000.0
                )
    
            # æµ‹è¯•é•¿åº¦ä¸åŒ¹é…é”™è¯¯
            with pytest.raises(ValueError, match="æ”¶ç›ŠçŽ‡åºåˆ—å’Œç»„åˆä»·å€¼åºåˆ—é•¿åº¦ä¸åŒ¹é…"):
                ReportData(
                    returns=pd.Series([0.01, 0.02]),
                    portfolio_values=pd.Series([1000000]),  # é•¿åº¦ä¸åŒ¹é…
                    benchmark_returns=pd.Series([0.01, 0.02]),
                    trades=[],
                    positions={},
                    start_date=date(2023, 1, 1),
                    end_date=date(2023, 12, 31),
                    initial_capital=1000000.0
                )
    
        def test_report_data_metrics_calculation(self, sample_report_data):
            """æµ‹è¯•æŠ¥å‘Šæ•°æ®æŒ‡æ ‡è®¡ç®—"""
            metrics = sample_report_data.calculate_metrics()
            
            # éªŒè¯æŒ‡æ ‡ç»“æž„
            assert 'return_metrics' in metrics
            assert 'risk_metrics' in metrics
            assert 'risk_adjusted_metrics' in metrics
            assert 'trading_metrics' in metrics
            
            # éªŒè¯å…·ä½“æŒ‡æ ‡
            assert 'total_return' in metrics['return_metrics']
            assert 'annualized_return' in metrics['return_metrics']
            assert 'volatility' in metrics['risk_metrics']
            assert 'max_drawdown' in metrics['risk_metrics']
            assert 'sharpe_ratio' in metrics['risk_adjusted_metrics']
    
    
    class TestChartGenerator:
        """å›¾è¡¨ç”Ÿæˆå™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def chart_generator(self):
            """åˆ›å»ºå›¾è¡¨ç”Ÿæˆå™¨"""
            return ChartGenerator()
    
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=100, freq='D')
            np.random.seed(42)
            
            returns = pd.Series(np.random.normal(0.001, 0.02, 100), index=dates)
            portfolio_values = pd.Series(np.random.uniform(900000, 1100000, 100), index=dates)
            benchmark_values = pd.Series(np.random.uniform(950000, 1050000, 100), index=dates)
            
            return {
                'returns': returns,
                'portfolio_values': portfolio_values,
                'benchmark_values': benchmark_values,
                'dates': dates
            }
    
        def test_chart_generator_initialization(self, chart_generator):
            """æµ‹è¯•å›¾è¡¨ç”Ÿæˆå™¨åˆå§‹åŒ–"""
            assert chart_generator.figure_size == (12, 8)
            assert chart_generator.style == 'seaborn-v0_8'
    
        def test_returns_chart_generation(self, chart_generator, sample_data):
            """æµ‹è¯•æ”¶ç›ŠçŽ‡å›¾è¡¨ç”Ÿæˆ"""
            chart_html = chart_generator.generate_returns_chart(
                portfolio_values=sample_data['portfolio_values'],
                benchmark_values=sample_data['benchmark_values']
            )
            
            # éªŒè¯è¿”å›žHTMLå­—ç¬¦ä¸²
            assert isinstance(chart_html, str)
            assert len(chart_html) > 0
            assert '<div>' in chart_html  # Plotlyç”Ÿæˆçš„HTMLåº”åŒ…å«divæ ‡ç­¾
    
        def test_drawdown_chart_generation(self, chart_generator, sample_data):
            """æµ‹è¯•å›žæ’¤å›¾è¡¨ç”Ÿæˆ"""
            chart_html = chart_generator.generate_drawdown_chart(
                portfolio_values=sample_data['portfolio_values']
            )
            
            assert isinstance(chart_html, str)
            assert len(chart_html) > 0
            assert '<div>' in chart_html
    
        def test_rolling_metrics_chart_generation(self, chart_generator, sample_data):
            """æµ‹è¯•æ»šåŠ¨æŒ‡æ ‡å›¾è¡¨ç”Ÿæˆ"""
            # åˆ›å»ºæ»šåŠ¨å¤æ™®æ¯”çŽ‡æ•°æ®
            rolling_sharpe = pd.Series(
                np.random.normal(1.5, 0.5, 70),  # 30å¤©çª—å£ï¼Œæ‰€ä»¥æ•°æ®ç‚¹è¾ƒå°‘
                index=sample_data['dates'][30:]
            )
            
            chart_html = chart_generator.generate_rolling_metrics_chart(
                rolling_sharpe, metric_name='å¤æ™®æ¯”çŽ‡'
            )
            
            assert isinstance(chart_html, str)
            assert len(chart_html) > 0
            assert '<div>' in chart_html
    
        def test_position_analysis_chart_generation(self, chart_generator):
            """æµ‹è¯•æŒä»“åˆ†æžå›¾è¡¨ç”Ÿæˆ"""
            positions_data = {
                '000001.SZ': {'weight': 0.4, 'market_value': 400000},
                '000002.SZ': {'weight': 0.3, 'market_value': 300000},
                '000003.SZ': {'weight': 0.2, 'market_value': 200000},
                'çŽ°é‡‘': {'weight': 0.1, 'market_value': 100000}
            }
            
            chart_html = chart_generator.generate_position_analysis_chart(positions_data)
            
            assert isinstance(chart_html, str)
            assert len(chart_html) > 0
            assert '<div>' in chart_html
    
        def test_monthly_returns_heatmap_generation(self, chart_generator, sample_data):
            """æµ‹è¯•æœˆåº¦æ”¶ç›ŠçŽ‡çƒ­åŠ›å›¾ç”Ÿæˆ"""
            # åˆ›å»ºæœˆåº¦æ”¶ç›ŠçŽ‡æ•°æ®
            monthly_returns = sample_data['returns'].groupby(
                sample_data['returns'].index.to_period('M')
            ).apply(lambda x: (1 + x).prod() - 1)
            
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼
            monthly_df = pd.DataFrame({
                'return': monthly_returns.values,
                'year': [p.year for p in monthly_returns.index],
                'month': [p.month for p in monthly_returns.index]
            })
            
            chart_html = chart_generator.generate_monthly_returns_heatmap(monthly_df)
            
            assert isinstance(chart_html, str)
            assert len(chart_html) > 0
            assert '<div>' in chart_html
    
        def test_risk_metrics_radar_chart_generation(self, chart_generator):
            """æµ‹è¯•é£Žé™©æŒ‡æ ‡é›·è¾¾å›¾ç”Ÿæˆ"""
            risk_metrics = {
                'volatility': 0.15,
                'max_drawdown': 0.08,
                'var_95': 0.03,
                'skewness': -0.1,
                'kurtosis': 0.2
            }
            
            chart_html = chart_generator.generate_risk_metrics_radar_chart(risk_metrics)
            
            assert isinstance(chart_html, str)
            assert len(chart_html) > 0
            assert '<div>' in chart_html
    
        def test_trading_analysis_chart_generation(self, chart_generator):
            """æµ‹è¯•äº¤æ˜“åˆ†æžå›¾è¡¨ç”Ÿæˆ"""
            trading_metrics = {
                'win_rate': 0.65,
                'profit_loss_ratio': 1.8,
                'average_win': 0.025,
                'average_loss': 0.015,
                'total_trades': 20
            }
            
            chart_html = chart_generator.generate_trading_analysis_chart(trading_metrics)
            
            assert isinstance(chart_html, str)
            assert len(chart_html) > 0
            assert '<div>' in chart_html
    
        def test_invalid_data_handling(self, chart_generator):
            """æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†"""
            # æµ‹è¯•ç©ºæ•°æ®
            with pytest.raises(ValueError, match="æ•°æ®ä¸èƒ½ä¸ºç©º"):
                chart_generator.generate_returns_chart(pd.Series([]), pd.Series([]))
    
            # æµ‹è¯•é•¿åº¦ä¸åŒ¹é…çš„æ•°æ®
            with pytest.raises(ValueError, match="ç»„åˆä»·å€¼å’ŒåŸºå‡†ä»·å€¼é•¿åº¦ä¸åŒ¹é…"):
                chart_generator.generate_returns_chart(
                    pd.Series([1000000, 1010000]),
                    pd.Series([1000000])  # é•¿åº¦ä¸åŒ¹é…
                )
    
    
    class TestHTMLReportGenerator:
        """HTMLæŠ¥å‘Šç”Ÿæˆå™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def html_generator(self):
            """åˆ›å»ºHTMLæŠ¥å‘Šç”Ÿæˆå™¨"""
            return HTMLReportGenerator()
    
        @pytest.fixture
        def sample_report_data(self):
            """åˆ›å»ºæ ·æœ¬æŠ¥å‘Šæ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=100, freq='D')
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 100)
            
            portfolio_values = [1000000]
            for ret in returns:
                portfolio_values.append(portfolio_values[-1] * (1 + ret))
            portfolio_values = pd.Series(portfolio_values[1:], index=dates)
            
            benchmark_returns = pd.Series(
                np.random.normal(0.0005, 0.015, 100),
                index=dates
            )
            
            trades = [
                Trade("000001.SZ", OrderType.BUY, 1000, Decimal("10.00"), datetime(2023, 1, 2), Decimal("10.00")),
                Trade("000002.SZ", OrderType.BUY, 2000, Decimal("5.00"), datetime(2023, 2, 15), Decimal("10.00")),
            ]
            
            positions = {
                '000001.SZ': {'quantity': 1000, 'market_value': 11000, 'weight': 0.4},
                '000002.SZ': {'quantity': 2000, 'market_value': 10000, 'weight': 0.35},
                'çŽ°é‡‘': {'quantity': 0, 'market_value': 7000, 'weight': 0.25}
            }
            
            return ReportData(
                returns=pd.Series(returns, index=dates),
                portfolio_values=portfolio_values,
                benchmark_returns=benchmark_returns,
                trades=trades,
                positions=positions,
                start_date=date(2023, 1, 1),
                end_date=date(2023, 4, 10),
                initial_capital=1000000.0
            )
    
        def test_html_generator_initialization(self, html_generator):
            """æµ‹è¯•HTMLç”Ÿæˆå™¨åˆå§‹åŒ–"""
            assert html_generator.template_dir is not None
            assert isinstance(html_generator.chart_generator, ChartGenerator)
    
        def test_summary_section_generation(self, html_generator, sample_report_data):
            """æµ‹è¯•æ‘˜è¦éƒ¨åˆ†ç”Ÿæˆ"""
            metrics = sample_report_data.calculate_metrics()
            summary_html = html_generator._generate_summary_section(
                sample_report_data, metrics
            )
            
            assert isinstance(summary_html, str)
            assert len(summary_html) > 0
            assert 'æ€»æ”¶ç›ŠçŽ‡' in summary_html
            assert 'å¹´åŒ–æ”¶ç›ŠçŽ‡' in summary_html
            assert 'æœ€å¤§å›žæ’¤' in summary_html
            assert 'å¤æ™®æ¯”çŽ‡' in summary_html
    
        def test_performance_section_generation(self, html_generator, sample_report_data):
            """æµ‹è¯•ç»©æ•ˆåˆ†æžéƒ¨åˆ†ç”Ÿæˆ"""
            metrics = sample_report_data.calculate_metrics()
            performance_html = html_generator._generate_performance_section(
                sample_report_data, metrics
            )
            
            assert isinstance(performance_html, str)
            assert len(performance_html) > 0
            assert 'ç»©æ•ˆåˆ†æž' in performance_html or 'Performance Analysis' in performance_html
    
        def test_risk_section_generation(self, html_generator, sample_report_data):
            """æµ‹è¯•é£Žé™©åˆ†æžéƒ¨åˆ†ç”Ÿæˆ"""
            metrics = sample_report_data.calculate_metrics()
            risk_html = html_generator._generate_risk_section(metrics)
            
            assert isinstance(risk_html, str)
            assert len(risk_html) > 0
            assert 'é£Žé™©åˆ†æž' in risk_html or 'Risk Analysis' in risk_html
    
        def test_trading_section_generation(self, html_generator, sample_report_data):
            """æµ‹è¯•äº¤æ˜“åˆ†æžéƒ¨åˆ†ç”Ÿæˆ"""
            metrics = sample_report_data.calculate_metrics()
            trading_html = html_generator._generate_trading_section(
                sample_report_data, metrics
            )
            
            assert isinstance(trading_html, str)
            assert len(trading_html) > 0
            assert 'äº¤æ˜“åˆ†æž' in trading_html or 'Trading Analysis' in trading_html
    
        def test_positions_section_generation(self, html_generator, sample_report_data):
            """æµ‹è¯•æŒä»“åˆ†æžéƒ¨åˆ†ç”Ÿæˆ"""
            positions_html = html_generator._generate_positions_section(sample_report_data)
            
            assert isinstance(positions_html, str)
            assert len(positions_html) > 0
            assert 'æŒä»“åˆ†æž' in positions_html or 'Positions Analysis' in positions_html
    
        def test_complete_report_generation(self, html_generator, sample_report_data):
            """æµ‹è¯•å®Œæ•´æŠ¥å‘Šç”Ÿæˆ"""
            # ä½¿ç”¨ä¸´æ—¶æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                temp_path = f.name
            
            # ç”ŸæˆæŠ¥å‘Š
            html_generator.generate_report(sample_report_data, temp_path)
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            assert os.path.exists(temp_path)
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            with open(temp_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            assert len(content) > 0
            assert '<html' in content
            assert '</html>' in content
            assert 'æŠ•èµ„ç»„åˆåˆ†æžæŠ¥å‘Š' in content or 'Portfolio Analysis Report' in content
            
            # éªŒè¯åŒ…å«ä¸»è¦éƒ¨åˆ†
            assert 'ç»©æ•ˆæ‘˜è¦' in content or 'Performance Summary' in content
            assert 'é£Žé™©åˆ†æž' in content or 'Risk Analysis' in content
            assert 'äº¤æ˜“åˆ†æž' in content or 'Trading Analysis' in content
            assert 'æŒä»“åˆ†æž' in content or 'Positions Analysis' in content
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)
    
        def test_report_with_benchmark_comparison(self, html_generator, sample_report_data):
            """æµ‹è¯•åŒ…å«åŸºå‡†æ¯”è¾ƒçš„æŠ¥å‘Šç”Ÿæˆ"""
            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                temp_path = f.name
            
            # ç”ŸæˆåŒ…å«åŸºå‡†æ¯”è¾ƒçš„æŠ¥å‘Š
            html_generator.generate_report(
                sample_report_data, 
                temp_path,
                include_benchmark=True
            )
            
            # éªŒè¯æ–‡ä»¶å†…å®¹
            with open(temp_path, 'r', encoding='utf-8') as f:
                content = f.read()
            
            assert 'åŸºå‡†æ¯”è¾ƒ' in content or 'Benchmark Comparison' in content
            assert 'è¶…é¢æ”¶ç›Š' in content or 'Excess Return' in content
            
            # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
            os.unlink(temp_path)
    
        def test_custom_template_usage(self, sample_report_data):
            """æµ‹è¯•è‡ªå®šä¹‰æ¨¡æ¿ä½¿ç”¨"""
            # åˆ›å»ºä¸´æ—¶æ¨¡æ¿ç›®å½•
            with tempfile.TemporaryDirectory() as temp_dir:
                template_path = Path(temp_dir) / "custom_template.html"
                
                # åˆ›å»ºç®€å•çš„è‡ªå®šä¹‰æ¨¡æ¿
                custom_template = """
                <!DOCTYPE html>
                <html>
                <head><title>Custom Report</title></head>
                <body>
                    <h1>è‡ªå®šä¹‰æŠ¥å‘Š</h1>
                    <p>æ€»æ”¶ç›ŠçŽ‡: {{total_return}}</p>
                    <p>å¤æ™®æ¯”çŽ‡: {{sharpe_ratio}}</p>
                </body>
                </html>
                """
                
                with open(template_path, 'w', encoding='utf-8') as f:
                    f.write(custom_template)
                
                # ä½¿ç”¨è‡ªå®šä¹‰æ¨¡æ¿ç”Ÿæˆå™¨
                custom_generator = HTMLReportGenerator(template_dir=temp_dir)
                
                with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                    report_path = f.name
                
                custom_generator.generate_report(
                    sample_report_data, 
                    report_path,
                    template_name="custom_template.html"
                )
                
                # éªŒè¯è‡ªå®šä¹‰æ¨¡æ¿ç”Ÿæ•ˆ
                with open(report_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                assert 'è‡ªå®šä¹‰æŠ¥å‘Š' in content
                assert 'Custom Report' in content
                
                # æ¸…ç†
                os.unlink(report_path)
    
        def test_invalid_template_error(self, html_generator, sample_report_data):
            """æµ‹è¯•æ— æ•ˆæ¨¡æ¿é”™è¯¯"""
            with tempfile.NamedTemporaryFile(mode='w', suffix='.html', delete=False) as f:
                temp_path = f.name
            
            # æµ‹è¯•ä¸å­˜åœ¨çš„æ¨¡æ¿
            with pytest.raises(FileNotFoundError, match="æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨|æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨"):
                html_generator.generate_report(
                    sample_report_data,
                    temp_path,
                    template_name="nonexistent_template.html"
                )
            
            # æ¸…ç†
            os.unlink(temp_path)
    
        def test_output_directory_creation(self, html_generator, sample_report_data):
            """æµ‹è¯•è¾“å‡ºç›®å½•åˆ›å»º"""
            with tempfile.TemporaryDirectory() as temp_dir:
                # ä½¿ç”¨ä¸å­˜åœ¨çš„å­ç›®å½•
                output_path = Path(temp_dir) / "reports" / "new_report.html"
                
                html_generator.generate_report(sample_report_data, str(output_path))
                
                # éªŒè¯ç›®å½•è¢«åˆ›å»º
                assert output_path.parent.exists()
                assert output_path.exists()
    
    
    class TestReportGenerator:
        """æŠ¥å‘Šç”Ÿæˆå™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def report_generator(self):
            """åˆ›å»ºæŠ¥å‘Šç”Ÿæˆå™¨"""
            return ReportGenerator()
    
        @pytest.fixture
        def sample_report_data(self):
            """åˆ›å»ºæ ·æœ¬æŠ¥å‘Šæ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=50, freq='D')
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 50)
            
            portfolio_values = [1000000]
            for ret in returns:
                portfolio_values.append(portfolio_values[-1] * (1 + ret))
            portfolio_values = pd.Series(portfolio_values[1:], index=dates)
            
            benchmark_returns = pd.Series(
                np.random.normal(0.0005, 0.015, 50),
                index=dates
            )
            
            trades = [
                Trade("000001.SZ", OrderType.BUY, 1000, Decimal("10.00"), datetime(2023, 1, 2), Decimal("10.00")),
            ]
            
            positions = {
                '000001.SZ': {'quantity': 1000, 'market_value': 11000, 'weight': 1.0}
            }
            
            return ReportData(
                returns=pd.Series(returns, index=dates),
                portfolio_values=portfolio_values,
                benchmark_returns=benchmark_returns,
                trades=trades,
                positions=positions,
                start_date=date(2023, 1, 1),
                end_date=date(2023, 2, 19),
                initial_capital=1000000.0
            )
    
        def test_report_generator_initialization(self, report_generator):
            """æµ‹è¯•æŠ¥å‘Šç”Ÿæˆå™¨åˆå§‹åŒ–"""
            assert isinstance(report_generator.html_generator, HTMLReportGenerator)
    
        def test_generate_comprehensive_report(self, report_generator, sample_report_data):
            """æµ‹è¯•ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
            with tempfile.TemporaryDirectory() as temp_dir:
                output_path = Path(temp_dir) / "comprehensive_report.html"
                
                report_generator.generate_comprehensive_report(
                    sample_report_data,
                    str(output_path)
                )
                
                # éªŒè¯æŠ¥å‘Šæ–‡ä»¶å­˜åœ¨
                assert output_path.exists()
                
                # éªŒè¯æŠ¥å‘Šå†…å®¹
                with open(output_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                assert len(content) > 1000  # ç¡®ä¿æŠ¥å‘Šæœ‰å®žè´¨å†…å®¹
                assert '<html' in content
                assert '</html>' in content
    
        def test_batch_report_generation(self, report_generator):
            """æµ‹è¯•æ‰¹é‡æŠ¥å‘Šç”Ÿæˆ"""
            # åˆ›å»ºå¤šä¸ªæŠ¥å‘Šæ•°æ®
            report_data_list = []
            for i in range(3):
                dates = pd.date_range(f'2023-0{i+1}-01', periods=30, freq='D')
                np.random.seed(42 + i)
                returns = np.random.normal(0.001, 0.02, 30)
                
                portfolio_values = [1000000]
                for ret in returns:
                    portfolio_values.append(portfolio_values[-1] * (1 + ret))
                portfolio_values = pd.Series(portfolio_values[1:], index=dates)
                
                report_data = ReportData(
                    returns=pd.Series(returns, index=dates),
                    portfolio_values=portfolio_values,
                    benchmark_returns=pd.Series(np.random.normal(0.0005, 0.015, 30), index=dates),
                    trades=[],
                    positions={},
                    start_date=dates[0].date(),
                    end_date=dates[-1].date(),
                    initial_capital=1000000.0
                )
                report_data_list.append((f"report_{i+1}", report_data))
            
            with tempfile.TemporaryDirectory() as temp_dir:
                report_generator.generate_batch_reports(
                    report_data_list,
                    temp_dir
                )
                
                # éªŒè¯æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶éƒ½è¢«åˆ›å»º
                report_files = list(Path(temp_dir).glob("*.html"))
                assert len(report_files) == 3
                
                # éªŒè¯æ–‡ä»¶åæ­£ç¡®
                expected_files = {"report_1.html", "report_2.html", "report_3.html"}
                actual_files = {f.name for f in report_files}
                assert actual_files == expected_files
    
        def test_report_comparison(self, report_generator):
            """æµ‹è¯•æŠ¥å‘Šå¯¹æ¯”åŠŸèƒ½"""
            # åˆ›å»ºä¸¤ä¸ªæŠ¥å‘Šæ•°æ®è¿›è¡Œå¯¹æ¯”
            dates = pd.date_range('2023-01-01', periods=50, freq='D')
            
            # ç­–ç•¥A
            np.random.seed(42)
            returns_a = np.random.normal(0.002, 0.02, 50)  # æ›´é«˜æ”¶ç›Š
            portfolio_values_a = [1000000]
            for ret in returns_a:
                portfolio_values_a.append(portfolio_values_a[-1] * (1 + ret))
            portfolio_values_a = pd.Series(portfolio_values_a[1:], index=dates)
            
            report_data_a = ReportData(
                returns=pd.Series(returns_a, index=dates),
                portfolio_values=portfolio_values_a,
                benchmark_returns=pd.Series(np.random.normal(0.0005, 0.015, 50), index=dates),
                trades=[],
                positions={},
                start_date=date(2023, 1, 1),
                end_date=date(2023, 2, 19),
                initial_capital=1000000.0
            )
            
            # ç­–ç•¥B
            np.random.seed(43)
            returns_b = np.random.normal(0.001, 0.015, 50)  # æ›´ä½Žé£Žé™©
            portfolio_values_b = [1000000]
            for ret in returns_b:
                portfolio_values_b.append(portfolio_values_b[-1] * (1 + ret))
            portfolio_values_b = pd.Series(portfolio_values_b[1:], index=dates)
            
            report_data_b = ReportData(
                returns=pd.Series(returns_b, index=dates),
                portfolio_values=portfolio_values_b,
                benchmark_returns=pd.Series(np.random.normal(0.0005, 0.015, 50), index=dates),
                trades=[],
                positions={},
                start_date=date(2023, 1, 1),
                end_date=date(2023, 2, 19),
                initial_capital=1000000.0
            )
            
            with tempfile.TemporaryDirectory() as temp_dir:
                output_path = Path(temp_dir) / "comparison_report.html"
                
                report_generator.generate_comparison_report(
                    {"ç­–ç•¥A": report_data_a, "ç­–ç•¥B": report_data_b},
                    str(output_path)
                )
                
                # éªŒè¯å¯¹æ¯”æŠ¥å‘Š
                assert output_path.exists()
                
                with open(output_path, 'r', encoding='utf-8') as f:
                    content = f.read()
                
                assert 'ç­–ç•¥A' in content
                assert 'ç­–ç•¥B' in content
                assert 'å¯¹æ¯”åˆ†æž' in content or 'Comparison Analysis' in content
    ]]></file>
  <file path="tests/unit/test_replay_buffer.py"><![CDATA[
    """
    æµ‹è¯•ç»éªŒå›žæ”¾ç¼“å†²åŒºçš„å•å…ƒæµ‹è¯•
    """
    import pytest
    import torch
    import numpy as np
    import multiprocessing as mp
    from unittest.mock import Mock, patch
    from dataclasses import dataclass
    from typing import List, Tuple, Dict, Any
    
    from src.rl_trading_system.models.replay_buffer import (
        ReplayBuffer, 
        PrioritizedReplayBuffer, 
        Experience, 
        ReplayBufferConfig
    )
    
    
    @dataclass
    class MockExperience:
        """æ¨¡æ‹Ÿç»éªŒæ•°æ®"""
        state: torch.Tensor
        action: torch.Tensor
        reward: float
        next_state: torch.Tensor
        done: bool
        info: Dict[str, Any] = None
    
    
    class TestReplayBuffer:
        """ç»éªŒå›žæ”¾ç¼“å†²åŒºæµ‹è¯•ç±»"""
        
        @pytest.fixture
        def buffer_config(self):
            """ç¼“å†²åŒºé…ç½®fixture"""
            return ReplayBufferConfig(
                capacity=1000,
                batch_size=32,
                state_dim=256,
                action_dim=100,
                device='cpu'
            )
        
        @pytest.fixture
        def replay_buffer(self, buffer_config):
            """å›žæ”¾ç¼“å†²åŒºfixture"""
            return ReplayBuffer(buffer_config)
        
        @pytest.fixture
        def sample_experience(self, buffer_config):
            """æ ·æœ¬ç»éªŒfixture"""
            return Experience(
                state=torch.randn(buffer_config.state_dim),
                action=torch.rand(buffer_config.action_dim),
                reward=np.random.uniform(-1, 1),
                next_state=torch.randn(buffer_config.state_dim),
                done=np.random.choice([True, False]),
                info={'step': 0}
            )
        
        def test_buffer_initialization(self, replay_buffer, buffer_config):
            """æµ‹è¯•ç¼“å†²åŒºåˆå§‹åŒ–"""
            assert replay_buffer.capacity == buffer_config.capacity
            assert replay_buffer.batch_size == buffer_config.batch_size
            assert replay_buffer.size == 0
            assert replay_buffer.position == 0
            assert len(replay_buffer.buffer) == buffer_config.capacity
            
        def test_add_experience(self, replay_buffer, sample_experience):
            """æµ‹è¯•æ·»åŠ ç»éªŒ"""
            initial_size = replay_buffer.size
            replay_buffer.add(sample_experience)
            
            assert replay_buffer.size == initial_size + 1
            assert replay_buffer.position == 1
            
            # æ£€æŸ¥å­˜å‚¨çš„ç»éªŒ
            stored_exp = replay_buffer.buffer[0]
            assert torch.allclose(stored_exp.state, sample_experience.state)
            assert torch.allclose(stored_exp.action, sample_experience.action)
            assert stored_exp.reward == sample_experience.reward
            assert stored_exp.done == sample_experience.done
            
        def test_buffer_overflow(self, replay_buffer, buffer_config):
            """æµ‹è¯•ç¼“å†²åŒºæº¢å‡ºå¤„ç†"""
            # å¡«æ»¡ç¼“å†²åŒº
            for i in range(buffer_config.capacity + 10):
                exp = Experience(
                    state=torch.randn(buffer_config.state_dim),
                    action=torch.rand(buffer_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(buffer_config.state_dim),
                    done=False,
                    info={'step': i}
                )
                replay_buffer.add(exp)
            
            # ç¼“å†²åŒºå¤§å°ä¸åº”è¶…è¿‡å®¹é‡
            assert replay_buffer.size == buffer_config.capacity
            
            # ä½ç½®åº”è¯¥å¾ªçŽ¯
            assert replay_buffer.position == 10
            
            # æ£€æŸ¥æœ€æ–°çš„ç»éªŒæ˜¯å¦æ­£ç¡®å­˜å‚¨
            latest_exp = replay_buffer.buffer[9]  # position - 1
            assert latest_exp.reward == float(buffer_config.capacity + 9)
            
        def test_sample_batch(self, replay_buffer, buffer_config):
            """æµ‹è¯•æ‰¹é‡é‡‡æ ·"""
            # æ·»åŠ è¶³å¤Ÿçš„ç»éªŒ
            experiences = []
            for i in range(buffer_config.batch_size * 2):
                exp = Experience(
                    state=torch.randn(buffer_config.state_dim),
                    action=torch.rand(buffer_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(buffer_config.state_dim),
                    done=i % 10 == 0,
                    info={'step': i}
                )
                experiences.append(exp)
                replay_buffer.add(exp)
            
            # é‡‡æ ·æ‰¹æ¬¡
            batch = replay_buffer.sample()
            
            assert len(batch) == buffer_config.batch_size
            assert all(isinstance(exp, Experience) for exp in batch)
            
            # æ£€æŸ¥æ‰¹æ¬¡æ•°æ®çš„å½¢çŠ¶
            states = torch.stack([exp.state for exp in batch])
            actions = torch.stack([exp.action for exp in batch])
            
            assert states.shape == (buffer_config.batch_size, buffer_config.state_dim)
            assert actions.shape == (buffer_config.batch_size, buffer_config.action_dim)
            
        def test_sample_insufficient_data(self, replay_buffer, buffer_config):
            """æµ‹è¯•æ•°æ®ä¸è¶³æ—¶çš„é‡‡æ ·"""
            # åªæ·»åŠ å°‘é‡ç»éªŒ
            for i in range(buffer_config.batch_size // 2):
                exp = Experience(
                    state=torch.randn(buffer_config.state_dim),
                    action=torch.rand(buffer_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(buffer_config.state_dim),
                    done=False
                )
                replay_buffer.add(exp)
            
            # åº”è¯¥æŠ›å‡ºå¼‚å¸¸æˆ–è¿”å›žæ‰€æœ‰å¯ç”¨æ•°æ®
            with pytest.raises(ValueError):
                replay_buffer.sample()
                
        def test_can_sample(self, replay_buffer, buffer_config):
            """æµ‹è¯•æ˜¯å¦å¯ä»¥é‡‡æ ·"""
            # åˆå§‹çŠ¶æ€ä¸èƒ½é‡‡æ ·
            assert not replay_buffer.can_sample()
            
            # æ·»åŠ è¶³å¤Ÿçš„ç»éªŒåŽå¯ä»¥é‡‡æ ·
            for i in range(buffer_config.batch_size):
                exp = Experience(
                    state=torch.randn(buffer_config.state_dim),
                    action=torch.rand(buffer_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(buffer_config.state_dim),
                    done=False
                )
                replay_buffer.add(exp)
            
            assert replay_buffer.can_sample()
            
        def test_clear_buffer(self, replay_buffer, buffer_config):
            """æµ‹è¯•æ¸…ç©ºç¼“å†²åŒº"""
            # æ·»åŠ ä¸€äº›ç»éªŒ
            for i in range(10):
                exp = Experience(
                    state=torch.randn(buffer_config.state_dim),
                    action=torch.rand(buffer_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(buffer_config.state_dim),
                    done=False
                )
                replay_buffer.add(exp)
            
            assert replay_buffer.size > 0
            
            # æ¸…ç©ºç¼“å†²åŒº
            replay_buffer.clear()
            
            assert replay_buffer.size == 0
            assert replay_buffer.position == 0
            
        def test_get_all_experiences(self, replay_buffer, buffer_config):
            """æµ‹è¯•èŽ·å–æ‰€æœ‰ç»éªŒ"""
            experiences = []
            for i in range(50):
                exp = Experience(
                    state=torch.randn(buffer_config.state_dim),
                    action=torch.rand(buffer_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(buffer_config.state_dim),
                    done=False,
                    info={'step': i}
                )
                experiences.append(exp)
                replay_buffer.add(exp)
            
            all_exp = replay_buffer.get_all_experiences()
            
            assert len(all_exp) == 50
            assert all(isinstance(exp, Experience) for exp in all_exp)
            
            # æ£€æŸ¥é¡ºåºæ˜¯å¦æ­£ç¡®
            for i, exp in enumerate(all_exp):
                assert exp.info['step'] == i
                
        def test_memory_efficiency(self, buffer_config):
            """æµ‹è¯•å†…å­˜æ•ˆçŽ‡"""
            # åˆ›å»ºå¤§å®¹é‡ç¼“å†²åŒº
            large_config = ReplayBufferConfig(
                capacity=10000,
                batch_size=64,
                state_dim=buffer_config.state_dim,
                action_dim=buffer_config.action_dim
            )
            
            large_buffer = ReplayBuffer(large_config)
            
            # æ·»åŠ ç»éªŒå¹¶æ£€æŸ¥å†…å­˜ä½¿ç”¨
            for i in range(1000):
                exp = Experience(
                    state=torch.randn(buffer_config.state_dim),
                    action=torch.rand(buffer_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(buffer_config.state_dim),
                    done=False
                )
                large_buffer.add(exp)
            
            # ç¼“å†²åŒºåº”è¯¥æ­£å¸¸å·¥ä½œ
            assert large_buffer.size == 1000
            assert large_buffer.can_sample()
            
            batch = large_buffer.sample()
            assert len(batch) == large_config.batch_size
    
    
    class TestPrioritizedReplayBuffer:
        """ä¼˜å…ˆçº§å›žæ”¾ç¼“å†²åŒºæµ‹è¯•ç±»"""
        
        @pytest.fixture
        def priority_config(self):
            """ä¼˜å…ˆçº§ç¼“å†²åŒºé…ç½®fixture"""
            return ReplayBufferConfig(
                capacity=1000,
                batch_size=32,
                state_dim=256,
                action_dim=100,
                alpha=0.6,
                beta=0.4,
                beta_increment=0.001,
                epsilon=1e-6,
                device='cpu'
            )
        
        @pytest.fixture
        def priority_buffer(self, priority_config):
            """ä¼˜å…ˆçº§å›žæ”¾ç¼“å†²åŒºfixture"""
            return PrioritizedReplayBuffer(priority_config)
        
        def test_priority_buffer_initialization(self, priority_buffer, priority_config):
            """æµ‹è¯•ä¼˜å…ˆçº§ç¼“å†²åŒºåˆå§‹åŒ–"""
            assert priority_buffer.capacity == priority_config.capacity
            assert priority_buffer.alpha == priority_config.alpha
            assert priority_buffer.beta == priority_config.beta
            assert priority_buffer.beta_increment == priority_config.beta_increment
            assert hasattr(priority_buffer, 'priorities')
            assert hasattr(priority_buffer, 'max_priority')
            
        def test_add_with_priority(self, priority_buffer, priority_config):
            """æµ‹è¯•å¸¦ä¼˜å…ˆçº§çš„æ·»åŠ """
            exp = Experience(
                state=torch.randn(priority_config.state_dim),
                action=torch.rand(priority_config.action_dim),
                reward=1.0,
                next_state=torch.randn(priority_config.state_dim),
                done=False
            )
            
            initial_max_priority = priority_buffer.max_priority
            priority_buffer.add(exp, priority=0.8)
            
            assert priority_buffer.size == 1
            assert priority_buffer.max_priority >= initial_max_priority
            
        def test_priority_sampling(self, priority_buffer, priority_config):
            """æµ‹è¯•ä¼˜å…ˆçº§é‡‡æ ·"""
            # æ·»åŠ ä¸åŒä¼˜å…ˆçº§çš„ç»éªŒ
            priorities = [0.1, 0.5, 0.9, 0.3, 0.7]
            
            for i, priority in enumerate(priorities * 10):  # é‡å¤ä»¥èŽ·å¾—è¶³å¤Ÿçš„æ ·æœ¬
                exp = Experience(
                    state=torch.randn(priority_config.state_dim),
                    action=torch.rand(priority_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(priority_config.state_dim),
                    done=False,
                    info={'priority': priority}
                )
                priority_buffer.add(exp, priority=priority)
            
            # é‡‡æ ·æ‰¹æ¬¡
            batch, indices, weights = priority_buffer.sample()
            
            assert len(batch) == priority_config.batch_size
            assert len(indices) == priority_config.batch_size
            assert len(weights) == priority_config.batch_size
            
            # æ£€æŸ¥é‡è¦æ€§æƒé‡
            assert torch.all(weights > 0)
            assert torch.all(torch.isfinite(weights))
            
        def test_update_priorities(self, priority_buffer, priority_config):
            """æµ‹è¯•æ›´æ–°ä¼˜å…ˆçº§"""
            # æ·»åŠ ä¸€äº›ç»éªŒ
            for i in range(priority_config.batch_size * 2):
                exp = Experience(
                    state=torch.randn(priority_config.state_dim),
                    action=torch.rand(priority_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(priority_config.state_dim),
                    done=False
                )
                priority_buffer.add(exp, priority=0.5)
            
            # é‡‡æ ·å¹¶æ›´æ–°ä¼˜å…ˆçº§
            batch, indices, weights = priority_buffer.sample()
            
            new_priorities = torch.rand(len(indices)) + 0.1  # é¿å…é›¶ä¼˜å…ˆçº§
            priority_buffer.update_priorities(indices, new_priorities)
            
            # éªŒè¯ä¼˜å…ˆçº§å·²æ›´æ–°ï¼ˆè€ƒè™‘alphaæŒ‡æ•°ï¼‰
            for idx, new_priority in zip(indices, new_priorities):
                stored_priority = priority_buffer.priorities[idx]
                expected_priority = (new_priority.item() ** priority_config.alpha)
                assert abs(stored_priority - expected_priority) < 1e-6
                
        def test_beta_annealing(self, priority_buffer, priority_config):
            """æµ‹è¯•betaå‚æ•°é€€ç«"""
            # å…ˆæ·»åŠ è¶³å¤Ÿçš„ç»éªŒ
            for i in range(priority_config.batch_size * 2):
                exp = Experience(
                    state=torch.randn(priority_config.state_dim),
                    action=torch.rand(priority_config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(priority_config.state_dim),
                    done=False
                )
                priority_buffer.add(exp, priority=0.5)
            
            initial_beta = priority_buffer.beta
            
            # å¤šæ¬¡é‡‡æ ·åº”è¯¥å¢žåŠ beta
            for _ in range(100):
                if priority_buffer.can_sample():
                    priority_buffer.sample()
            
            assert priority_buffer.beta > initial_beta
            assert priority_buffer.beta <= 1.0
            
        def test_importance_sampling_weights(self, priority_buffer, priority_config):
            """æµ‹è¯•é‡è¦æ€§é‡‡æ ·æƒé‡è®¡ç®—"""
            # æ·»åŠ ä¸åŒä¼˜å…ˆçº§çš„ç»éªŒ
            high_priority_exp = Experience(
                state=torch.randn(priority_config.state_dim),
                action=torch.rand(priority_config.action_dim),
                reward=1.0,
                next_state=torch.randn(priority_config.state_dim),
                done=False
            )
            
            low_priority_exp = Experience(
                state=torch.randn(priority_config.state_dim),
                action=torch.rand(priority_config.action_dim),
                reward=0.0,
                next_state=torch.randn(priority_config.state_dim),
                done=False
            )
            
            # æ·»åŠ å¤šä¸ªé«˜ä¼˜å…ˆçº§å’Œä½Žä¼˜å…ˆçº§ç»éªŒ
            for _ in range(priority_config.batch_size):
                priority_buffer.add(high_priority_exp, priority=0.9)
                priority_buffer.add(low_priority_exp, priority=0.1)
            
            # é‡‡æ ·å¤šæ¬¡å¹¶æ£€æŸ¥æƒé‡åˆ†å¸ƒ
            weight_sums = []
            for _ in range(10):
                batch, indices, weights = priority_buffer.sample()
                weight_sums.append(torch.sum(weights).item())
            
            # æƒé‡åº”è¯¥æœ‰åˆç†çš„åˆ†å¸ƒ
            avg_weight_sum = np.mean(weight_sums)
            assert avg_weight_sum > 0
            assert avg_weight_sum < priority_config.batch_size * 10  # ä¸åº”è¯¥è¿‡å¤§
    
    
    class TestMultiprocessReplayBuffer:
        """å¤šè¿›ç¨‹å›žæ”¾ç¼“å†²åŒºæµ‹è¯•ç±»"""
        
        @pytest.fixture
        def mp_config(self):
            """å¤šè¿›ç¨‹é…ç½®fixture"""
            return ReplayBufferConfig(
                capacity=1000,
                batch_size=32,
                state_dim=64,  # è¾ƒå°çš„ç»´åº¦ä»¥åŠ å¿«æµ‹è¯•
                action_dim=10,
                n_workers=2,
                device='cpu'
            )
        
        def test_multiprocess_add(self, mp_config):
            """æµ‹è¯•å¤šè¿›ç¨‹æ·»åŠ ç»éªŒ"""
            # è¿™ä¸ªæµ‹è¯•éœ€è¦å®žé™…çš„å¤šè¿›ç¨‹å®žçŽ°
            # è¿™é‡Œæä¾›æµ‹è¯•æ¡†æž¶
            pass
        
        def test_concurrent_sampling(self, mp_config):
            """æµ‹è¯•å¹¶å‘é‡‡æ ·"""
            # æµ‹è¯•å¤šä¸ªè¿›ç¨‹åŒæ—¶é‡‡æ ·çš„æƒ…å†µ
            pass
        
        def test_data_consistency(self, mp_config):
            """æµ‹è¯•æ•°æ®ä¸€è‡´æ€§"""
            # æµ‹è¯•å¤šè¿›ç¨‹çŽ¯å¢ƒä¸‹çš„æ•°æ®ä¸€è‡´æ€§
            pass
    
    
    class TestReplayBufferIntegration:
        """å›žæ”¾ç¼“å†²åŒºé›†æˆæµ‹è¯•"""
        
        def test_with_actor_critic(self):
            """æµ‹è¯•ä¸ŽActor-Criticç½‘ç»œçš„é›†æˆ"""
            # åˆ›å»ºæ¨¡æ‹Ÿçš„Actorå’ŒCriticç½‘ç»œ
            from src.rl_trading_system.models.actor_network import Actor, ActorConfig
            from src.rl_trading_system.models.critic_network import Critic, CriticConfig
            
            actor_config = ActorConfig(state_dim=64, action_dim=10, hidden_dim=128)
            critic_config = CriticConfig(state_dim=64, action_dim=10, hidden_dim=128)
            buffer_config = ReplayBufferConfig(capacity=1000, batch_size=16, state_dim=64, action_dim=10)
            
            actor = Actor(actor_config)
            critic = Critic(critic_config)
            buffer = ReplayBuffer(buffer_config)
            
            # ç”Ÿæˆä¸€äº›ç»éªŒ
            for i in range(50):
                state = torch.randn(64)
                action, _ = actor.get_action(state.unsqueeze(0), deterministic=True)
                action = action.squeeze(0)
                
                exp = Experience(
                    state=state,
                    action=action,
                    reward=np.random.uniform(-1, 1),
                    next_state=torch.randn(64),
                    done=np.random.choice([True, False])
                )
                buffer.add(exp)
            
            # é‡‡æ ·å¹¶æµ‹è¯•ç½‘ç»œ
            if buffer.can_sample():
                batch = buffer.sample()
                
                states = torch.stack([exp.state for exp in batch])
                actions = torch.stack([exp.action for exp in batch])
                
                # æµ‹è¯•Criticç½‘ç»œ
                q_values = critic(states, actions)
                assert q_values.shape == (len(batch), 1)
                
                # æµ‹è¯•Actorç½‘ç»œ
                new_actions, log_probs = actor.get_action(states)
                assert new_actions.shape == (len(batch), 10)
                assert log_probs.shape == (len(batch),)
                
        def test_memory_leak_prevention(self):
            """æµ‹è¯•å†…å­˜æ³„æ¼é¢„é˜²"""
            config = ReplayBufferConfig(capacity=100, batch_size=16, state_dim=32, action_dim=5)
            buffer = ReplayBuffer(config)
            
            # å¤§é‡æ·»åŠ å’Œé‡‡æ ·æ“ä½œ
            for epoch in range(10):
                # æ·»åŠ ç»éªŒ
                for i in range(config.capacity):
                    exp = Experience(
                        state=torch.randn(config.state_dim),
                        action=torch.rand(config.action_dim),
                        reward=np.random.uniform(-1, 1),
                        next_state=torch.randn(config.state_dim),
                        done=np.random.choice([True, False])
                    )
                    buffer.add(exp)
                
                # å¤šæ¬¡é‡‡æ ·
                for _ in range(20):
                    if buffer.can_sample():
                        batch = buffer.sample()
                        del batch  # æ˜¾å¼åˆ é™¤
            
            # ç¼“å†²åŒºåº”è¯¥ä»ç„¶æ­£å¸¸å·¥ä½œ
            assert buffer.size == config.capacity
            assert buffer.can_sample()
            
        def test_serialization(self):
            """æµ‹è¯•åºåˆ—åŒ–å’Œååºåˆ—åŒ–"""
            config = ReplayBufferConfig(capacity=100, batch_size=16, state_dim=32, action_dim=5)
            buffer = ReplayBuffer(config)
            
            # æ·»åŠ ä¸€äº›ç»éªŒ
            original_experiences = []
            for i in range(50):
                exp = Experience(
                    state=torch.randn(config.state_dim),
                    action=torch.rand(config.action_dim),
                    reward=float(i),
                    next_state=torch.randn(config.state_dim),
                    done=i % 10 == 0,
                    info={'step': i}
                )
                original_experiences.append(exp)
                buffer.add(exp)
            
            # ä¿å­˜çŠ¶æ€
            state_dict = buffer.state_dict()
            
            # åˆ›å»ºæ–°ç¼“å†²åŒºå¹¶åŠ è½½çŠ¶æ€
            new_buffer = ReplayBuffer(config)
            new_buffer.load_state_dict(state_dict)
            
            # éªŒè¯æ•°æ®ä¸€è‡´æ€§
            assert new_buffer.size == buffer.size
            assert new_buffer.position == buffer.position
            
            # éªŒè¯ç»éªŒæ•°æ®
            original_all = buffer.get_all_experiences()
            loaded_all = new_buffer.get_all_experiences()
            
            assert len(original_all) == len(loaded_all)
            
            for orig, loaded in zip(original_all, loaded_all):
                assert torch.allclose(orig.state, loaded.state)
                assert torch.allclose(orig.action, loaded.action)
                assert orig.reward == loaded.reward
                assert orig.done == loaded.done
    ]]></file>
  <file path="tests/unit/test_positional_encoding.py"><![CDATA[
    """
    æµ‹è¯•ä½ç½®ç¼–ç ç»„ä»¶çš„å•å…ƒæµ‹è¯•
    """
    
    import pytest
    import torch
    import numpy as np
    import math
    from typing import Tuple
    
    from src.rl_trading_system.models.positional_encoding import (
        PositionalEncoding,
        LearnablePositionalEncoding,
        RelativePositionalEncoding
    )
    
    
    class TestPositionalEncoding:
        """æµ‹è¯•æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç """
        
        @pytest.fixture
        def pos_encoding(self):
            """åˆ›å»ºä½ç½®ç¼–ç å®žä¾‹"""
            return PositionalEncoding(d_model=256, max_len=1000)
        
        def test_initialization(self, pos_encoding):
            """æµ‹è¯•ä½ç½®ç¼–ç åˆå§‹åŒ–"""
            assert pos_encoding.d_model == 256
            assert pos_encoding.max_len == 1000
            assert pos_encoding.pe.shape == (1000, 256)
        
        def test_sinusoidal_pattern(self, pos_encoding):
            """æµ‹è¯•æ­£å¼¦ä½™å¼¦ç¼–ç æ¨¡å¼"""
            pe = pos_encoding.pe
            
            # æ£€æŸ¥å¶æ•°ä½ç½®ä½¿ç”¨sinï¼Œå¥‡æ•°ä½ç½®ä½¿ç”¨cos
            pos = 10
            for i in range(0, 256, 2):
                expected_sin = math.sin(pos / (10000 ** (i / 256)))
                expected_cos = math.cos(pos / (10000 ** (i / 256)))
                
                assert abs(pe[pos, i] - expected_sin) < 1e-6
                assert abs(pe[pos, i + 1] - expected_cos) < 1e-6
        
        def test_forward_pass(self, pos_encoding):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_model = 2, 50, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ä»¥ç¦ç”¨dropout
            pos_encoding.eval()
            output = pos_encoding(x)
            
            assert output.shape == (batch_size, seq_len, d_model)
            
            # éªŒè¯ä½ç½®ç¼–ç è¢«æ­£ç¡®æ·»åŠ 
            expected = x + pos_encoding.pe[:seq_len].unsqueeze(0)
            torch.testing.assert_close(output, expected)
        
        def test_different_sequence_lengths(self, pos_encoding):
            """æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„ä½ç½®ç¼–ç æ•ˆæžœ"""
            d_model = 256
            
            # æµ‹è¯•ä¸åŒé•¿åº¦
            for seq_len in [10, 50, 100, 252]:
                x = torch.randn(1, seq_len, d_model)
                output = pos_encoding(x)
                
                assert output.shape == (1, seq_len, d_model)
                
                # éªŒè¯ä½ç½®ç¼–ç çš„å”¯ä¸€æ€§
                pe_slice = pos_encoding.pe[:seq_len]
                for i in range(seq_len - 1):
                    for j in range(i + 1, seq_len):
                        # ä¸åŒä½ç½®çš„ç¼–ç åº”è¯¥ä¸åŒ
                        assert not torch.allclose(pe_slice[i], pe_slice[j])
        
        def test_max_length_constraint(self, pos_encoding):
            """æµ‹è¯•æœ€å¤§é•¿åº¦çº¦æŸ"""
            d_model = 256
            max_len = pos_encoding.max_len
            
            # æµ‹è¯•è¶…è¿‡æœ€å¤§é•¿åº¦çš„æƒ…å†µ
            x = torch.randn(1, max_len + 10, d_model)
            
            with pytest.raises(IndexError):
                pos_encoding(x)
        
        def test_positional_encoding_properties(self, pos_encoding):
            """æµ‹è¯•ä½ç½®ç¼–ç çš„æ•°å­¦æ€§è´¨"""
            pe = pos_encoding.pe
            
            # æµ‹è¯•å‘¨æœŸæ€§ï¼šæŸäº›é¢‘çŽ‡çš„ç¼–ç åº”è¯¥å…·æœ‰å‘¨æœŸæ€§
            # å¯¹äºŽæœ€ä½Žé¢‘çŽ‡ï¼Œå‘¨æœŸåº”è¯¥æ˜¯2Ï€ * 10000
            lowest_freq_period = 2 * math.pi * 10000
            
            # ç”±äºŽåºåˆ—é•¿åº¦é™åˆ¶ï¼Œæˆ‘ä»¬æµ‹è¯•è¾ƒå°çš„å‘¨æœŸæ€§
            for pos in range(100):
                # æ£€æŸ¥ç›¸é‚»ä½ç½®çš„å·®å¼‚
                diff = torch.norm(pe[pos + 1] - pe[pos])
                assert diff > 0  # ç›¸é‚»ä½ç½®åº”è¯¥ä¸åŒ
        
        def test_gradient_flow(self, pos_encoding):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            x = torch.randn(2, 50, 256, requires_grad=True)
            output = pos_encoding(x)
            loss = output.sum()
            loss.backward()
            
            # ä½ç½®ç¼–ç ä¸åº”è¯¥æœ‰æ¢¯åº¦ï¼ˆå®ƒæ˜¯å›ºå®šçš„ï¼‰
            assert pos_encoding.pe.grad is None
            # è¾“å…¥åº”è¯¥æœ‰æ¢¯åº¦
            assert x.grad is not None
            assert x.grad.shape == x.shape
    
    
    class TestLearnablePositionalEncoding:
        """æµ‹è¯•å¯å­¦ä¹ ä½ç½®ç¼–ç """
        
        @pytest.fixture
        def learnable_pos_encoding(self):
            """åˆ›å»ºå¯å­¦ä¹ ä½ç½®ç¼–ç å®žä¾‹"""
            return LearnablePositionalEncoding(d_model=256, max_len=1000)
        
        def test_initialization(self, learnable_pos_encoding):
            """æµ‹è¯•å¯å­¦ä¹ ä½ç½®ç¼–ç åˆå§‹åŒ–"""
            assert learnable_pos_encoding.d_model == 256
            assert learnable_pos_encoding.max_len == 1000
            assert learnable_pos_encoding.pe.shape == (1000, 256)
            assert learnable_pos_encoding.pe.requires_grad == True
        
        def test_forward_pass(self, learnable_pos_encoding):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_model = 2, 50, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            output = learnable_pos_encoding(x)
            
            assert output.shape == (batch_size, seq_len, d_model)
        
        def test_gradient_flow(self, learnable_pos_encoding):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            x = torch.randn(2, 50, 256, requires_grad=True)
            output = learnable_pos_encoding(x)
            loss = output.sum()
            loss.backward()
            
            # å¯å­¦ä¹ ä½ç½®ç¼–ç åº”è¯¥æœ‰æ¢¯åº¦
            assert learnable_pos_encoding.pe.grad is not None
            # è¾“å…¥ä¹Ÿåº”è¯¥æœ‰æ¢¯åº¦
            assert x.grad is not None
        
        def test_parameter_update(self, learnable_pos_encoding):
            """æµ‹è¯•å‚æ•°æ›´æ–°"""
            # è®°å½•åˆå§‹å‚æ•°
            initial_pe = learnable_pos_encoding.pe.clone()
            
            # æ¨¡æ‹Ÿè®­ç»ƒæ­¥éª¤
            optimizer = torch.optim.Adam(learnable_pos_encoding.parameters(), lr=0.01)
            
            x = torch.randn(2, 50, 256)
            output = learnable_pos_encoding(x)
            loss = output.sum()
            
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            # å‚æ•°åº”è¯¥è¢«æ›´æ–°
            assert not torch.allclose(initial_pe, learnable_pos_encoding.pe)
        
        def test_different_sequence_lengths(self, learnable_pos_encoding):
            """æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦"""
            d_model = 256
            
            for seq_len in [10, 50, 100, 252]:
                x = torch.randn(1, seq_len, d_model)
                output = learnable_pos_encoding(x)
                assert output.shape == (1, seq_len, d_model)
    
    
    class TestRelativePositionalEncoding:
        """æµ‹è¯•ç›¸å¯¹ä½ç½®ç¼–ç """
        
        @pytest.fixture
        def relative_pos_encoding(self):
            """åˆ›å»ºç›¸å¯¹ä½ç½®ç¼–ç å®žä¾‹"""
            return RelativePositionalEncoding(d_model=256, max_relative_position=128)
        
        def test_initialization(self, relative_pos_encoding):
            """æµ‹è¯•ç›¸å¯¹ä½ç½®ç¼–ç åˆå§‹åŒ–"""
            assert relative_pos_encoding.d_model == 256
            assert relative_pos_encoding.max_relative_position == 128
            # ç›¸å¯¹ä½ç½®ç¼–ç è¡¨å¤§å°åº”è¯¥æ˜¯ 2 * max_relative_position + 1
            assert relative_pos_encoding.relative_pe.shape == (257, 256)
        
        def test_relative_position_calculation(self, relative_pos_encoding):
            """æµ‹è¯•ç›¸å¯¹ä½ç½®è®¡ç®—"""
            seq_len = 10
            relative_positions = relative_pos_encoding._get_relative_positions(seq_len)
            
            assert relative_positions.shape == (seq_len, seq_len)
            
            # æ£€æŸ¥å¯¹è§’çº¿ï¼ˆè‡ªå·±åˆ°è‡ªå·±çš„ç›¸å¯¹ä½ç½®åº”è¯¥æ˜¯0ï¼‰
            for i in range(seq_len):
                assert relative_positions[i, i] == 0
            
            # æ£€æŸ¥ç›¸å¯¹ä½ç½®çš„å¯¹ç§°æ€§
            for i in range(seq_len):
                for j in range(seq_len):
                    expected_relative_pos = j - i
                    # è£å‰ªåˆ°æœ€å¤§ç›¸å¯¹ä½ç½®èŒƒå›´å†…
                    expected_relative_pos = max(-128, min(128, expected_relative_pos))
                    assert relative_positions[i, j] == expected_relative_pos
        
        def test_forward_pass(self, relative_pos_encoding):
            """æµ‹è¯•å‰å‘ä¼ æ’­"""
            batch_size, seq_len, d_model = 2, 50, 256
            x = torch.randn(batch_size, seq_len, d_model)
            
            output = relative_pos_encoding(x)
            
            assert output.shape == (batch_size, seq_len, d_model)
        
        def test_attention_bias_generation(self, relative_pos_encoding):
            """æµ‹è¯•æ³¨æ„åŠ›åç½®ç”Ÿæˆ"""
            seq_len = 20
            attention_bias = relative_pos_encoding.get_attention_bias(seq_len)
            
            assert attention_bias.shape == (seq_len, seq_len)
            
            # æ£€æŸ¥å¯¹ç§°æ€§è´¨ï¼šbias[i,j] å’Œ bias[j,i] åº”è¯¥æœ‰ç‰¹å®šå…³ç³»
            # ç”±äºŽç›¸å¯¹ä½ç½®ç¼–ç ï¼Œbias[i,j] åº”è¯¥ç­‰äºŽ relative_pe[j-i]
            relative_positions = relative_pos_encoding._get_relative_positions(seq_len)
            
            for i in range(seq_len):
                for j in range(seq_len):
                    rel_pos = relative_positions[i, j]
                    # è½¬æ¢ä¸ºç›¸å¯¹ä½ç½®ç¼–ç è¡¨çš„ç´¢å¼•
                    pe_idx = rel_pos + relative_pos_encoding.max_relative_position
                    expected_bias = relative_pos_encoding.relative_pe[pe_idx].sum()  # ç®€åŒ–æ£€æŸ¥
                    # è¿™é‡Œåªæ£€æŸ¥å½¢çŠ¶å’ŒåŸºæœ¬å±žæ€§ï¼Œå…·ä½“å®žçŽ°å¯èƒ½æœ‰æ‰€ä¸åŒ
        
        def test_gradient_flow(self, relative_pos_encoding):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            x = torch.randn(2, 20, 256, requires_grad=True)
            output = relative_pos_encoding(x)
            loss = output.sum()
            loss.backward()
            
            # ç›¸å¯¹ä½ç½®ç¼–ç å‚æ•°åº”è¯¥æœ‰æ¢¯åº¦
            assert relative_pos_encoding.relative_pe.grad is not None
            # è¾“å…¥ä¹Ÿåº”è¯¥æœ‰æ¢¯åº¦
            assert x.grad is not None
        
        def test_max_relative_position_clipping(self, relative_pos_encoding):
            """æµ‹è¯•æœ€å¤§ç›¸å¯¹ä½ç½®è£å‰ª"""
            # æµ‹è¯•è¶…è¿‡æœ€å¤§ç›¸å¯¹ä½ç½®çš„åºåˆ—
            seq_len = 300  # è¶…è¿‡ max_relative_position * 2
            
            relative_positions = relative_pos_encoding._get_relative_positions(seq_len)
            
            # æ‰€æœ‰ç›¸å¯¹ä½ç½®éƒ½åº”è¯¥åœ¨ [-max_relative_position, max_relative_position] èŒƒå›´å†…
            assert torch.all(relative_positions >= -relative_pos_encoding.max_relative_position)
            assert torch.all(relative_positions <= relative_pos_encoding.max_relative_position)
        
        def test_different_sequence_lengths(self, relative_pos_encoding):
            """æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„ç›¸å¯¹ä½ç½®ç¼–ç æ•ˆæžœ"""
            d_model = 256
            
            for seq_len in [5, 20, 50, 100]:
                x = torch.randn(1, seq_len, d_model)
                output = relative_pos_encoding(x)
                
                assert output.shape == (1, seq_len, d_model)
                
                # æµ‹è¯•æ³¨æ„åŠ›åç½®
                attention_bias = relative_pos_encoding.get_attention_bias(seq_len)
                assert attention_bias.shape == (seq_len, seq_len)
    
    
    class TestPositionalEncodingComparison:
        """æ¯”è¾ƒä¸åŒä½ç½®ç¼–ç æ–¹æ³•çš„æµ‹è¯•"""
        
        def test_encoding_differences(self):
            """æµ‹è¯•ä¸åŒç¼–ç æ–¹æ³•çš„å·®å¼‚"""
            d_model, seq_len = 256, 50
            x = torch.randn(1, seq_len, d_model)
            
            # åˆ›å»ºä¸åŒçš„ä½ç½®ç¼–ç 
            sinusoidal_pe = PositionalEncoding(d_model, 1000)
            learnable_pe = LearnablePositionalEncoding(d_model, 1000)
            relative_pe = RelativePositionalEncoding(d_model, 128)
            
            # èŽ·å–è¾“å‡º
            sin_output = sinusoidal_pe(x)
            learn_output = learnable_pe(x)
            rel_output = relative_pe(x)
            
            # æ‰€æœ‰è¾“å‡ºå½¢çŠ¶åº”è¯¥ç›¸åŒ
            assert sin_output.shape == learn_output.shape == rel_output.shape
            
            # ä½†å†…å®¹åº”è¯¥ä¸åŒï¼ˆé™¤éžæžå…¶å·§åˆï¼‰
            assert not torch.allclose(sin_output, learn_output, atol=1e-3)
            assert not torch.allclose(sin_output, rel_output, atol=1e-3)
            assert not torch.allclose(learn_output, rel_output, atol=1e-3)
        
        def test_performance_characteristics(self):
            """æµ‹è¯•æ€§èƒ½ç‰¹å¾"""
            d_model, seq_len = 256, 100
            x = torch.randn(2, seq_len, d_model)
            
            encodings = [
                PositionalEncoding(d_model, 1000),
                LearnablePositionalEncoding(d_model, 1000),
                RelativePositionalEncoding(d_model, 128)
            ]
            
            for encoding in encodings:
                # æµ‹è¯•å‰å‘ä¼ æ’­æ—¶é—´
                import time
                start_time = time.time()
                
                for _ in range(10):
                    output = encoding(x)
                
                end_time = time.time()
                avg_time = (end_time - start_time) / 10
                
                # æ‰€æœ‰ç¼–ç æ–¹æ³•éƒ½åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆ
                assert avg_time < 0.1  # 100ms per forward pass should be reasonable
        
        @pytest.mark.parametrize("seq_len", [10, 50, 100, 252])
        def test_scalability_with_sequence_length(self, seq_len):
            """æµ‹è¯•éšåºåˆ—é•¿åº¦çš„å¯æ‰©å±•æ€§"""
            d_model = 256
            x = torch.randn(1, seq_len, d_model)
            
            # æµ‹è¯•æ‰€æœ‰ç¼–ç æ–¹æ³•
            sinusoidal_pe = PositionalEncoding(d_model, max(1000, seq_len))
            learnable_pe = LearnablePositionalEncoding(d_model, max(1000, seq_len))
            relative_pe = RelativePositionalEncoding(d_model, max(128, seq_len // 2))
            
            # æ‰€æœ‰æ–¹æ³•éƒ½åº”è¯¥èƒ½å¤„ç†ä¸åŒé•¿åº¦çš„åºåˆ—
            sin_output = sinusoidal_pe(x)
            learn_output = learnable_pe(x)
            rel_output = relative_pe(x)
            
            assert sin_output.shape == (1, seq_len, d_model)
            assert learn_output.shape == (1, seq_len, d_model)
            assert rel_output.shape == (1, seq_len, d_model)
    ]]></file>
  <file path="tests/unit/test_portfolio_environment.py"><![CDATA[
    """
    æŠ•èµ„ç»„åˆçŽ¯å¢ƒçš„å•å…ƒæµ‹è¯•
    æµ‹è¯•PortfolioEnvironmentçš„GymæŽ¥å£å…¼å®¹æ€§ã€çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±å‡½æ•°
    """
    import pytest
    import numpy as np
    import pandas as pd
    import gym
    from gym import spaces
    from datetime import datetime, timedelta
    from unittest.mock import Mock, patch, MagicMock
    from typing import Dict, List, Tuple, Any, Optional
    
    from src.rl_trading_system.data.data_models import (
        MarketData, TradingState, TradingAction, TransactionRecord
    )
    from src.rl_trading_system.trading.portfolio_environment import (
        PortfolioEnvironment, PortfolioConfig
    )
    from src.rl_trading_system.data.interfaces import DataInterface
    
    
    class MockDataInterface(DataInterface):
        """æ¨¡æ‹Ÿæ•°æ®æŽ¥å£"""
        
        def get_stock_list(self, market: str = 'A') -> List[str]:
            return ['000001.SZ', '000002.SZ', '600000.SH', '600036.SH']
        
        def get_price_data(self, symbols: List[str], 
                          start_date: str, end_date: str) -> pd.DataFrame:
            # è¿”å›žç©ºDataFrameï¼Œè®©çŽ¯å¢ƒä½¿ç”¨æ¨¡æ‹Ÿæ•°æ®
            return pd.DataFrame()
        
        def get_fundamental_data(self, symbols: List[str], 
                               start_date: str, end_date: str) -> pd.DataFrame:
            return pd.DataFrame()
    
    
    class TestPortfolioEnvironment:
        """æŠ•èµ„ç»„åˆçŽ¯å¢ƒæµ‹è¯•ç±»"""
        
        @pytest.fixture
        def env_config(self):
            """çŽ¯å¢ƒé…ç½®fixture"""
            return PortfolioConfig(
                stock_pool=['000001.SZ', '000002.SZ', '600000.SH', '600036.SH'],
                lookback_window=30,
                initial_cash=100000.0,
                commission_rate=0.001,
                stamp_tax_rate=0.001,
                max_position_size=0.4  # ä½¿ç”¨å¯è¡Œçš„æœ€å¤§æƒé‡é™åˆ¶
            )
        
        @pytest.fixture
        def mock_data_interface(self):
            """æ¨¡æ‹Ÿæ•°æ®æŽ¥å£fixture"""
            return MockDataInterface()
        
        @pytest.fixture
        def portfolio_env(self, env_config, mock_data_interface):
            """æŠ•èµ„ç»„åˆçŽ¯å¢ƒfixture"""
            return PortfolioEnvironment(env_config, mock_data_interface)
        
        def test_environment_initialization(self, portfolio_env, env_config):
            """æµ‹è¯•çŽ¯å¢ƒåˆå§‹åŒ–"""
            assert portfolio_env.n_stocks == len(env_config.stock_pool)
            assert portfolio_env.config.initial_cash == env_config.initial_cash
            assert portfolio_env.observation_space is not None
            assert portfolio_env.action_space is not None
            
            # æ£€æŸ¥è§‚å¯Ÿç©ºé—´
            obs_space = portfolio_env.observation_space
            assert isinstance(obs_space, spaces.Dict)
            assert 'features' in obs_space.spaces
            assert 'positions' in obs_space.spaces
            assert 'market_state' in obs_space.spaces
            
            # æ£€æŸ¥åŠ¨ä½œç©ºé—´
            action_space = portfolio_env.action_space
            assert isinstance(action_space, spaces.Box)
            assert action_space.shape == (portfolio_env.n_stocks,)
            
        def test_gym_interface_compatibility(self, portfolio_env):
            """æµ‹è¯•GymæŽ¥å£å…¼å®¹æ€§"""
            # æµ‹è¯•resetæ–¹æ³•
            obs = portfolio_env.reset()
            assert isinstance(obs, dict)
            assert 'features' in obs
            assert 'positions' in obs
            assert 'market_state' in obs
            
            # æµ‹è¯•stepæ–¹æ³•
            action = np.array([0.25, 0.25, 0.25, 0.25])
            obs, reward, done, info = portfolio_env.step(action)
            
            assert isinstance(obs, dict)
            assert isinstance(reward, (int, float))
            assert isinstance(done, bool)
            assert isinstance(info, dict)
            
            # æ£€æŸ¥è§‚å¯Ÿç©ºé—´å…¼å®¹æ€§
            assert portfolio_env.observation_space.contains(obs)
            
        def test_observation_space_structure(self, portfolio_env, env_config):
            """æµ‹è¯•è§‚å¯Ÿç©ºé—´ç»“æž„"""
            obs = portfolio_env.reset()
            
            # æ£€æŸ¥ç‰¹å¾ç»´åº¦
            features = obs['features']
            expected_shape = (env_config.lookback_window, portfolio_env.n_stocks, portfolio_env.n_features)
            assert features.shape == expected_shape
            assert features.dtype == np.float32
            
            # æ£€æŸ¥æŒä»“ç»´åº¦
            positions = obs['positions']
            assert positions.shape == (portfolio_env.n_stocks,)
            assert positions.dtype == np.float32
            assert np.all(positions >= 0)
            
            # æ£€æŸ¥å¸‚åœºçŠ¶æ€ç»´åº¦
            market_state = obs['market_state']
            assert market_state.shape == (portfolio_env.n_market_features,)
            assert market_state.dtype == np.float32
            
        def test_action_space_structure(self, portfolio_env):
            """æµ‹è¯•åŠ¨ä½œç©ºé—´ç»“æž„"""
            action_space = portfolio_env.action_space
            
            # æ£€æŸ¥åŠ¨ä½œç©ºé—´ç±»åž‹å’Œç»´åº¦
            assert isinstance(action_space, spaces.Box)
            assert action_space.shape == (portfolio_env.n_stocks,)
            assert action_space.dtype == np.float32
            
            # æ£€æŸ¥åŠ¨ä½œç©ºé—´è¾¹ç•Œ
            assert np.all(action_space.low == 0)
            assert np.all(action_space.high == 1)
            
            # æµ‹è¯•åŠ¨ä½œé‡‡æ ·
            action = action_space.sample()
            assert action_space.contains(action)
            assert action.shape == (portfolio_env.n_stocks,)
            
        def test_reward_function_components(self, portfolio_env):
            """æµ‹è¯•å¥–åŠ±å‡½æ•°ç»„æˆéƒ¨åˆ†"""
            portfolio_env.reset()
            
            # æµ‹è¯•ä¸åŒæƒé‡åˆ†é…çš„å¥–åŠ±
            actions = [
                np.array([1.0, 0.0, 0.0, 0.0]),  # é›†ä¸­æŠ•èµ„
                np.array([0.25, 0.25, 0.25, 0.25]),  # å‡åŒ€åˆ†æ•£
                np.array([0.4, 0.3, 0.2, 0.1])  # é€‚åº¦é›†ä¸­
            ]
            
            rewards = []
            for action in actions:
                obs, reward, done, info = portfolio_env.step(action)
                rewards.append(reward)
                
                # æ£€æŸ¥ä¿¡æ¯å­—å…¸åŒ…å«å¿…è¦ä¿¡æ¯
                assert 'portfolio_return' in info
                assert 'transaction_cost' in info
                assert 'concentration' in info
                assert 'drawdown' in info
                
                # æ£€æŸ¥é›†ä¸­åº¦è®¡ç®—ï¼ˆä½¿ç”¨å®žé™…æŒä»“è€Œä¸æ˜¯åŽŸå§‹åŠ¨ä½œï¼‰
                expected_concentration = np.sum(info['positions'] ** 2)
                assert abs(info['concentration'] - expected_concentration) < 1e-6
            
            # å¥–åŠ±åº”è¯¥æ˜¯æœ‰é™çš„æ•°å€¼
            for reward in rewards:
                assert np.isfinite(reward)
                assert isinstance(reward, (int, float))
        
        def test_position_weight_constraints(self, portfolio_env):
            """æµ‹è¯•æŒä»“æƒé‡çº¦æŸ"""
            portfolio_env.reset()
            
            # æµ‹è¯•æƒé‡æ ‡å‡†åŒ–
            test_actions = [
                np.array([2.0, 1.0, 1.0, 1.0]),  # éœ€è¦æ ‡å‡†åŒ–
                np.array([0.0, 0.0, 0.0, 0.0]),  # å…¨é›¶æƒé‡
                np.array([1.0, 0.0, 0.0, 0.0])   # å•ä¸€æƒé‡
            ]
            
            for action in test_actions:
                obs, reward, done, info = portfolio_env.step(action)
                positions = info['positions']
                
                # æ£€æŸ¥æƒé‡å’Œçº¦æŸ
                assert abs(positions.sum() - 1.0) < 1e-6
                
                # æ£€æŸ¥æƒé‡éžè´Ÿçº¦æŸ
                assert np.all(positions >= 0)
                
                # æ£€æŸ¥æœ€å¤§æƒé‡çº¦æŸ
                assert np.all(positions <= portfolio_env.config.max_position_size + 1e-6)
        
        def test_transaction_cost_calculation(self, portfolio_env):
            """æµ‹è¯•äº¤æ˜“æˆæœ¬è®¡ç®—"""
            portfolio_env.reset()
            
            # ç¬¬ä¸€æ­¥ï¼šå»ºç«‹åˆå§‹æŒä»“
            initial_action = np.array([0.25, 0.25, 0.25, 0.25])
            obs1, reward1, done1, info1 = portfolio_env.step(initial_action)
            initial_cost = info1['transaction_cost']
            
            # ç¬¬äºŒæ­¥ï¼šä¸æ”¹å˜æŒä»“
            same_action = np.array([0.25, 0.25, 0.25, 0.25])
            obs2, reward2, done2, info2 = portfolio_env.step(same_action)
            no_change_cost = info2['transaction_cost']
            
            # ç¬¬ä¸‰æ­¥ï¼šå¤§å¹…è°ƒæ•´æŒä»“
            different_action = np.array([0.7, 0.1, 0.1, 0.1])
            obs3, reward3, done3, info3 = portfolio_env.step(different_action)
            large_change_cost = info3['transaction_cost']
            
            # éªŒè¯äº¤æ˜“æˆæœ¬é€»è¾‘
            assert initial_cost >= 0  # åˆå§‹å»ºä»“åº”æœ‰éžè´Ÿæˆæœ¬
            assert no_change_cost >= 0  # æˆæœ¬åº”ä¸ºéžè´Ÿ
            assert large_change_cost >= 0  # æˆæœ¬åº”ä¸ºéžè´Ÿ
            
            # å¦‚æžœæœ‰å®žé™…çš„æƒé‡å˜åŒ–ï¼Œåº”è¯¥äº§ç”Ÿæˆæœ¬
            if np.any(np.abs(info3['positions'] - info2['positions']) > 1e-6):
                assert large_change_cost >= no_change_cost
        
        def test_a_share_trading_rules(self, portfolio_env):
            """æµ‹è¯•Aè‚¡äº¤æ˜“è§„åˆ™çº¦æŸ"""
            portfolio_env.reset()
            
            # æµ‹è¯•å•åªè‚¡ç¥¨æœ€å¤§æƒé‡é™åˆ¶
            concentrated_action = np.array([1.0, 0.0, 0.0, 0.0])
            obs, reward, done, info = portfolio_env.step(concentrated_action)
            positions = info['positions']
            
            # æ£€æŸ¥æ˜¯å¦åº”ç”¨äº†æœ€å¤§æƒé‡é™åˆ¶
            max_weight = np.max(positions)
            assert max_weight <= portfolio_env.config.max_position_size + 1e-6
            
            # æµ‹è¯•T+1è§„åˆ™ï¼ˆç®€åŒ–æµ‹è¯•ï¼‰
            if portfolio_env.config.t_plus_1:
                # éªŒè¯T+1è§„åˆ™çš„åŸºæœ¬çº¦æŸ
                # åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æŸ¥å½“æ—¥ä¹°å…¥è‚¡ç¥¨ä¸èƒ½å½“æ—¥å–å‡º
                assert portfolio_env.config.t_plus_1 == True
            
            # æµ‹è¯•æƒé‡çº¦æŸåŽçš„é‡æ–°æ ‡å‡†åŒ–
            assert abs(positions.sum() - 1.0) < 1e-6
            assert np.all(positions >= 0)
        
        def test_price_limit_constraints(self, portfolio_env):
            """æµ‹è¯•æ¶¨è·Œåœé™åˆ¶"""
            portfolio_env.reset()
            
            # æ¨¡æ‹Ÿæ¶¨è·Œåœæƒ…å†µ
            # åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æŸ¥ä»·æ ¼å˜åŠ¨æ˜¯å¦è¶…è¿‡é™åˆ¶
            action = np.array([0.25, 0.25, 0.25, 0.25])
            obs, reward, done, info = portfolio_env.step(action)
            
            # åŸºæœ¬æ£€æŸ¥ï¼šç¡®ä¿çŽ¯å¢ƒä»ç„¶æ­£å¸¸è¿è¡Œ
            assert np.isfinite(reward)
            assert isinstance(done, bool)
            
            # æ£€æŸ¥ä»·æ ¼é™åˆ¶é…ç½®
            assert portfolio_env.config.price_limit == 0.1  # 10%æ¶¨è·Œåœé™åˆ¶
            
            # éªŒè¯åœ¨æ¶¨è·Œåœæƒ…å†µä¸‹çŽ¯å¢ƒçš„ç¨³å®šæ€§
            for _ in range(5):
                action = portfolio_env.action_space.sample()
                obs, reward, done, info = portfolio_env.step(action)
                assert np.isfinite(reward)
                assert np.all(np.isfinite(obs['features']))
        
        def test_minimum_trade_amount_constraint(self, portfolio_env):
            """æµ‹è¯•æœ€å°äº¤æ˜“é‡‘é¢çº¦æŸ"""
            portfolio_env.reset()
            
            # æµ‹è¯•æžå°çš„æƒé‡å˜åŒ–
            small_action = np.array([0.2501, 0.2499, 0.25, 0.25])  # å¾ˆå°çš„å˜åŒ–
            obs, reward, done, info = portfolio_env.step(small_action)
            
            # æ£€æŸ¥æœ€å°äº¤æ˜“é‡‘é¢é…ç½®
            assert portfolio_env.config.min_trade_amount == 1000.0
            
            # éªŒè¯äº¤æ˜“æˆæœ¬è®¡ç®—è€ƒè™‘äº†æœ€å°äº¤æ˜“é‡‘é¢
            assert np.isfinite(info['transaction_cost'])
            assert info['transaction_cost'] >= 0
        
        def test_t_plus_1_trading_rule(self, portfolio_env):
            """æµ‹è¯•T+1äº¤æ˜“è§„åˆ™"""
            if not portfolio_env.config.t_plus_1:
                pytest.skip("T+1è§„åˆ™æœªå¯ç”¨")
            
            portfolio_env.reset()
            
            # ç¬¬ä¸€å¤©ï¼šä¹°å…¥è‚¡ç¥¨
            buy_action = np.array([0.5, 0.3, 0.2, 0.0])
            obs1, reward1, done1, info1 = portfolio_env.step(buy_action)
            
            # è®°å½•ä¹°å…¥çš„è‚¡ç¥¨
            bought_positions = info1['positions'].copy()
            
            # ç¬¬äºŒå¤©ï¼šå°è¯•å–å‡ºåˆšä¹°å…¥çš„è‚¡ç¥¨ï¼ˆåœ¨çœŸå®žå®žçŽ°ä¸­åº”è¯¥è¢«é™åˆ¶ï¼‰
            sell_action = np.array([0.0, 0.0, 0.0, 1.0])
            obs2, reward2, done2, info2 = portfolio_env.step(sell_action)
            
            # åœ¨ç®€åŒ–çš„æµ‹è¯•å®žçŽ°ä¸­ï¼Œæˆ‘ä»¬åªéªŒè¯åŸºæœ¬åŠŸèƒ½
            assert np.isfinite(reward2)
            assert np.all(info2['positions'] >= 0)
            assert abs(info2['positions'].sum() - 1.0) < 1e-6
        
        def test_market_impact_model(self, portfolio_env):
            """æµ‹è¯•å¸‚åœºå†²å‡»æ¨¡åž‹"""
            portfolio_env.reset()
            
            # æµ‹è¯•ä¸åŒè§„æ¨¡çš„äº¤æ˜“å¯¹å¸‚åœºå†²å‡»çš„å½±å“
            small_trade = np.array([0.26, 0.24, 0.25, 0.25])  # å°é¢äº¤æ˜“
            large_trade = np.array([0.7, 0.1, 0.1, 0.1])     # å¤§é¢äº¤æ˜“
            
            # å°é¢äº¤æ˜“
            obs1, reward1, done1, info1 = portfolio_env.step(small_trade)
            small_cost = info1['transaction_cost']
            
            # é‡ç½®çŽ¯å¢ƒ
            portfolio_env.reset()
            
            # å¤§é¢äº¤æ˜“
            obs2, reward2, done2, info2 = portfolio_env.step(large_trade)
            large_cost = info2['transaction_cost']
            
            # å¤§é¢äº¤æ˜“çš„æˆæœ¬åº”è¯¥æ›´é«˜ï¼ˆç”±äºŽå¸‚åœºå†²å‡»ï¼‰
            if large_cost > 0 and small_cost > 0:
                assert large_cost >= small_cost
            
            # éªŒè¯æˆæœ¬è®¡ç®—çš„åˆç†æ€§
            assert small_cost >= 0
            assert large_cost >= 0
        
        def test_episode_completion(self, portfolio_env):
            """æµ‹è¯•å®Œæ•´äº¤æ˜“å‘¨æœŸ"""
            obs = portfolio_env.reset()
            total_steps = 0
            episode_rewards = []
            
            while True:
                # éšæœºåŠ¨ä½œ
                action = portfolio_env.action_space.sample()
                obs, reward, done, info = portfolio_env.step(action)
                
                episode_rewards.append(reward)
                total_steps += 1
                
                # æ£€æŸ¥è§‚å¯Ÿç©ºé—´ä¸€è‡´æ€§
                assert portfolio_env.observation_space.contains(obs)
                
                if done:
                    break
                
                # é˜²æ­¢æ— é™å¾ªçŽ¯
                if total_steps > portfolio_env.max_steps + 10:
                    break
            
            # éªŒè¯episodeå®Œæˆ
            assert done
            assert total_steps <= portfolio_env.max_steps
            assert len(episode_rewards) == total_steps
            
            # æ£€æŸ¥æœ€ç»ˆæŠ•èµ„ç»„åˆæŒ‡æ ‡
            metrics = portfolio_env.get_portfolio_metrics()
            assert 'total_return' in metrics
            assert 'volatility' in metrics
            assert 'sharpe_ratio' in metrics
            assert 'max_drawdown' in metrics
        
        def test_risk_metrics_calculation(self, portfolio_env):
            """æµ‹è¯•é£Žé™©æŒ‡æ ‡è®¡ç®—"""
            portfolio_env.reset()
            
            # è¿è¡Œå‡ æ­¥ä»¥ç§¯ç´¯æ•°æ®
            for _ in range(10):
                action = portfolio_env.action_space.sample()
                obs, reward, done, info = portfolio_env.step(action)
                
                # æ£€æŸ¥é£Žé™©æŒ‡æ ‡
                assert 'drawdown' in info
                assert 'concentration' in info
                assert 'active_positions' in info
                
                # éªŒè¯æŒ‡æ ‡èŒƒå›´
                assert 0 <= info['drawdown'] <= 1
                assert 0 <= info['concentration'] <= 1
                assert 0 <= info['active_positions'] <= portfolio_env.n_stocks
            
            # æ£€æŸ¥æœ€ç»ˆæŒ‡æ ‡
            metrics = portfolio_env.get_portfolio_metrics()
            if metrics:  # å¦‚æžœæœ‰è¶³å¤Ÿæ•°æ®
                assert metrics['max_drawdown'] >= 0
                assert np.isfinite(metrics['volatility'])
                assert np.isfinite(metrics['sharpe_ratio'])
        
        def test_state_consistency(self, portfolio_env):
            """æµ‹è¯•çŠ¶æ€ä¸€è‡´æ€§"""
            obs1 = portfolio_env.reset()
            
            # æ‰§è¡Œç›¸åŒåŠ¨ä½œåºåˆ—
            actions = [
                np.array([0.4, 0.3, 0.2, 0.1]),
                np.array([0.3, 0.3, 0.2, 0.2]),
                np.array([0.25, 0.25, 0.25, 0.25])
            ]
            
            states = [obs1]
            for action in actions:
                obs, reward, done, info = portfolio_env.step(action)
                states.append(obs)
            
            # æ£€æŸ¥çŠ¶æ€æ¼”åŒ–çš„ä¸€è‡´æ€§
            for i, state in enumerate(states):
                assert 'features' in state
                assert 'positions' in state
                assert 'market_state' in state
                
                # æ£€æŸ¥æŒä»“çŠ¶æ€çš„ä¸€è‡´æ€§
                if i > 0:
                    # æŒä»“åº”è¯¥åæ˜ ä¸Šä¸€æ­¥çš„åŠ¨ä½œ
                    current_positions = state['positions']
                    # æ³¨æ„ï¼šç”±äºŽçº¦æŸå’Œæ ‡å‡†åŒ–ï¼Œå¯èƒ½ä¸å®Œå…¨ç›¸ç­‰
                    assert np.allclose(current_positions.sum(), 1.0, atol=1e-6)
        
        def test_edge_cases(self, portfolio_env):
            """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
            portfolio_env.reset()
            
            # æµ‹è¯•æžç«¯åŠ¨ä½œ
            edge_actions = [
                np.array([1.0, 0.0, 0.0, 0.0]),  # å…¨éƒ¨æŠ•èµ„ä¸€åªè‚¡ç¥¨
                np.array([0.0, 0.0, 0.0, 0.0]),  # å…¨éƒ¨çŽ°é‡‘
                np.ones(portfolio_env.n_stocks) * 1e-10,  # æžå°æƒé‡
                np.ones(portfolio_env.n_stocks) * 1e10   # æžå¤§æƒé‡
            ]
            
            for action in edge_actions:
                obs, reward, done, info = portfolio_env.step(action)
                
                # çŽ¯å¢ƒåº”è¯¥èƒ½å¤„ç†æ‰€æœ‰è¾¹ç•Œæƒ…å†µ
                assert np.isfinite(reward)
                assert isinstance(done, bool)
                assert portfolio_env.observation_space.contains(obs)
                
                # æƒé‡çº¦æŸåº”è¯¥å§‹ç»ˆæ»¡è¶³
                positions = info['positions']
                assert np.all(positions >= 0)
                assert abs(positions.sum() - 1.0) < 1e-5
        
        def test_performance_metrics(self, portfolio_env):
            """æµ‹è¯•æ€§èƒ½æŒ‡æ ‡"""
            portfolio_env.reset()
            
            # è¿è¡Œä¸€ä¸ªå®Œæ•´episode
            while True:
                action = portfolio_env.action_space.sample()
                obs, reward, done, info = portfolio_env.step(action)
                
                if done:
                    break
            
            # èŽ·å–æ€§èƒ½æŒ‡æ ‡
            metrics = portfolio_env.get_portfolio_metrics()
            
            if metrics:  # å¦‚æžœæœ‰è¶³å¤Ÿæ•°æ®è®¡ç®—æŒ‡æ ‡
                # æ£€æŸ¥æŒ‡æ ‡çš„åˆç†æ€§
                assert isinstance(metrics['total_return'], float)
                assert isinstance(metrics['volatility'], float)
                assert isinstance(metrics['sharpe_ratio'], float)
                assert isinstance(metrics['max_drawdown'], float)
                
                # æ£€æŸ¥æŒ‡æ ‡èŒƒå›´
                assert metrics['volatility'] >= 0
                assert 0 <= metrics['max_drawdown'] <= 1
                assert np.isfinite(metrics['sharpe_ratio'])
        
        @pytest.mark.parametrize("n_stocks", [2, 5, 10])
        def test_different_stock_pool_sizes(self, n_stocks):
            """æµ‹è¯•ä¸åŒè‚¡ç¥¨æ± å¤§å°"""
            stock_pool = [f"stock_{i:03d}" for i in range(n_stocks)]
            config = PortfolioConfig(
                stock_pool=stock_pool,
                lookback_window=20,
                initial_cash=50000.0
            )
            
            mock_data_interface = MockDataInterface()
            env = PortfolioEnvironment(config, mock_data_interface)
            obs = env.reset()
            
            # æ£€æŸ¥ç»´åº¦æ­£ç¡®æ€§
            assert obs['features'].shape[1] == n_stocks
            assert obs['positions'].shape[0] == n_stocks
            assert env.action_space.shape[0] == n_stocks
            
            # æµ‹è¯•åŠ¨ä½œæ‰§è¡Œ
            action = np.ones(n_stocks) / n_stocks
            obs, reward, done, info = env.step(action)
            
            assert np.isfinite(reward)
            assert info['positions'].shape[0] == n_stocks
        
        @pytest.mark.parametrize("lookback_window", [10, 30, 60])
        def test_different_lookback_windows(self, lookback_window):
            """æµ‹è¯•ä¸åŒå›žæœ›çª—å£"""
            config = PortfolioConfig(
                stock_pool=['A', 'B', 'C'],
                lookback_window=lookback_window,
                initial_cash=100000.0
            )
            
            mock_data_interface = MockDataInterface()
            env = PortfolioEnvironment(config, mock_data_interface)
            obs = env.reset()
            
            # æ£€æŸ¥ç‰¹å¾ç»´åº¦
            assert obs['features'].shape[0] == lookback_window
            
            # æµ‹è¯•æ­£å¸¸è¿è¡Œ
            action = np.array([0.33, 0.33, 0.34])
            obs, reward, done, info = env.step(action)
            
            assert np.isfinite(reward)
            assert obs['features'].shape[0] == lookback_window
    ]]></file>
  <file path="tests/unit/test_performance_metrics.py"><![CDATA[
    """
    ç»©æ•ˆæŒ‡æ ‡è®¡ç®—æ¨¡å—çš„å•å…ƒæµ‹è¯•
    æµ‹è¯•æ”¶ç›ŠçŽ‡ã€å¤æ™®æ¯”çŽ‡ã€æœ€å¤§å›žæ’¤ç­‰æŒ‡æ ‡è®¡ç®—ï¼Œé£Žé™©æŒ‡æ ‡ï¼ˆVaRã€CVaRã€æ³¢åŠ¨çŽ‡ï¼‰è®¡ç®—ï¼Œäº¤æ˜“æŒ‡æ ‡ï¼ˆæ¢æ‰‹çŽ‡ã€æˆæœ¬åˆ†æžï¼‰è®¡ç®—
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, date
    from typing import Dict, List, Optional
    from decimal import Decimal
    
    from src.rl_trading_system.evaluation.performance_metrics import (
        ReturnMetrics,
        RiskMetrics,
        RiskAdjustedMetrics,
        TradingMetrics,
        PortfolioMetrics
    )
    from src.rl_trading_system.backtest.multi_frequency_backtest import Trade, OrderType
    
    
    class TestReturnMetrics:
        """æ”¶ç›ŠçŽ‡æŒ‡æ ‡æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_returns(self):
            """åˆ›å»ºæ ·æœ¬æ”¶ç›ŠçŽ‡æ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            # åˆ›å»ºæ¨¡æ‹Ÿçš„æ—¥æ”¶ç›ŠçŽ‡åºåˆ—ï¼Œå¹´åŒ–æ”¶ç›Šçº¦10%
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)  # å‡å€¼0.1%ï¼Œæ ‡å‡†å·®2%
            return pd.Series(returns, index=dates)
    
        @pytest.fixture
        def sample_portfolio_values(self):
            """åˆ›å»ºæ ·æœ¬ç»„åˆä»·å€¼åºåˆ—"""
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)
            
            values = [1000000]  # åˆå§‹å€¼100ä¸‡
            for ret in returns:
                values.append(values[-1] * (1 + ret))
            
            return pd.Series(values[1:], index=dates)
    
        def test_return_metrics_initialization(self):
            """æµ‹è¯•æ”¶ç›ŠçŽ‡æŒ‡æ ‡åˆå§‹åŒ–"""
            returns = pd.Series([0.01, -0.005, 0.02, -0.01, 0.015])
            metrics = ReturnMetrics(returns)
    
            assert len(metrics.returns) == 5
            assert isinstance(metrics.returns, pd.Series)
    
        def test_total_return_calculation(self, sample_returns):
            """æµ‹è¯•æ€»æ”¶ç›ŠçŽ‡è®¡ç®—"""
            metrics = ReturnMetrics(sample_returns)
            total_return = metrics.calculate_total_return()
    
            # æ€»æ”¶ç›ŠçŽ‡ = ç´¯ç§¯æ”¶ç›Š
            expected_total_return = (1 + sample_returns).prod() - 1
            assert abs(total_return - expected_total_return) < 1e-10
    
        def test_annualized_return_calculation(self, sample_returns):
            """æµ‹è¯•å¹´åŒ–æ”¶ç›ŠçŽ‡è®¡ç®—"""
            metrics = ReturnMetrics(sample_returns)
            annualized_return = metrics.calculate_annualized_return()
    
            # å¹´åŒ–æ”¶ç›ŠçŽ‡ = (1 + æ€»æ”¶ç›ŠçŽ‡)^(252/å¤©æ•°) - 1
            total_return = metrics.calculate_total_return()
            expected_annualized = (1 + total_return) ** (252 / len(sample_returns)) - 1
            assert abs(annualized_return - expected_annualized) < 1e-10
    
        def test_monthly_returns_calculation(self, sample_returns):
            """æµ‹è¯•æœˆåº¦æ”¶ç›ŠçŽ‡è®¡ç®—"""
            metrics = ReturnMetrics(sample_returns)
            monthly_returns = metrics.calculate_monthly_returns()
    
            # éªŒè¯è¿”å›žçš„æ˜¯DataFrame
            assert isinstance(monthly_returns, pd.DataFrame)
            assert 'monthly_return' in monthly_returns.columns
            assert len(monthly_returns) > 0
    
        def test_cumulative_returns_calculation(self, sample_returns):
            """æµ‹è¯•ç´¯ç§¯æ”¶ç›ŠçŽ‡è®¡ç®—"""
            metrics = ReturnMetrics(sample_returns)
            cumulative_returns = metrics.calculate_cumulative_returns()
    
            # éªŒè¯ç´¯ç§¯æ”¶ç›ŠçŽ‡çš„è®¡ç®—
            assert isinstance(cumulative_returns, pd.Series)
            assert len(cumulative_returns) == len(sample_returns)
            assert abs(cumulative_returns.iloc[0] - sample_returns.iloc[0]) < 1e-10
            
            # æœ€åŽä¸€ä¸ªå€¼åº”è¯¥ç­‰äºŽæ€»æ”¶ç›ŠçŽ‡
            total_return = metrics.calculate_total_return()
            assert abs(cumulative_returns.iloc[-1] - total_return) < 1e-10
    
        def test_empty_returns_error(self):
            """æµ‹è¯•ç©ºæ”¶ç›ŠçŽ‡åºåˆ—é”™è¯¯"""
            with pytest.raises(ValueError, match="æ”¶ç›ŠçŽ‡åºåˆ—ä¸èƒ½ä¸ºç©º"):
                ReturnMetrics(pd.Series([]))
    
        def test_invalid_returns_error(self):
            """æµ‹è¯•æ— æ•ˆæ”¶ç›ŠçŽ‡é”™è¯¯"""
            # æµ‹è¯•åŒ…å«NaNçš„åºåˆ—
            returns_with_nan = pd.Series([0.01, np.nan, 0.02])
            with pytest.raises(ValueError, match="æ”¶ç›ŠçŽ‡åºåˆ—åŒ…å«æ— æ•ˆå€¼"):
                ReturnMetrics(returns_with_nan)
    
            # æµ‹è¯•åŒ…å«æ— ç©·å¤§çš„åºåˆ—
            returns_with_inf = pd.Series([0.01, np.inf, 0.02])
            with pytest.raises(ValueError, match="æ”¶ç›ŠçŽ‡åºåˆ—åŒ…å«æ— æ•ˆå€¼"):
                ReturnMetrics(returns_with_inf)
    
    
    class TestRiskMetrics:
        """é£Žé™©æŒ‡æ ‡æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_returns(self):
            """åˆ›å»ºæ ·æœ¬æ”¶ç›ŠçŽ‡æ•°æ®"""
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            return pd.Series(returns, index=dates)
    
        @pytest.fixture
        def sample_portfolio_values(self):
            """åˆ›å»ºæ ·æœ¬ç»„åˆä»·å€¼åºåˆ—"""
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)
            
            values = [1000000]
            for ret in returns:
                values.append(values[-1] * (1 + ret))
            
            return pd.Series(values[1:], index=dates)
    
        def test_risk_metrics_initialization(self, sample_returns):
            """æµ‹è¯•é£Žé™©æŒ‡æ ‡åˆå§‹åŒ–"""
            metrics = RiskMetrics(sample_returns)
            assert len(metrics.returns) == 252
    
        def test_volatility_calculation(self, sample_returns):
            """æµ‹è¯•æ³¢åŠ¨çŽ‡è®¡ç®—"""
            metrics = RiskMetrics(sample_returns)
            
            # æ—¥æ³¢åŠ¨çŽ‡
            daily_vol = metrics.calculate_volatility()
            expected_daily_vol = sample_returns.std()
            assert abs(daily_vol - expected_daily_vol) < 1e-10
    
            # å¹´åŒ–æ³¢åŠ¨çŽ‡
            annualized_vol = metrics.calculate_volatility(annualized=True)
            expected_annualized_vol = sample_returns.std() * np.sqrt(252)
            assert abs(annualized_vol - expected_annualized_vol) < 1e-10
    
        def test_max_drawdown_calculation(self, sample_portfolio_values):
            """æµ‹è¯•æœ€å¤§å›žæ’¤è®¡ç®—"""
            returns = sample_portfolio_values.pct_change().dropna()
            metrics = RiskMetrics(returns)
    
            max_drawdown = metrics.calculate_max_drawdown(sample_portfolio_values)
    
            # æ‰‹åŠ¨è®¡ç®—æœ€å¤§å›žæ’¤è¿›è¡ŒéªŒè¯
            peak = sample_portfolio_values.expanding().max()
            drawdown = (sample_portfolio_values - peak) / peak
            expected_max_drawdown = abs(drawdown.min())
    
            assert abs(max_drawdown - expected_max_drawdown) < 1e-10
            assert max_drawdown >= 0  # æœ€å¤§å›žæ’¤åº”è¯¥ä¸ºæ­£æ•°
    
        def test_var_calculation(self, sample_returns):
            """æµ‹è¯•VaRè®¡ç®—"""
            metrics = RiskMetrics(sample_returns)
    
            # 95% VaR
            var_95 = metrics.calculate_var(confidence_level=0.95)
            expected_var_95 = abs(sample_returns.quantile(0.05))
            assert abs(var_95 - expected_var_95) < 1e-10
    
            # 99% VaR
            var_99 = metrics.calculate_var(confidence_level=0.99)
            expected_var_99 = abs(sample_returns.quantile(0.01))
            assert abs(var_99 - expected_var_99) < 1e-10
    
            # VaRåº”è¯¥ä¸ºæ­£æ•°
            assert var_95 >= 0
            assert var_99 >= 0
            # 99% VaRåº”è¯¥å¤§äºŽ95% VaR
            assert var_99 >= var_95
    
        def test_cvar_calculation(self, sample_returns):
            """æµ‹è¯•CVaRè®¡ç®—"""
            metrics = RiskMetrics(sample_returns)
    
            # 95% CVaR
            cvar_95 = metrics.calculate_cvar(confidence_level=0.95)
            
            # æ‰‹åŠ¨è®¡ç®—CVaRè¿›è¡ŒéªŒè¯
            var_95 = metrics.calculate_var(confidence_level=0.95)
            tail_losses = sample_returns[sample_returns <= -var_95]
            expected_cvar_95 = abs(tail_losses.mean()) if len(tail_losses) > 0 else var_95
    
            assert abs(cvar_95 - expected_cvar_95) < 1e-10
            assert cvar_95 >= 0
    
        def test_downside_deviation_calculation(self, sample_returns):
            """æµ‹è¯•ä¸‹è¡Œåå·®è®¡ç®—"""
            metrics = RiskMetrics(sample_returns)
    
            # ç›¸å¯¹äºŽ0çš„ä¸‹è¡Œåå·®
            downside_dev = metrics.calculate_downside_deviation()
            negative_returns = sample_returns[sample_returns < 0]
            expected_downside_dev = np.sqrt((negative_returns ** 2).mean())
            assert abs(downside_dev - expected_downside_dev) < 1e-10
    
            # ç›¸å¯¹äºŽç›®æ ‡æ”¶ç›ŠçŽ‡çš„ä¸‹è¡Œåå·®
            target_return = 0.005
            downside_dev_target = metrics.calculate_downside_deviation(target_return=target_return)
            below_target = sample_returns[sample_returns < target_return] - target_return
            expected_downside_dev_target = np.sqrt((below_target ** 2).mean())
            assert abs(downside_dev_target - expected_downside_dev_target) < 1e-10
    
        def test_skewness_calculation(self, sample_returns):
            """æµ‹è¯•ååº¦è®¡ç®—"""
            metrics = RiskMetrics(sample_returns)
            skewness = metrics.calculate_skewness()
    
            # ä½¿ç”¨scipyçš„skewå‡½æ•°éªŒè¯
            from scipy.stats import skew
            expected_skewness = skew(sample_returns.values)
            assert abs(skewness - expected_skewness) < 1e-2  # æ”¾å®½ç²¾åº¦è¦æ±‚
    
        def test_kurtosis_calculation(self, sample_returns):
            """æµ‹è¯•å³°åº¦è®¡ç®—"""
            metrics = RiskMetrics(sample_returns)
            kurtosis = metrics.calculate_kurtosis()
    
            # ä½¿ç”¨scipyçš„kurtosiså‡½æ•°éªŒè¯
            from scipy.stats import kurtosis as scipy_kurtosis
            expected_kurtosis = scipy_kurtosis(sample_returns.values)
            assert abs(kurtosis - expected_kurtosis) < 0.1  # è¿›ä¸€æ­¥æ”¾å®½ç²¾åº¦è¦æ±‚
    
        def test_invalid_confidence_level_error(self, sample_returns):
            """æµ‹è¯•æ— æ•ˆç½®ä¿¡æ°´å¹³é”™è¯¯"""
            metrics = RiskMetrics(sample_returns)
    
            # æµ‹è¯•ç½®ä¿¡æ°´å¹³è¶…å‡ºèŒƒå›´
            with pytest.raises(ValueError, match="ç½®ä¿¡æ°´å¹³å¿…é¡»åœ¨0å’Œ1ä¹‹é—´"):
                metrics.calculate_var(confidence_level=1.5)
    
            with pytest.raises(ValueError, match="ç½®ä¿¡æ°´å¹³å¿…é¡»åœ¨0å’Œ1ä¹‹é—´"):
                metrics.calculate_var(confidence_level=-0.1)
    
    
    class TestRiskAdjustedMetrics:
        """é£Žé™©è°ƒæ•´æŒ‡æ ‡æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_returns(self):
            """åˆ›å»ºæ ·æœ¬æ”¶ç›ŠçŽ‡æ•°æ®"""
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            return pd.Series(returns, index=dates)
    
        @pytest.fixture
        def sample_portfolio_values(self):
            """åˆ›å»ºæ ·æœ¬ç»„åˆä»·å€¼åºåˆ—"""
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)
            
            values = [1000000]
            for ret in returns:
                values.append(values[-1] * (1 + ret))
            
            return pd.Series(values[1:], index=dates)
    
        def test_sharpe_ratio_calculation(self, sample_returns):
            """æµ‹è¯•å¤æ™®æ¯”çŽ‡è®¡ç®—"""
            metrics = RiskAdjustedMetrics(sample_returns)
    
            # é»˜è®¤æ— é£Žé™©åˆ©çŽ‡
            sharpe_ratio = metrics.calculate_sharpe_ratio()
            
            # æ‰‹åŠ¨è®¡ç®—éªŒè¯
            excess_returns = sample_returns - 0.03/252  # é»˜è®¤3%å¹´åŒ–æ— é£Žé™©åˆ©çŽ‡
            expected_sharpe = excess_returns.mean() / excess_returns.std() * np.sqrt(252)
            assert abs(sharpe_ratio - expected_sharpe) < 1e-10
    
            # è‡ªå®šä¹‰æ— é£Žé™©åˆ©çŽ‡
            risk_free_rate = 0.05  # 5%
            sharpe_ratio_custom = metrics.calculate_sharpe_ratio(risk_free_rate=risk_free_rate)
            excess_returns_custom = sample_returns - risk_free_rate/252
            expected_sharpe_custom = excess_returns_custom.mean() / excess_returns_custom.std() * np.sqrt(252)
            assert abs(sharpe_ratio_custom - expected_sharpe_custom) < 1e-10
    
        def test_sortino_ratio_calculation(self, sample_returns):
            """æµ‹è¯•ç´¢æè¯ºæ¯”çŽ‡è®¡ç®—"""
            metrics = RiskAdjustedMetrics(sample_returns)
    
            sortino_ratio = metrics.calculate_sortino_ratio()
    
            # æ‰‹åŠ¨è®¡ç®—éªŒè¯
            target_return = 0.03/252  # é»˜è®¤3%å¹´åŒ–ç›®æ ‡æ”¶ç›ŠçŽ‡
            excess_returns = sample_returns - target_return
            downside_returns = excess_returns[excess_returns < 0]
            downside_deviation = np.sqrt((downside_returns ** 2).mean())
            expected_sortino = excess_returns.mean() / downside_deviation * np.sqrt(252)
            
            assert abs(sortino_ratio - expected_sortino) < 1e-10
    
        def test_calmar_ratio_calculation(self, sample_returns, sample_portfolio_values):
            """æµ‹è¯•å¡çŽ›æ¯”çŽ‡è®¡ç®—"""
            metrics = RiskAdjustedMetrics(sample_returns)
    
            calmar_ratio = metrics.calculate_calmar_ratio(sample_portfolio_values)
    
            # æ‰‹åŠ¨è®¡ç®—éªŒè¯
            annualized_return = (1 + sample_returns).prod() ** (252 / len(sample_returns)) - 1
            
            # è®¡ç®—æœ€å¤§å›žæ’¤
            peak = sample_portfolio_values.expanding().max()
            drawdown = (sample_portfolio_values - peak) / peak
            max_drawdown = abs(drawdown.min())
            
            expected_calmar = annualized_return / max_drawdown if max_drawdown > 0 else 0
            assert abs(calmar_ratio - expected_calmar) < 1e-10
    
        def test_information_ratio_calculation(self, sample_returns):
            """æµ‹è¯•ä¿¡æ¯æ¯”çŽ‡è®¡ç®—"""
            # åˆ›å»ºåŸºå‡†æ”¶ç›ŠçŽ‡
            np.random.seed(43)
            benchmark_returns = pd.Series(
                np.random.normal(0.0005, 0.015, 252),
                index=sample_returns.index
            )
    
            metrics = RiskAdjustedMetrics(sample_returns)
            info_ratio = metrics.calculate_information_ratio(benchmark_returns)
    
            # æ‰‹åŠ¨è®¡ç®—éªŒè¯
            active_returns = sample_returns - benchmark_returns
            tracking_error = active_returns.std() * np.sqrt(252)
            expected_info_ratio = active_returns.mean() * 252 / tracking_error if tracking_error > 0 else 0
    
            assert abs(info_ratio - expected_info_ratio) < 1e-10
    
        def test_treynor_ratio_calculation(self, sample_returns):
            """æµ‹è¯•ç‰¹é›·è¯ºæ¯”çŽ‡è®¡ç®—"""
            metrics = RiskAdjustedMetrics(sample_returns)
    
            # å‡è®¾beta = 1.2
            beta = 1.2
            treynor_ratio = metrics.calculate_treynor_ratio(beta=beta)
    
            # æ‰‹åŠ¨è®¡ç®—éªŒè¯
            risk_free_rate = 0.03/252
            excess_returns = sample_returns - risk_free_rate
            expected_treynor = excess_returns.mean() * 252 / beta
    
            assert abs(treynor_ratio - expected_treynor) < 1e-10
    
        def test_zero_volatility_handling(self):
            """æµ‹è¯•é›¶æ³¢åŠ¨çŽ‡å¤„ç†"""
            # åˆ›å»ºé›¶æ³¢åŠ¨çŽ‡çš„æ”¶ç›Šåºåˆ—
            constant_returns = pd.Series([0.001] * 252)
            metrics = RiskAdjustedMetrics(constant_returns)
    
            # å¤æ™®æ¯”çŽ‡åº”è¯¥ä¸ºæ— ç©·å¤§æˆ–å¤„ç†ä¸ºç‰¹æ®Šå€¼
            sharpe_ratio = metrics.calculate_sharpe_ratio()
            # é›¶æ³¢åŠ¨çŽ‡æ—¶ï¼Œå¤æ™®æ¯”çŽ‡ä¼šéžå¸¸å¤§ï¼ˆæŽ¥è¿‘æ— ç©·å¤§ï¼‰
            assert np.isinf(sharpe_ratio) or abs(sharpe_ratio) > 1e10
    
        def test_invalid_benchmark_error(self, sample_returns):
            """æµ‹è¯•æ— æ•ˆåŸºå‡†é”™è¯¯"""
            metrics = RiskAdjustedMetrics(sample_returns)
    
            # é•¿åº¦ä¸åŒ¹é…çš„åŸºå‡†
            short_benchmark = pd.Series([0.001] * 100)
            with pytest.raises(ValueError, match="åŸºå‡†æ”¶ç›ŠçŽ‡åºåˆ—é•¿åº¦ä¸ŽæŠ•èµ„ç»„åˆæ”¶ç›ŠçŽ‡ä¸åŒ¹é…"):
                metrics.calculate_information_ratio(short_benchmark)
    
    
    class TestTradingMetrics:
        """äº¤æ˜“æŒ‡æ ‡æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_trades(self):
            """åˆ›å»ºæ ·æœ¬äº¤æ˜“æ•°æ®"""
            trades = [
                Trade("000001.SZ", OrderType.BUY, 1000, Decimal("10.00"), datetime(2023, 1, 2), Decimal("10.00")),
                Trade("000001.SZ", OrderType.SELL, 500, Decimal("11.00"), datetime(2023, 1, 15), Decimal("5.50")),
                Trade("000002.SZ", OrderType.BUY, 2000, Decimal("5.00"), datetime(2023, 1, 20), Decimal("10.00")),
                Trade("000001.SZ", OrderType.SELL, 500, Decimal("10.50"), datetime(2023, 2, 1), Decimal("5.25")),
                Trade("000002.SZ", OrderType.SELL, 1000, Decimal("5.50"), datetime(2023, 2, 10), Decimal("5.50"))
            ]
            return trades
    
        @pytest.fixture
        def sample_portfolio_values(self):
            """åˆ›å»ºæ ·æœ¬ç»„åˆä»·å€¼åºåˆ—"""
            dates = pd.date_range('2023-01-01', periods=60, freq='D')
            np.random.seed(42)
            values = np.random.uniform(900000, 1100000, 60)
            return pd.Series(values, index=dates)
    
        def test_trading_metrics_initialization(self, sample_trades, sample_portfolio_values):
            """æµ‹è¯•äº¤æ˜“æŒ‡æ ‡åˆå§‹åŒ–"""
            metrics = TradingMetrics(sample_trades, sample_portfolio_values)
            assert len(metrics.trades) == 5
            assert len(metrics.portfolio_values) == 60
    
        def test_turnover_rate_calculation(self, sample_trades, sample_portfolio_values):
            """æµ‹è¯•æ¢æ‰‹çŽ‡è®¡ç®—"""
            metrics = TradingMetrics(sample_trades, sample_portfolio_values)
    
            # æœˆåº¦æ¢æ‰‹çŽ‡
            monthly_turnover = metrics.calculate_turnover_rate(period='monthly')
            assert isinstance(monthly_turnover, pd.Series)
            assert len(monthly_turnover) > 0
    
            # å¹´åŒ–æ¢æ‰‹çŽ‡
            annual_turnover = metrics.calculate_turnover_rate(period='annual')
            assert isinstance(annual_turnover, float)
            assert annual_turnover >= 0
    
        def test_transaction_cost_analysis(self, sample_trades, sample_portfolio_values):
            """æµ‹è¯•äº¤æ˜“æˆæœ¬åˆ†æž"""
            metrics = TradingMetrics(sample_trades, sample_portfolio_values)
    
            cost_analysis = metrics.calculate_transaction_cost_analysis()
    
            # éªŒè¯è¿”å›žçš„å­—å…¸åŒ…å«å¿…è¦çš„å­—æ®µ
            assert 'total_commission' in cost_analysis
            assert 'commission_rate' in cost_analysis
            assert 'cost_per_trade' in cost_analysis
            assert 'cost_ratio_to_portfolio' in cost_analysis
    
            # éªŒè¯æ•°å€¼åˆç†æ€§
            assert cost_analysis['total_commission'] >= 0
            assert cost_analysis['commission_rate'] >= 0
            assert cost_analysis['cost_per_trade'] >= 0
            assert cost_analysis['cost_ratio_to_portfolio'] >= 0
    
        def test_holding_period_analysis(self, sample_trades):
            """æµ‹è¯•æŒä»“å‘¨æœŸåˆ†æž"""
            metrics = TradingMetrics(sample_trades, pd.Series([1000000]))
    
            holding_analysis = metrics.calculate_holding_period_analysis()
    
            # éªŒè¯è¿”å›žçš„å­—å…¸åŒ…å«å¿…è¦çš„å­—æ®µ
            assert 'average_holding_days' in holding_analysis
            assert 'median_holding_days' in holding_analysis
            assert 'max_holding_days' in holding_analysis
            assert 'min_holding_days' in holding_analysis
    
            # éªŒè¯æ•°å€¼åˆç†æ€§
            assert holding_analysis['average_holding_days'] >= 0
            assert holding_analysis['median_holding_days'] >= 0
            assert holding_analysis['max_holding_days'] >= holding_analysis['min_holding_days']
    
        def test_win_loss_analysis(self, sample_trades):
            """æµ‹è¯•ç›ˆäºåˆ†æž"""
            metrics = TradingMetrics(sample_trades, pd.Series([1000000]))
    
            win_loss_analysis = metrics.calculate_win_loss_analysis()
    
            # éªŒè¯è¿”å›žçš„å­—å…¸åŒ…å«å¿…è¦çš„å­—æ®µ
            assert 'win_rate' in win_loss_analysis
            assert 'profit_loss_ratio' in win_loss_analysis
            assert 'average_win' in win_loss_analysis
            assert 'average_loss' in win_loss_analysis
            assert 'total_trades' in win_loss_analysis
    
            # éªŒè¯æ•°å€¼åˆç†æ€§
            assert 0 <= win_loss_analysis['win_rate'] <= 1
            assert win_loss_analysis['total_trades'] == len([t for t in sample_trades if t.trade_type == OrderType.SELL])
    
        def test_position_concentration_analysis(self, sample_trades, sample_portfolio_values):
            """æµ‹è¯•æŒä»“é›†ä¸­åº¦åˆ†æž"""
            metrics = TradingMetrics(sample_trades, sample_portfolio_values)
    
            concentration = metrics.calculate_position_concentration()
    
            # éªŒè¯è¿”å›žçš„å­—å…¸åŒ…å«å¿…è¦çš„å­—æ®µ
            assert 'herfindahl_index' in concentration
            assert 'max_position_weight' in concentration
            assert 'top_5_concentration' in concentration
            assert 'effective_positions' in concentration
    
            # éªŒè¯æ•°å€¼åˆç†æ€§
            assert 0 <= concentration['herfindahl_index'] <= 1
            assert 0 <= concentration['max_position_weight'] <= 1
            assert concentration['effective_positions'] >= 1
    
        def test_empty_trades_handling(self):
            """æµ‹è¯•ç©ºäº¤æ˜“åˆ—è¡¨å¤„ç†"""
            empty_trades = []
            portfolio_values = pd.Series([1000000])
            
            # åº”è¯¥èƒ½å¤Ÿå¤„ç†ç©ºäº¤æ˜“åˆ—è¡¨è€Œä¸æŠ›å‡ºå¼‚å¸¸
            metrics = TradingMetrics(empty_trades, portfolio_values)
            
            # ä½†æŸäº›è®¡ç®—å¯èƒ½è¿”å›žé»˜è®¤å€¼æˆ–æŠ›å‡ºåˆç†çš„é”™è¯¯
            cost_analysis = metrics.calculate_transaction_cost_analysis()
            assert cost_analysis['total_commission'] == 0
    
    
    class TestPortfolioMetrics:
        """ç»„åˆæŒ‡æ ‡æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            np.random.seed(42)
            returns = np.random.normal(0.001, 0.02, 252)
            
            portfolio_values = [1000000]
            for ret in returns:
                portfolio_values.append(portfolio_values[-1] * (1 + ret))
            
            trades = [
                Trade("000001.SZ", OrderType.BUY, 1000, Decimal("10.00"), datetime(2023, 1, 2), Decimal("10.00")),
                Trade("000001.SZ", OrderType.SELL, 500, Decimal("11.00"), datetime(2023, 6, 15), Decimal("5.50")),
                Trade("000002.SZ", OrderType.BUY, 2000, Decimal("5.00"), datetime(2023, 3, 20), Decimal("10.00")),
            ]
    
            return {
                'returns': pd.Series(returns, index=dates),
                'portfolio_values': pd.Series(portfolio_values[1:], index=dates),
                'trades': trades
            }
    
        def test_comprehensive_metrics_calculation(self, sample_data):
            """æµ‹è¯•ç»¼åˆæŒ‡æ ‡è®¡ç®—"""
            metrics = PortfolioMetrics(
                returns=sample_data['returns'],
                portfolio_values=sample_data['portfolio_values'],
                trades=sample_data['trades']
            )
    
            comprehensive_metrics = metrics.calculate_comprehensive_metrics()
    
            # éªŒè¯è¿”å›žçš„æŒ‡æ ‡åŒ…å«æ‰€æœ‰ç±»åˆ«
            assert 'return_metrics' in comprehensive_metrics
            assert 'risk_metrics' in comprehensive_metrics
            assert 'risk_adjusted_metrics' in comprehensive_metrics
            assert 'trading_metrics' in comprehensive_metrics
    
            # éªŒè¯æ¯ä¸ªç±»åˆ«åŒ…å«åˆç†çš„æŒ‡æ ‡
            return_metrics = comprehensive_metrics['return_metrics']
            assert 'total_return' in return_metrics
            assert 'annualized_return' in return_metrics
    
            risk_metrics = comprehensive_metrics['risk_metrics']
            assert 'volatility' in risk_metrics
            assert 'max_drawdown' in risk_metrics
            assert 'var_95' in risk_metrics
    
            risk_adjusted = comprehensive_metrics['risk_adjusted_metrics']
            assert 'sharpe_ratio' in risk_adjusted
            assert 'sortino_ratio' in risk_adjusted
    
        def test_benchmark_comparison(self, sample_data):
            """æµ‹è¯•åŸºå‡†æ¯”è¾ƒ"""
            # åˆ›å»ºåŸºå‡†æ•°æ®
            np.random.seed(43)
            benchmark_returns = pd.Series(
                np.random.normal(0.0005, 0.015, 252),
                index=sample_data['returns'].index
            )
    
            metrics = PortfolioMetrics(
                returns=sample_data['returns'],
                portfolio_values=sample_data['portfolio_values'],
                trades=sample_data['trades']
            )
    
            comparison = metrics.compare_with_benchmark(benchmark_returns)
    
            # éªŒè¯æ¯”è¾ƒç»“æžœ
            assert 'portfolio_metrics' in comparison
            assert 'benchmark_metrics' in comparison
            assert 'relative_metrics' in comparison
    
            relative_metrics = comparison['relative_metrics']
            assert 'excess_return' in relative_metrics
            assert 'information_ratio' in relative_metrics
            assert 'tracking_error' in relative_metrics
    
        def test_rolling_metrics_calculation(self, sample_data):
            """æµ‹è¯•æ»šåŠ¨æŒ‡æ ‡è®¡ç®—"""
            metrics = PortfolioMetrics(
                returns=sample_data['returns'],
                portfolio_values=sample_data['portfolio_values'],
                trades=sample_data['trades']
            )
    
            # 30å¤©æ»šåŠ¨å¤æ™®æ¯”çŽ‡
            rolling_sharpe = metrics.calculate_rolling_metrics(window=30, metric='sharpe_ratio')
            assert isinstance(rolling_sharpe, pd.Series)
            # rolling()æ–¹æ³•ä¼šä¿æŒåŽŸåºåˆ—é•¿åº¦ï¼Œå‰é¢çš„å€¼ä¸ºNaN
            assert len(rolling_sharpe) == len(sample_data['returns'])
            # æœ‰æ•ˆå€¼çš„æ•°é‡åº”è¯¥æ˜¯ len - window + 1
            valid_values = rolling_sharpe.dropna()
            assert len(valid_values) == len(sample_data['returns']) - 30 + 1
    
            # 60å¤©æ»šåŠ¨æ³¢åŠ¨çŽ‡
            rolling_vol = metrics.calculate_rolling_metrics(window=60, metric='volatility')
            assert isinstance(rolling_vol, pd.Series)
            assert len(rolling_vol) == len(sample_data['returns'])  # ä¿æŒåŽŸåºåˆ—é•¿åº¦
    
        def test_sector_analysis(self, sample_data):
            """æµ‹è¯•è¡Œä¸šåˆ†æž"""
            # æ·»åŠ è¡Œä¸šä¿¡æ¯åˆ°äº¤æ˜“æ•°æ®
            sector_mapping = {
                "000001.SZ": "é‡‘èž",
                "000002.SZ": "åœ°äº§"
            }
    
            metrics = PortfolioMetrics(
                returns=sample_data['returns'],
                portfolio_values=sample_data['portfolio_values'],
                trades=sample_data['trades']
            )
    
            sector_analysis = metrics.calculate_sector_analysis(sector_mapping)
    
            # éªŒè¯è¡Œä¸šåˆ†æžç»“æžœ
            assert isinstance(sector_analysis, dict)
            assert len(sector_analysis) > 0
    
            # éªŒè¯æ¯ä¸ªè¡Œä¸šçš„æŒ‡æ ‡
            for sector, sector_metrics in sector_analysis.items():
                assert 'weight' in sector_metrics
                assert 'return_contribution' in sector_metrics
                assert 'trade_count' in sector_metrics
    
        def test_invalid_data_validation(self):
            """æµ‹è¯•æ— æ•ˆæ•°æ®éªŒè¯"""
            # æµ‹è¯•é•¿åº¦ä¸åŒ¹é…çš„æ•°æ®
            returns = pd.Series([0.01, 0.02])
            portfolio_values = pd.Series([1000000, 1010000, 1020000])  # é•¿åº¦ä¸åŒ¹é…
            trades = []
    
            with pytest.raises(ValueError, match="æ”¶ç›ŠçŽ‡åºåˆ—å’Œç»„åˆä»·å€¼åºåˆ—é•¿åº¦ä¸åŒ¹é…"):
                PortfolioMetrics(returns, portfolio_values, trades)
    ]]></file>
  <file path="tests/unit/test_multi_frequency_backtest.py"><![CDATA[
    """
    å¤šé¢‘çŽ‡å›žæµ‹å¼•æ“Žçš„å•å…ƒæµ‹è¯•
    æµ‹è¯•æ—¥é¢‘å’Œåˆ†é’Ÿé¢‘å›žæµ‹åŠŸèƒ½ã€äº¤æ˜“æ‰§è¡Œæ¨¡æ‹Ÿå’Œæˆäº¤ä»·æ ¼å¤„ç†ã€å›žæµ‹ç»“æžœçš„å‡†ç¡®æ€§å’Œæ€§èƒ½
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta, date, time
    from typing import Dict, List, Tuple, Any, Optional
    from decimal import Decimal
    from enum import Enum
    
    from src.rl_trading_system.backtest.multi_frequency_backtest import (
        MultiFrequencyBacktest,
        BacktestConfig,
        BacktestResult,
        ExecutionMode,
        PriceMode,
        OrderType,
        Order,
        Trade,
        Position,
        Portfolio
    )
    
    
    class TestExecutionMode:
        """äº¤æ˜“æ‰§è¡Œæ¨¡å¼æµ‹è¯•ç±»"""
    
        def test_execution_mode_enum_values(self):
            """æµ‹è¯•äº¤æ˜“æ‰§è¡Œæ¨¡å¼æžšä¸¾å€¼"""
            assert ExecutionMode.NEXT_BAR == "next_bar"
            assert ExecutionMode.NEXT_CLOSE == "next_close"
            assert ExecutionMode.NEXT_OPEN == "next_open"
            assert ExecutionMode.MARKET_ORDER == "market_order"
            assert ExecutionMode.LIMIT_ORDER == "limit_order"
    
    
    class TestPriceMode:
        """ä»·æ ¼æ¨¡å¼æµ‹è¯•ç±»"""
    
        def test_price_mode_enum_values(self):
            """æµ‹è¯•ä»·æ ¼æ¨¡å¼æžšä¸¾å€¼"""
            assert PriceMode.CLOSE == "close"
            assert PriceMode.OPEN == "open"
            assert PriceMode.HIGH == "high"
            assert PriceMode.LOW == "low"
            assert PriceMode.VWAP == "vwap"
            assert PriceMode.TWAP == "twap"
    
    
    class TestOrderType:
        """è®¢å•ç±»åž‹æµ‹è¯•ç±»"""
    
        def test_order_type_enum_values(self):
            """æµ‹è¯•è®¢å•ç±»åž‹æžšä¸¾å€¼"""
            assert OrderType.BUY == "buy"
            assert OrderType.SELL == "sell"
            assert OrderType.SHORT == "short"
            assert OrderType.COVER == "cover"
    
    
    class TestOrder:
        """è®¢å•æµ‹è¯•ç±»"""
    
        def test_order_creation(self):
            """æµ‹è¯•è®¢å•åˆ›å»º"""
            order = Order(
                symbol="000001.SZ",
                order_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50"),
                timestamp=datetime.now()
            )
    
            assert order.symbol == "000001.SZ"
            assert order.order_type == OrderType.BUY
            assert order.quantity == 1000
            assert order.price == Decimal("10.50")
            assert isinstance(order.timestamp, datetime)
            assert order.order_id is not None
            assert order.status == "pending"
    
        def test_order_validation(self):
            """æµ‹è¯•è®¢å•éªŒè¯"""
            # æµ‹è¯•æ— æ•ˆæ•°é‡
            with pytest.raises(ValueError, match="è®¢å•æ•°é‡å¿…é¡»ä¸ºæ­£æ•°"):
                Order(
                    symbol="000001.SZ",
                    order_type=OrderType.BUY,
                    quantity=0,
                    price=Decimal("10.50")
                )
    
            # æµ‹è¯•æ— æ•ˆä»·æ ¼
            with pytest.raises(ValueError, match="è®¢å•ä»·æ ¼å¿…é¡»ä¸ºæ­£æ•°"):
                Order(
                    symbol="000001.SZ",
                    order_type=OrderType.BUY,
                    quantity=1000,
                    price=Decimal("-10.50")
                )
    
            # æµ‹è¯•ç©ºè‚¡ç¥¨ä»£ç 
            with pytest.raises(ValueError, match="è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º"):
                Order(
                    symbol="",
                    order_type=OrderType.BUY,
                    quantity=1000,
                    price=Decimal("10.50")
                )
    
        def test_order_execution_price_calculation(self):
            """æµ‹è¯•è®¢å•æ‰§è¡Œä»·æ ¼è®¡ç®—"""
            order = Order(
                symbol="000001.SZ",
                order_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50")
            )
    
            # æ¨¡æ‹Ÿå¸‚åœºæ•°æ®
            market_data = {
                'open': 10.45,
                'high': 10.60,
                'low': 10.40,
                'close': 10.55,
                'volume': 1000000,
                'vwap': 10.52
            }
    
            # æµ‹è¯•ä¸åŒä»·æ ¼æ¨¡å¼ä¸‹çš„æ‰§è¡Œä»·æ ¼
            assert order.get_execution_price(market_data, PriceMode.CLOSE) == Decimal("10.55")
            assert order.get_execution_price(market_data, PriceMode.OPEN) == Decimal("10.45")
            assert order.get_execution_price(market_data, PriceMode.HIGH) == Decimal("10.60")
            assert order.get_execution_price(market_data, PriceMode.LOW) == Decimal("10.40")
            assert order.get_execution_price(market_data, PriceMode.VWAP) == Decimal("10.52")
    
    
    class TestTrade:
        """äº¤æ˜“æµ‹è¯•ç±»"""
    
        def test_trade_creation(self):
            """æµ‹è¯•äº¤æ˜“åˆ›å»º"""
            trade = Trade(
                symbol="000001.SZ",
                trade_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50"),
                timestamp=datetime.now(),
                commission=Decimal("10.50")
            )
    
            assert trade.symbol == "000001.SZ"
            assert trade.trade_type == OrderType.BUY
            assert trade.quantity == 1000
            assert trade.price == Decimal("10.50")
            assert trade.commission == Decimal("10.50")
            assert trade.trade_id is not None
    
        def test_trade_value_calculation(self):
            """æµ‹è¯•äº¤æ˜“ä»·å€¼è®¡ç®—"""
            trade = Trade(
                symbol="000001.SZ",
                trade_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50"),
                commission=Decimal("10.50")
            )
    
            # ä¹°å…¥äº¤æ˜“ä»·å€¼ = -(quantity * price + commission)
            expected_value = -(1000 * Decimal("10.50") + Decimal("10.50"))
            assert trade.get_trade_value() == expected_value
    
            # å–å‡ºäº¤æ˜“
            sell_trade = Trade(
                symbol="000001.SZ",
                trade_type=OrderType.SELL,
                quantity=1000,
                price=Decimal("11.00"),
                commission=Decimal("11.00")
            )
    
            # å–å‡ºäº¤æ˜“ä»·å€¼ = quantity * price - commission
            expected_sell_value = 1000 * Decimal("11.00") - Decimal("11.00")
            assert sell_trade.get_trade_value() == expected_sell_value
    
    
    class TestPosition:
        """æŒä»“æµ‹è¯•ç±»"""
    
        def test_position_creation(self):
            """æµ‹è¯•æŒä»“åˆ›å»º"""
            position = Position(symbol="000001.SZ")
    
            assert position.symbol == "000001.SZ"
            assert position.quantity == 0
            assert position.avg_price == Decimal("0")
            assert position.market_value == Decimal("0")
            assert position.unrealized_pnl == Decimal("0")
    
        def test_position_buy_operations(self):
            """æµ‹è¯•æŒä»“ä¹°å…¥æ“ä½œ"""
            position = Position(symbol="000001.SZ")
    
            # ç¬¬ä¸€æ¬¡ä¹°å…¥
            position.update_position(
                trade_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50")
            )
    
            assert position.quantity == 1000
            assert position.avg_price == Decimal("10.50")
    
            # ç¬¬äºŒæ¬¡ä¹°å…¥ï¼ˆä¸åŒä»·æ ¼ï¼‰
            position.update_position(
                trade_type=OrderType.BUY,
                quantity=500,
                price=Decimal("11.00")
            )
    
            assert position.quantity == 1500
            # å¹³å‡ä»·æ ¼ = (1000*10.50 + 500*11.00) / 1500
            expected_avg_price = (Decimal("10500") + Decimal("5500")) / Decimal("1500")
            assert abs(position.avg_price - expected_avg_price) < Decimal("0.01")
    
        def test_position_sell_operations(self):
            """æµ‹è¯•æŒä»“å–å‡ºæ“ä½œ"""
            position = Position(symbol="000001.SZ")
    
            # å…ˆä¹°å…¥
            position.update_position(
                trade_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50")
            )
    
            # éƒ¨åˆ†å–å‡º
            position.update_position(
                trade_type=OrderType.SELL,
                quantity=300,
                price=Decimal("11.00")
            )
    
            assert position.quantity == 700
            assert position.avg_price == Decimal("10.50")  # å–å‡ºä¸æ”¹å˜å¹³å‡æˆæœ¬
    
        def test_position_market_value_calculation(self):
            """æµ‹è¯•æŒä»“å¸‚å€¼è®¡ç®—"""
            position = Position(symbol="000001.SZ")
    
            position.update_position(
                trade_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50")
            )
    
            # æ›´æ–°å¸‚åœºä»·æ ¼
            current_price = Decimal("11.20")
            position.update_market_value(current_price)
    
            expected_market_value = 1000 * current_price
            assert position.market_value == expected_market_value
    
            # æœªå®žçŽ°ç›ˆäº = å¸‚å€¼ - æˆæœ¬
            expected_cost = 1000 * Decimal("10.50")
            expected_unrealized_pnl = expected_market_value - expected_cost
            assert position.unrealized_pnl == expected_unrealized_pnl
    
        def test_position_insufficient_quantity_error(self):
            """æµ‹è¯•æŒä»“æ•°é‡ä¸è¶³é”™è¯¯"""
            position = Position(symbol="000001.SZ")
    
            # æ²¡æœ‰æŒä»“å´è¦å–å‡º
            with pytest.raises(ValueError, match="æŒä»“æ•°é‡ä¸è¶³"):
                position.update_position(
                    trade_type=OrderType.SELL,
                    quantity=100,
                    price=Decimal("10.50")
                )
    
            # ä¹°å…¥åŽå–å‡ºè¶…è¿‡æŒä»“æ•°é‡
            position.update_position(
                trade_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50")
            )
    
            with pytest.raises(ValueError, match="æŒä»“æ•°é‡ä¸è¶³"):
                position.update_position(
                    trade_type=OrderType.SELL,
                    quantity=1500,
                    price=Decimal("11.00")
                )
    
    
    class TestPortfolio:
        """ç»„åˆæµ‹è¯•ç±»"""
    
        def test_portfolio_creation(self):
            """æµ‹è¯•ç»„åˆåˆ›å»º"""
            portfolio = Portfolio(initial_cash=Decimal("1000000"))
    
            assert portfolio.cash == Decimal("1000000")
            assert portfolio.initial_cash == Decimal("1000000")
            assert len(portfolio.positions) == 0
            assert portfolio.total_value == Decimal("1000000")
    
        def test_portfolio_order_execution(self):
            """æµ‹è¯•ç»„åˆè®¢å•æ‰§è¡Œ"""
            portfolio = Portfolio(initial_cash=Decimal("1000000"))
    
            # åˆ›å»ºä¹°å…¥è®¢å•
            order = Order(
                symbol="000001.SZ",
                order_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50")
            )
    
            # æ¨¡æ‹Ÿå¸‚åœºæ•°æ®
            market_data = pd.Series({
                'open': 10.45,
                'high': 10.60,
                'low': 10.40,
                'close': 10.55,
                'volume': 1000000
            })
    
            # æ‰§è¡Œè®¢å•
            trade = portfolio.execute_order(
                order=order,
                market_data=market_data,
                price_mode=PriceMode.CLOSE,
                commission_rate=Decimal("0.001"),
                stamp_tax_rate=Decimal("0.001")
            )
    
            # éªŒè¯äº¤æ˜“ç»“æžœ
            assert trade.symbol == "000001.SZ"
            assert trade.trade_type == OrderType.BUY
            assert trade.quantity == 1000
            assert trade.price == Decimal("10.55")
    
            # éªŒè¯ç»„åˆçŠ¶æ€
            assert "000001.SZ" in portfolio.positions
            assert portfolio.positions["000001.SZ"].quantity == 1000
    
            # éªŒè¯çŽ°é‡‘å‡å°‘
            expected_cost = 1000 * Decimal("10.55") + trade.commission
            expected_cash = Decimal("1000000") - expected_cost
            assert abs(portfolio.cash - expected_cash) < Decimal("0.01")
    
        def test_portfolio_insufficient_cash_error(self):
            """æµ‹è¯•ç»„åˆçŽ°é‡‘ä¸è¶³é”™è¯¯"""
            portfolio = Portfolio(initial_cash=Decimal("10000"))  # çŽ°é‡‘ä¸è¶³
    
            order = Order(
                symbol="000001.SZ",
                order_type=OrderType.BUY,
                quantity=10000,  # éœ€è¦çº¦10ä¸‡å…ƒ
                price=Decimal("10.50")
            )
    
            market_data = pd.Series({
                'close': 10.55
            })
    
            # åº”è¯¥æŠ›å‡ºçŽ°é‡‘ä¸è¶³å¼‚å¸¸
            with pytest.raises(ValueError, match="çŽ°é‡‘ä¸è¶³"):
                portfolio.execute_order(
                    order=order,
                    market_data=market_data,
                    price_mode=PriceMode.CLOSE,
                    commission_rate=Decimal("0.001"),
                    stamp_tax_rate=Decimal("0.001")
                )
    
        def test_portfolio_performance_calculation(self):
            """æµ‹è¯•ç»„åˆä¸šç»©è®¡ç®—"""
            portfolio = Portfolio(initial_cash=Decimal("1000000"))
    
            # ä¹°å…¥è‚¡ç¥¨
            order = Order(
                symbol="000001.SZ",
                order_type=OrderType.BUY,
                quantity=1000,
                price=Decimal("10.50")
            )
    
            market_data = pd.Series({'close': 10.50})
            portfolio.execute_order(order, market_data, PriceMode.CLOSE, Decimal("0.001"), Decimal("0.001"))
    
            # æ›´æ–°å¸‚åœºä»·æ ¼
            current_prices = {"000001.SZ": Decimal("11.50")}
            portfolio.update_market_values(current_prices)
    
            # è®¡ç®—æ€»ä»·å€¼å’Œæ”¶ç›ŠçŽ‡
            performance = portfolio.get_performance_metrics()
    
            assert performance['total_value'] > portfolio.initial_cash
            assert performance['total_return'] > 0
            assert performance['cash_ratio'] < 1.0
    
    
    class TestBacktestConfig:
        """å›žæµ‹é…ç½®æµ‹è¯•ç±»"""
    
        def test_backtest_config_creation(self):
            """æµ‹è¯•å›žæµ‹é…ç½®åˆ›å»º"""
            config = BacktestConfig(
                start_date=date(2023, 1, 1),
                end_date=date(2023, 12, 31),
                initial_capital=1000000.0,
                frequency="1d",
                execution_mode=ExecutionMode.NEXT_CLOSE,
                price_mode=PriceMode.CLOSE,
                commission_rate=0.001,
                stamp_tax_rate=0.001
            )
    
            assert config.start_date == date(2023, 1, 1)
            assert config.end_date == date(2023, 12, 31)
            assert config.initial_capital == 1000000.0
            assert config.frequency == "1d"
            assert config.execution_mode == ExecutionMode.NEXT_CLOSE
            assert config.price_mode == PriceMode.CLOSE
            assert config.commission_rate == 0.001
            assert config.stamp_tax_rate == 0.001
    
        def test_backtest_config_validation(self):
            """æµ‹è¯•å›žæµ‹é…ç½®éªŒè¯"""
            # æµ‹è¯•ç»“æŸæ—¥æœŸæ—©äºŽå¼€å§‹æ—¥æœŸ
            with pytest.raises(ValueError, match="ç»“æŸæ—¥æœŸä¸èƒ½æ—©äºŽå¼€å§‹æ—¥æœŸ"):
                BacktestConfig(
                    start_date=date(2023, 12, 31),
                    end_date=date(2023, 1, 1),
                    initial_capital=1000000.0
                )
    
            # æµ‹è¯•æ— æ•ˆçš„åˆå§‹èµ„é‡‘
            with pytest.raises(ValueError, match="åˆå§‹èµ„é‡‘å¿…é¡»ä¸ºæ­£æ•°"):
                BacktestConfig(
                    start_date=date(2023, 1, 1),
                    end_date=date(2023, 12, 31),
                    initial_capital=-1000.0
                )
    
            # æµ‹è¯•æ— æ•ˆçš„ä½£é‡‘çŽ‡
            with pytest.raises(ValueError, match="ä½£é‡‘çŽ‡å¿…é¡»ä¸ºéžè´Ÿæ•°"):
                BacktestConfig(
                    start_date=date(2023, 1, 1),
                    end_date=date(2023, 12, 31),
                    initial_capital=1000000.0,
                    commission_rate=-0.001
                )
    
        def test_backtest_config_frequency_validation(self):
            """æµ‹è¯•å›žæµ‹é¢‘çŽ‡éªŒè¯"""
            # æœ‰æ•ˆé¢‘çŽ‡
            valid_frequencies = ["1d", "1h", "30min", "15min", "5min", "1min"]
            for freq in valid_frequencies:
                config = BacktestConfig(
                    start_date=date(2023, 1, 1),
                    end_date=date(2023, 12, 31),
                    initial_capital=1000000.0,
                    frequency=freq
                )
                assert config.frequency == freq
    
            # æ— æ•ˆé¢‘çŽ‡
            with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„é¢‘çŽ‡"):
                BacktestConfig(
                    start_date=date(2023, 1, 1),
                    end_date=date(2023, 12, 31),
                    initial_capital=1000000.0,
                    frequency="invalid"
                )
    
    
    class TestBacktestResult:
        """å›žæµ‹ç»“æžœæµ‹è¯•ç±»"""
    
        def test_backtest_result_creation(self):
            """æµ‹è¯•å›žæµ‹ç»“æžœåˆ›å»º"""
            # æ¨¡æ‹Ÿäº¤æ˜“åŽ†å²
            trades = [
                Trade("000001.SZ", OrderType.BUY, 1000, Decimal("10.50"), datetime.now(), Decimal("10.50")),
                Trade("000001.SZ", OrderType.SELL, 500, Decimal("11.00"), datetime.now(), Decimal("5.50"))
            ]
    
            # æ¨¡æ‹Ÿç»„åˆä»·å€¼åŽ†å²
            portfolio_values = pd.Series(
                [1000000, 1005000, 1010000, 1008000, 1012000],
                index=pd.date_range('2023-01-01', periods=5, freq='D')
            )
    
            result = BacktestResult(
                trades=trades,
                portfolio_values=portfolio_values,
                positions={},
                final_cash=Decimal("950000")
            )
    
            assert len(result.trades) == 2
            assert len(result.portfolio_values) == 5
            assert result.final_cash == Decimal("950000")
    
        def test_backtest_result_performance_metrics(self):
            """æµ‹è¯•å›žæµ‹ç»“æžœæ€§èƒ½æŒ‡æ ‡è®¡ç®—"""
            # åˆ›å»ºä¸€ä¸ªç®€å•çš„æ”¶ç›Šåºåˆ—
            returns = [0.01, -0.005, 0.015, -0.01, 0.02]
            portfolio_values = [1000000]
            for r in returns:
                portfolio_values.append(portfolio_values[-1] * (1 + r))
    
            portfolio_values = pd.Series(
                portfolio_values,
                index=pd.date_range('2023-01-01', periods=6, freq='D')
            )
    
            result = BacktestResult(
                trades=[],
                portfolio_values=portfolio_values,
                positions={},
                final_cash=Decimal("0")
            )
    
            metrics = result.calculate_performance_metrics()
    
            # éªŒè¯åŸºæœ¬æŒ‡æ ‡
            assert 'total_return' in metrics
            assert 'annualized_return' in metrics
            assert 'volatility' in metrics
            assert 'sharpe_ratio' in metrics
            assert 'max_drawdown' in metrics
            assert 'win_rate' in metrics
    
            # éªŒè¯æ€»æ”¶ç›ŠçŽ‡
            expected_total_return = (portfolio_values.iloc[-1] / portfolio_values.iloc[0]) - 1
            assert abs(metrics['total_return'] - expected_total_return) < 0.0001
    
            # éªŒè¯æœ€å¤§å›žæ’¤ä¸ºæ­£æ•°
            assert metrics['max_drawdown'] >= 0
    
    
    class TestMultiFrequencyBacktest:
        """å¤šé¢‘çŽ‡å›žæµ‹å¼•æ“Žæµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬å¸‚åœºæ•°æ®"""
            dates = pd.date_range('2023-01-01', '2023-01-10', freq='D')
            symbols = ['000001.SZ', '000002.SZ']
    
            data_list = []
            for symbol in symbols:
                for date in dates:
                    data_list.append({
                        'symbol': symbol,
                        'datetime': date,
                        'open': 10.0 + np.random.randn() * 0.1,
                        'high': 10.2 + np.random.randn() * 0.1,
                        'low': 9.8 + np.random.randn() * 0.1,
                        'close': 10.0 + np.random.randn() * 0.1,
                        'volume': 1000000 + np.random.randint(-100000, 100000)
                    })
    
            df = pd.DataFrame(data_list)
            df = df.set_index(['datetime', 'symbol'])
            return df
    
        @pytest.fixture
        def backtest_config(self):
            """åˆ›å»ºå›žæµ‹é…ç½®"""
            return BacktestConfig(
                start_date=date(2023, 1, 1),
                end_date=date(2023, 1, 10),
                initial_capital=1000000.0,
                frequency="1d",
                execution_mode=ExecutionMode.NEXT_CLOSE,
                price_mode=PriceMode.CLOSE,
                commission_rate=0.001,
                stamp_tax_rate=0.001
            )
    
        @pytest.fixture
        def simple_strategy(self):
            """åˆ›å»ºç®€å•çš„ä¹°å…¥æŒæœ‰ç­–ç•¥"""
            def strategy_func(data, portfolio, timestamp):
                """ç®€å•ç­–ç•¥ï¼šç¬¬ä¸€å¤©ä¹°å…¥ï¼Œæœ€åŽä¸€å¤©å–å‡º"""
                orders = []
    
                if timestamp == pd.Timestamp('2023-01-02'):  # ç¬¬äºŒä¸ªäº¤æ˜“æ—¥ä¹°å…¥
                    orders.append(Order(
                        symbol="000001.SZ",
                        order_type=OrderType.BUY,
                        quantity=1000,
                        price=Decimal("10.0")
                    ))
                elif timestamp == pd.Timestamp('2023-01-09'):  # å€’æ•°ç¬¬äºŒä¸ªäº¤æ˜“æ—¥å–å‡º
                    if "000001.SZ" in portfolio.positions and portfolio.positions["000001.SZ"].quantity > 0:
                        orders.append(Order(
                            symbol="000001.SZ",
                            order_type=OrderType.SELL,
                            quantity=portfolio.positions["000001.SZ"].quantity,
                            price=Decimal("10.0")
                        ))
    
                return orders
    
            return strategy_func
    
        def test_backtest_engine_initialization(self, backtest_config):
            """æµ‹è¯•å›žæµ‹å¼•æ“Žåˆå§‹åŒ–"""
            engine = MultiFrequencyBacktest(backtest_config)
    
            assert engine.config == backtest_config
            assert engine.portfolio.initial_cash == Decimal(str(backtest_config.initial_capital))
            assert len(engine.trades) == 0
            assert len(engine.portfolio_values) == 0
    
        def test_backtest_data_validation(self, backtest_config):
            """æµ‹è¯•å›žæµ‹æ•°æ®éªŒè¯"""
            engine = MultiFrequencyBacktest(backtest_config)
    
            # æµ‹è¯•ç©ºæ•°æ®
            with pytest.raises(ValueError, match="å›žæµ‹æ•°æ®ä¸èƒ½ä¸ºç©º"):
                engine.run(data=pd.DataFrame(), strategy=lambda *args: [])
    
            # æµ‹è¯•ç¼ºå°‘å¿…è¦åˆ—çš„æ•°æ®
            invalid_data = pd.DataFrame({
                'symbol': ['000001.SZ'],
                'datetime': [pd.Timestamp('2023-01-01')],
                'close': [10.0]
                # ç¼ºå°‘ open, high, low, volume
            }).set_index(['datetime', 'symbol'])
    
            with pytest.raises(ValueError, match="æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—"):
                engine.run(data=invalid_data, strategy=lambda *args: [])
    
        def test_backtest_daily_frequency_execution(self, sample_data, backtest_config, simple_strategy):
            """æµ‹è¯•æ—¥é¢‘å›žæµ‹æ‰§è¡Œ"""
            engine = MultiFrequencyBacktest(backtest_config)
            result = engine.run(data=sample_data, strategy=simple_strategy)
    
            # éªŒè¯å›žæµ‹ç»“æžœ
            assert isinstance(result, BacktestResult)
            assert len(result.trades) >= 1  # è‡³å°‘æœ‰ä¹°å…¥äº¤æ˜“
            assert len(result.portfolio_values) > 0
    
            # éªŒè¯ç»„åˆä»·å€¼åºåˆ—çš„è¿žç»­æ€§
            assert result.portfolio_values.index.is_monotonic_increasing
    
            # éªŒè¯æœ€ç»ˆèµ„é‡‘ + æŒä»“å¸‚å€¼ = æ€»ä»·å€¼
            final_total_value = float(result.final_cash)
            for position in result.positions.values():
                final_total_value += float(position.market_value)
    
            assert abs(final_total_value - result.portfolio_values.iloc[-1]) < 1.0
    
        def test_backtest_minute_frequency_data_handling(self, backtest_config):
            """æµ‹è¯•åˆ†é’Ÿé¢‘å›žæµ‹æ•°æ®å¤„ç†"""
            # åˆ›å»ºåˆ†é’Ÿé¢‘æ•°æ®
            minute_dates = pd.date_range('2023-01-01 09:30:00', '2023-01-01 15:00:00', freq='1min')
            minute_data_list = []
    
            for ts in minute_dates:
                minute_data_list.append({
                    'symbol': '000001.SZ',
                    'datetime': ts,
                    'open': 10.0 + np.random.randn() * 0.01,
                    'high': 10.02 + np.random.randn() * 0.01,
                    'low': 9.98 + np.random.randn() * 0.01,
                    'close': 10.0 + np.random.randn() * 0.01,
                    'volume': 1000 + np.random.randint(-100, 100)
                })
    
            minute_data = pd.DataFrame(minute_data_list).set_index(['datetime', 'symbol'])
    
            # æ›´æ–°é…ç½®ä¸ºåˆ†é’Ÿé¢‘
            minute_config = BacktestConfig(
                start_date=date(2023, 1, 1),
                end_date=date(2023, 1, 1),
                initial_capital=1000000.0,
                frequency="1min",
                execution_mode=ExecutionMode.NEXT_CLOSE,
                price_mode=PriceMode.CLOSE
            )
    
            engine = MultiFrequencyBacktest(minute_config)
    
            # ç®€å•ç­–ç•¥ï¼šåœ¨ç¬¬ä¸€åˆ†é’Ÿä¹°å…¥
            def minute_strategy(data, portfolio, timestamp):
                orders = []
                if timestamp == minute_dates[0]:
                    orders.append(Order(
                        symbol="000001.SZ",
                        order_type=OrderType.BUY,
                        quantity=100,
                        price=Decimal("10.0")
                    ))
                return orders
    
            result = engine.run(data=minute_data, strategy=minute_strategy)
    
            # éªŒè¯åˆ†é’Ÿé¢‘å›žæµ‹ç»“æžœ
            assert isinstance(result, BacktestResult)
            assert len(result.portfolio_values) > 100  # åˆ†é’Ÿé¢‘åº”è¯¥æœ‰å¾ˆå¤šæ•°æ®ç‚¹
    
        def test_backtest_execution_modes(self, sample_data, simple_strategy):
            """æµ‹è¯•ä¸åŒæ‰§è¡Œæ¨¡å¼"""
            base_config = BacktestConfig(
                start_date=date(2023, 1, 1),
                end_date=date(2023, 1, 10),
                initial_capital=1000000.0,
                frequency="1d"
            )
    
            execution_modes = [
                ExecutionMode.NEXT_CLOSE,
                ExecutionMode.NEXT_OPEN,
                ExecutionMode.MARKET_ORDER
            ]
    
            results = {}
            for mode in execution_modes:
                config = BacktestConfig(
                    start_date=base_config.start_date,
                    end_date=base_config.end_date,
                    initial_capital=base_config.initial_capital,
                    frequency=base_config.frequency,
                    execution_mode=mode,
                    price_mode=PriceMode.CLOSE
                )
    
                engine = MultiFrequencyBacktest(config)
                result = engine.run(data=sample_data, strategy=simple_strategy)
                results[mode] = result
    
            # éªŒè¯ä¸åŒæ‰§è¡Œæ¨¡å¼éƒ½èƒ½æ­£å¸¸è¿è¡Œ
            for mode, result in results.items():
                assert isinstance(result, BacktestResult)
                assert len(result.portfolio_values) > 0
    
        def test_backtest_price_modes(self, sample_data, simple_strategy):
            """æµ‹è¯•ä¸åŒä»·æ ¼æ¨¡å¼"""
            base_config = BacktestConfig(
                start_date=date(2023, 1, 1),
                end_date=date(2023, 1, 10),
                initial_capital=1000000.0,
                frequency="1d",
                execution_mode=ExecutionMode.NEXT_CLOSE
            )
    
            price_modes = [PriceMode.CLOSE, PriceMode.OPEN, PriceMode.HIGH, PriceMode.LOW]
    
            results = {}
            for mode in price_modes:
                config = BacktestConfig(
                    start_date=base_config.start_date,
                    end_date=base_config.end_date,
                    initial_capital=base_config.initial_capital,
                    frequency=base_config.frequency,
                    execution_mode=base_config.execution_mode,
                    price_mode=mode
                )
    
                engine = MultiFrequencyBacktest(config)
                result = engine.run(data=sample_data, strategy=simple_strategy)
                results[mode] = result
    
            # éªŒè¯ä¸åŒä»·æ ¼æ¨¡å¼éƒ½èƒ½æ­£å¸¸è¿è¡Œï¼Œå¹¶ä¸”å¯èƒ½äº§ç”Ÿä¸åŒçš„ç»“æžœ
            for mode, result in results.items():
                assert isinstance(result, BacktestResult)
                assert len(result.trades) >= 1
    
        def test_backtest_transaction_cost_calculation(self, sample_data, backtest_config, simple_strategy):
            """æµ‹è¯•äº¤æ˜“æˆæœ¬è®¡ç®—"""
            # è®¾ç½®è¾ƒé«˜çš„äº¤æ˜“æˆæœ¬ä»¥ä¾¿æµ‹è¯•
            high_cost_config = BacktestConfig(
                start_date=backtest_config.start_date,
                end_date=backtest_config.end_date,
                initial_capital=backtest_config.initial_capital,
                frequency=backtest_config.frequency,
                execution_mode=backtest_config.execution_mode,
                price_mode=backtest_config.price_mode,
                commission_rate=0.01,  # 1% ä½£é‡‘çŽ‡
                stamp_tax_rate=0.01   # 1% å°èŠ±ç¨ŽçŽ‡
            )
    
            engine = MultiFrequencyBacktest(high_cost_config)
            result = engine.run(data=sample_data, strategy=simple_strategy)
    
            # éªŒè¯äº¤æ˜“æˆæœ¬è¢«æ­£ç¡®è®¡ç®—
            total_commission = sum(float(trade.commission) for trade in result.trades)
            assert total_commission > 0
    
            # éªŒè¯ä¹°å…¥å’Œå–å‡ºçš„ä½£é‡‘è®¡ç®—
            buy_trades = [t for t in result.trades if t.trade_type == OrderType.BUY]
            sell_trades = [t for t in result.trades if t.trade_type == OrderType.SELL]
    
            if buy_trades:
                buy_trade = buy_trades[0]
                expected_buy_commission = float(buy_trade.quantity) * float(buy_trade.price) * 0.01
                assert abs(float(buy_trade.commission) - expected_buy_commission) < 0.01
    
            if sell_trades:
                sell_trade = sell_trades[0]
                # å–å‡ºä½£é‡‘ = ä½£é‡‘çŽ‡ + å°èŠ±ç¨ŽçŽ‡
                expected_sell_commission = float(sell_trade.quantity) * float(sell_trade.price) * 0.02
                assert abs(float(sell_trade.commission) - expected_sell_commission) < 0.01
    
        def test_backtest_performance_accuracy(self, sample_data, backtest_config):
            """æµ‹è¯•å›žæµ‹æ€§èƒ½å‡†ç¡®æ€§"""
            # åˆ›å»ºä¸€ä¸ªç¡®å®šæ€§ç­–ç•¥ä»¥ä¾¿éªŒè¯å‡†ç¡®æ€§
            def deterministic_strategy(data, portfolio, timestamp):
                orders = []
    
                # åœ¨ç‰¹å®šæ—¥æœŸæ‰§è¡Œç‰¹å®šäº¤æ˜“
                if timestamp == pd.Timestamp('2023-01-02'):
                    orders.append(Order(
                        symbol="000001.SZ",
                        order_type=OrderType.BUY,
                        quantity=1000,
                        price=Decimal("10.0")  # æŒ‡å®šä»·æ ¼
                    ))
    
                return orders
    
            engine = MultiFrequencyBacktest(backtest_config)
            result = engine.run(data=sample_data, strategy=deterministic_strategy)
    
            # æ‰‹åŠ¨è®¡ç®—é¢„æœŸç»“æžœå¹¶éªŒè¯
            if result.trades:
                buy_trade = result.trades[0]
                expected_cost = float(buy_trade.quantity) * float(buy_trade.price) + float(buy_trade.commission)
                expected_remaining_cash = backtest_config.initial_capital - expected_cost
    
                # éªŒè¯çŽ°é‡‘ä½™é¢çš„å‡†ç¡®æ€§ï¼ˆå…è®¸å°çš„èˆå…¥è¯¯å·®ï¼‰
                assert abs(float(result.final_cash) - expected_remaining_cash) < 1.0
    
        def test_backtest_data_frequency_mismatch_handling(self, backtest_config):
            """æµ‹è¯•æ•°æ®é¢‘çŽ‡ä¸åŒ¹é…å¤„ç†"""
            # åˆ›å»ºå°æ—¶é¢‘æ•°æ®ä½†é…ç½®ä¸ºæ—¥é¢‘å›žæµ‹
            hourly_dates = pd.date_range('2023-01-01 09:00:00', '2023-01-02 16:00:00', freq='1h')
            hourly_data_list = []
    
            for ts in hourly_dates:
                hourly_data_list.append({
                    'symbol': '000001.SZ',
                    'datetime': ts,
                    'open': 10.0,
                    'high': 10.1,
                    'low': 9.9,
                    'close': 10.0,
                    'volume': 1000
                })
    
            hourly_data = pd.DataFrame(hourly_data_list).set_index(['datetime', 'symbol'])
    
            engine = MultiFrequencyBacktest(backtest_config)  # æ—¥é¢‘é…ç½®
    
            # ç­–ç•¥
            def strategy(data, portfolio, timestamp):
                return []
    
            # å›žæµ‹å¼•æ“Žåº”è¯¥èƒ½å¤Ÿå¤„ç†é¢‘çŽ‡ä¸åŒ¹é…ï¼Œè‡ªåŠ¨é‡é‡‡æ ·
            result = engine.run(data=hourly_data, strategy=strategy)
            assert isinstance(result, BacktestResult)
    
        def test_backtest_edge_cases(self, backtest_config):
            """æµ‹è¯•è¾¹ç•Œæƒ…å†µ"""
            # åˆ›å»ºåªæœ‰ä¸€å¤©æ•°æ®çš„æƒ…å†µ
            single_day_data = pd.DataFrame({
                'symbol': ['000001.SZ'],
                'datetime': [pd.Timestamp('2023-01-01')],
                'open': [10.0],
                'high': [10.1],
                'low': [9.9],
                'close': [10.0],
                'volume': [1000]
            }).set_index(['datetime', 'symbol'])
    
            single_day_config = BacktestConfig(
                start_date=date(2023, 1, 1),
                end_date=date(2023, 1, 1),
                initial_capital=1000000.0,
                frequency="1d"
            )
    
            engine = MultiFrequencyBacktest(single_day_config)
    
            def empty_strategy(data, portfolio, timestamp):
                return []
    
            result = engine.run(data=single_day_data, strategy=empty_strategy)
    
            # éªŒè¯å•æ—¥å›žæµ‹èƒ½æ­£å¸¸è¿è¡Œ
            assert isinstance(result, BacktestResult)
            assert len(result.portfolio_values) >= 1
            assert result.portfolio_values.iloc[0] == single_day_config.initial_capital
    
        def test_backtest_strategy_exception_handling(self, sample_data, backtest_config):
            """æµ‹è¯•ç­–ç•¥å¼‚å¸¸å¤„ç†"""
            def faulty_strategy(data, portfolio, timestamp):
                # æ•…æ„æŠ›å‡ºå¼‚å¸¸çš„ç­–ç•¥
                raise RuntimeError("ç­–ç•¥æ‰§è¡Œé”™è¯¯")
    
            engine = MultiFrequencyBacktest(backtest_config)
    
            # æ ¹æ®å¼€å‘è§„åˆ™1ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œåº”è¯¥è®©å¼‚å¸¸æš´éœ²
            # å› æ­¤è¿™é‡Œåº”è¯¥ç›´æŽ¥æŠ›å‡ºå¼‚å¸¸è€Œä¸æ˜¯è¢«æ•èŽ·
            with pytest.raises(RuntimeError, match="ç­–ç•¥æ‰§è¡Œé”™è¯¯"):
                engine.run(data=sample_data, strategy=faulty_strategy)
    
        def test_backtest_invalid_order_handling(self, sample_data, backtest_config):
            """æµ‹è¯•æ— æ•ˆè®¢å•å¤„ç†"""
            def invalid_order_strategy(data, portfolio, timestamp):
                if timestamp == pd.Timestamp('2023-01-02'):
                    # è¿”å›žæ— æ•ˆè®¢å•ï¼ˆè´Ÿæ•°é‡ï¼‰
                    return [Order(
                        symbol="000001.SZ",
                        order_type=OrderType.BUY,
                        quantity=-1000,  # æ— æ•ˆæ•°é‡
                        price=Decimal("10.0")
                    )]
                return []
    
            engine = MultiFrequencyBacktest(backtest_config)
    
            # æ ¹æ®å¼€å‘è§„åˆ™1ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œåº”è¯¥è®©å¼‚å¸¸æš´éœ²
            # æ— æ•ˆè®¢å•åº”è¯¥ç›´æŽ¥æŠ›å‡ºValueError
            with pytest.raises(ValueError, match="è®¢å•æ•°é‡å¿…é¡»ä¸ºæ­£æ•°"):
                engine.run(data=sample_data, strategy=invalid_order_strategy)
    ]]></file>
  <file path="tests/unit/test_feature_engineer.py"><![CDATA[
    """
    ç‰¹å¾å·¥ç¨‹æ¨¡å—æµ‹è¯•ç”¨ä¾‹
    æµ‹è¯•æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€åŸºæœ¬é¢å› å­å’Œå¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾è®¡ç®—
    """
    
    import pytest
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    from unittest.mock import Mock, patch
    
    from src.rl_trading_system.data.feature_engineer import FeatureEngineer
    from src.rl_trading_system.data.data_models import MarketData, FeatureVector
    
    
    # Global fixtures
    @pytest.fixture
    def feature_engineer():
        """åˆ›å»ºç‰¹å¾å·¥ç¨‹å™¨å®žä¾‹"""
        return FeatureEngineer()
    
    @pytest.fixture
    def sample_price_data():
        """åˆ›å»ºæ ·æœ¬ä»·æ ¼æ•°æ®"""
        dates = pd.date_range('2023-01-01', periods=100, freq='D')
        np.random.seed(42)
        
        # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
        base_price = 100.0
        returns = np.random.normal(0, 0.02, 100)
        prices = [base_price]
        
        for ret in returns[1:]:
            prices.append(prices[-1] * (1 + ret))
        
        data = pd.DataFrame({
            'datetime': dates,
            'symbol': ['000001.SZ'] * 100,
            'open': [p * (1 + np.random.normal(0, 0.001)) for p in prices],
            'high': [p * (1 + abs(np.random.normal(0, 0.005))) for p in prices],
            'low': [p * (1 - abs(np.random.normal(0, 0.005))) for p in prices],
            'close': prices,
            'volume': np.random.randint(1000000, 10000000, 100),
            'amount': [p * v for p, v in zip(prices, np.random.randint(1000000, 10000000, 100))]
        })
        
        # ç¡®ä¿ä»·æ ¼é€»è¾‘å…³ç³»æ­£ç¡®
        data['high'] = np.maximum(data['high'], data[['open', 'close']].max(axis=1))
        data['low'] = np.minimum(data['low'], data[['open', 'close']].min(axis=1))
        
        return data.set_index(['datetime', 'symbol'])
    
    @pytest.fixture
    def sample_fundamental_data():
        """åˆ›å»ºæ ·æœ¬åŸºæœ¬é¢æ•°æ®"""
        dates = pd.date_range('2023-01-01', periods=20, freq='Q')  # å­£åº¦æ•°æ®
        
        data = pd.DataFrame({
            'datetime': dates,
            'symbol': ['000001.SZ'] * 20,
            'pe_ratio': np.random.uniform(10, 30, 20),
            'pb_ratio': np.random.uniform(1, 5, 20),
            'roe': np.random.uniform(0.05, 0.25, 20),
            'roa': np.random.uniform(0.02, 0.15, 20),
            'debt_ratio': np.random.uniform(0.2, 0.8, 20),
            'current_ratio': np.random.uniform(1.0, 3.0, 20),
            'revenue_growth': np.random.uniform(-0.1, 0.3, 20),
            'profit_growth': np.random.uniform(-0.2, 0.5, 20)
        })
        
        return data.set_index(['datetime', 'symbol'])
    
    
    class TestFeatureEngineer:
        """ç‰¹å¾å·¥ç¨‹å™¨æµ‹è¯•ç±»"""
        pass
    
    
    class TestTechnicalIndicators:
        """æŠ€æœ¯æŒ‡æ ‡è®¡ç®—æµ‹è¯•"""
        
        def test_calculate_sma(self, feature_engineer, sample_price_data):
            """æµ‹è¯•ç®€å•ç§»åŠ¨å¹³å‡çº¿è®¡ç®—"""
            result = feature_engineer.calculate_sma(sample_price_data, window=20)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['sma_5', 'sma_10', 'sma_20', 'sma_60']
            for col in expected_columns:
                assert col in result.columns
            
            # éªŒè¯æ•°å€¼åˆç†æ€§
            assert not result['sma_20'].isnull().all()
            sma_20_valid = result['sma_20'].dropna()
            assert len(sma_20_valid) > 0
            assert (sma_20_valid > 0).all()
            
            # éªŒè¯ç§»åŠ¨å¹³å‡çº¿çš„å•è°ƒæ€§ï¼ˆåœ¨è¶‹åŠ¿æ˜Žæ˜¾æ—¶ï¼‰
            close_prices = sample_price_data['close'].values
            sma_20 = result['sma_20'].dropna().values
            
            # ç®€å•éªŒè¯ï¼šSMAåº”è¯¥å¹³æ»‘ä»·æ ¼æ³¢åŠ¨
            assert len(sma_20) > 0
        
        def test_calculate_ema(self, feature_engineer, sample_price_data):
            """æµ‹è¯•æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿è®¡ç®—"""
            result = feature_engineer.calculate_ema(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['ema_12', 'ema_26']
            for col in expected_columns:
                assert col in result.columns
            
            # éªŒè¯æ•°å€¼åˆç†æ€§
            assert not result['ema_12'].isnull().all()
            ema_12_valid = result['ema_12'].dropna()
            assert len(ema_12_valid) > 0
            assert (ema_12_valid > 0).all()
            
            # éªŒè¯EMAçš„å“åº”æ€§ï¼ˆåº”è¯¥æ¯”SMAæ›´å¿«å“åº”ä»·æ ¼å˜åŒ–ï¼‰
            sma_result = feature_engineer.calculate_sma(sample_price_data, window=12)
            if 'sma_12' in sma_result.columns:
                # EMAå’ŒSMAåº”è¯¥æœ‰ç›¸ä¼¼çš„è¶‹åŠ¿ä½†ä¸å®Œå…¨ç›¸åŒ
                correlation = result['ema_12'].corr(sma_result['sma_12'])
                assert correlation > 0.8  # é«˜ç›¸å…³æ€§ä½†ä¸å®Œå…¨ç›¸åŒ
        
        def test_calculate_rsi(self, feature_engineer, sample_price_data):
            """æµ‹è¯•ç›¸å¯¹å¼ºå¼±æŒ‡æ•°è®¡ç®—"""
            result = feature_engineer.calculate_rsi(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            assert 'rsi_14' in result.columns
            
            # éªŒè¯RSIèŒƒå›´åœ¨0-100ä¹‹é—´
            rsi_values = result['rsi_14'].dropna()
            assert (rsi_values >= 0).all()
            assert (rsi_values <= 100).all()
            
            # éªŒè¯RSIçš„åˆç†æ€§
            assert len(rsi_values) > 0
            assert rsi_values.std() > 0  # RSIåº”è¯¥æœ‰å˜åŒ–
        
        def test_calculate_macd(self, feature_engineer, sample_price_data):
            """æµ‹è¯•MACDæŒ‡æ ‡è®¡ç®—"""
            result = feature_engineer.calculate_macd(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['macd', 'macd_signal', 'macd_histogram']
            for col in expected_columns:
                assert col in result.columns
            
            # éªŒè¯MACDçš„æ•°å­¦å…³ç³»
            macd_values = result.dropna()
            if len(macd_values) > 0:
                # MACDç›´æ–¹å›¾ = MACD - ä¿¡å·çº¿
                calculated_histogram = macd_values['macd'] - macd_values['macd_signal']
                np.testing.assert_array_almost_equal(
                    calculated_histogram.values,
                    macd_values['macd_histogram'].values,
                    decimal=6
                )
        
        def test_calculate_bollinger_bands(self, feature_engineer, sample_price_data):
            """æµ‹è¯•å¸ƒæž—å¸¦è®¡ç®—"""
            result = feature_engineer.calculate_bollinger_bands(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['bb_upper', 'bb_middle', 'bb_lower', 'bb_width', 'bb_position']
            for col in expected_columns:
                assert col in result.columns
            
            # éªŒè¯å¸ƒæž—å¸¦çš„æ•°å­¦å…³ç³»
            bb_data = result.dropna()
            if len(bb_data) > 0:
                # ä¸Šè½¨ > ä¸­è½¨ > ä¸‹è½¨
                assert (bb_data['bb_upper'] >= bb_data['bb_middle']).all()
                assert (bb_data['bb_middle'] >= bb_data['bb_lower']).all()
                
                # å¸ƒæž—å¸¦å®½åº¦ = ä¸Šè½¨ - ä¸‹è½¨
                calculated_width = bb_data['bb_upper'] - bb_data['bb_lower']
                np.testing.assert_array_almost_equal(
                    calculated_width.values,
                    bb_data['bb_width'].values,
                    decimal=6
                )
                
                # å¸ƒæž—å¸¦ä½ç½®é€šå¸¸åœ¨0-1ä¹‹é—´ï¼Œä½†å¯èƒ½è¶…å‡ºèŒƒå›´ï¼ˆè¿™æ˜¯æ­£å¸¸çš„ï¼‰
                # éªŒè¯å¸ƒæž—å¸¦ä½ç½®çš„è®¡ç®—æ˜¯å¦æ­£ç¡®
                expected_position = (sample_price_data['close'] - bb_data['bb_lower']) / bb_data['bb_width']
                expected_position = expected_position.dropna()
                actual_position = bb_data['bb_position']
                
                # ç¡®ä¿ç´¢å¼•å¯¹é½
                common_index = expected_position.index.intersection(actual_position.index)
                if len(common_index) > 0:
                    np.testing.assert_array_almost_equal(
                        expected_position.loc[common_index].values,
                        actual_position.loc[common_index].values,
                        decimal=6
                    )
        
        def test_calculate_stochastic(self, feature_engineer, sample_price_data):
            """æµ‹è¯•éšæœºæŒ‡æ ‡è®¡ç®—"""
            result = feature_engineer.calculate_stochastic(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['stoch_k', 'stoch_d']
            for col in expected_columns:
                assert col in result.columns
            
            # éªŒè¯éšæœºæŒ‡æ ‡èŒƒå›´åœ¨0-100ä¹‹é—´
            stoch_data = result.dropna()
            if len(stoch_data) > 0:
                assert (stoch_data['stoch_k'] >= 0).all()
                assert (stoch_data['stoch_k'] <= 100).all()
                assert (stoch_data['stoch_d'] >= 0).all()
                assert (stoch_data['stoch_d'] <= 100).all()
        
        def test_calculate_atr(self, feature_engineer, sample_price_data):
            """æµ‹è¯•å¹³å‡çœŸå®žæ³¢å¹…è®¡ç®—"""
            result = feature_engineer.calculate_atr(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            assert 'atr_14' in result.columns
            
            # éªŒè¯ATRä¸ºæ­£å€¼
            atr_values = result['atr_14'].dropna()
            assert len(atr_values) > 0
            assert (atr_values > 0).all()
            
            # éªŒè¯ATRçš„åˆç†æ€§ï¼ˆåº”è¯¥åæ˜ ä»·æ ¼æ³¢åŠ¨ï¼‰
            assert len(atr_values) > 0
            assert atr_values.std() >= 0  # ATRåº”è¯¥æœ‰å˜åŒ–
        
        def test_calculate_volume_indicators(self, feature_engineer, sample_price_data):
            """æµ‹è¯•æˆäº¤é‡æŒ‡æ ‡è®¡ç®—"""
            result = feature_engineer.calculate_volume_indicators(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['volume_sma', 'volume_ratio', 'obv', 'vwap']
            for col in expected_columns:
                assert col in result.columns
            
            # éªŒè¯æˆäº¤é‡æŒ‡æ ‡çš„åˆç†æ€§
            volume_data = result.dropna()
            if len(volume_data) > 0:
                # æˆäº¤é‡ç§»åŠ¨å¹³å‡åº”è¯¥ä¸ºæ­£
                assert (volume_data['volume_sma'] > 0).all()
                
                # æˆäº¤é‡æ¯”çŽ‡åº”è¯¥ä¸ºæ­£
                assert (volume_data['volume_ratio'] > 0).all()
                
                # VWAPåº”è¯¥ä¸ºæ­£
                assert (volume_data['vwap'] > 0).all()
        
        @pytest.mark.parametrize("window", [5, 10, 20, 60])
        def test_technical_indicators_different_windows(self, feature_engineer, sample_price_data, window):
            """æµ‹è¯•ä¸åŒçª—å£æœŸçš„æŠ€æœ¯æŒ‡æ ‡è®¡ç®—"""
            result = feature_engineer.calculate_sma(sample_price_data, window=window)
            
            # éªŒè¯ç»“æžœåŒ…å«æŒ‡å®šçª—å£çš„æŒ‡æ ‡
            expected_col = f'sma_{window}'
            assert expected_col in result.columns
            
            # éªŒè¯å‰window-1ä¸ªå€¼ä¸ºNaN
            sma_values = result[expected_col]
            assert sma_values.iloc[:window-1].isnull().all()
            
            # éªŒè¯åŽç»­å€¼ä¸ä¸ºNaN
            if len(sma_values) > window:
                assert not sma_values.iloc[window:].isnull().all()
    
    
    class TestFundamentalFactors:
        """åŸºæœ¬é¢å› å­æµ‹è¯•"""
        
        def test_calculate_valuation_factors(self, feature_engineer, sample_fundamental_data):
            """æµ‹è¯•ä¼°å€¼å› å­è®¡ç®—"""
            result = feature_engineer.calculate_valuation_factors(sample_fundamental_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['pe_ratio', 'pb_ratio', 'ps_ratio', 'pcf_ratio']
            for col in expected_columns:
                if col in result.columns:
                    # éªŒè¯ä¼°å€¼å› å­ä¸ºæ­£å€¼
                    values = result[col].dropna()
                    if len(values) > 0:
                        assert (values > 0).all()
        
        def test_calculate_profitability_factors(self, feature_engineer, sample_fundamental_data):
            """æµ‹è¯•ç›ˆåˆ©èƒ½åŠ›å› å­è®¡ç®—"""
            result = feature_engineer.calculate_profitability_factors(sample_fundamental_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['roe', 'roa', 'gross_margin', 'net_margin']
            for col in expected_columns:
                if col in result.columns:
                    values = result[col].dropna()
                    if len(values) > 0:
                        # ROEå’ŒROAåº”è¯¥åœ¨åˆç†èŒƒå›´å†…
                        if col in ['roe', 'roa']:
                            assert (values >= -1).all()  # å…è®¸è´Ÿå€¼ä½†ä¸åº”è¿‡åˆ†æžç«¯
                            assert (values <= 2).all()   # ä¸åº”è¶…è¿‡200%
        
        def test_calculate_growth_factors(self, feature_engineer, sample_fundamental_data):
            """æµ‹è¯•æˆé•¿æ€§å› å­è®¡ç®—"""
            result = feature_engineer.calculate_growth_factors(sample_fundamental_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['revenue_growth', 'profit_growth', 'eps_growth']
            for col in expected_columns:
                if col in result.columns:
                    values = result[col].dropna()
                    if len(values) > 0:
                        # æˆé•¿çŽ‡å¯ä»¥ä¸ºè´Ÿï¼Œä½†åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
                        assert (values >= -2).all()  # ä¸åº”ä½ŽäºŽ-200%
                        assert (values <= 5).all()   # ä¸åº”è¶…è¿‡500%
        
        def test_calculate_leverage_factors(self, feature_engineer, sample_fundamental_data):
            """æµ‹è¯•æ æ†å› å­è®¡ç®—"""
            result = feature_engineer.calculate_leverage_factors(sample_fundamental_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['debt_ratio', 'debt_to_equity', 'current_ratio', 'quick_ratio']
            for col in expected_columns:
                if col in result.columns:
                    values = result[col].dropna()
                    if len(values) > 0:
                        # æ æ†æŒ‡æ ‡åº”è¯¥ä¸ºæ­£å€¼
                        assert (values >= 0).all()
                        
                        # å€ºåŠ¡æ¯”çŽ‡ä¸åº”è¶…è¿‡100%
                        if col == 'debt_ratio':
                            assert (values <= 1).all()
    
    
    class TestMarketMicrostructure:
        """å¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾æµ‹è¯•"""
        
        def test_calculate_liquidity_features(self, feature_engineer, sample_price_data):
            """æµ‹è¯•æµåŠ¨æ€§ç‰¹å¾è®¡ç®—"""
            result = feature_engineer.calculate_liquidity_features(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['turnover_rate', 'amihud_illiquidity', 'bid_ask_spread']
            for col in expected_columns:
                if col in result.columns:
                    values = result[col].dropna()
                    if len(values) > 0:
                        # æµåŠ¨æ€§æŒ‡æ ‡åº”è¯¥ä¸ºæ­£å€¼
                        assert (values >= 0).all()
        
        def test_calculate_volatility_features(self, feature_engineer, sample_price_data):
            """æµ‹è¯•æ³¢åŠ¨çŽ‡ç‰¹å¾è®¡ç®—"""
            result = feature_engineer.calculate_volatility_features(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['realized_volatility', 'garman_klass_volatility', 'parkinson_volatility']
            for col in expected_columns:
                if col in result.columns:
                    values = result[col].dropna()
                    if len(values) > 0:
                        # æ³¢åŠ¨çŽ‡åº”è¯¥ä¸ºæ­£å€¼
                        assert (values >= 0).all()
        
        def test_calculate_momentum_features(self, feature_engineer, sample_price_data):
            """æµ‹è¯•åŠ¨é‡ç‰¹å¾è®¡ç®—"""
            result = feature_engineer.calculate_momentum_features(sample_price_data)
            
            # éªŒè¯ç»“æžœä¸ä¸ºç©º
            assert not result.empty
            
            # éªŒè¯åˆ—å
            expected_columns = ['price_momentum_1m', 'price_momentum_3m', 'volume_momentum']
            for col in expected_columns:
                if col in result.columns:
                    values = result[col].dropna()
                    if len(values) > 0:
                        # åŠ¨é‡å¯ä»¥ä¸ºæ­£æˆ–è´Ÿ
                        assert not values.isnull().all()
    
    
    class TestFeatureNormalization:
        """ç‰¹å¾æ ‡å‡†åŒ–æµ‹è¯•"""
        
        @pytest.fixture
        def sample_features(self):
            """åˆ›å»ºæ ·æœ¬ç‰¹å¾æ•°æ®"""
            np.random.seed(42)
            data = pd.DataFrame({
                'feature1': np.random.normal(100, 20, 100),
                'feature2': np.random.exponential(2, 100),
                'feature3': np.random.uniform(-10, 10, 100),
                'feature4': np.random.normal(0, 1, 100)
            })
            return data
        
        def test_z_score_normalization(self, feature_engineer, sample_features):
            """æµ‹è¯•Z-scoreæ ‡å‡†åŒ–"""
            result = feature_engineer.normalize_features(sample_features, method='zscore')
            
            # éªŒè¯ç»“æžœå½¢çŠ¶
            assert result.shape == sample_features.shape
            
            # éªŒè¯æ ‡å‡†åŒ–åŽçš„ç»Ÿè®¡ç‰¹æ€§
            for col in result.columns:
                values = result[col].dropna()
                if len(values) > 1:
                    # å‡å€¼åº”è¯¥æŽ¥è¿‘0
                    assert abs(values.mean()) < 0.1
                    # æ ‡å‡†å·®åº”è¯¥æŽ¥è¿‘1
                    assert abs(values.std() - 1) < 0.1
        
        def test_min_max_normalization(self, feature_engineer, sample_features):
            """æµ‹è¯•Min-Maxæ ‡å‡†åŒ–"""
            result = feature_engineer.normalize_features(sample_features, method='minmax')
            
            # éªŒè¯ç»“æžœå½¢çŠ¶
            assert result.shape == sample_features.shape
            
            # éªŒè¯æ ‡å‡†åŒ–åŽçš„èŒƒå›´
            for col in result.columns:
                values = result[col].dropna()
                if len(values) > 0:
                    # å€¼åº”è¯¥åœ¨[0, 1]èŒƒå›´å†…
                    assert (values >= 0).all()
                    assert (values <= 1).all()
                    # æœ€å°å€¼åº”è¯¥æŽ¥è¿‘0ï¼Œæœ€å¤§å€¼åº”è¯¥æŽ¥è¿‘1
                    assert abs(values.min()) < 0.01
                    assert abs(values.max() - 1) < 0.01
        
        def test_robust_normalization(self, feature_engineer, sample_features):
            """æµ‹è¯•é²æ£’æ ‡å‡†åŒ–"""
            result = feature_engineer.normalize_features(sample_features, method='robust')
            
            # éªŒè¯ç»“æžœå½¢çŠ¶
            assert result.shape == sample_features.shape
            
            # éªŒè¯æ ‡å‡†åŒ–åŽçš„ç»Ÿè®¡ç‰¹æ€§
            for col in result.columns:
                values = result[col].dropna()
                if len(values) > 1:
                    # ä¸­ä½æ•°åº”è¯¥æŽ¥è¿‘0
                    assert abs(values.median()) < 0.5
        
        def test_handle_missing_values(self, feature_engineer):
            """æµ‹è¯•ç¼ºå¤±å€¼å¤„ç†"""
            # åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®
            data = pd.DataFrame({
                'feature1': [1, 2, np.nan, 4, 5],
                'feature2': [np.nan, 2, 3, 4, np.nan],
                'feature3': [1, 2, 3, 4, 5]
            })
            
            # æµ‹è¯•å‰å‘å¡«å……
            result_ffill = feature_engineer.handle_missing_values(data, method='ffill')
            assert result_ffill.isnull().sum().sum() <= data.isnull().sum().sum()
            
            # æµ‹è¯•å‡å€¼å¡«å……
            result_mean = feature_engineer.handle_missing_values(data, method='mean')
            assert result_mean.isnull().sum().sum() == 0
            
            # æµ‹è¯•ä¸­ä½æ•°å¡«å……
            result_median = feature_engineer.handle_missing_values(data, method='median')
            assert result_median.isnull().sum().sum() == 0
        
        def test_outlier_detection_and_treatment(self, feature_engineer, sample_features):
            """æµ‹è¯•å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†"""
            # æ·»åŠ ä¸€äº›å¼‚å¸¸å€¼
            data_with_outliers = sample_features.copy()
            data_with_outliers.iloc[0, 0] = 1000  # æžå¤§å€¼
            data_with_outliers.iloc[1, 1] = -1000  # æžå°å€¼
            
            # æ£€æµ‹å¼‚å¸¸å€¼
            outliers = feature_engineer.detect_outliers(data_with_outliers, method='iqr')
            assert outliers.sum().sum() > 0  # åº”è¯¥æ£€æµ‹åˆ°å¼‚å¸¸å€¼
            
            # å¤„ç†å¼‚å¸¸å€¼
            result = feature_engineer.treat_outliers(data_with_outliers, method='clip')
            
            # éªŒè¯å¼‚å¸¸å€¼è¢«å¤„ç†
            assert result.max().max() < data_with_outliers.max().max()
            assert result.min().min() > data_with_outliers.min().min()
    
    
    class TestFeatureSelection:
        """ç‰¹å¾é€‰æ‹©æµ‹è¯•"""
        
        @pytest.fixture
        def sample_features_with_target(self):
            """åˆ›å»ºå¸¦ç›®æ ‡å˜é‡çš„ç‰¹å¾æ•°æ®"""
            np.random.seed(42)
            n_samples = 100
            
            # åˆ›å»ºä¸€äº›æœ‰ç”¨çš„ç‰¹å¾
            useful_feature1 = np.random.normal(0, 1, n_samples)
            useful_feature2 = np.random.normal(0, 1, n_samples)
            
            # åˆ›å»ºç›®æ ‡å˜é‡ï¼ˆä¸Žæœ‰ç”¨ç‰¹å¾ç›¸å…³ï¼‰
            target = 0.5 * useful_feature1 + 0.3 * useful_feature2 + np.random.normal(0, 0.1, n_samples)
            
            # åˆ›å»ºä¸€äº›å™ªå£°ç‰¹å¾
            noise_features = np.random.normal(0, 1, (n_samples, 5))
            
            features = pd.DataFrame({
                'useful1': useful_feature1,
                'useful2': useful_feature2,
                'noise1': noise_features[:, 0],
                'noise2': noise_features[:, 1],
                'noise3': noise_features[:, 2],
                'noise4': noise_features[:, 3],
                'noise5': noise_features[:, 4]
            })
            
            return features, pd.Series(target)
        
        def test_correlation_based_selection(self, feature_engineer, sample_features_with_target):
            """æµ‹è¯•åŸºäºŽç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©"""
            features, target = sample_features_with_target
            
            selected_features = feature_engineer.select_features_by_correlation(
                features, target, threshold=0.1
            )
            
            # éªŒè¯é€‰æ‹©äº†ä¸€äº›ç‰¹å¾
            assert len(selected_features) > 0
            assert len(selected_features) <= len(features.columns)
            
            # éªŒè¯é€‰æ‹©çš„ç‰¹å¾ç¡®å®žä¸Žç›®æ ‡ç›¸å…³
            for feature in selected_features:
                correlation = abs(features[feature].corr(target))
                assert correlation >= 0.1
        
        def test_mutual_information_selection(self, feature_engineer, sample_features_with_target):
            """æµ‹è¯•åŸºäºŽäº’ä¿¡æ¯çš„ç‰¹å¾é€‰æ‹©"""
            features, target = sample_features_with_target
            
            selected_features = feature_engineer.select_features_by_mutual_info(
                features, target, k=3
            )
            
            # éªŒè¯é€‰æ‹©äº†æŒ‡å®šæ•°é‡çš„ç‰¹å¾
            assert len(selected_features) == 3
            
            # éªŒè¯é€‰æ‹©çš„ç‰¹å¾åœ¨åŽŸç‰¹å¾ä¸­
            for feature in selected_features:
                assert feature in features.columns
        
        def test_variance_threshold_selection(self, feature_engineer):
            """æµ‹è¯•åŸºäºŽæ–¹å·®é˜ˆå€¼çš„ç‰¹å¾é€‰æ‹©"""
            # åˆ›å»ºåŒ…å«ä½Žæ–¹å·®ç‰¹å¾çš„æ•°æ®
            data = pd.DataFrame({
                'high_var': np.random.normal(0, 10, 100),
                'medium_var': np.random.normal(0, 1, 100),
                'low_var': np.random.normal(0, 0.01, 100),
                'constant': [1] * 100
            })
            
            selected_features = feature_engineer.select_features_by_variance(
                data, threshold=0.1
            )
            
            # éªŒè¯ä½Žæ–¹å·®ç‰¹å¾è¢«è¿‡æ»¤
            assert 'constant' not in selected_features
            assert 'low_var' not in selected_features
            assert 'high_var' in selected_features
            assert 'medium_var' in selected_features
    
    
    class TestIntegrationTests:
        """é›†æˆæµ‹è¯•"""
        
        def test_complete_feature_pipeline(self, feature_engineer, sample_price_data, sample_fundamental_data):
            """æµ‹è¯•å®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµæ°´çº¿"""
            # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            technical_features = feature_engineer.calculate_technical_indicators(sample_price_data)
            assert not technical_features.empty
            
            # è®¡ç®—åŸºæœ¬é¢å› å­
            fundamental_features = feature_engineer.calculate_fundamental_factors(sample_fundamental_data)
            assert not fundamental_features.empty
            
            # è®¡ç®—å¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
            microstructure_features = feature_engineer.calculate_microstructure_features(sample_price_data)
            assert not microstructure_features.empty
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            all_features = feature_engineer.combine_features([
                technical_features,
                fundamental_features,
                microstructure_features
            ])
            assert not all_features.empty
            
            # æ ‡å‡†åŒ–ç‰¹å¾
            normalized_features = feature_engineer.normalize_features(all_features)
            assert normalized_features.shape == all_features.shape
            
            # éªŒè¯æœ€ç»ˆç‰¹å¾å‘é‡çš„åˆ›å»º
            feature_vector = feature_engineer.create_feature_vector(
                timestamp=datetime.now(),
                symbol='000001.SZ',
                normalized_features=normalized_features.iloc[-1]
            )
            
            assert isinstance(feature_vector, FeatureVector)
            assert feature_vector.symbol == '000001.SZ'
            assert len(feature_vector.technical_indicators) > 0
            assert len(feature_vector.fundamental_factors) > 0
            assert len(feature_vector.market_microstructure) > 0
        
        def test_error_handling(self, feature_engineer):
            """æµ‹è¯•é”™è¯¯å¤„ç†"""
            # æµ‹è¯•ç©ºæ•°æ®
            empty_data = pd.DataFrame()
            
            with pytest.raises(ValueError):
                feature_engineer.calculate_technical_indicators(empty_data)
            
            # æµ‹è¯•ç¼ºå°‘å¿…è¦åˆ—çš„æ•°æ®
            invalid_data = pd.DataFrame({'invalid_column': [1, 2, 3]})
            
            with pytest.raises(ValueError):
                feature_engineer.calculate_technical_indicators(invalid_data)
            
            # æµ‹è¯•åŒ…å«NaNçš„æ•°æ®å¤„ç†
            data_with_nan = pd.DataFrame({
                'close': [1, 2, np.nan, 4, 5],
                'volume': [100, 200, 300, np.nan, 500]
            })
            
            # åº”è¯¥èƒ½å¤Ÿå¤„ç†NaNå€¼è€Œä¸æŠ›å‡ºå¼‚å¸¸
            result = feature_engineer.handle_missing_values(data_with_nan)
            assert not result.isnull().all().all()
        
        @pytest.mark.parametrize("data_size", [10, 50, 100, 500])
        def test_performance_with_different_data_sizes(self, feature_engineer, data_size):
            """æµ‹è¯•ä¸åŒæ•°æ®å¤§å°ä¸‹çš„æ€§èƒ½"""
            # ç”Ÿæˆä¸åŒå¤§å°çš„æµ‹è¯•æ•°æ®
            dates = pd.date_range('2023-01-01', periods=data_size, freq='D')
            np.random.seed(42)
            
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': ['000001.SZ'] * data_size,
                'open': np.random.uniform(90, 110, data_size),
                'high': np.random.uniform(95, 115, data_size),
                'low': np.random.uniform(85, 105, data_size),
                'close': np.random.uniform(90, 110, data_size),
                'volume': np.random.randint(1000000, 10000000, data_size),
                'amount': np.random.uniform(1e8, 1e9, data_size)
            }).set_index(['datetime', 'symbol'])
            
            # ç¡®ä¿ä»·æ ¼é€»è¾‘å…³ç³»æ­£ç¡®
            data['high'] = np.maximum(data['high'], data[['open', 'close']].max(axis=1))
            data['low'] = np.minimum(data['low'], data[['open', 'close']].min(axis=1))
            
            # æµ‹è¯•æŠ€æœ¯æŒ‡æ ‡è®¡ç®—
            import time
            start_time = time.time()
            result = feature_engineer.calculate_technical_indicators(data)
            end_time = time.time()
            
            # éªŒè¯ç»“æžœ
            assert not result.empty
            
            # éªŒè¯æ€§èƒ½ï¼ˆåº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼‰
            execution_time = end_time - start_time
            assert execution_time < 10  # åº”è¯¥åœ¨10ç§’å†…å®Œæˆ
            
            # å¯¹äºŽè¾ƒå¤§çš„æ•°æ®é›†ï¼Œæ‰§è¡Œæ—¶é—´åº”è¯¥åˆç†å¢žé•¿
            if data_size >= 100:
                assert execution_time < data_size * 0.1  # æ¯100æ¡æ•°æ®ä¸è¶…è¿‡10ç§’
    ]]></file>
  <file path="tests/unit/test_data_split_strategy.py"><![CDATA[
    """
    æ•°æ®åˆ’åˆ†ç­–ç•¥çš„å•å…ƒæµ‹è¯•
    æµ‹è¯•æ—¶åºæ•°æ®çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†ã€æ»šåŠ¨çª—å£åˆ’åˆ†å’Œæ•°æ®æ³„éœ²é˜²æŠ¤
    """
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from typing import Dict, List, Tuple, Any, Optional
    from unittest.mock import Mock, MagicMock
    
    from src.rl_trading_system.training.data_split_strategy import (
        DataSplitStrategy, 
        TimeSeriesSplitStrategy,
        RollingWindowSplitStrategy,
        FixedSplitStrategy,
        SplitConfig,
        SplitResult
    )
    
    
    class TestSplitConfig:
        """æ•°æ®åˆ’åˆ†é…ç½®æµ‹è¯•ç±»"""
        
        def test_split_config_creation(self):
            """æµ‹è¯•é…ç½®åˆ›å»º"""
            config = SplitConfig(
                train_ratio=0.7,
                validation_ratio=0.2,
                test_ratio=0.1,
                min_train_samples=100,
                min_validation_samples=50,
                gap_days=1,
                rolling_window_size=252
            )
            
            assert config.train_ratio == 0.7
            assert config.validation_ratio == 0.2
            assert config.test_ratio == 0.1
            assert config.min_train_samples == 100
            assert config.min_validation_samples == 50
            assert config.gap_days == 1
            assert config.rolling_window_size == 252
        
        def test_split_config_validation(self):
            """æµ‹è¯•é…ç½®éªŒè¯"""
            # æµ‹è¯•æ¯”ä¾‹å’Œä¸ä¸º1çš„æƒ…å†µ
            with pytest.raises(ValueError, match="æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1"):
                SplitConfig(
                    train_ratio=0.6,
                    validation_ratio=0.2,
                    test_ratio=0.1
                )
            
            # æµ‹è¯•è´Ÿæ¯”ä¾‹
            with pytest.raises(ValueError, match="æ¯”ä¾‹ä¸èƒ½ä¸ºè´Ÿæ•°"):
                SplitConfig(
                    train_ratio=-0.1,
                    validation_ratio=0.6,
                    test_ratio=0.5
                )
            
            # æµ‹è¯•æœ€å°æ ·æœ¬æ•°
            with pytest.raises(ValueError, match="æœ€å°æ ·æœ¬æ•°å¿…é¡»ä¸ºæ­£æ•°"):
                SplitConfig(
                    train_ratio=0.7,
                    validation_ratio=0.2,
                    test_ratio=0.1,
                    min_train_samples=0
                )
    
    
    class TestSplitResult:
        """æ•°æ®åˆ’åˆ†ç»“æžœæµ‹è¯•ç±»"""
        
        def test_split_result_creation(self):
            """æµ‹è¯•ç»“æžœåˆ›å»º"""
            train_indices = np.array([0, 1, 2, 3, 4])
            val_indices = np.array([5, 6, 7])
            test_indices = np.array([8, 9])
            
            result = SplitResult(
                train_indices=train_indices,
                validation_indices=val_indices,
                test_indices=test_indices,
                split_dates={
                    'train_start': '2023-01-01',
                    'train_end': '2023-06-30',
                    'val_start': '2023-07-01',
                    'val_end': '2023-09-30',
                    'test_start': '2023-10-01',
                    'test_end': '2023-12-31'
                }
            )
            
            assert np.array_equal(result.train_indices, train_indices)
            assert np.array_equal(result.validation_indices, val_indices)
            assert np.array_equal(result.test_indices, test_indices)
            assert len(result.split_dates) == 6
        
        def test_split_result_validation(self):
            """æµ‹è¯•ç»“æžœéªŒè¯"""
            # æµ‹è¯•ç´¢å¼•é‡å 
            with pytest.raises(ValueError, match="è®­ç»ƒå’ŒéªŒè¯ç´¢å¼•ä¸èƒ½é‡å "):
                SplitResult(
                    train_indices=np.array([0, 1, 2]),
                    validation_indices=np.array([2, 3, 4]),  # ä¸Žè®­ç»ƒé‡å 
                    test_indices=np.array([5, 6])
                )
            
            # æµ‹è¯•ç´¢å¼•é‡å 
            with pytest.raises(ValueError, match="éªŒè¯å’Œæµ‹è¯•ç´¢å¼•ä¸èƒ½é‡å "):
                SplitResult(
                    train_indices=np.array([0, 1, 2]),
                    validation_indices=np.array([3, 4, 5]),
                    test_indices=np.array([5, 6, 7])  # ä¸ŽéªŒè¯é‡å 
                )
        
        def test_get_metrics(self):
            """æµ‹è¯•èŽ·å–ç»Ÿè®¡æŒ‡æ ‡"""
            result = SplitResult(
                train_indices=np.array([0, 1, 2, 3, 4]),
                validation_indices=np.array([5, 6, 7]),
                test_indices=np.array([8, 9])
            )
            
            metrics = result.get_metrics()
            
            assert metrics['train_size'] == 5
            assert metrics['validation_size'] == 3
            assert metrics['test_size'] == 2
            assert metrics['total_size'] == 10
            assert abs(metrics['train_ratio'] - 0.5) < 1e-6
            assert abs(metrics['validation_ratio'] - 0.3) < 1e-6
            assert abs(metrics['test_ratio'] - 0.2) < 1e-6
    
    
    class TestTimeSeriesSplitStrategy:
        """æ—¶åºæ•°æ®åˆ’åˆ†ç­–ç•¥æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ—¶åºæ•°æ®"""
            dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'TEST',
                'price': np.random.randn(len(dates)) * 10 + 100,
                'volume': np.random.randint(1000, 10000, len(dates))
            })
            return data.set_index(['datetime', 'symbol'])
        
        @pytest.fixture
        def split_config(self):
            """åˆ›å»ºåˆ’åˆ†é…ç½®"""
            return SplitConfig(
                train_ratio=0.7,
                validation_ratio=0.2,
                test_ratio=0.1,
                min_train_samples=50,
                min_validation_samples=20,
                gap_days=1
            )
        
        def test_time_series_split_basic(self, sample_data, split_config):
            """æµ‹è¯•åŸºæœ¬æ—¶åºåˆ’åˆ†"""
            strategy = TimeSeriesSplitStrategy(split_config)
            result = strategy.split(sample_data)
            
            # æ£€æŸ¥åŸºæœ¬å±žæ€§
            assert isinstance(result, SplitResult)
            assert len(result.train_indices) > 0
            assert len(result.validation_indices) > 0
            assert len(result.test_indices) > 0
            
            # æ£€æŸ¥æ—¶åºé¡ºåº
            assert result.train_indices[-1] < result.validation_indices[0]
            assert result.validation_indices[-1] < result.test_indices[0]
            
            # æ£€æŸ¥æ¯”ä¾‹
            metrics = result.get_metrics()
            assert abs(metrics['train_ratio'] - 0.7) < 0.1
            assert abs(metrics['validation_ratio'] - 0.2) < 0.1
            assert abs(metrics['test_ratio'] - 0.1) < 0.1
        
        def test_time_series_split_with_gap(self, sample_data):
            """æµ‹è¯•å¸¦é—´éš”çš„æ—¶åºåˆ’åˆ†"""
            config = SplitConfig(
                train_ratio=0.6,
                validation_ratio=0.2,
                test_ratio=0.2,
                gap_days=5  # 5å¤©é—´éš”
            )
            
            strategy = TimeSeriesSplitStrategy(config)
            result = strategy.split(sample_data)
            
            # æ£€æŸ¥é—´éš”
            train_end_date = sample_data.index.get_level_values('datetime')[result.train_indices[-1]]
            val_start_date = sample_data.index.get_level_values('datetime')[result.validation_indices[0]]
            gap = (val_start_date - train_end_date).days
            assert gap >= config.gap_days
            
            val_end_date = sample_data.index.get_level_values('datetime')[result.validation_indices[-1]]
            test_start_date = sample_data.index.get_level_values('datetime')[result.test_indices[0]]
            gap = (test_start_date - val_end_date).days
            assert gap >= config.gap_days
        
        def test_time_series_split_minimum_samples(self, split_config):
            """æµ‹è¯•æœ€å°æ ·æœ¬æ•°çº¦æŸ"""
            # åˆ›å»ºå°æ•°æ®é›†
            dates = pd.date_range(start='2023-01-01', end='2023-01-10', freq='D')
            small_data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'TEST',
                'price': np.random.randn(len(dates))
            }).set_index(['datetime', 'symbol'])
            
            strategy = TimeSeriesSplitStrategy(split_config)
            
            # åº”è¯¥æŠ›å‡ºå¼‚å¸¸ï¼Œå› ä¸ºæ•°æ®å¤ªå°‘
            with pytest.raises(ValueError, match="æ•°æ®é‡ä¸è¶³"):
                strategy.split(small_data)
        
        def test_data_leakage_prevention(self, sample_data, split_config):
            """æµ‹è¯•æ•°æ®æ³„éœ²é˜²æŠ¤"""
            strategy = TimeSeriesSplitStrategy(split_config)
            result = strategy.split(sample_data)
            
            # æ£€æŸ¥æ—¶é—´é¡ºåºï¼Œç¡®ä¿æ²¡æœ‰æœªæ¥æ•°æ®æ³„éœ²
            train_dates = sample_data.index.get_level_values('datetime')[result.train_indices]
            val_dates = sample_data.index.get_level_values('datetime')[result.validation_indices]
            test_dates = sample_data.index.get_level_values('datetime')[result.test_indices]
            
            # è®­ç»ƒæ•°æ®åº”è¯¥åœ¨éªŒè¯æ•°æ®ä¹‹å‰
            assert train_dates.max() < val_dates.min()
            
            # éªŒè¯æ•°æ®åº”è¯¥åœ¨æµ‹è¯•æ•°æ®ä¹‹å‰
            assert val_dates.max() < test_dates.min()
            
            # æ£€æŸ¥ç´¢å¼•çš„æ—¶åºæ€§
            assert np.all(np.diff(result.train_indices) >= 0)
            assert np.all(np.diff(result.validation_indices) >= 0)
            assert np.all(np.diff(result.test_indices) >= 0)
    
    
    class TestRollingWindowSplitStrategy:
        """æ»šåŠ¨çª—å£åˆ’åˆ†ç­–ç•¥æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ—¶åºæ•°æ®"""
            dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'TEST',
                'price': np.random.randn(len(dates)) * 10 + 100,
                'volume': np.random.randint(1000, 10000, len(dates))
            })
            return data.set_index(['datetime', 'symbol'])
        
        @pytest.fixture
        def rolling_config(self):
            """åˆ›å»ºæ»šåŠ¨çª—å£é…ç½®"""
            return SplitConfig(
                train_ratio=0.7,
                validation_ratio=0.2,
                test_ratio=0.1,
                rolling_window_size=252,  # ä¸€å¹´çš„äº¤æ˜“æ—¥
                step_size=63,  # å­£åº¦æ­¥é•¿
                min_train_samples=100
            )
        
        def test_rolling_window_split_basic(self, sample_data, rolling_config):
            """æµ‹è¯•åŸºæœ¬æ»šåŠ¨çª—å£åˆ’åˆ†"""
            strategy = RollingWindowSplitStrategy(rolling_config)
            splits = strategy.split_rolling(sample_data)
            
            # åº”è¯¥äº§ç”Ÿå¤šä¸ªåˆ’åˆ†
            assert len(splits) > 1
            
            # æ£€æŸ¥æ¯ä¸ªåˆ’åˆ†
            for i, result in enumerate(splits):
                assert isinstance(result, SplitResult)
                assert len(result.train_indices) > 0
                assert len(result.validation_indices) > 0
                assert len(result.test_indices) > 0
                
                # æ£€æŸ¥æ—¶åºé¡ºåº
                assert result.train_indices[-1] < result.validation_indices[0]
                assert result.validation_indices[-1] < result.test_indices[0]
        
        def test_rolling_window_progression(self, sample_data, rolling_config):
            """æµ‹è¯•æ»šåŠ¨çª—å£çš„æ—¶é—´æŽ¨è¿›"""
            strategy = RollingWindowSplitStrategy(rolling_config)
            splits = strategy.split_rolling(sample_data)
            
            # æ£€æŸ¥çª—å£æŽ¨è¿›
            for i in range(1, len(splits)):
                prev_split = splits[i-1]
                curr_split = splits[i]
                
                # å½“å‰åˆ’åˆ†çš„å¼€å§‹åº”è¯¥åœ¨å‰ä¸€ä¸ªåˆ’åˆ†ä¹‹åŽ
                prev_train_start = sample_data.index.get_level_values('datetime')[prev_split.train_indices[0]]
                curr_train_start = sample_data.index.get_level_values('datetime')[curr_split.train_indices[0]]
                
                assert curr_train_start > prev_train_start
        
        def test_rolling_window_overlap_prevention(self, sample_data, rolling_config):
            """æµ‹è¯•æ»šåŠ¨çª—å£çš„é‡å é˜²æŠ¤"""
            strategy = RollingWindowSplitStrategy(rolling_config)
            splits = strategy.split_rolling(sample_data)
            
            # æ£€æŸ¥ç›¸é‚»çª—å£çš„æ—¶é—´æŽ¨è¿›
            for i in range(1, len(splits)):
                prev_split = splits[i-1]
                curr_split = splits[i]
                
                # å½“å‰çª—å£çš„å¼€å§‹åº”è¯¥åœ¨å‰ä¸€ä¸ªçª—å£å¼€å§‹ä¹‹åŽï¼ˆå…è®¸è®­ç»ƒæ•°æ®é‡å ï¼Œä½†çª—å£æ•´ä½“åº”è¯¥æŽ¨è¿›ï¼‰
                prev_window_start = sample_data.index.get_level_values('datetime')[prev_split.train_indices[0]]
                curr_window_start = sample_data.index.get_level_values('datetime')[curr_split.train_indices[0]]
                
                assert curr_window_start > prev_window_start
                
                # æ£€æŸ¥æ­¥é•¿æŽ¨è¿›æ˜¯å¦åˆç†
                time_diff = (curr_window_start - prev_window_start).days
                expected_step = rolling_config.step_size or (rolling_config.rolling_window_size // 4)
                # å…è®¸ä¸€å®šçš„åå·®ï¼Œå› ä¸ºæ­¥é•¿æ˜¯æŒ‰æ—¥æœŸè®¡ç®—çš„
                assert time_diff >= expected_step * 0.5
        
        def test_rolling_window_size_consistency(self, sample_data, rolling_config):
            """æµ‹è¯•æ»šåŠ¨çª—å£å¤§å°ä¸€è‡´æ€§"""
            strategy = RollingWindowSplitStrategy(rolling_config)
            splits = strategy.split_rolling(sample_data)
            
            # æ¯ä¸ªçª—å£çš„æ€»å¤§å°åº”è¯¥æŽ¥è¿‘é…ç½®çš„çª—å£å¤§å°
            for result in splits[:-1]:  # æœ€åŽä¸€ä¸ªçª—å£å¯èƒ½è¾ƒå°
                total_size = (len(result.train_indices) + 
                             len(result.validation_indices) + 
                             len(result.test_indices))
                
                # å…è®¸ä¸€å®šçš„åå·®
                assert abs(total_size - rolling_config.rolling_window_size) < 50
    
    
    class TestFixedSplitStrategy:
        """å›ºå®šåˆ’åˆ†ç­–ç•¥æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ—¶åºæ•°æ®"""
            dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'TEST',
                'price': np.random.randn(len(dates)) * 10 + 100
            })
            return data.set_index(['datetime', 'symbol'])
        
        def test_fixed_split_by_date(self, sample_data):
            """æµ‹è¯•æŒ‰æ—¥æœŸå›ºå®šåˆ’åˆ†"""
            config = SplitConfig(
                train_end_date='2023-08-31',
                validation_end_date='2023-10-31'
            )
            
            strategy = FixedSplitStrategy(config)
            result = strategy.split(sample_data)
            
            # æ£€æŸ¥æ—¥æœŸè¾¹ç•Œ
            train_dates = sample_data.index.get_level_values('datetime')[result.train_indices]
            val_dates = sample_data.index.get_level_values('datetime')[result.validation_indices]
            test_dates = sample_data.index.get_level_values('datetime')[result.test_indices]
            
            assert train_dates.max() <= pd.Timestamp(config.train_end_date)
            assert val_dates.max() <= pd.Timestamp(config.validation_end_date)
            assert test_dates.min() > pd.Timestamp(config.validation_end_date)
        
        def test_fixed_split_by_ratio(self, sample_data):
            """æµ‹è¯•æŒ‰æ¯”ä¾‹å›ºå®šåˆ’åˆ†"""
            config = SplitConfig(
                train_ratio=0.6,
                validation_ratio=0.2,
                test_ratio=0.2
            )
            
            strategy = FixedSplitStrategy(config)
            result = strategy.split(sample_data)
            
            metrics = result.get_metrics()
            
            # æ£€æŸ¥æ¯”ä¾‹
            assert abs(metrics['train_ratio'] - 0.6) < 0.05
            assert abs(metrics['validation_ratio'] - 0.2) < 0.05
            assert abs(metrics['test_ratio'] - 0.2) < 0.05
    
    
    class TestDataLeakageDetection:
        """æ•°æ®æ³„éœ²æ£€æµ‹æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ•°æ®"""
            dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'TEST',
                'feature1': np.random.randn(len(dates)),
                'feature2': np.random.randn(len(dates)),
                'target': np.random.randn(len(dates))
            })
            return data.set_index(['datetime', 'symbol'])
        
        def test_temporal_leakage_detection(self, sample_data):
            """æµ‹è¯•æ—¶é—´æ³„éœ²æ£€æµ‹"""
            config = SplitConfig(
                train_ratio=0.7,
                validation_ratio=0.2,
                test_ratio=0.1
            )
            
            strategy = TimeSeriesSplitStrategy(config)
            result = strategy.split(sample_data)
            
            # æ£€æµ‹æ˜¯å¦å­˜åœ¨æ—¶é—´æ³„éœ²
            leakage_detected = strategy.detect_temporal_leakage(result, sample_data)
            assert not leakage_detected  # æ­£ç¡®çš„åˆ’åˆ†ä¸åº”è¯¥æœ‰æ³„éœ²
        
        def test_feature_leakage_detection(self, sample_data):
            """æµ‹è¯•ç‰¹å¾æ³„éœ²æ£€æµ‹"""
            # åˆ›å»ºæœ‰æ³„éœ²çš„ç‰¹å¾ï¼ˆä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼‰
            future_feature = sample_data['feature1'].shift(-5)  # ä½¿ç”¨5å¤©åŽçš„æ•°æ®
            sample_data['leaked_feature'] = future_feature
            
            config = SplitConfig(train_ratio=0.7, validation_ratio=0.2, test_ratio=0.1)
            strategy = TimeSeriesSplitStrategy(config)
            result = strategy.split(sample_data)
            
            # æ£€æµ‹ç‰¹å¾æ³„éœ²
            leakage_detected = strategy.detect_feature_leakage(
                result, sample_data, ['leaked_feature']
            )
            assert leakage_detected  # åº”è¯¥æ£€æµ‹åˆ°æ³„éœ²
        
        def test_target_leakage_detection(self, sample_data):
            """æµ‹è¯•ç›®æ ‡å˜é‡æ³„éœ²æ£€æµ‹"""
            # åˆ›å»ºä½¿ç”¨æœªæ¥ç›®æ ‡çš„ç‰¹å¾
            sample_data['target_lag'] = sample_data['target'].shift(-1)
            
            config = SplitConfig(train_ratio=0.7, validation_ratio=0.2, test_ratio=0.1)
            strategy = TimeSeriesSplitStrategy(config)
            result = strategy.split(sample_data)
            
            # æ£€æµ‹ç›®æ ‡æ³„éœ²
            leakage_detected = strategy.detect_target_leakage(
                result, sample_data, 'target', ['target_lag']
            )
            assert leakage_detected  # åº”è¯¥æ£€æµ‹åˆ°æ³„éœ²
    
    
    class TestSplitStrategyComparison:
        """åˆ’åˆ†ç­–ç•¥å¯¹æ¯”æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ•°æ®"""
            dates = pd.date_range(start='2020-01-01', end='2023-12-31', freq='D')
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'TEST',
                'price': np.cumsum(np.random.randn(len(dates))) + 100,
                'return': np.random.randn(len(dates)) * 0.02
            })
            return data.set_index(['datetime', 'symbol'])
        
        def test_strategy_consistency(self, sample_data):
            """æµ‹è¯•ä¸åŒç­–ç•¥çš„ä¸€è‡´æ€§"""
            config = SplitConfig(
                train_ratio=0.7,
                validation_ratio=0.2,
                test_ratio=0.1
            )
            
            # æµ‹è¯•æ—¶åºç­–ç•¥
            ts_strategy = TimeSeriesSplitStrategy(config)
            ts_result = ts_strategy.split(sample_data)
            
            # æµ‹è¯•å›ºå®šç­–ç•¥
            fixed_strategy = FixedSplitStrategy(config)
            fixed_result = fixed_strategy.split(sample_data)
            
            # ä¸¤ç§ç­–ç•¥çš„ç»“æžœåº”è¯¥ç›¸ä¼¼ï¼ˆä½†ä¸å®Œå…¨ç›¸åŒï¼‰
            ts_metrics = ts_result.get_metrics()
            fixed_metrics = fixed_result.get_metrics()
            
            assert abs(ts_metrics['train_ratio'] - fixed_metrics['train_ratio']) < 0.1
            assert abs(ts_metrics['validation_ratio'] - fixed_metrics['validation_ratio']) < 0.1
            assert abs(ts_metrics['test_ratio'] - fixed_metrics['test_ratio']) < 0.1
        
        def test_strategy_robustness(self, sample_data):
            """æµ‹è¯•ç­–ç•¥ç¨³å¥æ€§"""
            config = SplitConfig(
                train_ratio=0.7,
                validation_ratio=0.2,
                test_ratio=0.1,
                random_seed=42
            )
            
            strategy = TimeSeriesSplitStrategy(config)
            
            # å¤šæ¬¡è¿è¡Œåº”è¯¥äº§ç”Ÿç›¸åŒç»“æžœ
            result1 = strategy.split(sample_data)
            result2 = strategy.split(sample_data)
            
            assert np.array_equal(result1.train_indices, result2.train_indices)
            assert np.array_equal(result1.validation_indices, result2.validation_indices)
            assert np.array_equal(result1.test_indices, result2.test_indices)
        
        @pytest.mark.parametrize("strategy_name,strategy_class", [
            ("time_series", TimeSeriesSplitStrategy),
            ("fixed", FixedSplitStrategy)
        ])
        def test_strategy_validity(self, sample_data, strategy_name, strategy_class):
            """æµ‹è¯•ä¸åŒç­–ç•¥çš„æœ‰æ•ˆæ€§"""
            config = SplitConfig(
                train_ratio=0.7,
                validation_ratio=0.2,
                test_ratio=0.1
            )
            
            strategy = strategy_class(config)
            result = strategy.split(sample_data)
            
            # åŸºæœ¬æœ‰æ•ˆæ€§æ£€æŸ¥
            assert len(result.train_indices) > 0
            assert len(result.validation_indices) > 0
            assert len(result.test_indices) > 0
            
            # æ— é‡å æ£€æŸ¥
            assert len(np.intersect1d(result.train_indices, result.validation_indices)) == 0
            assert len(np.intersect1d(result.validation_indices, result.test_indices)) == 0
            assert len(np.intersect1d(result.train_indices, result.test_indices)) == 0
            
            # è¦†ç›–å®Œæ•´æ€§æ£€æŸ¥
            all_indices = np.concatenate([
                result.train_indices, 
                result.validation_indices, 
                result.test_indices
            ])
            expected_indices = np.arange(len(sample_data))
            
            # å…è®¸æœ‰é—´éš”ï¼Œä½†æ€»ä½“åº”è¯¥è¦†ç›–å¤§éƒ¨åˆ†æ•°æ®
            coverage = len(all_indices) / len(expected_indices)
            assert coverage > 0.8  # è‡³å°‘è¦†ç›–80%çš„æ•°æ®
    
    
    class TestEdgeCases:
        """è¾¹ç•Œæƒ…å†µæµ‹è¯•ç±»"""
        
        def test_single_symbol_data(self):
            """æµ‹è¯•å•ä¸€è‚¡ç¥¨æ•°æ®"""
            dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'SINGLE',
                'price': np.random.randn(len(dates))
            }).set_index(['datetime', 'symbol'])
            
            config = SplitConfig(train_ratio=0.7, validation_ratio=0.2, test_ratio=0.1)
            strategy = TimeSeriesSplitStrategy(config)
            result = strategy.split(data)
            
            assert len(result.train_indices) > 0
            assert len(result.validation_indices) > 0
            assert len(result.test_indices) > 0
        
        def test_multiple_symbols_data(self):
            """æµ‹è¯•å¤šè‚¡ç¥¨æ•°æ®"""
            dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
            symbols = ['A', 'B', 'C']
            
            data_list = []
            for symbol in symbols:
                symbol_data = pd.DataFrame({
                    'datetime': dates,
                    'symbol': symbol,
                    'price': np.random.randn(len(dates))
                })
                data_list.append(symbol_data)
            
            data = pd.concat(data_list).set_index(['datetime', 'symbol'])
            
            config = SplitConfig(train_ratio=0.7, validation_ratio=0.2, test_ratio=0.1)
            strategy = TimeSeriesSplitStrategy(config)
            result = strategy.split(data)
            
            # åº”è¯¥èƒ½å¤Ÿå¤„ç†å¤šè‚¡ç¥¨æ•°æ®
            assert len(result.train_indices) > 0
            assert len(result.validation_indices) > 0
            assert len(result.test_indices) > 0
        
        def test_irregular_time_series(self):
            """æµ‹è¯•éžè§„å¾‹æ—¶åºæ•°æ®"""
            # åˆ›å»ºæœ‰ç¼ºå¤±æ—¥æœŸçš„æ•°æ®
            all_dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')
            # éšæœºåˆ é™¤ä¸€äº›æ—¥æœŸ
            keep_indices = np.random.choice(len(all_dates), size=int(len(all_dates) * 0.8), replace=False)
            dates = all_dates[np.sort(keep_indices)]
            
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'IRREGULAR',
                'price': np.random.randn(len(dates))
            }).set_index(['datetime', 'symbol'])
            
            config = SplitConfig(train_ratio=0.7, validation_ratio=0.2, test_ratio=0.1)
            strategy = TimeSeriesSplitStrategy(config)
            result = strategy.split(data)
            
            # åº”è¯¥èƒ½å¤Ÿå¤„ç†éžè§„å¾‹æ—¶åº
            assert len(result.train_indices) > 0
            assert len(result.validation_indices) > 0
            assert len(result.test_indices) > 0
        
        def test_minimum_data_requirements(self):
            """æµ‹è¯•æœ€å°æ•°æ®è¦æ±‚"""
            # åˆ›å»ºæžå°æ•°æ®é›†
            dates = pd.date_range(start='2023-01-01', end='2023-01-05', freq='D')
            data = pd.DataFrame({
                'datetime': dates,
                'symbol': 'TINY',
                'price': [1, 2, 3, 4, 5]
            }).set_index(['datetime', 'symbol'])
            
            config = SplitConfig(
                train_ratio=0.6,
                validation_ratio=0.2,
                test_ratio=0.2,
                min_train_samples=10,  # è¦æ±‚æœ€å°‘10ä¸ªè®­ç»ƒæ ·æœ¬
                min_validation_samples=2
            )
            
            strategy = TimeSeriesSplitStrategy(config)
            
            with pytest.raises(ValueError, match="æ•°æ®é‡ä¸è¶³"):
                strategy.split(data)
    ]]></file>
  <file path="tests/unit/test_data_processor.py"><![CDATA[
    """
    æ•°æ®é¢„å¤„ç†ç®¡é“æµ‹è¯•ç”¨ä¾‹
    æµ‹è¯•æ•°æ®æ¸…æ´—ã€é¢„å¤„ç†æµæ°´çº¿ã€æ•°æ®è´¨é‡æ£€æŸ¥å’Œç¼“å­˜åŠŸèƒ½
    """
    
    import pytest
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    from unittest.mock import Mock, patch, MagicMock
    import tempfile
    import os
    import shutil
    
    from src.rl_trading_system.data.data_processor import DataProcessor
    from src.rl_trading_system.data.data_cache import DataCache
    from src.rl_trading_system.data.data_quality import DataQualityChecker
    from src.rl_trading_system.data.data_models import MarketData, FeatureVector
    from src.rl_trading_system.data.feature_engineer import FeatureEngineer
    
    
    class TestDataProcessor:
        """æ•°æ®é¢„å¤„ç†å™¨æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def sample_price_data(self):
            """åˆ›å»ºæ ·æœ¬ä»·æ ¼æ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=100, freq='D')
            np.random.seed(42)
            
            data = {
                'open': 100 + np.random.randn(100) * 2,
                'high': 102 + np.random.randn(100) * 2,
                'low': 98 + np.random.randn(100) * 2,
                'close': 100 + np.random.randn(100) * 2,
                'volume': np.random.randint(1000000, 10000000, 100),
                'amount': np.random.randint(100000000, 1000000000, 100)
            }
            
            # ç¡®ä¿ä»·æ ¼å…³ç³»æ­£ç¡®
            for i in range(100):
                data['high'][i] = max(data['open'][i], data['high'][i], 
                                    data['low'][i], data['close'][i])
                data['low'][i] = min(data['open'][i], data['high'][i], 
                                   data['low'][i], data['close'][i])
            
            df = pd.DataFrame(data, index=dates)
            return df
        
        @pytest.fixture
        def sample_fundamental_data(self):
            """åˆ›å»ºæ ·æœ¬åŸºæœ¬é¢æ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=100, freq='D')
            np.random.seed(42)
            
            data = {
                'pe_ratio': 10 + np.random.randn(100) * 3,
                'pb_ratio': 1.5 + np.random.randn(100) * 0.5,
                'roe': 0.1 + np.random.randn(100) * 0.05,
                'roa': 0.05 + np.random.randn(100) * 0.02,
                'revenue_growth': 0.1 + np.random.randn(100) * 0.1,
                'profit_growth': 0.15 + np.random.randn(100) * 0.15
            }
            
            df = pd.DataFrame(data, index=dates)
            return df
        
        @pytest.fixture
        def dirty_price_data(self):
            """åˆ›å»ºåŒ…å«è„æ•°æ®çš„ä»·æ ¼æ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=50, freq='D')
            
            data = {
                'open': [100, 101, np.nan, 103, -5, 105, 106, 107, 108, 109] + [100] * 40,
                'high': [102, 103, 104, 105, 106, 107, 108, 109, 110, 111] + [102] * 40,
                'low': [98, 99, 100, 101, 102, 103, 104, 105, 106, 107] + [98] * 40,
                'close': [101, 102, 103, 104, 105, 106, 107, 108, 109, 110] + [101] * 40,
                'volume': [1000000, 2000000, np.nan, 4000000, -1000000, 
                          6000000, 7000000, 8000000, 9000000, 10000000] + [1000000] * 40,
                'amount': [100000000, 200000000, 300000000, 400000000, 500000000,
                          600000000, 700000000, 800000000, 900000000, 1000000000] + [100000000] * 40
            }
            
            # æ•…æ„åˆ¶é€ ä»·æ ¼å…³ç³»é”™è¯¯
            data['low'][5] = 120  # æœ€ä½Žä»·é«˜äºŽæœ€é«˜ä»·
            
            df = pd.DataFrame(data, index=dates)
            return df
        
        @pytest.fixture
        def data_processor(self):
            """åˆ›å»ºæ•°æ®é¢„å¤„ç†å™¨å®žä¾‹"""
            return DataProcessor()
        
        @pytest.fixture
        def temp_cache_dir(self):
            """åˆ›å»ºä¸´æ—¶ç¼“å­˜ç›®å½•"""
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir)
        
        def test_data_processor_initialization(self, data_processor):
            """æµ‹è¯•æ•°æ®é¢„å¤„ç†å™¨åˆå§‹åŒ–"""
            assert data_processor is not None
            assert hasattr(data_processor, 'feature_engineer')
            assert hasattr(data_processor, 'quality_checker')
            assert hasattr(data_processor, 'cache')
        
        def test_process_clean_data(self, data_processor, sample_price_data):
            """æµ‹è¯•å¤„ç†å¹²å‡€æ•°æ®"""
            result = data_processor.process_data(
                data=sample_price_data,
                symbols=['000001.SZ'],
                data_type='price'
            )
            
            assert isinstance(result, dict)
            assert 'processed_data' in result
            assert 'quality_report' in result
            assert 'feature_vectors' in result
            
            # æ£€æŸ¥å¤„ç†åŽçš„æ•°æ®
            processed_data = result['processed_data']
            assert not processed_data.empty
            assert len(processed_data) <= len(sample_price_data)
            
            # æ£€æŸ¥è´¨é‡æŠ¥å‘Š
            quality_report = result['quality_report']
            assert quality_report['status'] in ['good', 'warning', 'error']
            assert 'score' in quality_report
            assert isinstance(quality_report['score'], float)
        
        def test_process_dirty_data(self, data_processor, dirty_price_data):
            """æµ‹è¯•å¤„ç†è„æ•°æ®"""
            result = data_processor.process_data(
                data=dirty_price_data,
                symbols=['000001.SZ'],
                data_type='price',
                clean_strategy='aggressive'
            )
            
            # æ£€æŸ¥æ•°æ®æ¸…æ´—æ•ˆæžœ
            processed_data = result['processed_data']
            assert len(processed_data) < len(dirty_price_data)  # åº”è¯¥åˆ é™¤äº†ä¸€äº›è„æ•°æ®
            
            # æ£€æŸ¥æ²¡æœ‰è´Ÿå€¼
            numeric_columns = ['open', 'high', 'low', 'close', 'volume']
            for col in numeric_columns:
                if col in processed_data.columns:
                    assert (processed_data[col] >= 0).all()
            
            # æ£€æŸ¥ä»·æ ¼å…³ç³»
            if all(col in processed_data.columns for col in ['high', 'low']):
                assert (processed_data['high'] >= processed_data['low']).all()
        
        def test_batch_processing(self, data_processor, sample_price_data):
            """æµ‹è¯•æ‰¹å¤„ç†åŠŸèƒ½"""
            # åˆ›å»ºå¤šä¸ªè‚¡ç¥¨çš„æ•°æ®
            symbols = ['000001.SZ', '000002.SZ', '600000.SH']
            batch_data = {}
            
            for symbol in symbols:
                # ä¸ºæ¯ä¸ªè‚¡ç¥¨åˆ›å»ºç•¥æœ‰ä¸åŒçš„æ•°æ®
                data = sample_price_data.copy()
                data = data * (1 + np.random.randn() * 0.1)  # æ·»åŠ éšæœºå˜åŒ–
                batch_data[symbol] = data
            
            result = data_processor.process_batch(
                batch_data=batch_data,
                data_type='price',
                parallel=True
            )
            
            assert isinstance(result, dict)
            assert len(result) == len(symbols)
            
            for symbol in symbols:
                assert symbol in result
                assert 'processed_data' in result[symbol]
                assert 'quality_report' in result[symbol]
        
        def test_feature_engineering_integration(self, data_processor, sample_price_data):
            """æµ‹è¯•ç‰¹å¾å·¥ç¨‹é›†æˆ"""
            result = data_processor.process_data(
                data=sample_price_data,
                symbols=['000001.SZ'],
                data_type='price',
                calculate_features=True
            )
            
            assert 'feature_vectors' in result
            feature_vectors = result['feature_vectors']
            assert len(feature_vectors) > 0
            
            # æ£€æŸ¥ç‰¹å¾å‘é‡ç»“æž„
            first_vector = feature_vectors[0]
            assert isinstance(first_vector, FeatureVector)
            assert first_vector.symbol == '000001.SZ'
            assert len(first_vector.technical_indicators) > 0
            assert len(first_vector.fundamental_factors) > 0
            assert len(first_vector.market_microstructure) > 0
        
        def test_data_normalization(self, data_processor, sample_price_data):
            """æµ‹è¯•æ•°æ®æ ‡å‡†åŒ–"""
            result = data_processor.process_data(
                data=sample_price_data,
                symbols=['000001.SZ'],
                data_type='price',
                normalize=True,
                normalization_method='zscore'
            )
            
            processed_data = result['processed_data']
            
            # æ£€æŸ¥æ ‡å‡†åŒ–æ•ˆæžœï¼ˆæŠ€æœ¯æŒ‡æ ‡åº”è¯¥è¢«æ ‡å‡†åŒ–ï¼‰
            if 'sma_20' in processed_data.columns:
                sma_mean = processed_data['sma_20'].mean()
                sma_std = processed_data['sma_20'].std()
                assert abs(sma_mean) < 0.1  # å‡å€¼æŽ¥è¿‘0
                assert abs(sma_std - 1.0) < 0.1  # æ ‡å‡†å·®æŽ¥è¿‘1
        
        def test_missing_value_handling(self, data_processor):
            """æµ‹è¯•ç¼ºå¤±å€¼å¤„ç†"""
            # åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®
            dates = pd.date_range('2023-01-01', periods=20, freq='D')
            data = pd.DataFrame({
                'open': [100, np.nan, 102, 103, np.nan] * 4,
                'high': [102, 103, np.nan, 105, 106] * 4,
                'low': [98, 99, 100, np.nan, 102] * 4,
                'close': [101, 102, 103, 104, np.nan] * 4,
                'volume': [1000000] * 20,
                'amount': [100000000] * 20
            }, index=dates)
            
            # æµ‹è¯•å‰å‘å¡«å……
            result_ffill = data_processor.process_data(
                data=data,
                symbols=['000001.SZ'],
                data_type='price',
                missing_value_method='ffill'
            )
            
            processed_ffill = result_ffill['processed_data']
            assert processed_ffill.isnull().sum().sum() < data.isnull().sum().sum()
            
            # æµ‹è¯•åˆ é™¤ç¼ºå¤±å€¼
            result_drop = data_processor.process_data(
                data=data,
                symbols=['000001.SZ'],
                data_type='price',
                missing_value_method='drop'
            )
            
            processed_drop = result_drop['processed_data']
            assert processed_drop.isnull().sum().sum() == 0
            assert len(processed_drop) < len(data)
        
        def test_outlier_detection_and_treatment(self, data_processor):
            """æµ‹è¯•å¼‚å¸¸å€¼æ£€æµ‹å’Œå¤„ç†"""
            # åˆ›å»ºåŒ…å«å¼‚å¸¸å€¼çš„æ•°æ®
            dates = pd.date_range('2023-01-01', periods=100, freq='D')
            np.random.seed(42)
            
            data = pd.DataFrame({
                'open': 100 + np.random.randn(100) * 2,
                'high': 102 + np.random.randn(100) * 2,
                'low': 98 + np.random.randn(100) * 2,
                'close': 100 + np.random.randn(100) * 2,
                'volume': np.random.randint(1000000, 10000000, 100),
                'amount': np.random.randint(100000000, 1000000000, 100)
            }, index=dates)
            
            # äººä¸ºæ·»åŠ å¼‚å¸¸å€¼
            data.loc[data.index[10], 'close'] = 1000  # æžå¤§å¼‚å¸¸å€¼
            data.loc[data.index[20], 'volume'] = 100000000  # æžå¤§æˆäº¤é‡
            
            result = data_processor.process_data(
                data=data,
                symbols=['000001.SZ'],
                data_type='price',
                outlier_treatment='clip'
            )
            
            processed_data = result['processed_data']
            
            # æ£€æŸ¥å¼‚å¸¸å€¼æ˜¯å¦è¢«å¤„ç†
            close_max = processed_data['close'].max()
            assert close_max < 200  # å¼‚å¸¸å€¼åº”è¯¥è¢«è£å‰ª
        
        def test_data_validation(self, data_processor, sample_price_data):
            """æµ‹è¯•æ•°æ®éªŒè¯"""
            # æµ‹è¯•æœ‰æ•ˆæ•°æ®
            is_valid = data_processor.validate_data(
                data=sample_price_data,
                data_type='price'
            )
            assert is_valid
            
            # æµ‹è¯•æ— æ•ˆæ•°æ®
            invalid_data = sample_price_data.copy()
            invalid_data['high'] = invalid_data['low'] - 10  # åˆ¶é€ ä»·æ ¼å…³ç³»é”™è¯¯
            
            is_valid = data_processor.validate_data(
                data=invalid_data,
                data_type='price'
            )
            assert not is_valid
        
        def test_pipeline_configuration(self, data_processor):
            """æµ‹è¯•æµæ°´çº¿é…ç½®"""
            config = {
                'clean_strategy': 'conservative',
                'missing_value_method': 'ffill',
                'outlier_treatment': 'clip',
                'normalize': True,
                'normalization_method': 'minmax',
                'calculate_features': True,
                'feature_selection': True,
                'cache_enabled': True
            }
            
            data_processor.configure_pipeline(config)
            
            assert data_processor.config['clean_strategy'] == 'conservative'
            assert data_processor.config['normalize'] is True
            assert data_processor.config['cache_enabled'] is True
    
    
    class TestDataProcessorCache:
        """æ•°æ®é¢„å¤„ç†å™¨ç¼“å­˜æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def temp_cache_dir(self):
            """åˆ›å»ºä¸´æ—¶ç¼“å­˜ç›®å½•"""
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir)
        
        @pytest.fixture
        def cache_processor(self, temp_cache_dir):
            """åˆ›å»ºå¸¦ç¼“å­˜çš„æ•°æ®é¢„å¤„ç†å™¨"""
            cache = DataCache(cache_dir=temp_cache_dir, default_ttl=3600)
            processor = DataProcessor(cache=cache)
            return processor
        
        @pytest.fixture
        def sample_data(self):
            """åˆ›å»ºæ ·æœ¬æ•°æ®"""
            dates = pd.date_range('2023-01-01', periods=50, freq='D')
            np.random.seed(42)
            data = pd.DataFrame({
                'open': 100 + np.random.randn(50),
                'high': 102 + np.random.randn(50),
                'low': 98 + np.random.randn(50),
                'close': 100 + np.random.randn(50),
                'volume': np.random.randint(1000000, 10000000, 50),
                'amount': np.random.randint(100000000, 1000000000, 50)
            }, index=dates)
            
            # ç¡®ä¿ä»·æ ¼å…³ç³»æ­£ç¡®
            for i in range(50):
                data.iloc[i, 1] = max(data.iloc[i, [0, 1, 2, 3]])  # high
                data.iloc[i, 2] = min(data.iloc[i, [0, 1, 2, 3]])  # low
            
            return data
        
        def test_cache_hit(self, cache_processor, sample_data):
            """æµ‹è¯•ç¼“å­˜å‘½ä¸­"""
            # ç¬¬ä¸€æ¬¡å¤„ç†ï¼Œåº”è¯¥ç¼“å­˜ç»“æžœ
            result1 = cache_processor.process_data(
                data=sample_data,
                symbols=['000001.SZ'],
                data_type='price',
                use_cache=True
            )
            
            # ç¬¬äºŒæ¬¡å¤„ç†ï¼Œåº”è¯¥ä»Žç¼“å­˜èŽ·å–
            with patch.object(cache_processor.feature_engineer, 'calculate_technical_indicators') as mock_calc:
                result2 = cache_processor.process_data(
                    data=sample_data,
                    symbols=['000001.SZ'],
                    data_type='price',
                    use_cache=True
                )
                
                # å¦‚æžœä»Žç¼“å­˜èŽ·å–ï¼Œä¸åº”è¯¥è°ƒç”¨ç‰¹å¾è®¡ç®—
                mock_calc.assert_not_called()
            
            # ç»“æžœåº”è¯¥ç›¸åŒ
            pd.testing.assert_frame_equal(
                result1['processed_data'], 
                result2['processed_data']
            )
        
        def test_cache_miss_on_different_params(self, cache_processor, sample_data):
            """æµ‹è¯•ä¸åŒå‚æ•°å¯¼è‡´çš„ç¼“å­˜æœªå‘½ä¸­"""
            # ä½¿ç”¨ä¸åŒå‚æ•°å¤„ç†
            result1 = cache_processor.process_data(
                data=sample_data,
                symbols=['000001.SZ'],
                data_type='price',
                normalize=True,
                use_cache=True
            )
            
            result2 = cache_processor.process_data(
                data=sample_data,
                symbols=['000001.SZ'],
                data_type='price',
                normalize=False,  # ä¸åŒçš„å‚æ•°
                use_cache=True
            )
            
            # ç»“æžœåº”è¯¥ä¸åŒ
            assert not result1['processed_data'].equals(result2['processed_data'])
        
        def test_cache_expiration(self, temp_cache_dir, sample_data):
            """æµ‹è¯•ç¼“å­˜è¿‡æœŸ"""
            # åˆ›å»ºçŸ­TTLçš„ç¼“å­˜
            cache = DataCache(cache_dir=temp_cache_dir, default_ttl=1)  # 1ç§’TTL
            processor = DataProcessor(cache=cache)
            
            # ç¬¬ä¸€æ¬¡å¤„ç†
            result1 = processor.process_data(
                data=sample_data,
                symbols=['000001.SZ'],
                data_type='price',
                use_cache=True
            )
            
            # ç­‰å¾…ç¼“å­˜è¿‡æœŸ
            import time
            time.sleep(2)
            
            # ç¬¬äºŒæ¬¡å¤„ç†ï¼Œåº”è¯¥é‡æ–°è®¡ç®—
            with patch.object(processor.feature_engineer, 'calculate_technical_indicators') as mock_calc:
                mock_calc.return_value = pd.DataFrame()  # æ¨¡æ‹Ÿè¿”å›ž
                
                result2 = processor.process_data(
                    data=sample_data,
                    symbols=['000001.SZ'],
                    data_type='price',
                    use_cache=True
                )
                
                # åº”è¯¥é‡æ–°è°ƒç”¨ç‰¹å¾è®¡ç®—
                mock_calc.assert_called()
        
        def test_cache_disable(self, cache_processor, sample_data):
            """æµ‹è¯•ç¦ç”¨ç¼“å­˜"""
            # ç¦ç”¨ç¼“å­˜å¤„ç†
            result1 = cache_processor.process_data(
                data=sample_data,
                symbols=['000001.SZ'],
                data_type='price',
                use_cache=False
            )
            
            # å†æ¬¡å¤„ç†ï¼Œåº”è¯¥é‡æ–°è®¡ç®—
            with patch.object(cache_processor.feature_engineer, 'calculate_technical_indicators') as mock_calc:
                mock_calc.return_value = pd.DataFrame()
                
                result2 = cache_processor.process_data(
                    data=sample_data,
                    symbols=['000001.SZ'],
                    data_type='price',
                    use_cache=False
                )
                
                # åº”è¯¥è°ƒç”¨ç‰¹å¾è®¡ç®—
                mock_calc.assert_called()
    
    
    class TestDataProcessorQuality:
        """æ•°æ®é¢„å¤„ç†å™¨è´¨é‡æ£€æŸ¥æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def quality_processor(self):
            """åˆ›å»ºæ•°æ®é¢„å¤„ç†å™¨"""
            return DataProcessor()
        
        def test_quality_check_good_data(self, quality_processor):
            """æµ‹è¯•é«˜è´¨é‡æ•°æ®çš„è´¨é‡æ£€æŸ¥"""
            # åˆ›å»ºé«˜è´¨é‡æ•°æ®
            dates = pd.date_range('2023-01-01', periods=100, freq='D')
            data = pd.DataFrame({
                'open': 100 + np.random.randn(100) * 0.5,
                'high': 101 + np.random.randn(100) * 0.5,
                'low': 99 + np.random.randn(100) * 0.5,
                'close': 100 + np.random.randn(100) * 0.5,
                'volume': np.random.randint(1000000, 2000000, 100),
                'amount': np.random.randint(100000000, 200000000, 100)
            }, index=dates)
            
            # ç¡®ä¿ä»·æ ¼å…³ç³»æ­£ç¡®
            for i in range(100):
                data.loc[data.index[i], 'high'] = max(
                    data.loc[data.index[i], ['open', 'high', 'low', 'close']]
                )
                data.loc[data.index[i], 'low'] = min(
                    data.loc[data.index[i], ['open', 'high', 'low', 'close']]
                )
            
            quality_report = quality_processor.check_data_quality(
                data=data,
                data_type='price'
            )
            
            assert quality_report['status'] == 'good'
            assert quality_report['score'] >= 0.8
            assert len(quality_report['issues']) == 0
        
        def test_quality_check_poor_data(self, quality_processor):
            """æµ‹è¯•ä½Žè´¨é‡æ•°æ®çš„è´¨é‡æ£€æŸ¥"""
            # åˆ›å»ºä½Žè´¨é‡æ•°æ®
            dates = pd.date_range('2023-01-01', periods=20, freq='D')
            data = pd.DataFrame({
                'open': [100, np.nan, -50, 103, np.nan] * 4,
                'high': [102, np.nan, 104, 105, 106] * 4,
                'low': [150, 99, 100, 101, 102] * 4,  # æ•…æ„åˆ¶é€ é”™è¯¯å…³ç³»
                'close': [101, np.nan, 103, 104, np.nan] * 4,
                'volume': [-1000000, 2000000, np.nan, 4000000, 5000000] * 4,
                'amount': [100000000] * 20
            }, index=dates)
            
            quality_report = quality_processor.check_data_quality(
                data=data,
                data_type='price'
            )
            
            assert quality_report['status'] in ['warning', 'error']
            assert quality_report['score'] < 0.6
            assert len(quality_report['issues']) > 0
        
        def test_quality_improvement_after_cleaning(self, quality_processor):
            """æµ‹è¯•æ¸…æ´—åŽæ•°æ®è´¨é‡æ”¹å–„"""
            # åˆ›å»ºè„æ•°æ®
            dates = pd.date_range('2023-01-01', periods=50, freq='D')
            dirty_data = pd.DataFrame({
                'open': [100, np.nan, -50, 103] + [100] * 46,
                'high': [102, 103, 104, 105] + [102] * 46,
                'low': [98, 99, 100, 101] + [98] * 46,
                'close': [101, np.nan, 103, 104] + [101] * 46,
                'volume': [-1000000, 2000000, 3000000, 4000000] + [1000000] * 46,
                'amount': [100000000] * 50
            }, index=dates)
            
            # æ¸…æ´—å‰çš„è´¨é‡
            quality_before = quality_processor.check_data_quality(
                data=dirty_data,
                data_type='price'
            )
            
            # æ¸…æ´—æ•°æ®
            result = quality_processor.process_data(
                data=dirty_data,
                symbols=['000001.SZ'],
                data_type='price',
                clean_strategy='aggressive'
            )
            
            # æ¸…æ´—åŽçš„è´¨é‡
            quality_after = result['quality_report']
            
            # è´¨é‡åº”è¯¥æœ‰æ‰€æ”¹å–„æˆ–è‡³å°‘ä¸å˜å·®
            assert quality_after['score'] >= quality_before['score'] * 0.9  # å…è®¸è½»å¾®ä¸‹é™
            assert len(quality_after['issues']) <= len(quality_before['issues'])
        
        def test_quality_metrics_calculation(self, quality_processor):
            """æµ‹è¯•è´¨é‡æŒ‡æ ‡è®¡ç®—"""
            dates = pd.date_range('2023-01-01', periods=100, freq='D')
            data = pd.DataFrame({
                'open': 100 + np.random.randn(100),
                'high': 102 + np.random.randn(100),
                'low': 98 + np.random.randn(100),
                'close': 100 + np.random.randn(100),
                'volume': np.random.randint(1000000, 10000000, 100),
                'amount': np.random.randint(100000000, 1000000000, 100)
            }, index=dates)
            
            # æ·»åŠ ä¸€äº›ç¼ºå¤±å€¼
            data.iloc[10:15, 0] = np.nan  # 5ä¸ªç¼ºå¤±å€¼
            
            quality_report = quality_processor.check_data_quality(
                data=data,
                data_type='price'
            )
            
            # æ£€æŸ¥ç»Ÿè®¡ä¿¡æ¯
            stats = quality_report['statistics']
            assert 'row_count' in stats
            assert 'missing_values' in stats
            assert stats['row_count'] == 100
            assert stats['missing_values']['open'] == 5
    
    
    class TestDataProcessorIntegration:
        """æ•°æ®é¢„å¤„ç†å™¨é›†æˆæµ‹è¯•ç±»"""
        
        @pytest.fixture
        def temp_cache_dir(self):
            """åˆ›å»ºä¸´æ—¶ç¼“å­˜ç›®å½•"""
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir)
        
        @pytest.fixture
        def full_processor(self, temp_cache_dir):
            """åˆ›å»ºå®Œæ•´é…ç½®çš„æ•°æ®é¢„å¤„ç†å™¨"""
            cache = DataCache(cache_dir=temp_cache_dir)
            processor = DataProcessor(cache=cache)
            
            config = {
                'clean_strategy': 'conservative',
                'missing_value_method': 'ffill',
                'outlier_treatment': 'clip',
                'normalize': True,
                'normalization_method': 'zscore',
                'calculate_features': True,
                'feature_selection': True,
                'cache_enabled': True
            }
            processor.configure_pipeline(config)
            
            return processor
        
        def test_end_to_end_processing(self, full_processor):
            """æµ‹è¯•ç«¯åˆ°ç«¯å¤„ç†æµç¨‹"""
            # åˆ›å»ºçœŸå®žåœºæ™¯çš„æ•°æ®
            dates = pd.date_range('2023-01-01', periods=252, freq='D')  # ä¸€å¹´äº¤æ˜“æ—¥
            np.random.seed(42)
            
            # æ¨¡æ‹Ÿè‚¡ä»·éšæœºæ¸¸èµ°
            returns = np.random.randn(252) * 0.02
            prices = 100 * np.exp(np.cumsum(returns))
            
            data = pd.DataFrame({
                'open': prices * (1 + np.random.randn(252) * 0.001),
                'high': prices * (1 + np.abs(np.random.randn(252)) * 0.002),
                'low': prices * (1 - np.abs(np.random.randn(252)) * 0.002),
                'close': prices,
                'volume': np.random.randint(1000000, 10000000, 252),
                'amount': prices * np.random.randint(1000000, 10000000, 252)
            }, index=dates)
            
            # ç¡®ä¿ä»·æ ¼å…³ç³»æ­£ç¡®
            for i in range(252):
                data.iloc[i, 1] = max(data.iloc[i, [0, 1, 2, 3]])  # high
                data.iloc[i, 2] = min(data.iloc[i, [0, 1, 2, 3]])  # low
            
            # æ·»åŠ ä¸€äº›çœŸå®žçš„æ•°æ®é—®é¢˜
            data.iloc[50:55, 0] = np.nan  # ç¼ºå¤±å€¼
            data.iloc[100, 3] = data.iloc[100, 3] * 1.5  # å¼‚å¸¸å€¼
            
            # å¤„ç†æ•°æ®
            result = full_processor.process_data(
                data=data,
                symbols=['000001.SZ'],
                data_type='price'
            )
            
            # éªŒè¯ç»“æžœå®Œæ•´æ€§
            assert 'processed_data' in result
            assert 'quality_report' in result
            assert 'feature_vectors' in result
            
            processed_data = result['processed_data']
            quality_report = result['quality_report']
            feature_vectors = result['feature_vectors']
            
            # éªŒè¯æ•°æ®è´¨é‡
            assert not processed_data.empty
            assert quality_report['status'] in ['good', 'warning']
            assert len(feature_vectors) > 0
            
            # éªŒè¯ç‰¹å¾å·¥ç¨‹
            first_vector = feature_vectors[0]
            assert len(first_vector.technical_indicators) > 5
            assert len(first_vector.fundamental_factors) >= 1
            assert len(first_vector.market_microstructure) > 0
            
            # éªŒè¯æ•°æ®æ ‡å‡†åŒ–
            if 'sma_20' in processed_data.columns:
                sma_mean = processed_data['sma_20'].mean()
                assert abs(sma_mean) < 0.2  # æ ‡å‡†åŒ–åŽå‡å€¼æŽ¥è¿‘0
        
        def test_multi_symbol_processing(self, full_processor):
            """æµ‹è¯•å¤šè‚¡ç¥¨å¤„ç†"""
            symbols = ['000001.SZ', '000002.SZ', '600000.SH']
            batch_data = {}
            
            # ä¸ºæ¯ä¸ªè‚¡ç¥¨åˆ›å»ºæ•°æ®
            for i, symbol in enumerate(symbols):
                dates = pd.date_range('2023-01-01', periods=100, freq='D')
                base_price = 100 + i * 50  # ä¸åŒçš„åŸºç¡€ä»·æ ¼
                
                data = pd.DataFrame({
                    'open': base_price + np.random.randn(100) * 2,
                    'high': base_price + 2 + np.random.randn(100) * 2,
                    'low': base_price - 2 + np.random.randn(100) * 2,
                    'close': base_price + np.random.randn(100) * 2,
                    'volume': np.random.randint(1000000, 10000000, 100),
                    'amount': np.random.randint(100000000, 1000000000, 100)
                }, index=dates)
                
                # ç¡®ä¿ä»·æ ¼å…³ç³»æ­£ç¡®
                for j in range(100):
                    data.iloc[j, 1] = max(data.iloc[j, [0, 1, 2, 3]])
                    data.iloc[j, 2] = min(data.iloc[j, [0, 1, 2, 3]])
                
                batch_data[symbol] = data
            
            # æ‰¹å¤„ç†
            results = full_processor.process_batch(
                batch_data=batch_data,
                data_type='price',
                parallel=True
            )
            
            # éªŒè¯ç»“æžœ
            assert len(results) == len(symbols)
            
            for symbol in symbols:
                assert symbol in results
                result = results[symbol]
                assert 'processed_data' in result
                assert 'quality_report' in result
                assert 'feature_vectors' in result
                
                # éªŒè¯æ¯ä¸ªè‚¡ç¥¨çš„å¤„ç†ç»“æžœ
                assert not result['processed_data'].empty
                assert len(result['feature_vectors']) > 0
        
        def test_performance_monitoring(self, full_processor):
            """æµ‹è¯•æ€§èƒ½ç›‘æŽ§"""
            # åˆ›å»ºå¤§é‡æ•°æ®æµ‹è¯•æ€§èƒ½
            dates = pd.date_range('2023-01-01', periods=1000, freq='D')
            data = pd.DataFrame({
                'open': 100 + np.random.randn(1000),
                'high': 102 + np.random.randn(1000),
                'low': 98 + np.random.randn(1000),
                'close': 100 + np.random.randn(1000),
                'volume': np.random.randint(1000000, 10000000, 1000),
                'amount': np.random.randint(100000000, 1000000000, 1000)
            }, index=dates)
            
            # ç¡®ä¿ä»·æ ¼å…³ç³»æ­£ç¡®
            for i in range(1000):
                data.iloc[i, 1] = max(data.iloc[i, [0, 1, 2, 3]])
                data.iloc[i, 2] = min(data.iloc[i, [0, 1, 2, 3]])
            
            import time
            start_time = time.time()
            
            result = full_processor.process_data(
                data=data,
                symbols=['000001.SZ'],
                data_type='price'
            )
            
            end_time = time.time()
            processing_time = end_time - start_time
            
            # éªŒè¯å¤„ç†æ—¶é—´åˆç†ï¼ˆåº”è¯¥åœ¨å‡ ç§’å†…å®Œæˆï¼‰
            assert processing_time < 30  # 30ç§’å†…å®Œæˆ
            
            # éªŒè¯ç»“æžœè´¨é‡
            assert not result['processed_data'].empty
            assert result['quality_report']['status'] in ['good', 'warning']
            
            # éªŒè¯ç¼“å­˜æ•ˆæžœ
            start_time2 = time.time()
            result2 = full_processor.process_data(
                data=data,
                symbols=['000001.SZ'],
                data_type='price',
                use_cache=True
            )
            end_time2 = time.time()
            cached_time = end_time2 - start_time2
            
            # ç¼“å­˜åº”è¯¥æé«˜é€Ÿåº¦æˆ–è‡³å°‘ä¸æ…¢å¤ªå¤š
            assert cached_time <= processing_time * 1.2  # å…è®¸20%çš„è¯¯å·®
    ]]></file>
  <file path="tests/unit/test_data_models.py"><![CDATA[
    """
    æµ‹è¯•æ ¸å¿ƒæ•°æ®æ¨¡åž‹
    æµ‹è¯•MarketDataã€FeatureVectorã€TradingStateç­‰æ•°æ®ç±»çš„åŠŸèƒ½
    """
    
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, timezone
    from typing import Dict, Any
    import json
    import pickle
    
    from src.rl_trading_system.data.data_models import (
        MarketData,
        FeatureVector,
        TradingState,
        TradingAction,
        TransactionRecord
    )
    
    
    class TestMarketData:
        """æµ‹è¯•MarketDataæ•°æ®ç±»"""
        
        def test_market_data_creation(self):
            """æµ‹è¯•MarketDataæ­£å¸¸åˆ›å»º"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            market_data = MarketData(
                timestamp=timestamp,
                symbol="000001.SZ",
                open_price=10.0,
                high_price=10.5,
                low_price=9.8,
                close_price=10.2,
                volume=1000000,
                amount=10200000.0
            )
            
            assert market_data.timestamp == timestamp
            assert market_data.symbol == "000001.SZ"
            assert market_data.open_price == 10.0
            assert market_data.high_price == 10.5
            assert market_data.low_price == 9.8
            assert market_data.close_price == 10.2
            assert market_data.volume == 1000000
            assert market_data.amount == 10200000.0
        
        def test_market_data_validation_price_order(self):
            """æµ‹è¯•ä»·æ ¼é¡ºåºéªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            # æµ‹è¯•high < lowçš„æƒ…å†µ
            with pytest.raises(ValueError, match="æœ€é«˜ä»·ä¸èƒ½ä½ŽäºŽæœ€ä½Žä»·"):
                MarketData(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    open_price=10.0,
                    high_price=9.5,  # high < low
                    low_price=9.8,
                    close_price=10.2,
                    volume=1000000,
                    amount=10200000.0
                )
        
        def test_market_data_validation_negative_volume(self):
            """æµ‹è¯•è´Ÿæˆäº¤é‡éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°"):
                MarketData(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    open_price=10.0,
                    high_price=10.5,
                    low_price=9.8,
                    close_price=10.2,
                    volume=-1000,  # è´Ÿæˆäº¤é‡
                    amount=10200000.0
                )
        
        def test_market_data_validation_negative_amount(self):
            """æµ‹è¯•è´Ÿæˆäº¤é¢éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="æˆäº¤é¢ä¸èƒ½ä¸ºè´Ÿæ•°"):
                MarketData(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    open_price=10.0,
                    high_price=10.5,
                    low_price=9.8,
                    close_price=10.2,
                    volume=1000000,
                    amount=-10200000.0  # è´Ÿæˆäº¤é¢
                )
        
        def test_market_data_serialization(self):
            """æµ‹è¯•MarketDataåºåˆ—åŒ–"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            market_data = MarketData(
                timestamp=timestamp,
                symbol="000001.SZ",
                open_price=10.0,
                high_price=10.5,
                low_price=9.8,
                close_price=10.2,
                volume=1000000,
                amount=10200000.0
            )
            
            # æµ‹è¯•to_dict
            data_dict = market_data.to_dict()
            assert isinstance(data_dict, dict)
            assert data_dict['symbol'] == "000001.SZ"
            assert data_dict['open_price'] == 10.0
            
            # æµ‹è¯•from_dict
            restored_data = MarketData.from_dict(data_dict)
            assert restored_data.symbol == market_data.symbol
            assert restored_data.open_price == market_data.open_price
            assert restored_data.timestamp == market_data.timestamp
        
        def test_market_data_json_serialization(self):
            """æµ‹è¯•JSONåºåˆ—åŒ–"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            market_data = MarketData(
                timestamp=timestamp,
                symbol="000001.SZ",
                open_price=10.0,
                high_price=10.5,
                low_price=9.8,
                close_price=10.2,
                volume=1000000,
                amount=10200000.0
            )
            
            # æµ‹è¯•JSONåºåˆ—åŒ–
            json_str = market_data.to_json()
            assert isinstance(json_str, str)
            
            # æµ‹è¯•JSONååºåˆ—åŒ–
            restored_data = MarketData.from_json(json_str)
            assert restored_data.symbol == market_data.symbol
            assert restored_data.open_price == market_data.open_price
    
    
    class TestFeatureVector:
        """æµ‹è¯•FeatureVectoræ•°æ®ç±»"""
        
        def test_feature_vector_creation(self):
            """æµ‹è¯•FeatureVectoræ­£å¸¸åˆ›å»º"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            feature_vector = FeatureVector(
                timestamp=timestamp,
                symbol="000001.SZ",
                technical_indicators={"rsi": 65.5, "macd": 0.12},
                fundamental_factors={"pe_ratio": 15.2, "pb_ratio": 1.8},
                market_microstructure={"bid_ask_spread": 0.01, "order_imbalance": 0.05}
            )
            
            assert feature_vector.timestamp == timestamp
            assert feature_vector.symbol == "000001.SZ"
            assert feature_vector.technical_indicators["rsi"] == 65.5
            assert feature_vector.fundamental_factors["pe_ratio"] == 15.2
            assert feature_vector.market_microstructure["bid_ask_spread"] == 0.01
        
        def test_feature_vector_validation_empty_features(self):
            """æµ‹è¯•ç©ºç‰¹å¾éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="æŠ€æœ¯æŒ‡æ ‡ä¸èƒ½ä¸ºç©º"):
                FeatureVector(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    technical_indicators={},  # ç©ºå­—å…¸
                    fundamental_factors={"pe_ratio": 15.2},
                    market_microstructure={"bid_ask_spread": 0.01}
                )
        
        def test_feature_vector_validation_nan_values(self):
            """æµ‹è¯•NaNå€¼éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="ç‰¹å¾å€¼ä¸èƒ½åŒ…å«NaN"):
                FeatureVector(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    technical_indicators={"rsi": float('nan')},  # NaNå€¼
                    fundamental_factors={"pe_ratio": 15.2},
                    market_microstructure={"bid_ask_spread": 0.01}
                )
        
        def test_feature_vector_serialization(self):
            """æµ‹è¯•FeatureVectoråºåˆ—åŒ–"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            feature_vector = FeatureVector(
                timestamp=timestamp,
                symbol="000001.SZ",
                technical_indicators={"rsi": 65.5, "macd": 0.12},
                fundamental_factors={"pe_ratio": 15.2, "pb_ratio": 1.8},
                market_microstructure={"bid_ask_spread": 0.01, "order_imbalance": 0.05}
            )
            
            # æµ‹è¯•to_dict
            data_dict = feature_vector.to_dict()
            assert isinstance(data_dict, dict)
            assert data_dict['technical_indicators']['rsi'] == 65.5
            
            # æµ‹è¯•from_dict
            restored_vector = FeatureVector.from_dict(data_dict)
            assert restored_vector.symbol == feature_vector.symbol
            assert restored_vector.technical_indicators['rsi'] == feature_vector.technical_indicators['rsi']
    
    
    class TestTradingState:
        """æµ‹è¯•TradingStateæ•°æ®ç±»"""
        
        def test_trading_state_creation(self):
            """æµ‹è¯•TradingStateæ­£å¸¸åˆ›å»º"""
            features = np.random.randn(60, 100, 50)  # lookback_window, n_stocks, n_features
            positions = np.random.rand(100)
            positions = positions / positions.sum()  # æ ‡å‡†åŒ–æƒé‡
            market_state = np.random.randn(10)
            
            trading_state = TradingState(
                features=features,
                positions=positions,
                market_state=market_state,
                cash=100000.0,
                total_value=1000000.0
            )
            
            assert trading_state.features.shape == (60, 100, 50)
            assert trading_state.positions.shape == (100,)
            assert trading_state.market_state.shape == (10,)
            assert trading_state.cash == 100000.0
            assert trading_state.total_value == 1000000.0
        
        def test_trading_state_validation_features_shape(self):
            """æµ‹è¯•ç‰¹å¾ç»´åº¦éªŒè¯"""
            features = np.random.randn(60, 50)  # é”™è¯¯çš„ç»´åº¦
            positions = np.random.rand(100)
            market_state = np.random.randn(10)
            
            with pytest.raises(ValueError, match="ç‰¹å¾æ•°ç»„å¿…é¡»æ˜¯3ç»´"):
                TradingState(
                    features=features,
                    positions=positions,
                    market_state=market_state,
                    cash=100000.0,
                    total_value=1000000.0
                )
        
        def test_trading_state_validation_positions_sum(self):
            """æµ‹è¯•æŒä»“æƒé‡å’ŒéªŒè¯"""
            features = np.random.randn(60, 100, 50)
            positions = np.array([0.5, 0.6])  # æƒé‡å’Œä¸ä¸º1
            market_state = np.random.randn(10)
            
            with pytest.raises(ValueError, match="æŒä»“æƒé‡å’Œå¿…é¡»æŽ¥è¿‘1"):
                TradingState(
                    features=features,
                    positions=positions,
                    market_state=market_state,
                    cash=100000.0,
                    total_value=1000000.0
                )
        
        def test_trading_state_validation_negative_cash(self):
            """æµ‹è¯•è´ŸçŽ°é‡‘éªŒè¯"""
            features = np.random.randn(60, 100, 50)
            positions = np.random.rand(100)
            positions = positions / positions.sum()
            market_state = np.random.randn(10)
            
            with pytest.raises(ValueError, match="çŽ°é‡‘ä¸èƒ½ä¸ºè´Ÿæ•°"):
                TradingState(
                    features=features,
                    positions=positions,
                    market_state=market_state,
                    cash=-1000.0,  # è´ŸçŽ°é‡‘
                    total_value=1000000.0
                )
        
        def test_trading_state_serialization(self):
            """æµ‹è¯•TradingStateåºåˆ—åŒ–"""
            features = np.random.randn(60, 100, 50)
            positions = np.random.rand(100)
            positions = positions / positions.sum()
            market_state = np.random.randn(10)
            
            trading_state = TradingState(
                features=features,
                positions=positions,
                market_state=market_state,
                cash=100000.0,
                total_value=1000000.0
            )
            
            # æµ‹è¯•to_dict
            data_dict = trading_state.to_dict()
            assert isinstance(data_dict, dict)
            assert 'features' in data_dict
            assert 'positions' in data_dict
            
            # æµ‹è¯•from_dict
            restored_state = TradingState.from_dict(data_dict)
            assert np.allclose(restored_state.features, trading_state.features)
            assert np.allclose(restored_state.positions, trading_state.positions)
            assert restored_state.cash == trading_state.cash
    
    
    class TestTradingAction:
        """æµ‹è¯•TradingActionæ•°æ®ç±»"""
        
        def test_trading_action_creation(self):
            """æµ‹è¯•TradingActionæ­£å¸¸åˆ›å»º"""
            target_weights = np.random.rand(100)
            target_weights = target_weights / target_weights.sum()
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            trading_action = TradingAction(
                target_weights=target_weights,
                confidence=0.85,
                timestamp=timestamp
            )
            
            assert trading_action.target_weights.shape == (100,)
            assert abs(trading_action.target_weights.sum() - 1.0) < 1e-6
            assert trading_action.confidence == 0.85
            assert trading_action.timestamp == timestamp
        
        def test_trading_action_validation_weights_sum(self):
            """æµ‹è¯•æƒé‡å’ŒéªŒè¯"""
            target_weights = np.array([0.5, 0.6])  # æƒé‡å’Œä¸ä¸º1
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="ç›®æ ‡æƒé‡å’Œå¿…é¡»æŽ¥è¿‘1"):
                TradingAction(
                    target_weights=target_weights,
                    confidence=0.85,
                    timestamp=timestamp
                )
        
        def test_trading_action_validation_confidence_range(self):
            """æµ‹è¯•ç½®ä¿¡åº¦èŒƒå›´éªŒè¯"""
            target_weights = np.array([0.5, 0.5])
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="ç½®ä¿¡åº¦å¿…é¡»åœ¨0åˆ°1ä¹‹é—´"):
                TradingAction(
                    target_weights=target_weights,
                    confidence=1.5,  # è¶…å‡ºèŒƒå›´
                    timestamp=timestamp
                )
        
        def test_trading_action_serialization(self):
            """æµ‹è¯•TradingActionåºåˆ—åŒ–"""
            target_weights = np.array([0.6, 0.4])
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            trading_action = TradingAction(
                target_weights=target_weights,
                confidence=0.85,
                timestamp=timestamp
            )
            
            # æµ‹è¯•to_dict
            data_dict = trading_action.to_dict()
            assert isinstance(data_dict, dict)
            assert 'target_weights' in data_dict
            assert 'confidence' in data_dict
            
            # æµ‹è¯•from_dict
            restored_action = TradingAction.from_dict(data_dict)
            assert np.allclose(restored_action.target_weights, trading_action.target_weights)
            assert restored_action.confidence == trading_action.confidence
    
    
    class TestTransactionRecord:
        """æµ‹è¯•TransactionRecordæ•°æ®ç±»"""
        
        def test_transaction_record_creation(self):
            """æµ‹è¯•TransactionRecordæ­£å¸¸åˆ›å»º"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            record = TransactionRecord(
                timestamp=timestamp,
                symbol="000001.SZ",
                action_type="buy",
                quantity=1000,
                price=10.5,
                commission=10.5,
                stamp_tax=0.0,
                slippage=2.1,
                total_cost=12.6
            )
            
            assert record.timestamp == timestamp
            assert record.symbol == "000001.SZ"
            assert record.action_type == "buy"
            assert record.quantity == 1000
            assert record.price == 10.5
            assert record.commission == 10.5
            assert record.stamp_tax == 0.0
            assert record.slippage == 2.1
            assert record.total_cost == 12.6
        
        def test_transaction_record_validation_action_type(self):
            """æµ‹è¯•äº¤æ˜“ç±»åž‹éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="äº¤æ˜“ç±»åž‹å¿…é¡»æ˜¯'buy'æˆ–'sell'"):
                TransactionRecord(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    action_type="invalid",  # æ— æ•ˆç±»åž‹
                    quantity=1000,
                    price=10.5,
                    commission=10.5,
                    stamp_tax=0.0,
                    slippage=2.1,
                    total_cost=12.6
                )
        
        def test_transaction_record_validation_negative_quantity(self):
            """æµ‹è¯•è´Ÿæ•°é‡éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="äº¤æ˜“æ•°é‡ä¸èƒ½ä¸ºè´Ÿæ•°"):
                TransactionRecord(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    action_type="buy",
                    quantity=-1000,  # è´Ÿæ•°é‡
                    price=10.5,
                    commission=10.5,
                    stamp_tax=0.0,
                    slippage=2.1,
                    total_cost=12.6
                )
        
        def test_transaction_record_validation_negative_price(self):
            """æµ‹è¯•è´Ÿä»·æ ¼éªŒè¯"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            with pytest.raises(ValueError, match="ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿæ•°"):
                TransactionRecord(
                    timestamp=timestamp,
                    symbol="000001.SZ",
                    action_type="buy",
                    quantity=1000,
                    price=-10.5,  # è´Ÿä»·æ ¼
                    commission=10.5,
                    stamp_tax=0.0,
                    slippage=2.1,
                    total_cost=12.6
                )
        
        def test_transaction_record_serialization(self):
            """æµ‹è¯•TransactionRecordåºåˆ—åŒ–"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            record = TransactionRecord(
                timestamp=timestamp,
                symbol="000001.SZ",
                action_type="buy",
                quantity=1000,
                price=10.5,
                commission=10.5,
                stamp_tax=0.0,
                slippage=2.1,
                total_cost=12.6
            )
            
            # æµ‹è¯•to_dict
            data_dict = record.to_dict()
            assert isinstance(data_dict, dict)
            assert data_dict['symbol'] == "000001.SZ"
            assert data_dict['action_type'] == "buy"
            
            # æµ‹è¯•from_dict
            restored_record = TransactionRecord.from_dict(data_dict)
            assert restored_record.symbol == record.symbol
            assert restored_record.action_type == record.action_type
            assert restored_record.quantity == record.quantity
    
    
    class TestDataModelsBoundaryConditions:
        """æµ‹è¯•æ•°æ®æ¨¡åž‹è¾¹ç•Œæ¡ä»¶"""
        
        def test_zero_values(self):
            """æµ‹è¯•é›¶å€¼å¤„ç†"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            # MarketDataå…è®¸é›¶ä»·æ ¼ï¼ˆåœç‰Œæƒ…å†µï¼‰
            market_data = MarketData(
                timestamp=timestamp,
                symbol="000001.SZ",
                open_price=0.0,
                high_price=0.0,
                low_price=0.0,
                close_price=0.0,
                volume=0,
                amount=0.0
            )
            assert market_data.open_price == 0.0
        
        def test_extreme_values(self):
            """æµ‹è¯•æžå€¼å¤„ç†"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            # æµ‹è¯•æžå¤§å€¼
            market_data = MarketData(
                timestamp=timestamp,
                symbol="000001.SZ",
                open_price=1e6,
                high_price=1e6,
                low_price=1e6,
                close_price=1e6,
                volume=int(1e9),
                amount=1e15
            )
            assert market_data.open_price == 1e6
        
        def test_unicode_symbols(self):
            """æµ‹è¯•Unicodeç¬¦å·å¤„ç†"""
            timestamp = datetime(2023, 1, 1, 9, 30, 0, tzinfo=timezone.utc)
            
            market_data = MarketData(
                timestamp=timestamp,
                symbol="å¹³å®‰é“¶è¡Œ.SZ",  # ä¸­æ–‡ç¬¦å·
                open_price=10.0,
                high_price=10.5,
                low_price=9.8,
                close_price=10.2,
                volume=1000000,
                amount=10200000.0
            )
            assert market_data.symbol == "å¹³å®‰é“¶è¡Œ.SZ"
    
    
    class TestDataModelsPerformance:
        """æµ‹è¯•æ•°æ®æ¨¡åž‹æ€§èƒ½"""
        
        def test_large_array_serialization(self):
            """æµ‹è¯•å¤§æ•°ç»„åºåˆ—åŒ–æ€§èƒ½"""
            # åˆ›å»ºå¤§åž‹ç‰¹å¾æ•°ç»„
            features = np.random.randn(252, 1000, 100)  # ä¸€å¹´æ•°æ®ï¼Œ1000åªè‚¡ç¥¨ï¼Œ100ä¸ªç‰¹å¾
            positions = np.random.rand(1000)
            positions = positions / positions.sum()
            market_state = np.random.randn(50)
            
            trading_state = TradingState(
                features=features,
                positions=positions,
                market_state=market_state,
                cash=1000000.0,
                total_value=10000000.0
            )
            
            # æµ‹è¯•åºåˆ—åŒ–æ—¶é—´ï¼ˆåº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼‰
            import time
            start_time = time.time()
            data_dict = trading_state.to_dict()
            serialization_time = time.time() - start_time
            
            # åºåˆ—åŒ–æ—¶é—´åº”è¯¥å°äºŽ1ç§’
            assert serialization_time < 1.0
            
            # æµ‹è¯•ååºåˆ—åŒ–
            start_time = time.time()
            restored_state = TradingState.from_dict(data_dict)
            deserialization_time = time.time() - start_time
            
            # ååºåˆ—åŒ–æ—¶é—´åº”è¯¥å°äºŽ1ç§’
            assert deserialization_time < 1.0
            
            # éªŒè¯æ•°æ®å®Œæ•´æ€§
            assert np.allclose(restored_state.features, trading_state.features)
    ]]></file>
  <file path="tests/unit/test_data_interfaces.py"><![CDATA[
    """
    æ•°æ®æŽ¥å£æµ‹è¯•ç”¨ä¾‹
    æµ‹è¯•DataInterfaceæŠ½è±¡ç±»å’Œå…·ä½“å®žçŽ°ï¼ŒåŒ…æ‹¬Qlibå’ŒAkshareæ•°æ®èŽ·å–åŠŸèƒ½
    """
    
    import pytest
    import pandas as pd
    import numpy as np
    from datetime import datetime, timedelta
    from unittest.mock import Mock, patch, MagicMock
    from typing import List, Dict, Any
    
    from src.rl_trading_system.data.interfaces import DataInterface
    from src.rl_trading_system.data.qlib_interface import QlibDataInterface
    from src.rl_trading_system.data.akshare_interface import AkshareDataInterface
    from src.rl_trading_system.data.data_models import MarketData
    
    
    class TestDataInterface:
        """æµ‹è¯•DataInterfaceæŠ½è±¡ç±»"""
        
        def test_abstract_class_cannot_be_instantiated(self):
            """æµ‹è¯•æŠ½è±¡ç±»ä¸èƒ½ç›´æŽ¥å®žä¾‹åŒ–"""
            with pytest.raises(TypeError):
                DataInterface()
        
        def test_abstract_methods_must_be_implemented(self):
            """æµ‹è¯•æŠ½è±¡æ–¹æ³•å¿…é¡»è¢«å®žçŽ°"""
            class IncompleteInterface(DataInterface):
                pass
            
            with pytest.raises(TypeError):
                IncompleteInterface()
        
        def test_complete_implementation_can_be_instantiated(self):
            """æµ‹è¯•å®Œæ•´å®žçŽ°å¯ä»¥è¢«å®žä¾‹åŒ–"""
            class CompleteInterface(DataInterface):
                def get_stock_list(self, market: str = 'A') -> List[str]:
                    return ['000001.SZ']
                
                def get_price_data(self, symbols: List[str], 
                                  start_date: str, end_date: str) -> pd.DataFrame:
                    return pd.DataFrame()
                
                def get_fundamental_data(self, symbols: List[str], 
                                       start_date: str, end_date: str) -> pd.DataFrame:
                    return pd.DataFrame()
            
            interface = CompleteInterface()
            assert isinstance(interface, DataInterface)
    
    
    class TestQlibDataInterface:
        """æµ‹è¯•QlibDataInterfaceå®žçŽ°"""
        
        @pytest.fixture
        def qlib_interface(self):
            """åˆ›å»ºQlibDataInterfaceå®žä¾‹"""
            return QlibDataInterface(provider_uri="test://provider")
        
        @pytest.fixture
        def sample_stock_list(self):
            """ç¤ºä¾‹è‚¡ç¥¨åˆ—è¡¨"""
            return ['000001.SZ', '000002.SZ', '600000.SH', '600036.SH']
        
        @pytest.fixture
        def sample_price_data(self):
            """ç¤ºä¾‹ä»·æ ¼æ•°æ®"""
            dates = pd.date_range('2023-01-01', '2023-01-10', freq='D')
            symbols = ['000001.SZ', '000002.SZ']
            
            data = []
            for date in dates:
                for symbol in symbols:
                    data.append({
                        'datetime': date,
                        'instrument': symbol,
                        '$open': 10.0 + np.random.random(),
                        '$high': 11.0 + np.random.random(),
                        '$low': 9.0 + np.random.random(),
                        '$close': 10.5 + np.random.random(),
                        '$volume': 1000000 + np.random.randint(0, 500000),
                        '$amount': 10000000 + np.random.randint(0, 5000000)
                    })
            
            df = pd.DataFrame(data)
            df.set_index(['datetime', 'instrument'], inplace=True)
            return df
        
        @pytest.fixture
        def sample_fundamental_data(self):
            """ç¤ºä¾‹åŸºæœ¬é¢æ•°æ®"""
            dates = pd.date_range('2023-01-01', '2023-01-10', freq='D')
            symbols = ['000001.SZ', '000002.SZ']
            
            data = []
            for date in dates:
                for symbol in symbols:
                    data.append({
                        'datetime': date,
                        'instrument': symbol,
                        'pe_ratio': 15.0 + np.random.random() * 10,
                        'pb_ratio': 1.5 + np.random.random() * 2,
                        'market_cap': 1000000000 + np.random.randint(0, 500000000),
                        'total_revenue': 100000000 + np.random.randint(0, 50000000)
                    })
            
            df = pd.DataFrame(data)
            df.set_index(['datetime', 'instrument'], inplace=True)
            return df
        
        def test_initialization(self, qlib_interface):
            """æµ‹è¯•åˆå§‹åŒ–"""
            assert qlib_interface.provider_uri == "test://provider"
            assert isinstance(qlib_interface, DataInterface)
        
        def test_initialization_with_default_provider(self):
            """æµ‹è¯•ä½¿ç”¨é»˜è®¤provideråˆå§‹åŒ–"""
            interface = QlibDataInterface()
            assert interface.provider_uri is None
        
        @patch('qlib.init')
        @patch('qlib.D.instruments')
        def test_get_stock_list_success(self, mock_instruments, mock_init, 
                                       qlib_interface, sample_stock_list):
            """æµ‹è¯•æˆåŠŸèŽ·å–è‚¡ç¥¨åˆ—è¡¨"""
            mock_instruments.return_value = sample_stock_list
            
            result = qlib_interface.get_stock_list('A')
            
            assert result == sample_stock_list
            mock_init.assert_called_once()
            mock_instruments.assert_called_once_with(market='A')
        
        @patch('qlib.init')
        @patch('qlib.D.instruments')
        def test_get_stock_list_different_markets(self, mock_instruments, mock_init, 
                                                 qlib_interface):
            """æµ‹è¯•èŽ·å–ä¸åŒå¸‚åœºçš„è‚¡ç¥¨åˆ—è¡¨"""
            mock_instruments.return_value = ['000001.SZ']
            
            # æµ‹è¯•Aè‚¡å¸‚åœº
            qlib_interface.get_stock_list('A')
            mock_instruments.assert_called_with(market='A')
            
            # æµ‹è¯•æ¸¯è‚¡å¸‚åœº
            qlib_interface.get_stock_list('HK')
            mock_instruments.assert_called_with(market='HK')
        
        @patch('qlib.init')
        @patch('qlib.D.instruments')
        def test_get_stock_list_exception_handling(self, mock_instruments, mock_init, 
                                                  qlib_interface):
            """æµ‹è¯•èŽ·å–è‚¡ç¥¨åˆ—è¡¨å¼‚å¸¸å¤„ç†"""
            mock_instruments.side_effect = Exception("Qlib connection error")
            
            with pytest.raises(Exception) as exc_info:
                qlib_interface.get_stock_list('A')
            
            assert "Qlib connection error" in str(exc_info.value)
        
        @patch('qlib.init')
        @patch('qlib.D.features')
        def test_get_price_data_success(self, mock_features, mock_init, 
                                       qlib_interface, sample_price_data):
            """æµ‹è¯•æˆåŠŸèŽ·å–ä»·æ ¼æ•°æ®"""
            mock_features.return_value = sample_price_data
            
            symbols = ['000001.SZ', '000002.SZ']
            start_date = '2023-01-01'
            end_date = '2023-01-10'
            
            result = qlib_interface.get_price_data(symbols, start_date, end_date)
            
            assert isinstance(result, pd.DataFrame)
            assert not result.empty
            mock_init.assert_called_once()
            mock_features.assert_called_once()
        
        @patch('qlib.init')
        @patch('qlib.D.features')
        def test_get_price_data_empty_symbols(self, mock_features, mock_init, 
                                             qlib_interface):
            """æµ‹è¯•ç©ºè‚¡ç¥¨åˆ—è¡¨"""
            mock_features.return_value = pd.DataFrame()
            
            result = qlib_interface.get_price_data([], '2023-01-01', '2023-01-10')
            
            assert isinstance(result, pd.DataFrame)
            assert result.empty
        
        @patch('qlib.init')
        @patch('qlib.D.features')
        def test_get_price_data_invalid_date_range(self, mock_features, mock_init, 
                                                  qlib_interface):
            """æµ‹è¯•æ— æ•ˆæ—¥æœŸèŒƒå›´"""
            mock_features.side_effect = ValueError("Invalid date range")
            
            with pytest.raises(ValueError) as exc_info:
                qlib_interface.get_price_data(['000001.SZ'], '2023-01-10', '2023-01-01')
            
            assert "Invalid date range" in str(exc_info.value)
        
        @patch('qlib.init')
        @patch('qlib.D.features')
        def test_get_fundamental_data_success(self, mock_features, mock_init, 
                                             qlib_interface, sample_fundamental_data):
            """æµ‹è¯•æˆåŠŸèŽ·å–åŸºæœ¬é¢æ•°æ®"""
            mock_features.return_value = sample_fundamental_data
            
            symbols = ['000001.SZ', '000002.SZ']
            start_date = '2023-01-01'
            end_date = '2023-01-10'
            
            result = qlib_interface.get_fundamental_data(symbols, start_date, end_date)
            
            assert isinstance(result, pd.DataFrame)
            assert not result.empty
            mock_init.assert_called_once()
            mock_features.assert_called_once()
        
        @patch('qlib.init')
        @patch('qlib.D.features')
        def test_get_fundamental_data_missing_data(self, mock_features, mock_init, 
                                                  qlib_interface):
            """æµ‹è¯•åŸºæœ¬é¢æ•°æ®ç¼ºå¤±"""
            mock_features.return_value = pd.DataFrame()
            
            result = qlib_interface.get_fundamental_data(['000001.SZ'], 
                                                        '2023-01-01', '2023-01-10')
            
            assert isinstance(result, pd.DataFrame)
            assert result.empty
    
    
    class TestAkshareDataInterface:
        """æµ‹è¯•AkshareDataInterfaceå®žçŽ°"""
        
        @pytest.fixture
        def akshare_interface(self):
            """åˆ›å»ºAkshareDataInterfaceå®žä¾‹"""
            return AkshareDataInterface()
        
        @pytest.fixture
        def sample_stock_list_akshare(self):
            """ç¤ºä¾‹Akshareè‚¡ç¥¨åˆ—è¡¨"""
            return pd.DataFrame({
                'code': ['000001', '000002', '600000', '600036'],
                'name': ['å¹³å®‰é“¶è¡Œ', 'ä¸‡ç§‘A', 'æµ¦å‘é“¶è¡Œ', 'æ‹›å•†é“¶è¡Œ'],
                'market': ['sz', 'sz', 'sh', 'sh']
            })
        
        @pytest.fixture
        def sample_price_data_akshare(self):
            """ç¤ºä¾‹Akshareä»·æ ¼æ•°æ®"""
            dates = pd.date_range('2023-01-01', '2023-01-10', freq='D')
            
            data = []
            for date in dates:
                data.append({
                    'date': date.strftime('%Y-%m-%d'),
                    'open': 10.0 + np.random.random(),
                    'high': 11.0 + np.random.random(),
                    'low': 9.0 + np.random.random(),
                    'close': 10.5 + np.random.random(),
                    'volume': 1000000 + np.random.randint(0, 500000),
                    'amount': 10000000 + np.random.randint(0, 5000000)
                })
            
            return pd.DataFrame(data)
        
        def test_initialization(self, akshare_interface):
            """æµ‹è¯•åˆå§‹åŒ–"""
            assert isinstance(akshare_interface, DataInterface)
        
        @patch('akshare.stock_info_a_code_name')
        def test_get_stock_list_success(self, mock_stock_info, akshare_interface, 
                                       sample_stock_list_akshare):
            """æµ‹è¯•æˆåŠŸèŽ·å–è‚¡ç¥¨åˆ—è¡¨"""
            mock_stock_info.return_value = sample_stock_list_akshare
            
            result = akshare_interface.get_stock_list('A')
            
            assert isinstance(result, list)
            assert len(result) > 0
            mock_stock_info.assert_called_once()
        
        @patch('akshare.stock_info_a_code_name')
        def test_get_stock_list_exception_handling(self, mock_stock_info, 
                                                  akshare_interface):
            """æµ‹è¯•èŽ·å–è‚¡ç¥¨åˆ—è¡¨å¼‚å¸¸å¤„ç†"""
            mock_stock_info.side_effect = Exception("Akshare API error")
            
            with pytest.raises(Exception) as exc_info:
                akshare_interface.get_stock_list('A')
            
            assert "Akshare API error" in str(exc_info.value)
        
        @patch('akshare.stock_zh_a_hist')
        def test_get_price_data_success(self, mock_stock_hist, akshare_interface, 
                                       sample_price_data_akshare):
            """æµ‹è¯•æˆåŠŸèŽ·å–ä»·æ ¼æ•°æ®"""
            mock_stock_hist.return_value = sample_price_data_akshare
            
            symbols = ['000001']
            start_date = '2023-01-01'
            end_date = '2023-01-10'
            
            result = akshare_interface.get_price_data(symbols, start_date, end_date)
            
            assert isinstance(result, pd.DataFrame)
            mock_stock_hist.assert_called()
        
        @patch('akshare.stock_zh_a_hist')
        def test_get_price_data_multiple_symbols(self, mock_stock_hist, 
                                                akshare_interface, sample_price_data_akshare):
            """æµ‹è¯•èŽ·å–å¤šä¸ªè‚¡ç¥¨çš„ä»·æ ¼æ•°æ®"""
            mock_stock_hist.return_value = sample_price_data_akshare
            
            symbols = ['000001', '000002', '600000']
            start_date = '2023-01-01'
            end_date = '2023-01-10'
            
            result = akshare_interface.get_price_data(symbols, start_date, end_date)
            
            assert isinstance(result, pd.DataFrame)
            # åº”è¯¥ä¸ºæ¯ä¸ªè‚¡ç¥¨è°ƒç”¨ä¸€æ¬¡API
            assert mock_stock_hist.call_count == len(symbols)
        
        @patch('akshare.stock_zh_a_hist')
        def test_get_price_data_api_failure(self, mock_stock_hist, akshare_interface):
            """æµ‹è¯•APIè°ƒç”¨å¤±è´¥"""
            mock_stock_hist.side_effect = Exception("API rate limit exceeded")
            
            with pytest.raises(Exception) as exc_info:
                akshare_interface.get_price_data(['000001'], '2023-01-01', '2023-01-10')
            
            assert "API rate limit exceeded" in str(exc_info.value)
        
        def test_get_fundamental_data_not_implemented(self, akshare_interface):
            """æµ‹è¯•åŸºæœ¬é¢æ•°æ®èŽ·å–ï¼ˆå½“å‰æœªå®žçŽ°ï¼‰"""
            result = akshare_interface.get_fundamental_data(['000001'], 
                                                           '2023-01-01', '2023-01-10')
            
            assert isinstance(result, pd.DataFrame)
            # å½“å‰å®žçŽ°è¿”å›žç©ºDataFrame
    
    
    class TestDataFormatUnification:
        """æµ‹è¯•æ•°æ®æ ¼å¼ç»Ÿä¸€"""
        
        @pytest.fixture
        def qlib_interface(self):
            return QlibDataInterface()
        
        @pytest.fixture
        def akshare_interface(self):
            return AkshareDataInterface()
        
        def test_price_data_format_consistency(self, qlib_interface, akshare_interface):
            """æµ‹è¯•ä»·æ ¼æ•°æ®æ ¼å¼ä¸€è‡´æ€§"""
            symbols = ['000001.SZ']
            start_date = '2023-01-01'
            end_date = '2023-01-10'
            
            with patch('qlib.init'), patch('qlib.D.features') as mock_qlib_features:
                # æ¨¡æ‹ŸQlibæ•°æ®æ ¼å¼
                qlib_data = pd.DataFrame({
                    'datetime': pd.date_range('2023-01-01', '2023-01-10'),
                    'instrument': ['000001.SZ'] * 10,
                    '$open': np.random.random(10) * 10 + 10,
                    '$high': np.random.random(10) * 10 + 11,
                    '$low': np.random.random(10) * 10 + 9,
                    '$close': np.random.random(10) * 10 + 10,
                    '$volume': np.random.randint(1000000, 2000000, 10),
                    '$amount': np.random.randint(10000000, 20000000, 10)
                }).set_index(['datetime', 'instrument'])
                mock_qlib_features.return_value = qlib_data
                
                qlib_result = qlib_interface.get_price_data(symbols, start_date, end_date)
            
            with patch('akshare.stock_zh_a_hist') as mock_akshare_hist:
                # æ¨¡æ‹ŸAkshareæ•°æ®æ ¼å¼
                akshare_data = pd.DataFrame({
                    'date': pd.date_range('2023-01-01', '2023-01-10').strftime('%Y-%m-%d'),
                    'open': np.random.random(10) * 10 + 10,
                    'high': np.random.random(10) * 10 + 11,
                    'low': np.random.random(10) * 10 + 9,
                    'close': np.random.random(10) * 10 + 10,
                    'volume': np.random.randint(1000000, 2000000, 10),
                    'amount': np.random.randint(10000000, 20000000, 10)
                })
                mock_akshare_hist.return_value = akshare_data
                
                akshare_result = akshare_interface.get_price_data(['000001'], 
                                                                 start_date, end_date)
            
            # éªŒè¯ä¸¤ä¸ªæŽ¥å£è¿”å›žçš„æ•°æ®æ ¼å¼ä¸€è‡´
            assert isinstance(qlib_result, pd.DataFrame)
            assert isinstance(akshare_result, pd.DataFrame)
        
        def test_data_validation_with_market_data_model(self):
            """æµ‹è¯•ä½¿ç”¨MarketDataæ¨¡åž‹è¿›è¡Œæ•°æ®éªŒè¯"""
            # åˆ›å»ºæœ‰æ•ˆçš„å¸‚åœºæ•°æ®
            valid_data = MarketData(
                timestamp=datetime.now(),
                symbol='000001.SZ',
                open_price=10.0,
                high_price=11.0,
                low_price=9.0,
                close_price=10.5,
                volume=1000000,
                amount=10500000
            )
            
            assert valid_data.symbol == '000001.SZ'
            assert valid_data.high_price > valid_data.low_price
            
            # æµ‹è¯•æ— æ•ˆæ•°æ®
            with pytest.raises(ValueError):
                MarketData(
                    timestamp=datetime.now(),
                    symbol='000001.SZ',
                    open_price=10.0,
                    high_price=9.0,  # æœ€é«˜ä»·ä½ŽäºŽæœ€ä½Žä»·
                    low_price=11.0,
                    close_price=10.5,
                    volume=1000000,
                    amount=10500000
                )
    
    
    class TestDataQualityChecks:
        """æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥"""
        
        def test_missing_data_detection(self):
            """æµ‹è¯•ç¼ºå¤±æ•°æ®æ£€æµ‹"""
            # åˆ›å»ºåŒ…å«ç¼ºå¤±å€¼çš„æ•°æ®
            data = pd.DataFrame({
                'open': [10.0, np.nan, 12.0],
                'high': [11.0, 13.0, np.nan],
                'low': [9.0, 11.0, 11.5],
                'close': [10.5, 12.5, 12.0],
                'volume': [1000000, 1200000, 1100000]
            })
            
            # æ£€æµ‹ç¼ºå¤±å€¼
            missing_data = data.isnull().sum()
            assert missing_data['open'] == 1
            assert missing_data['high'] == 1
            assert missing_data['low'] == 0
        
        def test_outlier_detection(self):
            """æµ‹è¯•å¼‚å¸¸å€¼æ£€æµ‹"""
            # åˆ›å»ºåŒ…å«å¼‚å¸¸å€¼çš„æ•°æ®
            normal_prices = np.random.normal(10, 1, 100)
            outlier_prices = np.append(normal_prices, [100, -5])  # æ·»åŠ å¼‚å¸¸å€¼
            
            # ä½¿ç”¨3Ïƒè§„åˆ™æ£€æµ‹å¼‚å¸¸å€¼
            mean_price = np.mean(normal_prices)
            std_price = np.std(normal_prices)
            
            outliers = np.abs(outlier_prices - mean_price) > 3 * std_price
            assert np.sum(outliers) >= 2  # è‡³å°‘æ£€æµ‹åˆ°2ä¸ªå¼‚å¸¸å€¼
        
        def test_data_consistency_checks(self):
            """æµ‹è¯•æ•°æ®ä¸€è‡´æ€§æ£€æŸ¥"""
            # æµ‹è¯•ä»·æ ¼å…³ç³»ä¸€è‡´æ€§
            data = pd.DataFrame({
                'open': [10.0, 11.0, 12.0],
                'high': [11.0, 12.0, 13.0],
                'low': [9.0, 10.0, 11.0],
                'close': [10.5, 11.5, 12.5],
                'volume': [1000000, 1200000, 1100000]
            })
            
            # æ£€æŸ¥high >= low
            assert (data['high'] >= data['low']).all()
            
            # æ£€æŸ¥ä»·æ ¼åœ¨åˆç†èŒƒå›´å†…
            assert (data['high'] >= data['open']).all() or (data['high'] >= data['close']).all()
            assert (data['low'] <= data['open']).all() or (data['low'] <= data['close']).all()
        
        def test_volume_amount_consistency(self):
            """æµ‹è¯•æˆäº¤é‡å’Œæˆäº¤é¢ä¸€è‡´æ€§"""
            data = pd.DataFrame({
                'close': [10.0, 11.0, 12.0],
                'volume': [1000000, 1200000, 1100000],
                'amount': [10000000, 13200000, 13200000]
            })
            
            # è®¡ç®—å¹³å‡ä»·æ ¼
            avg_price = data['amount'] / data['volume']
            
            # éªŒè¯å¹³å‡ä»·æ ¼åœ¨åˆç†èŒƒå›´å†…ï¼ˆæŽ¥è¿‘æ”¶ç›˜ä»·ï¼‰
            price_diff_ratio = np.abs(avg_price - data['close']) / data['close']
            assert (price_diff_ratio < 0.1).all()  # å·®å¼‚å°äºŽ10%
    
    
    class TestErrorHandling:
        """æµ‹è¯•é”™è¯¯å¤„ç†"""
        
        @pytest.fixture
        def qlib_interface(self):
            return QlibDataInterface()
        
        @pytest.fixture
        def akshare_interface(self):
            return AkshareDataInterface()
        
        def test_network_error_handling(self, qlib_interface):
            """æµ‹è¯•ç½‘ç»œé”™è¯¯å¤„ç†"""
            with patch('qlib.init') as mock_init:
                mock_init.side_effect = ConnectionError("Network connection failed")
                
                with pytest.raises(ConnectionError):
                    qlib_interface.get_stock_list('A')
        
        def test_api_rate_limit_handling(self, akshare_interface):
            """æµ‹è¯•APIé™æµå¤„ç†"""
            with patch('akshare.stock_info_a_code_name') as mock_api:
                mock_api.side_effect = Exception("API rate limit exceeded")
                
                with pytest.raises(Exception) as exc_info:
                    akshare_interface.get_stock_list('A')
                
                assert "rate limit" in str(exc_info.value).lower()
        
        def test_invalid_symbol_handling(self, qlib_interface):
            """æµ‹è¯•æ— æ•ˆè‚¡ç¥¨ä»£ç å¤„ç†"""
            with patch('qlib.init'), patch('qlib.D.features') as mock_features:
                mock_features.side_effect = ValueError("Invalid symbol")
                
                with pytest.raises(ValueError):
                    qlib_interface.get_price_data(['INVALID'], '2023-01-01', '2023-01-10')
        
        def test_date_format_validation(self, qlib_interface):
            """æµ‹è¯•æ—¥æœŸæ ¼å¼éªŒè¯"""
            with patch('qlib.init'), patch('qlib.D.features') as mock_features:
                mock_features.side_effect = ValueError("Invalid date format")
                
                with pytest.raises(ValueError):
                    qlib_interface.get_price_data(['000001.SZ'], 'invalid-date', '2023-01-10')
    
    
    @pytest.mark.integration
    class TestDataInterfaceIntegration:
        """æ•°æ®æŽ¥å£é›†æˆæµ‹è¯•"""
        
        def test_data_pipeline_integration(self):
            """æµ‹è¯•æ•°æ®ç®¡é“é›†æˆ"""
            # è¿™æ˜¯ä¸€ä¸ªé›†æˆæµ‹è¯•ç¤ºä¾‹ï¼Œå®žé™…è¿è¡Œéœ€è¦çœŸå®žçš„æ•°æ®æº
            pass
        
        def test_cache_mechanism_integration(self):
            """æµ‹è¯•ç¼“å­˜æœºåˆ¶é›†æˆ"""
            # æµ‹è¯•æ•°æ®ç¼“å­˜åŠŸèƒ½
            pass
        
        def test_data_source_failover(self):
            """æµ‹è¯•æ•°æ®æºæ•…éšœè½¬ç§»"""
            # æµ‹è¯•å½“ä¸»æ•°æ®æºå¤±è´¥æ—¶ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº
            pass
    
    
    if __name__ == "__main__":
        pytest.main([__file__, "-v"])
    ]]></file>
  <file path="tests/unit/test_critic_network.py"><![CDATA[
    """
    æµ‹è¯•Criticç½‘ç»œçš„å•å…ƒæµ‹è¯•
    """
    import pytest
    import torch
    import torch.nn as nn
    import numpy as np
    from unittest.mock import Mock, patch
    
    from src.rl_trading_system.models.critic_network import Critic, CriticConfig
    
    
    class TestCriticNetwork:
        """Criticç½‘ç»œæµ‹è¯•ç±»"""
        
        @pytest.fixture
        def critic_config(self):
            """Criticé…ç½®fixture"""
            return CriticConfig(
                state_dim=256,
                action_dim=100,
                hidden_dim=512,
                n_layers=3,
                activation='relu',
                dropout=0.1
            )
        
        @pytest.fixture
        def critic_network(self, critic_config):
            """Criticç½‘ç»œfixture"""
            return Critic(critic_config)
        
        @pytest.fixture
        def sample_state(self, critic_config):
            """æ ·æœ¬çŠ¶æ€fixture"""
            batch_size = 32
            return torch.randn(batch_size, critic_config.state_dim)
        
        @pytest.fixture
        def sample_action(self, critic_config):
            """æ ·æœ¬åŠ¨ä½œfixture"""
            batch_size = 32
            # ç”Ÿæˆæ ‡å‡†åŒ–çš„æŠ•èµ„ç»„åˆæƒé‡
            action = torch.rand(batch_size, critic_config.action_dim)
            action = action / action.sum(dim=1, keepdim=True)
            return action
        
        def test_critic_initialization(self, critic_network, critic_config):
            """æµ‹è¯•Criticç½‘ç»œåˆå§‹åŒ–"""
            assert isinstance(critic_network, nn.Module)
            assert critic_network.config.state_dim == critic_config.state_dim
            assert critic_network.config.action_dim == critic_config.action_dim
            assert critic_network.config.hidden_dim == critic_config.hidden_dim
            
            # æ£€æŸ¥ç½‘ç»œå±‚æ˜¯å¦æ­£ç¡®åˆ›å»º
            assert hasattr(critic_network, 'state_encoder')
            assert hasattr(critic_network, 'action_encoder')
            assert hasattr(critic_network, 'q_network')
            
        def test_forward_pass_shape(self, critic_network, sample_state, sample_action):
            """æµ‹è¯•å‰å‘ä¼ æ’­è¾“å‡ºå½¢çŠ¶"""
            q_value = critic_network.forward(sample_state, sample_action)
            
            batch_size = sample_state.size(0)
            expected_shape = (batch_size, 1)
            
            assert q_value.shape == expected_shape
            
        def test_forward_pass_values(self, critic_network, sample_state, sample_action):
            """æµ‹è¯•å‰å‘ä¼ æ’­è¾“å‡ºå€¼çš„åˆç†æ€§"""
            q_value = critic_network.forward(sample_state, sample_action)
            
            # æ£€æŸ¥Qå€¼æ˜¯å¦æœ‰é™
            assert torch.all(torch.isfinite(q_value))
            
            # Qå€¼åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆä¸åº”è¯¥è¿‡å¤§æˆ–è¿‡å°ï¼‰
            assert torch.all(q_value > -1000) and torch.all(q_value < 1000)
            
        def test_q_value_estimation_consistency(self, critic_network, sample_state, sample_action):
            """æµ‹è¯•Qå€¼ä¼°è®¡çš„ä¸€è‡´æ€§"""
            # ç›¸åŒè¾“å…¥åº”è¯¥äº§ç”Ÿç›¸åŒè¾“å‡º
            critic_network.eval()
            
            q_value1 = critic_network.forward(sample_state, sample_action)
            q_value2 = critic_network.forward(sample_state, sample_action)
            
            assert torch.allclose(q_value1, q_value2, atol=1e-6)
            
        def test_different_actions_different_q_values(self, critic_network, sample_state, critic_config):
            """æµ‹è¯•ä¸åŒåŠ¨ä½œäº§ç”Ÿä¸åŒQå€¼"""
            critic_network.eval()
            
            # ç”Ÿæˆä¸¤ä¸ªæžç«¯ä¸åŒçš„åŠ¨ä½œ
            batch_size = sample_state.size(0)
            
            # åŠ¨ä½œ1ï¼šæ‰€æœ‰æƒé‡ç»™ç¬¬ä¸€ä¸ªèµ„äº§
            action1 = torch.zeros(batch_size, critic_config.action_dim)
            action1[:, 0] = 1.0
            
            # åŠ¨ä½œ2ï¼šå‡åŒ€åˆ†å¸ƒæƒé‡
            action2 = torch.ones(batch_size, critic_config.action_dim) / critic_config.action_dim
            
            q_value1 = critic_network.forward(sample_state, action1)
            q_value2 = critic_network.forward(sample_state, action2)
            
            # æžç«¯ä¸åŒçš„åŠ¨ä½œåº”è¯¥äº§ç”Ÿä¸åŒçš„Qå€¼
            different_count = torch.sum(torch.abs(q_value1 - q_value2) > 1e-6)
            assert different_count > 0, "æžç«¯ä¸åŒçš„åŠ¨ä½œåº”è¯¥äº§ç”Ÿä¸åŒçš„Qå€¼"
            
            # æ£€æŸ¥Qå€¼çš„å˜å¼‚æ€§
            q_diff = torch.abs(q_value1 - q_value2)
            max_diff = torch.max(q_diff)
            assert max_diff > 1e-6, f"Qå€¼å·®å¼‚è¿‡å°: {max_diff}"
            
        def test_gradient_flow(self, critic_network, sample_state, sample_action):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            sample_state.requires_grad_(True)
            sample_action.requires_grad_(True)
            
            # å‰å‘ä¼ æ’­
            q_value = critic_network.forward(sample_state, sample_action)
            
            # è®¡ç®—æŸå¤±
            loss = torch.mean(q_value ** 2)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥è¾“å…¥æ¢¯åº¦
            assert sample_state.grad is not None
            assert sample_action.grad is not None
            assert torch.all(torch.isfinite(sample_state.grad))
            assert torch.all(torch.isfinite(sample_action.grad))
            
            # æ£€æŸ¥ç½‘ç»œå‚æ•°æ¢¯åº¦
            for name, param in critic_network.named_parameters():
                if param.requires_grad:
                    assert param.grad is not None, f"å‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
                    assert torch.all(torch.isfinite(param.grad)), f"å‚æ•° {name} çš„æ¢¯åº¦ä¸æ˜¯æœ‰é™å€¼"
                    
        def test_batch_processing(self, critic_network, critic_config):
            """æµ‹è¯•æ‰¹å¤„ç†èƒ½åŠ›"""
            batch_sizes = [1, 16, 32, 64]
            
            for batch_size in batch_sizes:
                state = torch.randn(batch_size, critic_config.state_dim)
                action = torch.rand(batch_size, critic_config.action_dim)
                action = action / action.sum(dim=1, keepdim=True)
                
                q_value = critic_network.forward(state, action)
                
                assert q_value.shape == (batch_size, 1)
                assert torch.all(torch.isfinite(q_value))
                
        def test_network_parameter_count(self, critic_network, critic_config):
            """æµ‹è¯•ç½‘ç»œå‚æ•°æ•°é‡çš„åˆç†æ€§"""
            total_params = sum(p.numel() for p in critic_network.parameters())
            
            # ä¼°ç®—å‚æ•°æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
            expected_min_params = (
                critic_config.state_dim * critic_config.hidden_dim +  # çŠ¶æ€ç¼–ç å™¨
                critic_config.action_dim * critic_config.hidden_dim +  # åŠ¨ä½œç¼–ç å™¨
                critic_config.hidden_dim * 1  # è¾“å‡ºå±‚
            )
            
            assert total_params >= expected_min_params
            assert total_params < expected_min_params * 20  # ä¸åº”è¯¥è¿‡å¤§
            
        def test_different_activation_functions(self, critic_config):
            """æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°"""
            activations = ['relu', 'tanh', 'gelu']
            
            for activation in activations:
                config = CriticConfig(
                    state_dim=critic_config.state_dim,
                    action_dim=critic_config.action_dim,
                    hidden_dim=critic_config.hidden_dim,
                    activation=activation
                )
                
                critic = Critic(config)
                state = torch.randn(16, critic_config.state_dim)
                action = torch.rand(16, critic_config.action_dim)
                action = action / action.sum(dim=1, keepdim=True)
                
                # åº”è¯¥èƒ½å¤Ÿæ­£å¸¸å‰å‘ä¼ æ’­
                q_value = critic.forward(state, action)
                assert q_value.shape == (16, 1)
                assert torch.all(torch.isfinite(q_value))
                
        def test_numerical_stability(self, critic_network, critic_config):
            """æµ‹è¯•æ•°å€¼ç¨³å®šæ€§"""
            # æµ‹è¯•æžç«¯è¾“å…¥å€¼
            extreme_states = [
                torch.full((4, critic_config.state_dim), 1e6),   # å¾ˆå¤§çš„å€¼
                torch.full((4, critic_config.state_dim), -1e6),  # å¾ˆå°çš„å€¼
                torch.zeros(4, critic_config.state_dim),         # é›¶å€¼
            ]
            
            extreme_actions = [
                torch.ones(4, critic_config.action_dim) / critic_config.action_dim,  # å‡åŒ€åˆ†å¸ƒ
                torch.zeros(4, critic_config.action_dim),  # é›¶æƒé‡ï¼ˆéœ€è¦å¤„ç†ï¼‰
            ]
            
            # å¤„ç†é›¶æƒé‡åŠ¨ä½œ
            extreme_actions[1][:, 0] = 1.0  # å…¨éƒ¨æƒé‡ç»™ç¬¬ä¸€ä¸ªèµ„äº§
            
            for i, state in enumerate(extreme_states):
                for j, action in enumerate(extreme_actions):
                    q_value = critic_network.forward(state, action)
                    
                    # è¾“å‡ºåº”è¯¥æ˜¯æœ‰é™çš„
                    assert torch.all(torch.isfinite(q_value)), f"æžç«¯è¾“å…¥ state_{i}, action_{j} äº§ç”Ÿäº†æ— é™å€¼"
                    
        def test_state_action_interaction(self, critic_network, critic_config):
            """æµ‹è¯•çŠ¶æ€-åŠ¨ä½œäº¤äº’"""
            batch_size = 16
            
            # å›ºå®šçŠ¶æ€ï¼Œå˜åŒ–åŠ¨ä½œ
            fixed_state = torch.randn(1, critic_config.state_dim).repeat(batch_size, 1)
            
            # ç”Ÿæˆä¸åŒçš„åŠ¨ä½œ
            actions = []
            for i in range(batch_size):
                action = torch.zeros(critic_config.action_dim)
                action[i % critic_config.action_dim] = 1.0  # å•ä¸€èµ„äº§æƒé‡ä¸º1
                actions.append(action)
            
            actions = torch.stack(actions)
            
            q_values = critic_network.forward(fixed_state, actions)
            
            # ä¸åŒåŠ¨ä½œåº”è¯¥äº§ç”Ÿä¸åŒçš„Qå€¼
            q_values_unique = torch.unique(q_values.round(decimals=4))
            assert len(q_values_unique) > 1, "ç›¸åŒçŠ¶æ€ä¸‹ä¸åŒåŠ¨ä½œåº”è¯¥äº§ç”Ÿä¸åŒQå€¼"
            
        def test_weight_initialization(self, critic_config):
            """æµ‹è¯•æƒé‡åˆå§‹åŒ–"""
            critic = Critic(critic_config)
            
            # æ£€æŸ¥æƒé‡æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            for name, param in critic.named_parameters():
                if 'weight' in name:
                    # æƒé‡åº”è¯¥ä¸å…¨ä¸ºé›¶
                    assert not torch.all(param == 0), f"æƒé‡ {name} å…¨ä¸ºé›¶"
                    
                    # æƒé‡åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
                    assert torch.all(torch.abs(param) < 10), f"æƒé‡ {name} è¿‡å¤§"
                    
                elif 'bias' in name:
                    # åç½®é€šå¸¸åˆå§‹åŒ–ä¸ºé›¶æˆ–å°å€¼
                    assert torch.all(torch.abs(param) < 1), f"åç½® {name} è¿‡å¤§"
                    
        def test_output_sensitivity(self, critic_network, sample_state, sample_action):
            """æµ‹è¯•è¾“å‡ºå¯¹è¾“å…¥çš„æ•æ„Ÿæ€§"""
            sample_state.requires_grad_(True)
            sample_action.requires_grad_(True)
            
            q_value = critic_network.forward(sample_state, sample_action)
            loss = torch.mean(q_value)
            loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦çš„å¤§å°
            state_grad_norm = torch.norm(sample_state.grad)
            action_grad_norm = torch.norm(sample_action.grad)
            
            # æ¢¯åº¦ä¸åº”è¯¥è¿‡å¤§æˆ–è¿‡å°
            assert state_grad_norm > 1e-6, "çŠ¶æ€æ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±"
            assert state_grad_norm < 1e3, "çŠ¶æ€æ¢¯åº¦è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸"
            assert action_grad_norm > 1e-6, "åŠ¨ä½œæ¢¯åº¦è¿‡å°ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦æ¶ˆå¤±"
            assert action_grad_norm < 1e3, "åŠ¨ä½œæ¢¯åº¦è¿‡å¤§ï¼Œå¯èƒ½å­˜åœ¨æ¢¯åº¦çˆ†ç‚¸"
            
        @pytest.mark.parametrize("state_dim,action_dim,hidden_dim", [
            (128, 50, 256),
            (512, 200, 1024),
            (64, 10, 128)
        ])
        def test_different_dimensions(self, state_dim, action_dim, hidden_dim):
            """æµ‹è¯•ä¸åŒç»´åº¦é…ç½®"""
            config = CriticConfig(
                state_dim=state_dim,
                action_dim=action_dim,
                hidden_dim=hidden_dim
            )
            
            critic = Critic(config)
            state = torch.randn(8, state_dim)
            action = torch.rand(8, action_dim)
            action = action / action.sum(dim=1, keepdim=True)
            
            q_value = critic.forward(state, action)
            
            assert q_value.shape == (8, 1)
            assert torch.all(torch.isfinite(q_value))
            
        def test_training_vs_eval_mode(self, critic_network, sample_state, sample_action):
            """æµ‹è¯•è®­ç»ƒæ¨¡å¼vsè¯„ä¼°æ¨¡å¼"""
            # è®­ç»ƒæ¨¡å¼
            critic_network.train()
            q_value_train1 = critic_network.forward(sample_state, sample_action)
            q_value_train2 = critic_network.forward(sample_state, sample_action)
            
            # è¯„ä¼°æ¨¡å¼
            critic_network.eval()
            q_value_eval1 = critic_network.forward(sample_state, sample_action)
            q_value_eval2 = critic_network.forward(sample_state, sample_action)
            
            # è¯„ä¼°æ¨¡å¼ä¸‹åº”è¯¥æ˜¯ç¡®å®šæ€§çš„
            assert torch.allclose(q_value_eval1, q_value_eval2, atol=1e-6)
            
            # å¦‚æžœæœ‰dropoutï¼Œè®­ç»ƒæ¨¡å¼å¯èƒ½æœ‰éšæœºæ€§
            if critic_network.config.dropout > 0:
                # è®­ç»ƒæ¨¡å¼å¯èƒ½æœ‰è½»å¾®å·®å¼‚ï¼ˆç”±äºŽdropoutï¼‰
                pass
            else:
                # æ²¡æœ‰dropoutæ—¶ï¼Œè®­ç»ƒæ¨¡å¼ä¹Ÿåº”è¯¥æ˜¯ç¡®å®šæ€§çš„
                assert torch.allclose(q_value_train1, q_value_train2, atol=1e-6)
    ]]></file>
  <file path="tests/unit/test_containerized_deployment.py"><![CDATA[
    #!/usr/bin/env python3
    """
    å®¹å™¨åŒ–éƒ¨ç½²æµ‹è¯•ç”¨ä¾‹
    
    æµ‹è¯•Dockerå®¹å™¨æž„å»ºå’Œè¿è¡Œã€Kuberneteséƒ¨ç½²å’ŒæœåŠ¡å‘çŽ°ã€CI/CDæµæ°´çº¿å’Œè‡ªåŠ¨åŒ–éƒ¨ç½²
    éœ€æ±‚: 8.1, 8.4
    """
    
    import pytest
    import subprocess
    import tempfile
    import yaml
    import json
    import os
    from pathlib import Path
    from unittest.mock import Mock, patch, MagicMock
    from typing import Dict, List, Any
    
    from src.rl_trading_system.deployment.containerized_deployment import (
        DockerManager,
        KubernetesManager,
        CICDPipeline,
        HealthChecker,
        ServiceDiscovery,
        ContainerConfig,
        KubernetesConfig,
        DeploymentStatus,
        ServiceStatus
    )
    
    
    class TestDockerManager:
        """Dockerç®¡ç†å™¨æµ‹è¯•"""
        
        @pytest.fixture
        def container_config(self):
            """åˆ›å»ºå®¹å™¨é…ç½®"""
            return ContainerConfig(
                image_name="rl-trading-system",
                image_tag="v1.0.0",
                dockerfile_path="./Dockerfile",
                build_context=".",
                environment_vars={
                    "PYTHONPATH": "/app/src",
                    "LOG_LEVEL": "INFO"
                },
                ports={"8000": "8000", "8888": "8888"},
                volumes={
                    "./data": "/app/data",
                    "./logs": "/app/logs"
                },
                health_check_cmd="curl -f http://localhost:8000/health || exit 1",
                health_check_interval=30,
                health_check_timeout=30,
                health_check_retries=3
            )
        
        @pytest.fixture
        def docker_manager(self, container_config):
            """åˆ›å»ºDockerç®¡ç†å™¨"""
            return DockerManager(container_config)
        
        def test_docker_manager_initialization(self, docker_manager, container_config):
            """æµ‹è¯•Dockerç®¡ç†å™¨åˆå§‹åŒ–"""
            assert docker_manager.config == container_config
            assert docker_manager.client is not None
            assert docker_manager.image_name == "rl-trading-system:v1.0.0"
        
        @patch('docker.from_env')
        def test_build_image_success(self, mock_docker, docker_manager):
            """æµ‹è¯•æˆåŠŸæž„å»ºDockeré•œåƒ"""
            # æ¨¡æ‹ŸDockerå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_docker.return_value = mock_client
            
            # æ¨¡æ‹Ÿæž„å»ºæˆåŠŸ
            mock_image = Mock()
            mock_image.id = "sha256:abc123"
            mock_client.images.build.return_value = (mock_image, [])
            
            # æ‰§è¡Œæž„å»º
            result = docker_manager.build_image()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_client.images.build.assert_called_once_with(
                path=".",
                dockerfile="./Dockerfile",
                tag="rl-trading-system:v1.0.0",
                rm=True,
                forcerm=True
            )
        
        @patch('docker.from_env')
        def test_build_image_failure(self, mock_docker, docker_manager):
            """æµ‹è¯•Dockeré•œåƒæž„å»ºå¤±è´¥"""
            # æ¨¡æ‹ŸDockerå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_docker.return_value = mock_client
            
            # æ¨¡æ‹Ÿæž„å»ºå¤±è´¥
            mock_client.images.build.side_effect = Exception("Build failed")
            
            # æ‰§è¡Œæž„å»º
            result = docker_manager.build_image()
            
            # éªŒè¯ç»“æžœ
            assert result is False
        
        @patch('docker.from_env')
        def test_run_container_success(self, mock_docker, docker_manager):
            """æµ‹è¯•æˆåŠŸè¿è¡Œå®¹å™¨"""
            # æ¨¡æ‹ŸDockerå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_docker.return_value = mock_client
            
            # æ¨¡æ‹Ÿå®¹å™¨è¿è¡Œ
            mock_container = Mock()
            mock_container.id = "container123"
            mock_container.status = "running"
            mock_client.containers.run.return_value = mock_container
            
            # æ‰§è¡Œè¿è¡Œ
            container_id = docker_manager.run_container("test-container")
            
            # éªŒè¯ç»“æžœ
            assert container_id == "container123"
            mock_client.containers.run.assert_called_once_with(
                image="rl-trading-system:v1.0.0",
                name="test-container",
                ports={"8000": "8000", "8888": "8888"},
                volumes={
                    "./data": {"bind": "/app/data", "mode": "rw"},
                    "./logs": {"bind": "/app/logs", "mode": "rw"}
                },
                environment={
                    "PYTHONPATH": "/app/src",
                    "LOG_LEVEL": "INFO"
                },
                detach=True,
                restart_policy={"Name": "unless-stopped"}
            )
        
        @patch('docker.from_env')
        def test_stop_container(self, mock_docker, docker_manager):
            """æµ‹è¯•åœæ­¢å®¹å™¨"""
            # æ¨¡æ‹ŸDockerå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_docker.return_value = mock_client
            
            # æ¨¡æ‹Ÿå®¹å™¨
            mock_container = Mock()
            mock_client.containers.get.return_value = mock_container
            
            # æ‰§è¡Œåœæ­¢
            result = docker_manager.stop_container("container123")
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_container.stop.assert_called_once()
            mock_container.remove.assert_called_once()
        
        @patch('docker.from_env')
        def test_get_container_status(self, mock_docker, docker_manager):
            """æµ‹è¯•èŽ·å–å®¹å™¨çŠ¶æ€"""
            # æ¨¡æ‹ŸDockerå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_docker.return_value = mock_client
            
            # æ¨¡æ‹Ÿå®¹å™¨çŠ¶æ€
            mock_container = Mock()
            mock_container.status = "running"
            mock_container.attrs = {
                "State": {
                    "Health": {"Status": "healthy"}
                }
            }
            mock_client.containers.get.return_value = mock_container
            
            # æ‰§è¡ŒèŽ·å–çŠ¶æ€
            status = docker_manager.get_container_status("container123")
            
            # éªŒè¯ç»“æžœ
            assert status["status"] == "running"
            assert status["health"] == "healthy"
        
        def test_generate_dockerfile(self, docker_manager):
            """æµ‹è¯•ç”ŸæˆDockerfile"""
            dockerfile_content = docker_manager.generate_dockerfile()
            
            # éªŒè¯Dockerfileå†…å®¹
            assert "FROM python:3.9-slim" in dockerfile_content
            assert "WORKDIR /app" in dockerfile_content
            assert "COPY requirements.txt ." in dockerfile_content
            assert "RUN pip install --no-cache-dir -r requirements.txt" in dockerfile_content
            assert "EXPOSE 8000 8888" in dockerfile_content
            assert "HEALTHCHECK" in dockerfile_content
        
        def test_generate_docker_compose(self, docker_manager):
            """æµ‹è¯•ç”Ÿæˆdocker-compose.yml"""
            compose_content = docker_manager.generate_docker_compose()
            
            # è§£æžYAMLå†…å®¹
            compose_data = yaml.safe_load(compose_content)
            
            # éªŒè¯composeæ–‡ä»¶ç»“æž„
            assert "version" in compose_data
            assert "services" in compose_data
            assert "rl-trading-system" in compose_data["services"]
            
            service = compose_data["services"]["rl-trading-system"]
            assert "ports" in service
            assert "volumes" in service
            assert "environment" in service
    
    
    class TestKubernetesManager:
        """Kubernetesç®¡ç†å™¨æµ‹è¯•"""
        
        @pytest.fixture
        def k8s_config(self):
            """åˆ›å»ºKubernetesé…ç½®"""
            return KubernetesConfig(
                namespace="rl-trading",
                deployment_name="rl-trading-system",
                service_name="rl-trading-service",
                image_name="rl-trading-system:v1.0.0",
                replicas=3,
                cpu_request="500m",
                cpu_limit="2000m",
                memory_request="1Gi",
                memory_limit="4Gi",
                ports=[8000, 8888],
                environment_vars={
                    "PYTHONPATH": "/app/src",
                    "LOG_LEVEL": "INFO"
                },
                config_maps=["trading-config"],
                secrets=["trading-secrets"],
                health_check_path="/health",
                readiness_check_path="/ready"
            )
        
        @pytest.fixture
        def k8s_manager(self, k8s_config):
            """åˆ›å»ºKubernetesç®¡ç†å™¨"""
            return KubernetesManager(k8s_config)
        
        def test_k8s_manager_initialization(self, k8s_manager, k8s_config):
            """æµ‹è¯•Kubernetesç®¡ç†å™¨åˆå§‹åŒ–"""
            assert k8s_manager.config == k8s_config
            assert k8s_manager.namespace == "rl-trading"
        
        @patch('kubernetes.config.load_incluster_config')
        @patch('kubernetes.client.AppsV1Api')
        def test_create_deployment_success(self, mock_apps_api, mock_load_config, k8s_manager):
            """æµ‹è¯•æˆåŠŸåˆ›å»ºKuberneteséƒ¨ç½²"""
            # æ¨¡æ‹ŸKubernetes API
            mock_api = Mock()
            mock_apps_api.return_value = mock_api
            
            # æ¨¡æ‹Ÿéƒ¨ç½²åˆ›å»ºæˆåŠŸ
            mock_deployment = Mock()
            mock_deployment.metadata.name = "rl-trading-system"
            mock_api.create_namespaced_deployment.return_value = mock_deployment
            
            # æ‰§è¡Œåˆ›å»ºéƒ¨ç½²
            result = k8s_manager.create_deployment()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_api.create_namespaced_deployment.assert_called_once()
        
        @patch('kubernetes.config.load_incluster_config')
        @patch('kubernetes.client.CoreV1Api')
        def test_create_service_success(self, mock_core_api, mock_load_config, k8s_manager):
            """æµ‹è¯•æˆåŠŸåˆ›å»ºKubernetesæœåŠ¡"""
            # æ¨¡æ‹ŸKubernetes API
            mock_api = Mock()
            mock_core_api.return_value = mock_api
            
            # æ¨¡æ‹ŸæœåŠ¡åˆ›å»ºæˆåŠŸ
            mock_service = Mock()
            mock_service.metadata.name = "rl-trading-service"
            mock_api.create_namespaced_service.return_value = mock_service
            
            # æ‰§è¡Œåˆ›å»ºæœåŠ¡
            result = k8s_manager.create_service()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_api.create_namespaced_service.assert_called_once()
        
        @patch('kubernetes.config.load_incluster_config')
        @patch('kubernetes.client.AppsV1Api')
        def test_update_deployment(self, mock_apps_api, mock_load_config, k8s_manager):
            """æµ‹è¯•æ›´æ–°Kuberneteséƒ¨ç½²"""
            # æ¨¡æ‹ŸKubernetes API
            mock_api = Mock()
            mock_apps_api.return_value = mock_api
            
            # æ¨¡æ‹Ÿéƒ¨ç½²æ›´æ–°æˆåŠŸ
            mock_deployment = Mock()
            mock_api.patch_namespaced_deployment.return_value = mock_deployment
            
            # æ‰§è¡Œæ›´æ–°éƒ¨ç½²
            result = k8s_manager.update_deployment("rl-trading-system:v1.1.0")
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_api.patch_namespaced_deployment.assert_called_once()
        
        @patch('kubernetes.config.load_incluster_config')
        @patch('kubernetes.client.AppsV1Api')
        def test_scale_deployment(self, mock_apps_api, mock_load_config, k8s_manager):
            """æµ‹è¯•æ‰©ç¼©å®¹éƒ¨ç½²"""
            # æ¨¡æ‹ŸKubernetes API
            mock_api = Mock()
            mock_apps_api.return_value = mock_api
            
            # æ¨¡æ‹Ÿæ‰©ç¼©å®¹æˆåŠŸ
            mock_deployment = Mock()
            mock_api.patch_namespaced_deployment_scale.return_value = mock_deployment
            
            # æ‰§è¡Œæ‰©ç¼©å®¹
            result = k8s_manager.scale_deployment(5)
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_api.patch_namespaced_deployment_scale.assert_called_once()
        
        @patch('kubernetes.config.load_incluster_config')
        @patch('kubernetes.client.AppsV1Api')
        def test_get_deployment_status(self, mock_apps_api, mock_load_config, k8s_manager):
            """æµ‹è¯•èŽ·å–éƒ¨ç½²çŠ¶æ€"""
            # æ¨¡æ‹ŸKubernetes API
            mock_api = Mock()
            mock_apps_api.return_value = mock_api
            
            # æ¨¡æ‹Ÿéƒ¨ç½²çŠ¶æ€
            mock_deployment = Mock()
            mock_deployment.status.replicas = 3
            mock_deployment.status.ready_replicas = 3
            mock_deployment.status.available_replicas = 3
            mock_api.read_namespaced_deployment.return_value = mock_deployment
            
            # æ‰§è¡ŒèŽ·å–çŠ¶æ€
            status = k8s_manager.get_deployment_status()
            
            # éªŒè¯ç»“æžœ
            assert status["replicas"] == 3
            assert status["ready_replicas"] == 3
            assert status["available_replicas"] == 3
            assert status["is_ready"] is True
        
        def test_generate_deployment_yaml(self, k8s_manager):
            """æµ‹è¯•ç”Ÿæˆéƒ¨ç½²YAML"""
            deployment_yaml = k8s_manager.generate_deployment_yaml()
            
            # è§£æžYAMLå†…å®¹
            deployment_data = yaml.safe_load(deployment_yaml)
            
            # éªŒè¯éƒ¨ç½²é…ç½®
            assert deployment_data["kind"] == "Deployment"
            assert deployment_data["metadata"]["name"] == "rl-trading-system"
            assert deployment_data["spec"]["replicas"] == 3
            
            container = deployment_data["spec"]["template"]["spec"]["containers"][0]
            assert container["image"] == "rl-trading-system:v1.0.0"
            assert container["resources"]["requests"]["cpu"] == "500m"
            assert container["resources"]["limits"]["memory"] == "4Gi"
        
        def test_generate_service_yaml(self, k8s_manager):
            """æµ‹è¯•ç”ŸæˆæœåŠ¡YAML"""
            service_yaml = k8s_manager.generate_service_yaml()
            
            # è§£æžYAMLå†…å®¹
            service_data = yaml.safe_load(service_yaml)
            
            # éªŒè¯æœåŠ¡é…ç½®
            assert service_data["kind"] == "Service"
            assert service_data["metadata"]["name"] == "rl-trading-service"
            assert len(service_data["spec"]["ports"]) == 2
            assert service_data["spec"]["selector"]["app"] == "rl-trading-system"
    
    
    class TestCICDPipeline:
        """CI/CDæµæ°´çº¿æµ‹è¯•"""
        
        @pytest.fixture
        def pipeline_config(self):
            """åˆ›å»ºæµæ°´çº¿é…ç½®"""
            return {
                "repository": "https://github.com/rl-trading/rl-trading-system.git",
                "branch": "main",
                "build_stages": ["test", "build", "deploy"],
                "test_commands": ["pytest tests/", "flake8 src/", "mypy src/"],
                "build_commands": ["docker build -t rl-trading-system:latest ."],
                "deploy_commands": ["kubectl apply -f k8s/"],
                "notifications": {
                    "slack_webhook": "https://hooks.slack.com/services/xxx",
                    "email": "team@rltrading.com"
                }
            }
        
        @pytest.fixture
        def cicd_pipeline(self, pipeline_config):
            """åˆ›å»ºCI/CDæµæ°´çº¿"""
            return CICDPipeline(pipeline_config)
        
        def test_pipeline_initialization(self, cicd_pipeline, pipeline_config):
            """æµ‹è¯•æµæ°´çº¿åˆå§‹åŒ–"""
            assert cicd_pipeline.config == pipeline_config
            assert cicd_pipeline.repository == "https://github.com/rl-trading/rl-trading-system.git"
            assert cicd_pipeline.branch == "main"
        
        @patch('subprocess.run')
        def test_run_tests_success(self, mock_subprocess, cicd_pipeline):
            """æµ‹è¯•æˆåŠŸè¿è¡Œæµ‹è¯•"""
            # æ¨¡æ‹Ÿæµ‹è¯•æˆåŠŸ
            mock_result = Mock()
            mock_result.returncode = 0
            mock_result.stdout = "All tests passed"
            mock_subprocess.return_value = mock_result
            
            # æ‰§è¡Œæµ‹è¯•
            result = cicd_pipeline.run_tests()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            assert mock_subprocess.call_count == 3  # ä¸‰ä¸ªæµ‹è¯•å‘½ä»¤
        
        @patch('subprocess.run')
        def test_run_tests_failure(self, mock_subprocess, cicd_pipeline):
            """æµ‹è¯•æµ‹è¯•å¤±è´¥"""
            # æ¨¡æ‹Ÿæµ‹è¯•å¤±è´¥
            mock_result = Mock()
            mock_result.returncode = 1
            mock_result.stdout = "Test failed"
            mock_subprocess.return_value = mock_result
            
            # æ‰§è¡Œæµ‹è¯•
            result = cicd_pipeline.run_tests()
            
            # éªŒè¯ç»“æžœ
            assert result is False
        
        @patch('subprocess.run')
        def test_build_image_success(self, mock_subprocess, cicd_pipeline):
            """æµ‹è¯•æˆåŠŸæž„å»ºé•œåƒ"""
            # æ¨¡æ‹Ÿæž„å»ºæˆåŠŸ
            mock_result = Mock()
            mock_result.returncode = 0
            mock_result.stdout = "Successfully built"
            mock_subprocess.return_value = mock_result
            
            # æ‰§è¡Œæž„å»º
            result = cicd_pipeline.build_image()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_subprocess.assert_called_with(
                ["docker", "build", "-t", "rl-trading-system:latest", "."],
                capture_output=True,
                text=True,
                timeout=1800
            )
        
        @patch('subprocess.run')
        def test_deploy_to_kubernetes(self, mock_subprocess, cicd_pipeline):
            """æµ‹è¯•éƒ¨ç½²åˆ°Kubernetes"""
            # æ¨¡æ‹Ÿéƒ¨ç½²æˆåŠŸ
            mock_result = Mock()
            mock_result.returncode = 0
            mock_result.stdout = "deployment.apps/rl-trading-system created"
            mock_subprocess.return_value = mock_result
            
            # æ‰§è¡Œéƒ¨ç½²
            result = cicd_pipeline.deploy_to_kubernetes()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_subprocess.assert_called_with(
                ["kubectl", "apply", "-f", "k8s/"],
                capture_output=True,
                text=True,
                timeout=600
            )
        
        def test_generate_github_actions_workflow(self, cicd_pipeline):
            """æµ‹è¯•ç”ŸæˆGitHub Actionså·¥ä½œæµ"""
            workflow_yaml = cicd_pipeline.generate_github_actions_workflow()
            
            # è§£æžYAMLå†…å®¹
            workflow_data = yaml.safe_load(workflow_yaml)
            
            # éªŒè¯å·¥ä½œæµé…ç½®
            assert workflow_data["name"] == "CI/CD Pipeline"
            assert "push" in workflow_data["on"]
            assert "pull_request" in workflow_data["on"]
            
            jobs = workflow_data["jobs"]
            assert "test" in jobs
            assert "build" in jobs
            assert "deploy" in jobs
        
        def test_generate_jenkins_pipeline(self, cicd_pipeline):
            """æµ‹è¯•ç”ŸæˆJenkinsæµæ°´çº¿"""
            pipeline_script = cicd_pipeline.generate_jenkins_pipeline()
            
            # éªŒè¯æµæ°´çº¿è„šæœ¬
            assert "pipeline {" in pipeline_script
            assert "agent any" in pipeline_script
            assert "stage('Test')" in pipeline_script
            assert "stage('Build')" in pipeline_script
            assert "stage('Deploy')" in pipeline_script
    
    
    class TestHealthChecker:
        """å¥åº·æ£€æŸ¥å™¨æµ‹è¯•"""
        
        @pytest.fixture
        def health_checker(self):
            """åˆ›å»ºå¥åº·æ£€æŸ¥å™¨"""
            return HealthChecker(
                endpoints=[
                    "http://localhost:8000/health",
                    "http://localhost:8888/health"
                ],
                timeout=30,
                retry_count=3,
                retry_interval=5
            )
        
        @patch('requests.get')
        def test_check_endpoint_healthy(self, mock_get, health_checker):
            """æµ‹è¯•ç«¯ç‚¹å¥åº·æ£€æŸ¥æˆåŠŸ"""
            # æ¨¡æ‹Ÿå¥åº·å“åº”
            mock_response = Mock()
            mock_response.status_code = 200
            mock_response.json.return_value = {"status": "healthy"}
            mock_get.return_value = mock_response
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            result = health_checker.check_endpoint("http://localhost:8000/health")
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_get.assert_called_once_with(
                "http://localhost:8000/health",
                timeout=30
            )
        
        @patch('requests.get')
        def test_check_endpoint_unhealthy(self, mock_get, health_checker):
            """æµ‹è¯•ç«¯ç‚¹å¥åº·æ£€æŸ¥å¤±è´¥"""
            # æ¨¡æ‹Ÿä¸å¥åº·å“åº”
            mock_response = Mock()
            mock_response.status_code = 500
            mock_get.return_value = mock_response
            
            # æ‰§è¡Œå¥åº·æ£€æŸ¥
            result = health_checker.check_endpoint("http://localhost:8000/health")
            
            # éªŒè¯ç»“æžœ
            assert result is False
        
        @patch('requests.get')
        def test_check_all_endpoints(self, mock_get, health_checker):
            """æµ‹è¯•æ£€æŸ¥æ‰€æœ‰ç«¯ç‚¹"""
            # æ¨¡æ‹Ÿæ··åˆå“åº”
            responses = [
                Mock(status_code=200, json=lambda: {"status": "healthy"}),
                Mock(status_code=500)
            ]
            mock_get.side_effect = responses
            
            # æ‰§è¡Œæ£€æŸ¥
            results = health_checker.check_all_endpoints()
            
            # éªŒè¯ç»“æžœ
            assert len(results) == 2
            assert results["http://localhost:8000/health"] is True
            assert results["http://localhost:8888/health"] is False
        
        def test_wait_for_healthy_success(self, health_checker):
            """æµ‹è¯•ç­‰å¾…å¥åº·çŠ¶æ€æˆåŠŸ"""
            with patch.object(health_checker, 'check_endpoint', return_value=True):
                result = health_checker.wait_for_healthy("http://localhost:8000/health", max_wait=10)
                assert result is True
        
        def test_wait_for_healthy_timeout(self, health_checker):
            """æµ‹è¯•ç­‰å¾…å¥åº·çŠ¶æ€è¶…æ—¶"""
            with patch.object(health_checker, 'check_endpoint', return_value=False):
                result = health_checker.wait_for_healthy("http://localhost:8000/health", max_wait=1)
                assert result is False
    
    
    class TestServiceDiscovery:
        """æœåŠ¡å‘çŽ°æµ‹è¯•"""
        
        @pytest.fixture
        def service_discovery(self):
            """åˆ›å»ºæœåŠ¡å‘çŽ°"""
            return ServiceDiscovery(
                consul_host="localhost",
                consul_port=8500,
                service_name="rl-trading-system",
                service_port=8000,
                health_check_url="http://localhost:8000/health"
            )
        
        @patch('consul.Consul')
        def test_register_service_success(self, mock_consul, service_discovery):
            """æµ‹è¯•æˆåŠŸæ³¨å†ŒæœåŠ¡"""
            # æ¨¡æ‹ŸConsulå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_consul.return_value = mock_client
            
            # æ‰§è¡ŒæœåŠ¡æ³¨å†Œ
            result = service_discovery.register_service()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_client.agent.service.register.assert_called_once()
        
        @patch('consul.Consul')
        def test_deregister_service(self, mock_consul, service_discovery):
            """æµ‹è¯•æ³¨é”€æœåŠ¡"""
            # æ¨¡æ‹ŸConsulå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_consul.return_value = mock_client
            
            # æ‰§è¡ŒæœåŠ¡æ³¨é”€
            result = service_discovery.deregister_service()
            
            # éªŒè¯ç»“æžœ
            assert result is True
            mock_client.agent.service.deregister.assert_called_once()
        
        @patch('consul.Consul')
        def test_discover_services(self, mock_consul, service_discovery):
            """æµ‹è¯•æœåŠ¡å‘çŽ°"""
            # æ¨¡æ‹ŸConsulå®¢æˆ·ç«¯
            mock_client = Mock()
            mock_consul.return_value = mock_client
            
            # æ¨¡æ‹ŸæœåŠ¡åˆ—è¡¨
            mock_services = {
                "rl-trading-system-1": {
                    "Service": "rl-trading-system",
                    "Address": "192.168.1.10",
                    "Port": 8000
                },
                "rl-trading-system-2": {
                    "Service": "rl-trading-system",
                    "Address": "192.168.1.11",
                    "Port": 8000
                }
            }
            mock_client.health.service.return_value = (None, mock_services.values())
            
            # æ‰§è¡ŒæœåŠ¡å‘çŽ°
            services = service_discovery.discover_services("rl-trading-system")
            
            # éªŒè¯ç»“æžœ
            assert len(services) == 2
            assert services[0]["address"] == "192.168.1.10"
            assert services[1]["address"] == "192.168.1.11"
    
    
    class TestContainerizedDeploymentIntegration:
        """å®¹å™¨åŒ–éƒ¨ç½²é›†æˆæµ‹è¯•"""
        
        @pytest.fixture
        def temp_project_dir(self):
            """åˆ›å»ºä¸´æ—¶é¡¹ç›®ç›®å½•"""
            with tempfile.TemporaryDirectory() as temp_dir:
                project_dir = Path(temp_dir)
                
                # åˆ›å»ºåŸºæœ¬é¡¹ç›®ç»“æž„
                (project_dir / "src").mkdir()
                (project_dir / "tests").mkdir()
                (project_dir / "k8s").mkdir()
                
                # åˆ›å»ºåŸºæœ¬æ–‡ä»¶
                (project_dir / "requirements.txt").write_text("torch>=1.12.0\nnumpy>=1.21.0")
                (project_dir / "setup.py").write_text("from setuptools import setup\nsetup(name='test')")
                
                yield project_dir
        
        def test_full_docker_deployment_flow(self, temp_project_dir):
            """æµ‹è¯•å®Œæ•´çš„Dockeréƒ¨ç½²æµç¨‹"""
            # åˆ›å»ºå®¹å™¨é…ç½®
            config = ContainerConfig(
                image_name="test-app",
                image_tag="v1.0.0",
                dockerfile_path=str(temp_project_dir / "Dockerfile"),
                build_context=str(temp_project_dir)
            )
            
            # åˆ›å»ºDockerç®¡ç†å™¨
            docker_manager = DockerManager(config)
            
            # ç”ŸæˆDockerfile
            dockerfile_content = docker_manager.generate_dockerfile()
            (temp_project_dir / "Dockerfile").write_text(dockerfile_content)
            
            # éªŒè¯Dockerfileå­˜åœ¨
            assert (temp_project_dir / "Dockerfile").exists()
            
            # ç”Ÿæˆdocker-compose.yml
            compose_content = docker_manager.generate_docker_compose()
            (temp_project_dir / "docker-compose.yml").write_text(compose_content)
            
            # éªŒè¯composeæ–‡ä»¶å­˜åœ¨
            assert (temp_project_dir / "docker-compose.yml").exists()
        
        def test_full_kubernetes_deployment_flow(self, temp_project_dir):
            """æµ‹è¯•å®Œæ•´çš„Kuberneteséƒ¨ç½²æµç¨‹"""
            # åˆ›å»ºKubernetesé…ç½®
            config = KubernetesConfig(
                namespace="test-namespace",
                deployment_name="test-app",
                service_name="test-service",
                image_name="test-app:v1.0.0"
            )
            
            # åˆ›å»ºKubernetesç®¡ç†å™¨
            k8s_manager = KubernetesManager(config)
            
            # ç”Ÿæˆéƒ¨ç½²YAML
            deployment_yaml = k8s_manager.generate_deployment_yaml()
            (temp_project_dir / "k8s" / "deployment.yaml").write_text(deployment_yaml)
            
            # ç”ŸæˆæœåŠ¡YAML
            service_yaml = k8s_manager.generate_service_yaml()
            (temp_project_dir / "k8s" / "service.yaml").write_text(service_yaml)
            
            # éªŒè¯YAMLæ–‡ä»¶å­˜åœ¨
            assert (temp_project_dir / "k8s" / "deployment.yaml").exists()
            assert (temp_project_dir / "k8s" / "service.yaml").exists()
            
            # éªŒè¯YAMLå†…å®¹
            deployment_data = yaml.safe_load((temp_project_dir / "k8s" / "deployment.yaml").read_text())
            assert deployment_data["kind"] == "Deployment"
            assert deployment_data["metadata"]["name"] == "test-app"
        
        def test_cicd_pipeline_integration(self, temp_project_dir):
            """æµ‹è¯•CI/CDæµæ°´çº¿é›†æˆ"""
            # åˆ›å»ºæµæ°´çº¿é…ç½®
            config = {
                "repository": "https://github.com/test/test-app.git",
                "branch": "main",
                "build_stages": ["test", "build", "deploy"]
            }
            
            # åˆ›å»ºCI/CDæµæ°´çº¿
            pipeline = CICDPipeline(config)
            
            # ç”ŸæˆGitHub Actionså·¥ä½œæµ
            workflow_yaml = pipeline.generate_github_actions_workflow()
            workflow_dir = temp_project_dir / ".github" / "workflows"
            workflow_dir.mkdir(parents=True)
            (workflow_dir / "ci-cd.yml").write_text(workflow_yaml)
            
            # ç”ŸæˆJenkinsæµæ°´çº¿
            jenkins_script = pipeline.generate_jenkins_pipeline()
            (temp_project_dir / "Jenkinsfile").write_text(jenkins_script)
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            assert (workflow_dir / "ci-cd.yml").exists()
            assert (temp_project_dir / "Jenkinsfile").exists()
            
            # éªŒè¯å·¥ä½œæµå†…å®¹
            workflow_data = yaml.safe_load((workflow_dir / "ci-cd.yml").read_text())
            assert workflow_data["name"] == "CI/CD Pipeline"
            assert "jobs" in workflow_data
    ]]></file>
  <file path="tests/unit/test_config_manager.py"><![CDATA[
    """
    é…ç½®ç®¡ç†å™¨æµ‹è¯•ç”¨ä¾‹
    
    æµ‹è¯•YAMLé…ç½®æ–‡ä»¶åŠ è½½å’ŒéªŒè¯ã€çŽ¯å¢ƒå˜é‡è¦†ç›–æœºåˆ¶ã€é…ç½®å‚æ•°ç±»åž‹æ£€æŸ¥å’Œé»˜è®¤å€¼
    éœ€æ±‚: 10.1
    """
    
    import os
    import tempfile
    import pytest
    from pathlib import Path
    from typing import Dict, Any
    from unittest.mock import patch, mock_open
    
    import yaml
    
    from src.rl_trading_system.config.config_manager import (
        ConfigManager,
        ConfigValidationError,
        ConfigLoadError
    )
    
    
    # å…¨å±€fixtures
    @pytest.fixture
    def sample_config_dict() -> Dict[str, Any]:
        """ç¤ºä¾‹é…ç½®å­—å…¸"""
        return {
            'model': {
                'transformer': {
                    'd_model': 256,
                    'n_heads': 8,
                    'n_layers': 6,
                    'dropout': 0.1,
                    'max_seq_len': 252
                },
                'sac': {
                    'lr_actor': 3e-4,
                    'lr_critic': 3e-4,
                    'gamma': 0.99,
                    'buffer_size': 1000000
                }
            },
            'trading': {
                'environment': {
                    'initial_cash': 1000000.0,
                    'commission_rate': 0.001,
                    'lookback_window': 60
                }
            }
        }
    
    @pytest.fixture
    def sample_yaml_content(sample_config_dict) -> str:
        """ç¤ºä¾‹YAMLé…ç½®å†…å®¹"""
        return yaml.dump(sample_config_dict, default_flow_style=False)
    
    @pytest.fixture
    def temp_config_file(sample_yaml_content) -> Path:
        """åˆ›å»ºä¸´æ—¶é…ç½®æ–‡ä»¶"""
        with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
            f.write(sample_yaml_content)
            temp_path = Path(f.name)
        
        yield temp_path
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_path.exists():
            temp_path.unlink()
    
    @pytest.fixture
    def config_manager() -> ConfigManager:
        """é…ç½®ç®¡ç†å™¨å®žä¾‹"""
        return ConfigManager()
    
    
    class TestConfigManager:
        """é…ç½®ç®¡ç†å™¨æµ‹è¯•ç±»"""
        pass
    
    
    class TestConfigLoading:
        """é…ç½®åŠ è½½æµ‹è¯•"""
        
        def test_load_yaml_file_success(self, config_manager, temp_config_file, sample_config_dict):
            """æµ‹è¯•æˆåŠŸåŠ è½½YAMLæ–‡ä»¶"""
            config = config_manager.load_config(temp_config_file)
            
            assert config == sample_config_dict
            assert config['model']['transformer']['d_model'] == 256
            assert config['trading']['environment']['initial_cash'] == 1000000.0
        
        def test_load_nonexistent_file(self, config_manager):
            """æµ‹è¯•åŠ è½½ä¸å­˜åœ¨çš„æ–‡ä»¶"""
            with pytest.raises(ConfigLoadError, match="é…ç½®æ–‡ä»¶ä¸å­˜åœ¨"):
                config_manager.load_config("nonexistent.yaml")
        
        def test_load_invalid_yaml(self, config_manager):
            """æµ‹è¯•åŠ è½½æ— æ•ˆçš„YAMLæ–‡ä»¶"""
            invalid_yaml = "invalid: yaml: content: ["
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                f.write(invalid_yaml)
                temp_path = Path(f.name)
            
            try:
                with pytest.raises(ConfigLoadError, match="YAMLè§£æžé”™è¯¯"):
                    config_manager.load_config(temp_path)
            finally:
                temp_path.unlink()
        
        def test_load_empty_file(self, config_manager):
            """æµ‹è¯•åŠ è½½ç©ºæ–‡ä»¶"""
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                f.write("")
                temp_path = Path(f.name)
            
            try:
                config = config_manager.load_config(temp_path)
                assert config == {}
            finally:
                temp_path.unlink()
        
        def test_load_multiple_configs(self, config_manager, sample_config_dict):
            """æµ‹è¯•åŠ è½½å¤šä¸ªé…ç½®æ–‡ä»¶å¹¶åˆå¹¶"""
            config1 = {'model': {'transformer': {'d_model': 256}}}
            config2 = {'trading': {'environment': {'initial_cash': 1000000.0}}}
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f1:
                yaml.dump(config1, f1)
                temp_path1 = Path(f1.name)
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f2:
                yaml.dump(config2, f2)
                temp_path2 = Path(f2.name)
            
            try:
                merged_config = config_manager.load_configs([temp_path1, temp_path2])
                
                assert 'model' in merged_config
                assert 'trading' in merged_config
                assert merged_config['model']['transformer']['d_model'] == 256
                assert merged_config['trading']['environment']['initial_cash'] == 1000000.0
            finally:
                temp_path1.unlink()
                temp_path2.unlink()
    
    
    class TestEnvironmentVariableOverride:
        """çŽ¯å¢ƒå˜é‡è¦†ç›–æµ‹è¯•"""
        
        def test_simple_env_override(self, config_manager, temp_config_file):
            """æµ‹è¯•ç®€å•çŽ¯å¢ƒå˜é‡è¦†ç›–"""
            with patch.dict(os.environ, {'MODEL_TRANSFORMER_D_MODEL': '512'}):
                config = config_manager.load_config(temp_config_file, enable_env_override=True)
                
                assert config['model']['transformer']['d_model'] == 512
        
        def test_nested_env_override(self, config_manager, temp_config_file):
            """æµ‹è¯•åµŒå¥—è·¯å¾„çŽ¯å¢ƒå˜é‡è¦†ç›–"""
            env_vars = {
                'TRADING_ENVIRONMENT_INITIAL_CASH': '2000000.0',
                'MODEL_SAC_LR_ACTOR': '1e-3'
            }
            
            with patch.dict(os.environ, env_vars):
                config = config_manager.load_config(temp_config_file, enable_env_override=True)
                
                assert config['trading']['environment']['initial_cash'] == 2000000.0
                assert config['model']['sac']['lr_actor'] == 1e-3
        
        def test_env_override_type_conversion(self, config_manager, temp_config_file):
            """æµ‹è¯•çŽ¯å¢ƒå˜é‡ç±»åž‹è½¬æ¢"""
            env_vars = {
                'MODEL_TRANSFORMER_N_HEADS': '16',  # int
                'MODEL_TRANSFORMER_DROPOUT': '0.2',  # float
                'TRADING_ENVIRONMENT_COMMISSION_RATE': '0.002'  # float
            }
            
            with patch.dict(os.environ, env_vars):
                config = config_manager.load_config(temp_config_file, enable_env_override=True)
                
                assert config['model']['transformer']['n_heads'] == 16
                assert isinstance(config['model']['transformer']['n_heads'], int)
                assert config['model']['transformer']['dropout'] == 0.2
                assert isinstance(config['model']['transformer']['dropout'], float)
                assert config['trading']['environment']['commission_rate'] == 0.002
        
        def test_env_override_boolean_values(self, config_manager):
            """æµ‹è¯•å¸ƒå°”å€¼çŽ¯å¢ƒå˜é‡è¦†ç›–"""
            config_dict = {'feature': {'enabled': False, 'debug': True}}
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(config_dict, f)
                temp_path = Path(f.name)
            
            env_vars = {
                'FEATURE_ENABLED': 'true',
                'FEATURE_DEBUG': 'false'
            }
            
            try:
                with patch.dict(os.environ, env_vars):
                    config = config_manager.load_config(temp_path, enable_env_override=True)
                    
                    assert config['feature']['enabled'] is True
                    assert config['feature']['debug'] is False
            finally:
                temp_path.unlink()
        
        def test_env_override_list_values(self, config_manager):
            """æµ‹è¯•åˆ—è¡¨å€¼çŽ¯å¢ƒå˜é‡è¦†ç›–"""
            config_dict = {'trading': {'stock_pool': ['000001.SZ', '000002.SZ']}}
            
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(config_dict, f)
                temp_path = Path(f.name)
            
            try:
                with patch.dict(os.environ, {'TRADING_STOCK_POOL': '000001.SZ,000002.SZ,000003.SZ'}):
                    config = config_manager.load_config(temp_path, enable_env_override=True)
                    
                    expected_pool = ['000001.SZ', '000002.SZ', '000003.SZ']
                    assert config['trading']['stock_pool'] == expected_pool
            finally:
                temp_path.unlink()
        
        def test_env_override_disabled(self, config_manager, temp_config_file):
            """æµ‹è¯•ç¦ç”¨çŽ¯å¢ƒå˜é‡è¦†ç›–"""
            with patch.dict(os.environ, {'MODEL_TRANSFORMER_D_MODEL': '512'}):
                config = config_manager.load_config(temp_config_file, enable_env_override=False)
                
                # åº”è¯¥ä¿æŒåŽŸå§‹å€¼
                assert config['model']['transformer']['d_model'] == 256
    
    
    class TestConfigValidation:
        """é…ç½®éªŒè¯æµ‹è¯•"""
        
        def test_validate_required_fields(self, config_manager):
            """æµ‹è¯•å¿…éœ€å­—æ®µéªŒè¯"""
            schema = {
                'model': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'transformer': {
                            'required': True,
                            'type': dict,
                            'schema': {
                                'd_model': {'required': True, 'type': int, 'min': 1}
                            }
                        }
                    }
                }
            }
            
            # ç¼ºå°‘å¿…éœ€å­—æ®µ
            invalid_config = {'model': {'transformer': {}}}
            
            with pytest.raises(ConfigValidationError, match="å¿…éœ€å­—æ®µç¼ºå¤±"):
                config_manager.validate_config(invalid_config, schema)
        
        def test_validate_type_checking(self, config_manager):
            """æµ‹è¯•ç±»åž‹æ£€æŸ¥"""
            schema = {
                'model': {
                    'type': dict,
                    'schema': {
                        'lr': {'type': float, 'min': 0.0, 'max': 1.0},
                        'epochs': {'type': int, 'min': 1},
                        'enabled': {'type': bool}
                    }
                }
            }
            
            # ç±»åž‹é”™è¯¯çš„é…ç½®
            invalid_configs = [
                {'model': {'lr': 'invalid'}},  # åº”è¯¥æ˜¯float
                {'model': {'epochs': 0.5}},    # åº”è¯¥æ˜¯int
                {'model': {'enabled': 'yes'}}  # åº”è¯¥æ˜¯bool
            ]
            
            for invalid_config in invalid_configs:
                with pytest.raises(ConfigValidationError, match="ç±»åž‹é”™è¯¯"):
                    config_manager.validate_config(invalid_config, schema)
        
        def test_validate_range_constraints(self, config_manager):
            """æµ‹è¯•èŒƒå›´çº¦æŸéªŒè¯"""
            schema = {
                'model': {
                    'type': dict,
                    'schema': {
                        'lr': {'type': float, 'min': 0.0, 'max': 1.0},
                        'batch_size': {'type': int, 'min': 1, 'max': 1024}
                    }
                }
            }
            
            # è¶…å‡ºèŒƒå›´çš„é…ç½®
            invalid_configs = [
                {'model': {'lr': -0.1}},      # å°äºŽæœ€å°å€¼
                {'model': {'lr': 1.5}},       # å¤§äºŽæœ€å¤§å€¼
                {'model': {'batch_size': 0}}, # å°äºŽæœ€å°å€¼
                {'model': {'batch_size': 2048}} # å¤§äºŽæœ€å¤§å€¼
            ]
            
            for invalid_config in invalid_configs:
                with pytest.raises(ConfigValidationError, match="å€¼è¶…å‡ºèŒƒå›´"):
                    config_manager.validate_config(invalid_config, schema)
        
        def test_validate_allowed_values(self, config_manager):
            """æµ‹è¯•å…è®¸å€¼éªŒè¯"""
            schema = {
                'data': {
                    'type': dict,
                    'schema': {
                        'provider': {'type': str, 'allowed': ['qlib', 'akshare']},
                        'freq': {'type': str, 'allowed': ['1d', '1h', '1min']}
                    }
                }
            }
            
            # ä¸å…è®¸çš„å€¼
            invalid_configs = [
                {'data': {'provider': 'yahoo'}},
                {'data': {'freq': '5min'}}
            ]
            
            for invalid_config in invalid_configs:
                with pytest.raises(ConfigValidationError, match="ä¸å…è®¸çš„å€¼"):
                    config_manager.validate_config(invalid_config, schema)
        
        def test_validate_custom_validator(self, config_manager):
            """æµ‹è¯•è‡ªå®šä¹‰éªŒè¯å™¨"""
            def validate_stock_code(field, value, error):
                """éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼"""
                if not isinstance(value, str) or len(value) != 9:
                    error(field, "è‚¡ç¥¨ä»£ç æ ¼å¼é”™è¯¯")
                if not (value.endswith('.SZ') or value.endswith('.SH')):
                    error(field, "è‚¡ç¥¨ä»£ç å¿…é¡»ä»¥.SZæˆ–.SHç»“å°¾")
            
            schema = {
                'trading': {
                    'type': dict,
                    'schema': {
                        'benchmark': {
                            'type': str,
                            'validator': validate_stock_code
                        }
                    }
                }
            }
            
            # æ— æ•ˆçš„è‚¡ç¥¨ä»£ç 
            invalid_configs = [
                {'trading': {'benchmark': '000300'}},      # é•¿åº¦ä¸å¤Ÿ
                {'trading': {'benchmark': '000300.XX'}},   # é”™è¯¯åŽç¼€
                {'trading': {'benchmark': 123}}            # é”™è¯¯ç±»åž‹
            ]
            
            for invalid_config in invalid_configs:
                with pytest.raises(ConfigValidationError):
                    config_manager.validate_config(invalid_config, schema)
    
    
    class TestDefaultValues:
        """é»˜è®¤å€¼æµ‹è¯•"""
        
        def test_apply_default_values(self, config_manager):
            """æµ‹è¯•åº”ç”¨é»˜è®¤å€¼"""
            schema = {
                'model': {
                    'type': dict,
                    'default': {},
                    'schema': {
                        'lr': {'type': float, 'default': 1e-3},
                        'batch_size': {'type': int, 'default': 32},
                        'enabled': {'type': bool, 'default': True}
                    }
                }
            }
            
            # ç©ºé…ç½®
            config = {}
            filled_config = config_manager.apply_defaults(config, schema)
            
            expected = {
                'model': {
                    'lr': 1e-3,
                    'batch_size': 32,
                    'enabled': True
                }
            }
            
            assert filled_config == expected
        
        def test_partial_default_application(self, config_manager):
            """æµ‹è¯•éƒ¨åˆ†é»˜è®¤å€¼åº”ç”¨"""
            schema = {
                'model': {
                    'type': dict,
                    'schema': {
                        'lr': {'type': float, 'default': 1e-3},
                        'batch_size': {'type': int, 'default': 32},
                        'epochs': {'type': int, 'default': 100}
                    }
                }
            }
            
            # éƒ¨åˆ†é…ç½®
            config = {'model': {'lr': 2e-3, 'epochs': 200}}
            filled_config = config_manager.apply_defaults(config, schema)
            
            expected = {
                'model': {
                    'lr': 2e-3,      # ä¿æŒç”¨æˆ·è®¾ç½®çš„å€¼
                    'batch_size': 32, # åº”ç”¨é»˜è®¤å€¼
                    'epochs': 200     # ä¿æŒç”¨æˆ·è®¾ç½®çš„å€¼
                }
            }
            
            assert filled_config == expected
        
        def test_nested_default_values(self, config_manager):
            """æµ‹è¯•åµŒå¥—é»˜è®¤å€¼"""
            schema = {
                'model': {
                    'type': dict,
                    'default': {},
                    'schema': {
                        'transformer': {
                            'type': dict,
                            'default': {},
                            'schema': {
                                'd_model': {'type': int, 'default': 256},
                                'n_heads': {'type': int, 'default': 8}
                            }
                        }
                    }
                }
            }
            
            config = {}
            filled_config = config_manager.apply_defaults(config, schema)
            
            expected = {
                'model': {
                    'transformer': {
                        'd_model': 256,
                        'n_heads': 8
                    }
                }
            }
            
            assert filled_config == expected
    
    
    class TestConfigManagerIntegration:
        """é…ç½®ç®¡ç†å™¨é›†æˆæµ‹è¯•"""
        
        def test_full_workflow(self, config_manager, sample_config_dict):
            """æµ‹è¯•å®Œæ•´å·¥ä½œæµç¨‹"""
            # åˆ›å»ºé…ç½®æ–‡ä»¶
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(sample_config_dict, f)
                temp_path = Path(f.name)
            
            # å®šä¹‰éªŒè¯æ¨¡å¼
            schema = {
                'model': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'transformer': {
                            'type': dict,
                            'schema': {
                                'd_model': {'type': int, 'min': 1, 'default': 256}
                            }
                        }
                    }
                }
            }
            
            try:
                # è®¾ç½®çŽ¯å¢ƒå˜é‡
                with patch.dict(os.environ, {'MODEL_TRANSFORMER_D_MODEL': '512'}):
                    # åŠ è½½ã€éªŒè¯å’Œåº”ç”¨é»˜è®¤å€¼
                    config = config_manager.load_and_validate_config(
                        temp_path, 
                        schema, 
                        enable_env_override=True
                    )
                    
                    # éªŒè¯ç»“æžœ
                    assert config['model']['transformer']['d_model'] == 512  # çŽ¯å¢ƒå˜é‡è¦†ç›–
                    assert 'sac' in config['model']  # åŽŸå§‹é…ç½®ä¿ç•™
                    
            finally:
                temp_path.unlink()
        
        def test_config_caching(self, config_manager, temp_config_file):
            """æµ‹è¯•é…ç½®ç¼“å­˜"""
            # é¦–æ¬¡åŠ è½½
            config1 = config_manager.load_config(temp_config_file, use_cache=True)
            
            # å†æ¬¡åŠ è½½ï¼ˆåº”è¯¥ä½¿ç”¨ç¼“å­˜ï¼Œä½†è¿”å›žæ·±æ‹·è´ä»¥ç¡®ä¿å®‰å…¨ï¼‰
            config2 = config_manager.load_config(temp_config_file, use_cache=True)
            
            assert config1 is not config2  # åº”è¯¥æ˜¯ä¸åŒå¯¹è±¡ï¼ˆæ·±æ‹·è´ï¼‰
            assert config1 == config2      # ä½†å†…å®¹ç›¸åŒ
            
            # ç¦ç”¨ç¼“å­˜
            config3 = config_manager.load_config(temp_config_file, use_cache=False)
            
            assert config1 is not config3  # åº”è¯¥æ˜¯ä¸åŒå¯¹è±¡
            assert config1 == config3      # ä½†å†…å®¹ç›¸åŒ
        
        def test_config_reload(self, config_manager, sample_config_dict):
            """æµ‹è¯•é…ç½®é‡æ–°åŠ è½½"""
            with tempfile.NamedTemporaryFile(mode='w', suffix='.yaml', delete=False) as f:
                yaml.dump(sample_config_dict, f)
                temp_path = Path(f.name)
            
            try:
                # é¦–æ¬¡åŠ è½½
                config1 = config_manager.load_config(temp_path, use_cache=True)
                
                # ä¿®æ”¹æ–‡ä»¶
                modified_config = sample_config_dict.copy()
                modified_config['model']['transformer']['d_model'] = 512
                
                with open(temp_path, 'w') as f:
                    yaml.dump(modified_config, f)
                
                # å¼ºåˆ¶é‡æ–°åŠ è½½
                config2 = config_manager.reload_config(temp_path)
                
                assert config1['model']['transformer']['d_model'] == 256
                assert config2['model']['transformer']['d_model'] == 512
                
            finally:
                temp_path.unlink()
    
    
    @pytest.mark.parametrize("env_value,expected_type,expected_value", [
        ("123", int, 123),
        ("123.45", float, 123.45),
        ("true", bool, True),
        ("false", bool, False),
        ("hello", str, "hello"),
        ("1,2,3", list, ["1", "2", "3"]),
    ])
    def test_env_value_type_conversion(env_value, expected_type, expected_value):
        """å‚æ•°åŒ–æµ‹è¯•çŽ¯å¢ƒå˜é‡ç±»åž‹è½¬æ¢"""
        from src.rl_trading_system.config.config_manager import ConfigManager
        
        manager = ConfigManager()
        converted = manager._convert_env_value(env_value, expected_type)
        
        assert type(converted) == expected_type
        assert converted == expected_value
    
    
    class TestErrorHandling:
        """é”™è¯¯å¤„ç†æµ‹è¯•"""
        
        def test_file_permission_error(self, config_manager):
            """æµ‹è¯•æ–‡ä»¶æƒé™é”™è¯¯"""
            # åˆ›å»ºä¸€ä¸ªæ— æƒé™è¯»å–çš„æ–‡ä»¶ï¼ˆåœ¨æ”¯æŒçš„ç³»ç»Ÿä¸Šï¼‰
            with tempfile.NamedTemporaryFile(delete=False) as f:
                temp_path = Path(f.name)
            
            try:
                # å°è¯•ç§»é™¤è¯»æƒé™
                temp_path.chmod(0o000)
                
                with pytest.raises(ConfigLoadError):
                    config_manager.load_config(temp_path)
                    
            except (OSError, PermissionError):
                # å¦‚æžœæ— æ³•è®¾ç½®æƒé™ï¼Œè·³è¿‡æ­¤æµ‹è¯•
                pytest.skip("æ— æ³•è®¾ç½®æ–‡ä»¶æƒé™")
            finally:
                # æ¢å¤æƒé™å¹¶åˆ é™¤æ–‡ä»¶
                try:
                    temp_path.chmod(0o644)
                    temp_path.unlink()
                except (OSError, PermissionError):
                    pass
        
        def test_circular_reference_detection(self, config_manager):
            """æµ‹è¯•å¾ªçŽ¯å¼•ç”¨æ£€æµ‹"""
            # è¿™ä¸ªæµ‹è¯•å¯èƒ½éœ€è¦åœ¨å®žé™…å®žçŽ°ä¸­æ ¹æ®å…·ä½“çš„å¼•ç”¨æœºåˆ¶æ¥è°ƒæ•´
            pass
    ]]></file>
  <file path="tests/unit/test_checkpoint_manager.py"><![CDATA[
    """
    æ£€æŸ¥ç‚¹ç®¡ç†å™¨çš„å•å…ƒæµ‹è¯•
    æµ‹è¯•æ¨¡åž‹ä¿å­˜ã€åŠ è½½å’Œç‰ˆæœ¬ç®¡ç†åŠŸèƒ½ï¼Œæ£€æŸ¥ç‚¹å®Œæ•´æ€§å’Œæ¢å¤èƒ½åŠ›ï¼Œæ¨¡åž‹åŽ‹ç¼©å’Œä¼˜åŒ–åŠŸèƒ½
    """
    import pytest
    import numpy as np
    import pandas as pd
    import torch
    import torch.nn as nn
    import os
    import shutil
    import json
    import tempfile
    from datetime import datetime, timedelta
    from unittest.mock import Mock, patch, MagicMock
    from typing import Dict, List, Tuple, Any, Optional
    from pathlib import Path
    import pickle
    import hashlib
    
    from src.rl_trading_system.model_management.checkpoint_manager import (
        CheckpointManager,
        CheckpointConfig,
        ModelCheckpoint,
        CheckpointMetadata,
        ModelCompressor
    )
    
    
    class MockModel(nn.Module):
        """æ¨¡æ‹ŸPyTorchæ¨¡åž‹"""
        
        def __init__(self, input_size=10, hidden_size=64, output_size=4):
            super(MockModel, self).__init__()
            self.fc1 = nn.Linear(input_size, hidden_size)
            self.fc2 = nn.Linear(hidden_size, hidden_size)
            self.fc3 = nn.Linear(hidden_size, output_size)
            self.relu = nn.ReLU()
            
        def forward(self, x):
            x = self.relu(self.fc1(x))
            x = self.relu(self.fc2(x))
            x = self.fc3(x)
            return x
        
        def get_model_size(self):
            """èŽ·å–æ¨¡åž‹å‚æ•°æ•°é‡"""
            return sum(p.numel() for p in self.parameters())
    
    
    class MockAgent:
        """æ¨¡æ‹Ÿå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“"""
        
        def __init__(self):
            self.actor = MockModel(input_size=20, hidden_size=128, output_size=4)
            self.critic = MockModel(input_size=24, hidden_size=128, output_size=1)
            self.optimizer_actor = torch.optim.Adam(self.actor.parameters(), lr=0.001)
            self.optimizer_critic = torch.optim.Adam(self.critic.parameters(), lr=0.001)
            self.training_step = 0
            
        def state_dict(self):
            """èŽ·å–æ¨¡åž‹çŠ¶æ€å­—å…¸"""
            return {
                'actor': self.actor.state_dict(),
                'critic': self.critic.state_dict(),
                'optimizer_actor': self.optimizer_actor.state_dict(),
                'optimizer_critic': self.optimizer_critic.state_dict(),
                'training_step': self.training_step
            }
        
        def load_state_dict(self, state_dict):
            """åŠ è½½æ¨¡åž‹çŠ¶æ€å­—å…¸"""
            self.actor.load_state_dict(state_dict['actor'])
            self.critic.load_state_dict(state_dict['critic'])
            self.optimizer_actor.load_state_dict(state_dict['optimizer_actor'])
            self.optimizer_critic.load_state_dict(state_dict['optimizer_critic'])
            self.training_step = state_dict['training_step']
        
        def get_model_info(self):
            """èŽ·å–æ¨¡åž‹ä¿¡æ¯"""
            return {
                'actor_params': self.actor.get_model_size(),
                'critic_params': self.critic.get_model_size(),
                'total_params': self.actor.get_model_size() + self.critic.get_model_size()
            }
    
    
    class TestCheckpointConfig:
        """æ£€æŸ¥ç‚¹é…ç½®æµ‹è¯•ç±»"""
        
        def test_checkpoint_config_creation(self):
            """æµ‹è¯•æ£€æŸ¥ç‚¹é…ç½®åˆ›å»º"""
            config = CheckpointConfig(
                save_dir="./checkpoints",
                max_checkpoints=10,
                save_frequency=100,
                compression_enabled=True,
                model_format="torch"
            )
            
            assert config.save_dir == "./checkpoints"
            assert config.max_checkpoints == 10
            assert config.save_frequency == 100
            assert config.compression_enabled == True
            assert config.model_format == "torch"
        
        def test_checkpoint_config_defaults(self):
            """æµ‹è¯•æ£€æŸ¥ç‚¹é…ç½®é»˜è®¤å€¼"""
            config = CheckpointConfig()
            
            assert config.save_dir == "./checkpoints"
            assert config.max_checkpoints == 5
            assert config.save_frequency == 1000
            assert config.compression_enabled == False
            assert config.model_format == "torch"
            assert config.auto_save_best == True
        
        def test_checkpoint_config_validation(self):
            """æµ‹è¯•æ£€æŸ¥ç‚¹é…ç½®éªŒè¯"""
            # æµ‹è¯•æ— æ•ˆçš„max_checkpoints
            with pytest.raises(ValueError, match="max_checkpointså¿…é¡»ä¸ºæ­£æ•°"):
                CheckpointConfig(max_checkpoints=0)
            
            # æµ‹è¯•æ— æ•ˆçš„save_frequency
            with pytest.raises(ValueError, match="save_frequencyå¿…é¡»ä¸ºæ­£æ•°"):
                CheckpointConfig(save_frequency=-1)
            
            # æµ‹è¯•æ— æ•ˆçš„model_format
            with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„æ¨¡åž‹æ ¼å¼"):
                CheckpointConfig(model_format="invalid")
    
    
    class TestModelCheckpoint:
        """æ¨¡åž‹æ£€æŸ¥ç‚¹æµ‹è¯•ç±»"""
        
        def test_model_checkpoint_creation(self):
            """æµ‹è¯•æ¨¡åž‹æ£€æŸ¥ç‚¹åˆ›å»º"""
            metadata = CheckpointMetadata(
                episode=1000,
                timestamp=datetime.now(),
                model_hash="abc123",
                performance_metrics={"reward": 100.0, "loss": 0.1}
            )
            
            checkpoint = ModelCheckpoint(
                checkpoint_id="checkpoint_1000",
                file_path="/path/to/checkpoint.pth",
                metadata=metadata
            )
            
            assert checkpoint.checkpoint_id == "checkpoint_1000"
            assert checkpoint.file_path == "/path/to/checkpoint.pth"
            assert checkpoint.metadata.episode == 1000
            assert checkpoint.metadata.model_hash == "abc123"
        
        def test_checkpoint_metadata_serialization(self):
            """æµ‹è¯•æ£€æŸ¥ç‚¹å…ƒæ•°æ®åºåˆ—åŒ–"""
            metadata = CheckpointMetadata(
                episode=500,
                timestamp=datetime.now(),
                model_hash="def456",
                performance_metrics={"accuracy": 0.95},
                model_info={"params": 10000}
            )
            
            # åºåˆ—åŒ–
            serialized = metadata.to_dict()
            
            assert serialized['episode'] == 500
            assert serialized['model_hash'] == "def456"
            assert serialized['performance_metrics']['accuracy'] == 0.95
            assert 'timestamp' in serialized
            
            # ååºåˆ—åŒ–
            restored = CheckpointMetadata.from_dict(serialized)
            
            assert restored.episode == metadata.episode
            assert restored.model_hash == metadata.model_hash
            assert restored.performance_metrics == metadata.performance_metrics
        
        def test_checkpoint_comparison(self):
            """æµ‹è¯•æ£€æŸ¥ç‚¹æ¯”è¾ƒ"""
            metadata1 = CheckpointMetadata(
                episode=1000,
                timestamp=datetime.now(),
                performance_metrics={"reward": 100.0}
            )
            
            metadata2 = CheckpointMetadata(
                episode=2000,
                timestamp=datetime.now(),
                performance_metrics={"reward": 150.0}
            )
            
            checkpoint1 = ModelCheckpoint("cp1", "path1", metadata1)
            checkpoint2 = ModelCheckpoint("cp2", "path2", metadata2)
            
            # æµ‹è¯•æ€§èƒ½æ¯”è¾ƒ
            assert checkpoint2.is_better_than(checkpoint1, metric="reward", mode="max")
            assert not checkpoint1.is_better_than(checkpoint2, metric="reward", mode="max")
    
    
    class TestCheckpointManager:
        """æ£€æŸ¥ç‚¹ç®¡ç†å™¨æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def temp_dir(self):
            """ä¸´æ—¶ç›®å½•fixture"""
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        @pytest.fixture
        def checkpoint_config(self, temp_dir):
            """æ£€æŸ¥ç‚¹é…ç½®fixture"""
            return CheckpointConfig(
                save_dir=temp_dir,
                max_checkpoints=3,
                save_frequency=10,
                auto_save_best=True
            )
        
        @pytest.fixture
        def mock_agent(self):
            """æ¨¡æ‹Ÿæ™ºèƒ½ä½“fixture"""
            return MockAgent()
        
        @pytest.fixture
        def checkpoint_manager(self, checkpoint_config):
            """æ£€æŸ¥ç‚¹ç®¡ç†å™¨fixture"""
            return CheckpointManager(checkpoint_config)
        
        def test_checkpoint_manager_initialization(self, checkpoint_manager, checkpoint_config):
            """æµ‹è¯•æ£€æŸ¥ç‚¹ç®¡ç†å™¨åˆå§‹åŒ–"""
            assert checkpoint_manager.config == checkpoint_config
            assert os.path.exists(checkpoint_config.save_dir)
            assert len(checkpoint_manager.checkpoints) == 0
            assert checkpoint_manager.best_checkpoint is None
        
        def test_save_checkpoint_basic(self, checkpoint_manager, mock_agent, temp_dir):
            """æµ‹è¯•åŸºæœ¬æ£€æŸ¥ç‚¹ä¿å­˜"""
            metrics = {"reward": 100.0, "loss": 0.1}
            
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model=mock_agent,
                episode=100,
                metrics=metrics
            )
            
            # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦åˆ›å»º
            assert os.path.exists(checkpoint_path)
            
            # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦è¢«è®°å½•
            assert len(checkpoint_manager.checkpoints) == 1
            
            checkpoint = checkpoint_manager.checkpoints[0]
            assert checkpoint.metadata.episode == 100
            assert checkpoint.metadata.performance_metrics == metrics
        
        def test_load_checkpoint_basic(self, checkpoint_manager, mock_agent, temp_dir):
            """æµ‹è¯•åŸºæœ¬æ£€æŸ¥ç‚¹åŠ è½½"""
            # å…ˆä¿å­˜ä¸€ä¸ªæ£€æŸ¥ç‚¹
            original_step = mock_agent.training_step = 500
            metrics = {"reward": 200.0}
            
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model=mock_agent,
                episode=200,
                metrics=metrics
            )
            
            # ä¿®æ”¹æ¨¡åž‹çŠ¶æ€
            mock_agent.training_step = 0
            
            # åŠ è½½æ£€æŸ¥ç‚¹
            loaded_metadata = checkpoint_manager.load_checkpoint(checkpoint_path, mock_agent)
            
            # éªŒè¯åŠ è½½ç»“æžœ
            assert mock_agent.training_step == original_step
            assert loaded_metadata.episode == 200
            assert loaded_metadata.performance_metrics == metrics
        
        def test_checkpoint_versioning(self, checkpoint_manager, mock_agent):
            """æµ‹è¯•æ£€æŸ¥ç‚¹ç‰ˆæœ¬ç®¡ç†"""
            # ä¿å­˜å¤šä¸ªæ£€æŸ¥ç‚¹
            for i in range(5):
                metrics = {"reward": i * 10.0}
                checkpoint_manager.save_checkpoint(
                    model=mock_agent,
                    episode=(i + 1) * 100,
                    metrics=metrics
                )
            
            # æ£€æŸ¥æ˜¯å¦åªä¿ç•™äº†æœ€å¤§æ•°é‡çš„æ£€æŸ¥ç‚¹
            assert len(checkpoint_manager.checkpoints) <= checkpoint_manager.config.max_checkpoints
            
            # æ£€æŸ¥æ£€æŸ¥ç‚¹æ˜¯å¦æŒ‰æ—¶é—´æŽ’åº
            timestamps = [cp.metadata.timestamp for cp in checkpoint_manager.checkpoints]
            assert timestamps == sorted(timestamps)
        
        def test_best_checkpoint_tracking(self, checkpoint_manager, mock_agent):
            """æµ‹è¯•æœ€ä½³æ£€æŸ¥ç‚¹è·Ÿè¸ª"""
            # ä¿å­˜å¤šä¸ªæ£€æŸ¥ç‚¹ï¼Œå¥–åŠ±é€’å¢ž
            rewards = [50.0, 100.0, 75.0, 150.0, 120.0]
            
            for i, reward in enumerate(rewards):
                metrics = {"reward": reward}
                checkpoint_manager.save_checkpoint(
                    model=mock_agent,
                    episode=(i + 1) * 100,
                    metrics=metrics,
                    is_best_metric="reward"
                )
            
            # æ£€æŸ¥æœ€ä½³æ£€æŸ¥ç‚¹
            assert checkpoint_manager.best_checkpoint is not None
            assert checkpoint_manager.best_checkpoint.metadata.performance_metrics["reward"] == 150.0
        
        def test_checkpoint_cleanup(self, checkpoint_manager, mock_agent):
            """æµ‹è¯•æ£€æŸ¥ç‚¹æ¸…ç†"""
            initial_count = 10
            max_checkpoints = checkpoint_manager.config.max_checkpoints
            
            # ä¿å­˜è¶…è¿‡æœ€å¤§æ•°é‡çš„æ£€æŸ¥ç‚¹
            for i in range(initial_count):
                metrics = {"reward": i * 5.0}
                checkpoint_manager.save_checkpoint(
                    model=mock_agent,
                    episode=(i + 1) * 50,
                    metrics=metrics
                )
            
            # éªŒè¯åªä¿ç•™äº†æœ€å¤§æ•°é‡çš„æ£€æŸ¥ç‚¹
            assert len(checkpoint_manager.checkpoints) == max_checkpoints
            
            # éªŒè¯ä¿ç•™çš„æ˜¯æœ€æ–°çš„æ£€æŸ¥ç‚¹
            episodes = [cp.metadata.episode for cp in checkpoint_manager.checkpoints]
            expected_episodes = list(range((initial_count - max_checkpoints + 1) * 50, 
                                         (initial_count + 1) * 50, 50))
            assert episodes == expected_episodes
        
        def test_checkpoint_integrity_verification(self, checkpoint_manager, mock_agent, temp_dir):
            """æµ‹è¯•æ£€æŸ¥ç‚¹å®Œæ•´æ€§éªŒè¯"""
            # ä¿å­˜æ£€æŸ¥ç‚¹
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model=mock_agent,
                episode=300,
                metrics={"reward": 300.0}
            )
            
            # éªŒè¯æ£€æŸ¥ç‚¹å®Œæ•´æ€§
            is_valid = checkpoint_manager.verify_checkpoint_integrity(checkpoint_path)
            assert is_valid
            
            # æŸåæ£€æŸ¥ç‚¹æ–‡ä»¶
            with open(checkpoint_path, 'rb') as f:
                data = f.read()
            
            # å†™å…¥æŸåçš„æ•°æ®
            with open(checkpoint_path, 'wb') as f:
                f.write(data[:len(data)//2])  # æˆªæ–­æ–‡ä»¶
            
            # éªŒè¯æŸåçš„æ£€æŸ¥ç‚¹
            is_valid = checkpoint_manager.verify_checkpoint_integrity(checkpoint_path)
            assert not is_valid
        
        def test_checkpoint_recovery(self, checkpoint_manager, mock_agent, temp_dir):
            """æµ‹è¯•æ£€æŸ¥ç‚¹æ¢å¤èƒ½åŠ›"""
            # ä¿å­˜å¤šä¸ªæ£€æŸ¥ç‚¹
            saved_paths = []
            for i in range(3):
                metrics = {"reward": (i + 1) * 100.0}
                path = checkpoint_manager.save_checkpoint(
                    model=mock_agent,
                    episode=(i + 1) * 200,
                    metrics=metrics
                )
                saved_paths.append(path)
            
            # æ¨¡æ‹Ÿéƒ¨åˆ†æ£€æŸ¥ç‚¹æŸå
            os.remove(saved_paths[1])  # åˆ é™¤ä¸­é—´çš„æ£€æŸ¥ç‚¹
            
            # æ‰«æå¹¶æ¢å¤å¯ç”¨çš„æ£€æŸ¥ç‚¹
            recovered_checkpoints = checkpoint_manager.scan_and_recover_checkpoints()
            
            # éªŒè¯æ¢å¤ç»“æžœ
            assert len(recovered_checkpoints) == 2  # åº”è¯¥æ¢å¤2ä¸ªæœ‰æ•ˆæ£€æŸ¥ç‚¹
            episodes = [cp.metadata.episode for cp in recovered_checkpoints]
            assert 200 in episodes and 600 in episodes
            assert 400 not in episodes  # æŸåçš„æ£€æŸ¥ç‚¹ä¸åº”è¯¥è¢«æ¢å¤
        
        def test_checkpoint_metadata_persistence(self, checkpoint_manager, mock_agent):
            """æµ‹è¯•æ£€æŸ¥ç‚¹å…ƒæ•°æ®æŒä¹…åŒ–"""
            # ä¿å­˜æ£€æŸ¥ç‚¹
            metrics = {"reward": 500.0, "steps": 10000}
            model_info = mock_agent.get_model_info()
            
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model=mock_agent,
                episode=500,
                metrics=metrics,
                model_info=model_info
            )
            
            # é‡æ–°åˆ›å»ºç®¡ç†å™¨å¹¶æ‰«ææ£€æŸ¥ç‚¹
            new_manager = CheckpointManager(checkpoint_manager.config)
            new_manager.scan_and_recover_checkpoints()
            
            # éªŒè¯å…ƒæ•°æ®æ˜¯å¦æ­£ç¡®æ¢å¤
            assert len(new_manager.checkpoints) == 1
            recovered_checkpoint = new_manager.checkpoints[0]
            
            assert recovered_checkpoint.metadata.episode == 500
            assert recovered_checkpoint.metadata.performance_metrics == metrics
            assert recovered_checkpoint.metadata.model_info == model_info
        
        def test_checkpoint_format_conversion(self, checkpoint_manager, mock_agent):
            """æµ‹è¯•æ£€æŸ¥ç‚¹æ ¼å¼è½¬æ¢"""
            # ä¿å­˜PyTorchæ ¼å¼çš„æ£€æŸ¥ç‚¹
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model=mock_agent,
                episode=100,
                metrics={"reward": 100.0}
            )
            
            # è½¬æ¢ä¸ºONNXæ ¼å¼ï¼ˆæ¨¡æ‹Ÿï¼‰
            onnx_path = checkpoint_manager.convert_checkpoint_format(
                checkpoint_path, 
                target_format="onnx",
                input_shape=(1, 20)  # ç¤ºä¾‹è¾“å…¥å½¢çŠ¶
            )
            
            assert os.path.exists(onnx_path)
            assert onnx_path.endswith('.onnx')
        
        @pytest.mark.parametrize("compression_method", ["gzip", "lzma", "bz2"])
        def test_checkpoint_compression(self, checkpoint_manager, mock_agent, compression_method):
            """æµ‹è¯•æ£€æŸ¥ç‚¹åŽ‹ç¼©"""
            # å¯ç”¨åŽ‹ç¼©
            checkpoint_manager.config.compression_enabled = True
            checkpoint_manager.config.compression_method = compression_method
            
            # ä¿å­˜åŽ‹ç¼©çš„æ£€æŸ¥ç‚¹
            checkpoint_path = checkpoint_manager.save_checkpoint(
                model=mock_agent,
                episode=100,
                metrics={"reward": 150.0}
            )
            
            # éªŒè¯åŽ‹ç¼©æ–‡ä»¶å­˜åœ¨ï¼ˆæ£€æŸ¥ç‚¹è·¯å¾„æœ¬èº«å°±æ˜¯åŽ‹ç¼©åŽçš„è·¯å¾„ï¼‰
            assert os.path.exists(checkpoint_path)
            assert checkpoint_path.endswith(f".{compression_method}")
            
            # éªŒè¯å¯ä»¥æ­£ç¡®åŠ è½½åŽ‹ç¼©çš„æ£€æŸ¥ç‚¹
            loaded_metadata = checkpoint_manager.load_checkpoint(checkpoint_path, mock_agent)
            assert loaded_metadata.episode == 100
            assert loaded_metadata.performance_metrics["reward"] == 150.0
        
        def test_checkpoint_size_optimization(self, checkpoint_manager, mock_agent):
            """æµ‹è¯•æ£€æŸ¥ç‚¹å¤§å°ä¼˜åŒ–"""
            # ä¿å­˜åŽŸå§‹æ£€æŸ¥ç‚¹
            original_path = checkpoint_manager.save_checkpoint(
                model=mock_agent,
                episode=100,
                metrics={"reward": 100.0}
            )
            original_size = os.path.getsize(original_path)
            
            # åº”ç”¨å¤§å°ä¼˜åŒ–
            optimized_path = checkpoint_manager.optimize_checkpoint_size(
                original_path,
                remove_optimizer_state=True,
                quantize_weights=True
            )
            optimized_size = os.path.getsize(optimized_path)
            
            # éªŒè¯ä¼˜åŒ–æ•ˆæžœï¼ˆç”±äºŽMockAgentå¯èƒ½æ²¡æœ‰çœŸæ­£çš„ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œæ‰€ä»¥å¤§å°å¯èƒ½ç›¸ä¼¼ï¼‰
            # ä¸»è¦éªŒè¯ä¼˜åŒ–è¿‡ç¨‹æ²¡æœ‰å‡ºé”™
            assert optimized_size > 0
            
            # éªŒè¯ä¼˜åŒ–åŽçš„æ£€æŸ¥ç‚¹ä»ç„¶å¯ç”¨
            new_agent = MockAgent()
            loaded_metadata = checkpoint_manager.load_checkpoint(optimized_path, new_agent)
            assert loaded_metadata.episode == 100
    
    
    class TestModelCompressor:
        """æ¨¡åž‹åŽ‹ç¼©å™¨æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def temp_dir(self):
            """ä¸´æ—¶ç›®å½•fixture"""
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        @pytest.fixture
        def mock_model(self):
            """æ¨¡æ‹Ÿæ¨¡åž‹fixture"""
            return MockModel()
        
        @pytest.fixture
        def compressor(self, temp_dir):
            """æ¨¡åž‹åŽ‹ç¼©å™¨fixture"""
            return ModelCompressor(temp_dir)
        
        def test_model_quantization(self, compressor, mock_model, temp_dir):
            """æµ‹è¯•æ¨¡åž‹é‡åŒ–"""
            # ä¿å­˜åŽŸå§‹æ¨¡åž‹
            original_path = os.path.join(temp_dir, "original_model.pth")
            torch.save(mock_model.state_dict(), original_path)
            original_size = os.path.getsize(original_path)
            
            # é‡åŒ–æ¨¡åž‹
            quantized_path = compressor.quantize_model(
                model=mock_model,
                model_path=original_path,
                quantization_type="dynamic"
            )
            
            # éªŒè¯é‡åŒ–ç»“æžœ
            assert os.path.exists(quantized_path)
            quantized_size = os.path.getsize(quantized_path)
            
            # é‡åŒ–åŽçš„æ¨¡åž‹åº”è¯¥æ›´å°
            assert quantized_size < original_size
        
        def test_model_pruning(self, compressor, mock_model, temp_dir):
            """æµ‹è¯•æ¨¡åž‹å‰ªæž"""
            # è®¡ç®—åŽŸå§‹å‚æ•°æ•°é‡
            original_params = mock_model.get_model_size()
            
            # å‰ªæžæ¨¡åž‹
            pruned_model = compressor.prune_model(
                model=mock_model,
                pruning_ratio=0.2  # å‰ªæž20%çš„å‚æ•°
            )
            
            # éªŒè¯å‰ªæžæ•ˆæžœ
            pruned_params = pruned_model.get_model_size()
            
            # æ³¨æ„ï¼šç”±äºŽæˆ‘ä»¬çš„æ¨¡æ‹Ÿæ¨¡åž‹æ¯”è¾ƒç®€å•ï¼Œè¿™é‡Œä¸»è¦æµ‹è¯•æŽ¥å£
            assert isinstance(pruned_model, nn.Module)
        
        def test_onnx_conversion(self, compressor, mock_model, temp_dir):
            """æµ‹è¯•ONNXè½¬æ¢"""
            # è½¬æ¢ä¸ºONNXæ ¼å¼
            onnx_path = compressor.convert_to_onnx(
                model=mock_model,
                input_shape=(1, 10),
                output_path=os.path.join(temp_dir, "model.onnx")
            )
            
            # éªŒè¯ONNXæ–‡ä»¶
            assert os.path.exists(onnx_path)
            assert onnx_path.endswith('.onnx')
        
        def test_torchscript_conversion(self, compressor, mock_model, temp_dir):
            """æµ‹è¯•TorchScriptè½¬æ¢"""
            # è½¬æ¢ä¸ºTorchScriptæ ¼å¼
            script_path = compressor.convert_to_torchscript(
                model=mock_model,
                example_input=torch.randn(1, 10),
                output_path=os.path.join(temp_dir, "model.pt")
            )
            
            # éªŒè¯TorchScriptæ–‡ä»¶
            assert os.path.exists(script_path)
            assert script_path.endswith('.pt')
            
            # éªŒè¯å¯ä»¥åŠ è½½TorchScriptæ¨¡åž‹
            loaded_model = torch.jit.load(script_path)
            assert isinstance(loaded_model, torch.jit.ScriptModule)
        
        def test_compression_pipeline(self, compressor, mock_model, temp_dir):
            """æµ‹è¯•å®Œæ•´çš„åŽ‹ç¼©æµæ°´çº¿"""
            # æ‰§è¡Œå®Œæ•´çš„åŽ‹ç¼©æµæ°´çº¿
            results = compressor.compress_model_pipeline(
                model=mock_model,
                input_shape=(1, 10),
                output_dir=temp_dir,
                enable_quantization=True,
                enable_pruning=True,
                enable_onnx=True,
                enable_torchscript=True
            )
            
            # éªŒè¯æ‰€æœ‰è¾“å‡ºæ–‡ä»¶
            assert 'quantized' in results
            assert 'pruned' in results
            assert 'onnx' in results
            assert 'torchscript' in results
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            for format_name, file_path in results.items():
                assert os.path.exists(file_path)
    
    
    class TestIntegrationScenarios:
        """é›†æˆåœºæ™¯æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def temp_dir(self):
            """ä¸´æ—¶ç›®å½•fixture"""
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        def test_training_checkpoint_workflow(self, temp_dir):
            """æµ‹è¯•è®­ç»ƒæ£€æŸ¥ç‚¹å·¥ä½œæµ"""
            # åˆ›å»ºé…ç½®å’Œç®¡ç†å™¨
            config = CheckpointConfig(
                save_dir=temp_dir,
                max_checkpoints=3,
                auto_save_best=True,
                compression_enabled=True
            )
            manager = CheckpointManager(config)
            agent = MockAgent()
            
            # æ¨¡æ‹Ÿè®­ç»ƒè¿‡ç¨‹
            best_reward = 0
            for episode in range(1, 11):
                # æ¨¡æ‹Ÿè®­ç»ƒ
                agent.training_step += 100
                reward = np.random.uniform(0, 200) + episode * 10  # æ€»ä½“è¶‹åŠ¿ä¸Šå‡
                
                metrics = {
                    "reward": reward,
                    "loss": np.random.uniform(0.1, 1.0),
                    "episode": episode
                }
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                is_best = reward > best_reward
                if is_best:
                    best_reward = reward
                
                checkpoint_path = manager.save_checkpoint(
                    model=agent,
                    episode=episode * 100,
                    metrics=metrics,
                    is_best_metric="reward" if is_best else None
                )
                
                assert os.path.exists(checkpoint_path)
            
            # éªŒè¯æœ€ç»ˆçŠ¶æ€
            assert len(manager.checkpoints) <= config.max_checkpoints
            assert manager.best_checkpoint is not None
            assert manager.best_checkpoint.metadata.performance_metrics["reward"] == best_reward
        
        def test_checkpoint_disaster_recovery(self, temp_dir):
            """æµ‹è¯•æ£€æŸ¥ç‚¹ç¾éš¾æ¢å¤"""
            # åˆ›å»ºæ£€æŸ¥ç‚¹ç®¡ç†å™¨
            config = CheckpointConfig(save_dir=temp_dir, max_checkpoints=5)
            manager = CheckpointManager(config)
            agent = MockAgent()
            
            # ä¿å­˜å¤šä¸ªæ£€æŸ¥ç‚¹
            saved_episodes = []
            for i in range(5):
                episode = (i + 1) * 200
                metrics = {"reward": i * 50.0}
                
                manager.save_checkpoint(
                    model=agent,
                    episode=episode,
                    metrics=metrics
                )
                saved_episodes.append(episode)
            
            # æ¨¡æ‹Ÿéƒ¨åˆ†æ–‡ä»¶æŸå
            checkpoints = manager.checkpoints
            # æŸåç¬¬2å’Œç¬¬4ä¸ªæ£€æŸ¥ç‚¹
            os.remove(checkpoints[1].file_path)
            
            # åœ¨ç¬¬3ä¸ªæ£€æŸ¥ç‚¹ä¸­å†™å…¥æ— æ•ˆæ•°æ®
            with open(checkpoints[2].file_path, 'w') as f:
                f.write("invalid data")
            
            # åˆ›å»ºæ–°çš„ç®¡ç†å™¨å®žä¾‹ï¼Œæ¨¡æ‹Ÿé‡å¯åŽçš„æ¢å¤
            recovery_manager = CheckpointManager(config)
            recovered = recovery_manager.scan_and_recover_checkpoints()
            
            # éªŒè¯æ¢å¤ç»“æžœï¼ˆè‡³å°‘æ¢å¤éƒ¨åˆ†æ£€æŸ¥ç‚¹ï¼Œå…è®¸ä¸€äº›æ£€æŸ¥ç‚¹è¢«æˆåŠŸæ¢å¤ï¼‰
            assert len(recovered) >= 2  # è‡³å°‘åº”è¯¥æ¢å¤2ä¸ªæ£€æŸ¥ç‚¹
            recovered_episodes = [cp.metadata.episode for cp in recovered]
            
            # åº”è¯¥æ¢å¤ç¬¬1å’Œç¬¬5ä¸ªæ£€æŸ¥ç‚¹
            assert saved_episodes[0] in recovered_episodes
            assert saved_episodes[4] in recovered_episodes
            # ç¬¬2ä¸ªæ£€æŸ¥ç‚¹è¢«åˆ é™¤ï¼Œåº”è¯¥ä¸å­˜åœ¨
            assert saved_episodes[1] not in recovered_episodes
        
        def test_model_format_migration(self, temp_dir):
            """æµ‹è¯•æ¨¡åž‹æ ¼å¼è¿ç§»"""
            # åˆ›å»ºåŽŸå§‹PyTorchæ£€æŸ¥ç‚¹
            config = CheckpointConfig(save_dir=temp_dir, model_format="torch")
            manager = CheckpointManager(config)
            agent = MockAgent()
            
            # ä¿å­˜PyTorchæ ¼å¼
            torch_path = manager.save_checkpoint(
                model=agent,
                episode=1000,
                metrics={"reward": 500.0}
            )
            
            # åˆ›å»ºåŽ‹ç¼©å™¨
            compressor = ModelCompressor(temp_dir)
            
            # æ‰¹é‡è½¬æ¢ä¸ºä¸åŒæ ¼å¼
            conversion_results = {}
            
            # è½¬æ¢ä¸ºONNX
            try:
                onnx_path = compressor.convert_to_onnx(
                    model=agent.actor,  # åªè½¬æ¢actorç½‘ç»œä½œä¸ºä¾‹å­
                    input_shape=(1, 20),
                    output_path=os.path.join(temp_dir, "model.onnx")
                )
                conversion_results['onnx'] = onnx_path
            except Exception as e:
                # ONNXè½¬æ¢å¯èƒ½å› ä¸ºçŽ¯å¢ƒé—®é¢˜å¤±è´¥ï¼Œè¿™é‡Œè·³è¿‡
                print(f"ONNX conversion skipped: {e}")
            
            # è½¬æ¢ä¸ºTorchScript
            script_path = compressor.convert_to_torchscript(
                model=agent.actor,
                example_input=torch.randn(1, 20),
                output_path=os.path.join(temp_dir, "model_script.pt")
            )
            conversion_results['torchscript'] = script_path
            
            # éªŒè¯è½¬æ¢ç»“æžœ
            for format_name, path in conversion_results.items():
                assert os.path.exists(path)
                print(f"Successfully converted to {format_name}: {path}")
        
        def test_checkpoint_performance_monitoring(self, temp_dir):
            """æµ‹è¯•æ£€æŸ¥ç‚¹æ€§èƒ½ç›‘æŽ§"""
            config = CheckpointConfig(save_dir=temp_dir, max_checkpoints=10)
            manager = CheckpointManager(config)
            agent = MockAgent()
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            performance_history = []
            
            for episode in range(1, 21):
                # æ¨¡æ‹Ÿä¸åŒçš„æ€§èƒ½æŒ‡æ ‡
                metrics = {
                    "reward": np.random.uniform(50, 200),
                    "loss": np.random.uniform(0.01, 0.5),
                    "success_rate": np.random.uniform(0.3, 0.9),
                    "steps_per_episode": np.random.randint(100, 500)
                }
                
                performance_history.append(metrics)
                
                # ä¿å­˜æ£€æŸ¥ç‚¹
                manager.save_checkpoint(
                    model=agent,
                    episode=episode * 50,
                    metrics=metrics,
                    is_best_metric="reward"
                )
            
            # åˆ†æžæ€§èƒ½è¶‹åŠ¿
            rewards = [m["reward"] for m in performance_history]
            losses = [m["loss"] for m in performance_history]
            
            # éªŒè¯æ£€æŸ¥ç‚¹ä¸­è®°å½•äº†å®Œæ•´çš„æ€§èƒ½åŽ†å²
            assert len(manager.checkpoints) <= config.max_checkpoints
            
            # éªŒè¯æœ€ä½³æ£€æŸ¥ç‚¹ç¡®å®žå¯¹åº”æœ€é«˜å¥–åŠ±
            if manager.best_checkpoint:
                best_reward = manager.best_checkpoint.metadata.performance_metrics["reward"]
                assert best_reward == max(rewards)
            
            # ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            performance_report = manager.generate_performance_report()
            
            assert "total_checkpoints" in performance_report
            assert "best_performance" in performance_report
            assert "performance_trend" in performance_report
            
            print(f"Performance report: {performance_report}")
    
    
    class TestErrorHandling:
        """é”™è¯¯å¤„ç†æµ‹è¯•ç±»"""
        
        @pytest.fixture
        def temp_dir(self):
            """ä¸´æ—¶ç›®å½•fixture"""
            temp_dir = tempfile.mkdtemp()
            yield temp_dir
            shutil.rmtree(temp_dir, ignore_errors=True)
        
        def test_invalid_checkpoint_file(self, temp_dir):
            """æµ‹è¯•æ— æ•ˆæ£€æŸ¥ç‚¹æ–‡ä»¶å¤„ç†"""
            config = CheckpointConfig(save_dir=temp_dir)
            manager = CheckpointManager(config)
            agent = MockAgent()
            
            # åˆ›å»ºæ— æ•ˆçš„æ£€æŸ¥ç‚¹æ–‡ä»¶
            invalid_path = os.path.join(temp_dir, "invalid.pth")
            with open(invalid_path, 'w') as f:
                f.write("not a valid checkpoint")
            
            # å°è¯•åŠ è½½æ— æ•ˆæ£€æŸ¥ç‚¹
            with pytest.raises(Exception):
                manager.load_checkpoint(invalid_path, agent)
        
        def test_disk_space_handling(self, temp_dir):
            """æµ‹è¯•ç£ç›˜ç©ºé—´ä¸è¶³å¤„ç†"""
            config = CheckpointConfig(save_dir=temp_dir)
            manager = CheckpointManager(config)
            agent = MockAgent()
            
            # æ¨¡æ‹Ÿç£ç›˜ç©ºé—´æ£€æŸ¥
            available_space = manager.check_available_disk_space()
            assert available_space > 0
            
            # éªŒè¯ç©ºé—´ä¸è¶³æ—¶çš„å¤„ç†
            with patch('shutil.disk_usage', return_value=(1000, 1000, 100)):  # åªæœ‰100å­—èŠ‚å¯ç”¨
                with pytest.raises(RuntimeError, match="ç£ç›˜ç©ºé—´ä¸è¶³"):
                    manager.save_checkpoint(
                        model=agent,
                        episode=100,
                        metrics={"reward": 100.0}
                    )
        
        def test_concurrent_access_handling(self, temp_dir):
            """æµ‹è¯•å¹¶å‘è®¿é—®å¤„ç†"""
            config = CheckpointConfig(save_dir=temp_dir)
            manager = CheckpointManager(config)
            agent = MockAgent()
            
            # æ¨¡æ‹Ÿå¹¶å‘ä¿å­˜
            import threading
            import time
            
            results = []
            errors = []
            
            def save_checkpoint_worker(worker_id):
                try:
                    time.sleep(0.1 * worker_id)  # æ¨¡æ‹Ÿä¸åŒçš„å¯åŠ¨æ—¶é—´
                    path = manager.save_checkpoint(
                        model=agent,
                        episode=worker_id * 100,
                        metrics={"reward": worker_id * 10.0}
                    )
                    results.append(path)
                except Exception as e:
                    errors.append(str(e))
            
            # å¯åŠ¨å¤šä¸ªçº¿ç¨‹
            threads = []
            for i in range(3):
                thread = threading.Thread(target=save_checkpoint_worker, args=(i,))
                threads.append(thread)
                thread.start()
            
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()
            
            # éªŒè¯ç»“æžœ
            assert len(errors) == 0  # ä¸åº”è¯¥æœ‰é”™è¯¯
            assert len(results) == 3  # åº”è¯¥æˆåŠŸä¿å­˜3ä¸ªæ£€æŸ¥ç‚¹
            assert len(manager.checkpoints) == 3
    ]]></file>
  <file path="tests/unit/test_canary_deployment.py"><![CDATA[
    """
    é‡‘ä¸é›€éƒ¨ç½²ç³»ç»Ÿçš„å•å…ƒæµ‹è¯•
    æµ‹è¯•æ–°æ¨¡åž‹çš„æ¸è¿›å¼éƒ¨ç½²å’Œè¯„ä¼°æœºåˆ¶ï¼ŒA/Bæµ‹è¯•æ¡†æž¶å’Œæ€§èƒ½å¯¹æ¯”ï¼Œéƒ¨ç½²è¿‡ç¨‹çš„å®‰å…¨æ€§å’Œå›žæ»šèƒ½åŠ›
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional, Any
    from unittest.mock import Mock, patch, MagicMock
    import json
    import threading
    import time
    import uuid
    
    from src.rl_trading_system.deployment.canary_deployment import (
        CanaryDeployment,
        ABTestFramework,
        ModelPerformanceComparator,
        DeploymentSafetyController,
        DeploymentStatus,
        RollbackManager,
        TrafficRouter,
        PerformanceMetrics,
        DeploymentConfig
    )
    
    
    # å…±äº«çš„æµ‹è¯•fixtures
    @pytest.fixture
    def mock_model():
        """åˆ›å»ºæ¨¡æ‹Ÿæ¨¡åž‹"""
        model = Mock()
        model.predict = Mock(return_value=np.array([0.1, 0.2, 0.7]))
        model.version = "v1.2.0"
        model.created_at = datetime.now()
        return model
    
    
    @pytest.fixture
    def mock_baseline_model():
        """åˆ›å»ºåŸºçº¿æ¨¡åž‹"""
        model = Mock()
        model.predict = Mock(return_value=np.array([0.2, 0.3, 0.5]))
        model.version = "v1.1.0"
        model.created_at = datetime.now() - timedelta(days=7)
        return model
    
    
    class TestCanaryDeployment:
        """é‡‘ä¸é›€éƒ¨ç½²æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def deployment_config(self):
            """åˆ›å»ºéƒ¨ç½²é…ç½®"""
            return DeploymentConfig(
                canary_percentage=10.0,
                evaluation_period=3600,  # 1å°æ—¶
                success_threshold=0.95,
                error_threshold=0.05,
                performance_threshold=0.02,
                rollback_threshold=0.1,
                max_canary_duration=7200  # 2å°æ—¶
            )
    
    
    
        @pytest.fixture
        def canary_deployment(self, deployment_config, mock_model, mock_baseline_model):
            """åˆ›å»ºé‡‘ä¸é›€éƒ¨ç½²å®žä¾‹"""
            return CanaryDeployment(
                canary_model=mock_model,
                baseline_model=mock_baseline_model,
                config=deployment_config
            )
    
        def test_canary_deployment_initialization(self, canary_deployment, deployment_config):
            """æµ‹è¯•é‡‘ä¸é›€éƒ¨ç½²åˆå§‹åŒ–"""
            assert canary_deployment.canary_model is not None
            assert canary_deployment.baseline_model is not None
            assert canary_deployment.config == deployment_config
            assert canary_deployment.status == DeploymentStatus.PENDING
            assert canary_deployment.start_time is None
            assert canary_deployment.traffic_percentage == 0.0
    
        def test_deployment_start(self, canary_deployment):
            """æµ‹è¯•éƒ¨ç½²å¯åŠ¨"""
            # å¯åŠ¨éƒ¨ç½²
            canary_deployment.start_deployment()
            
            # éªŒè¯çŠ¶æ€å˜åŒ–
            assert canary_deployment.status == DeploymentStatus.ACTIVE
            assert canary_deployment.start_time is not None
            assert canary_deployment.traffic_percentage == canary_deployment.config.canary_percentage
            
            # éªŒè¯æ— æ³•é‡å¤å¯åŠ¨
            with pytest.raises(ValueError, match="éƒ¨ç½²å·²ç»åœ¨è¿è¡Œä¸­"):
                canary_deployment.start_deployment()
    
        def test_gradual_traffic_increase(self, canary_deployment):
            """æµ‹è¯•æ¸è¿›å¼æµé‡å¢žé•¿"""
            canary_deployment.start_deployment()
            
            # æ¨¡æ‹ŸæˆåŠŸçš„è¯„ä¼°ç»“æžœ
            canary_deployment.performance_comparator.latest_comparison = {
                'success_rate_diff': -0.01,  # é‡‘ä¸é›€æ¨¡åž‹æ›´å¥½
                'error_rate_diff': -0.02,
                'performance_improvement': 0.03,
                'statistical_significance': True
            }
            
            # å¢žåŠ æµé‡
            initial_traffic = canary_deployment.traffic_percentage
            canary_deployment.increase_traffic(step_size=10.0)
            
            assert canary_deployment.traffic_percentage == initial_traffic + 10.0
    
        def test_deployment_success_criteria(self, canary_deployment):
            """æµ‹è¯•éƒ¨ç½²æˆåŠŸæ ‡å‡†"""
            canary_deployment.start_deployment()
            
            # æ¨¡æ‹Ÿæ»¡è¶³æˆåŠŸæ¡ä»¶çš„æŒ‡æ ‡
            metrics = PerformanceMetrics(
                success_rate=0.98,
                error_rate=0.01,
                avg_response_time=0.15,
                throughput=1000.0,
                accuracy=0.92,
                precision=0.88,
                recall=0.85,
                f1_score=0.86
            )
            
            canary_deployment.update_canary_metrics(metrics)
            
            # è¯„ä¼°æ˜¯å¦æ»¡è¶³æˆåŠŸæ ‡å‡†
            meets_criteria = canary_deployment.evaluate_success_criteria()
            assert meets_criteria == True
    
        def test_deployment_failure_detection(self, canary_deployment):
            """æµ‹è¯•éƒ¨ç½²å¤±è´¥æ£€æµ‹"""
            canary_deployment.start_deployment()
            
            # æ¨¡æ‹Ÿä¸æ»¡è¶³æˆåŠŸæ¡ä»¶çš„æŒ‡æ ‡
            metrics = PerformanceMetrics(
                success_rate=0.85,  # ä½ŽäºŽé˜ˆå€¼
                error_rate=0.15,    # é«˜äºŽé˜ˆå€¼
                avg_response_time=0.5,
                throughput=500.0,
                accuracy=0.75,
                precision=0.70,
                recall=0.68,
                f1_score=0.69
            )
            
            canary_deployment.update_canary_metrics(metrics)
            
            # è¯„ä¼°æ˜¯å¦æ»¡è¶³æˆåŠŸæ ‡å‡†
            meets_criteria = canary_deployment.evaluate_success_criteria()
            assert meets_criteria == False
    
        def test_automatic_rollback_trigger(self, canary_deployment):
            """æµ‹è¯•è‡ªåŠ¨å›žæ»šè§¦å‘"""
            canary_deployment.start_deployment()
            
            # æ¨¡æ‹Ÿä¸¥é‡çš„æ€§èƒ½ä¸‹é™
            canary_deployment.performance_comparator.latest_comparison = {
                'success_rate_diff': 0.15,  # é‡‘ä¸é›€æ¨¡åž‹æ›´å·®
                'error_rate_diff': 0.12,
                'performance_improvement': -0.2,
                'statistical_significance': True
            }
            
            # æ£€æŸ¥æ˜¯å¦è§¦å‘å›žæ»š
            should_rollback = canary_deployment.should_trigger_rollback()
            assert should_rollback is True
    
        def test_deployment_completion_successful(self, canary_deployment):
            """æµ‹è¯•æˆåŠŸçš„éƒ¨ç½²å®Œæˆ"""
            canary_deployment.start_deployment()
            
            # æ¨¡æ‹ŸæŒç»­æˆåŠŸçš„è¡¨çŽ°
            for i in range(5):
                metrics = PerformanceMetrics(
                    success_rate=0.98,
                    error_rate=0.01,
                    avg_response_time=0.1,
                    throughput=1200.0,
                    accuracy=0.93,
                    precision=0.90,
                    recall=0.88,
                    f1_score=0.89
                )
                canary_deployment.update_canary_metrics(metrics)
                canary_deployment.increase_traffic(step_size=20.0)
            
            # å®Œæˆéƒ¨ç½²
            canary_deployment.complete_deployment()
            
            assert canary_deployment.status == DeploymentStatus.COMPLETED
            assert canary_deployment.traffic_percentage == 100.0
    
        def test_deployment_rollback(self, canary_deployment):
            """æµ‹è¯•éƒ¨ç½²å›žæ»š"""
            canary_deployment.start_deployment()
            
            # è§¦å‘å›žæ»š
            rollback_reason = "æ€§èƒ½ä¸¥é‡ä¸‹é™"
            canary_deployment.rollback_deployment(reason=rollback_reason)
            
            assert canary_deployment.status == DeploymentStatus.ROLLED_BACK
            assert canary_deployment.traffic_percentage == 0.0
            assert rollback_reason in canary_deployment.deployment_history[-1]['reason']
    
        def test_deployment_timeout_handling(self, canary_deployment):
            """æµ‹è¯•éƒ¨ç½²è¶…æ—¶å¤„ç†"""
            canary_deployment.start_deployment()
            
            # æ¨¡æ‹Ÿè¶…æ—¶
            canary_deployment.start_time = datetime.now() - timedelta(seconds=canary_deployment.config.max_canary_duration + 1)
            
            is_timeout = canary_deployment.is_deployment_timeout()
            assert is_timeout is True
    
        def test_concurrent_deployment_prevention(self, canary_deployment):
            """æµ‹è¯•å¹¶å‘éƒ¨ç½²é˜²æŠ¤"""
            canary_deployment.start_deployment()
            
            # å°è¯•å†æ¬¡å¯åŠ¨åŒä¸€ä¸ªéƒ¨ç½²å®žä¾‹
            with pytest.raises(ValueError, match="éƒ¨ç½²å·²ç»åœ¨è¿è¡Œä¸­"):
                canary_deployment.start_deployment()
    
        def test_deployment_metrics_collection(self, canary_deployment):
            """æµ‹è¯•éƒ¨ç½²æŒ‡æ ‡æ”¶é›†"""
            canary_deployment.start_deployment()
            
            # æ·»åŠ å¤šä¸ªæŒ‡æ ‡æ•°æ®ç‚¹
            for i in range(10):
                metrics = PerformanceMetrics(
                    success_rate=0.95 + i * 0.001,
                    error_rate=0.02 - i * 0.001,
                    avg_response_time=0.1 + i * 0.01,
                    throughput=1000 + i * 10,
                    accuracy=0.90 + i * 0.001,
                    precision=0.85 + i * 0.001,
                    recall=0.82 + i * 0.001,
                    f1_score=0.83 + i * 0.001
                )
                canary_deployment.update_canary_metrics(metrics)
            
            # éªŒè¯æŒ‡æ ‡åŽ†å²è®°å½•
            metrics_history = canary_deployment.get_metrics_history()
            assert len(metrics_history) == 10
            assert metrics_history[-1].success_rate > metrics_history[0].success_rate
    
        def test_deployment_configuration_validation(self):
            """æµ‹è¯•éƒ¨ç½²é…ç½®éªŒè¯"""
            # æµ‹è¯•æ— æ•ˆçš„é‡‘ä¸é›€ç™¾åˆ†æ¯”
            with pytest.raises(ValueError, match="é‡‘ä¸é›€æµé‡ç™¾åˆ†æ¯”å¿…é¡»åœ¨0-100ä¹‹é—´"):
                DeploymentConfig(
                    canary_percentage=150.0,  # æ— æ•ˆ
                    evaluation_period=3600,
                    success_threshold=0.95,
                    error_threshold=0.05,
                    performance_threshold=0.02,
                    rollback_threshold=0.1,
                    max_canary_duration=7200
                )
            
            # æµ‹è¯•æ— æ•ˆçš„é˜ˆå€¼
            with pytest.raises(ValueError, match="æˆåŠŸçŽ‡é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´"):
                DeploymentConfig(
                    canary_percentage=10.0,
                    evaluation_period=3600,
                    success_threshold=1.5,  # æ— æ•ˆ
                    error_threshold=0.05,
                    performance_threshold=0.02,
                    rollback_threshold=0.1,
                    max_canary_duration=7200
                )
    
    
    class TestABTestFramework:
        """A/Bæµ‹è¯•æ¡†æž¶æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def ab_test_framework(self, mock_model, mock_baseline_model):
            """åˆ›å»ºA/Bæµ‹è¯•æ¡†æž¶"""
            return ABTestFramework(
                model_a=mock_baseline_model,  # åŸºçº¿æ¨¡åž‹
                model_b=mock_model,           # æ–°æ¨¡åž‹
                traffic_split=0.5,            # 50/50åˆ†æµ
                minimum_sample_size=100,      # é™ä½Žæœ€å°æ ·æœ¬é‡ä»¥ä¾¿æµ‹è¯•
                confidence_level=0.95,
                test_duration=86400           # 24å°æ—¶
            )
    
        def test_ab_framework_initialization(self, ab_test_framework):
            """æµ‹è¯•A/Bæµ‹è¯•æ¡†æž¶åˆå§‹åŒ–"""
            assert ab_test_framework.model_a is not None
            assert ab_test_framework.model_b is not None
            assert ab_test_framework.traffic_split == 0.5
            assert ab_test_framework.minimum_sample_size == 100
            assert ab_test_framework.confidence_level == 0.95
            assert ab_test_framework.test_status == "pending"
    
        def test_traffic_routing(self, ab_test_framework):
            """æµ‹è¯•æµé‡è·¯ç”±"""
            # æ¨¡æ‹Ÿ1000æ¬¡è¯·æ±‚
            model_a_count = 0
            model_b_count = 0
            
            for i in range(1000):
                user_id = f"user_{i}"
                selected_model = ab_test_framework.route_traffic(user_id)
                
                if selected_model == ab_test_framework.model_a:
                    model_a_count += 1
                else:
                    model_b_count += 1
            
            # éªŒè¯æµé‡åˆ†é…æŽ¥è¿‘50/50
            total_requests = model_a_count + model_b_count
            model_a_ratio = model_a_count / total_requests
            model_b_ratio = model_b_count / total_requests
            
            assert abs(model_a_ratio - 0.5) < 0.1  # å…è®¸10%çš„åå·®
            assert abs(model_b_ratio - 0.5) < 0.1
    
        def test_consistent_user_routing(self, ab_test_framework):
            """æµ‹è¯•ç”¨æˆ·è·¯ç”±ä¸€è‡´æ€§"""
            user_id = "test_user_123"
            
            # åŒä¸€ç”¨æˆ·å¤šæ¬¡è¯·æ±‚åº”è¯¥è·¯ç”±åˆ°åŒä¸€æ¨¡åž‹
            first_model = ab_test_framework.route_traffic(user_id)
            for _ in range(10):
                current_model = ab_test_framework.route_traffic(user_id)
                assert current_model == first_model
    
        def test_experiment_data_collection(self, ab_test_framework):
            """æµ‹è¯•å®žéªŒæ•°æ®æ”¶é›†"""
            # æ¨¡æ‹ŸA/Bæµ‹è¯•æ•°æ®æ”¶é›†
            for i in range(100):
                user_id = f"user_{i}"
                selected_model = ab_test_framework.route_traffic(user_id)
                
                # æ¨¡æ‹Ÿé¢„æµ‹ç»“æžœå’Œå®žé™…ç»“æžœ
                prediction = 0.8 if selected_model == ab_test_framework.model_a else 0.9
                actual = 1.0 if i % 3 == 0 else 0.0  # æ¨¡æ‹Ÿå®žé™…ç»“æžœ
                
                ab_test_framework.record_result(user_id, selected_model, prediction, actual)
            
            # éªŒè¯æ•°æ®æ”¶é›†
            experiment_data = ab_test_framework.get_experiment_data()
            assert len(experiment_data) == 100
            assert all('user_id' in record for record in experiment_data)
            assert all('model' in record for record in experiment_data)
            assert all('prediction' in record for record in experiment_data)
            assert all('actual' in record for record in experiment_data)
    
        def test_statistical_significance_testing(self, ab_test_framework):
            """æµ‹è¯•ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
            # æ·»åŠ æ¨¡æ‹Ÿæ•°æ®
            np.random.seed(42)
            
            # æ¨¡åž‹Açš„æ•°æ®ï¼ˆè¾ƒå·®æ€§èƒ½ï¼‰
            for i in range(500):
                ab_test_framework.record_result(
                    f"user_a_{i}", 
                    ab_test_framework.model_a,
                    np.random.normal(0.7, 0.1),  # é¢„æµ‹
                    1.0 if np.random.random() > 0.4 else 0.0  # 60%æˆåŠŸçŽ‡
                )
            
            # æ¨¡åž‹Bçš„æ•°æ®ï¼ˆè¾ƒå¥½æ€§èƒ½ï¼‰
            for i in range(500):
                ab_test_framework.record_result(
                    f"user_b_{i}",
                    ab_test_framework.model_b,
                    np.random.normal(0.8, 0.1),  # é¢„æµ‹
                    1.0 if np.random.random() > 0.3 else 0.0  # 70%æˆåŠŸçŽ‡
                )
            
            # è¿›è¡Œç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
            significance_result = ab_test_framework.calculate_statistical_significance()
            
            assert isinstance(significance_result, dict)
            assert 'p_value' in significance_result
            assert 'is_significant' in significance_result
            assert 'confidence_interval' in significance_result
            assert 'effect_size' in significance_result
    
        def test_minimum_sample_size_check(self, ab_test_framework):
            """æµ‹è¯•æœ€å°æ ·æœ¬é‡æ£€æŸ¥"""
            # æ ·æœ¬é‡ä¸è¶³æ—¶ - åªæ·»åŠ å°‘é‡æ•°æ®åˆ°æ¯ä¸ªæ¨¡åž‹
            for i in range(50):  # å°‘äºŽminimum_sample_size
                ab_test_framework.record_result(f"user_a_{i}", ab_test_framework.model_a, 0.8, 1.0)
                ab_test_framework.record_result(f"user_b_{i}", ab_test_framework.model_b, 0.8, 1.0)
            
            has_sufficient_data = ab_test_framework.has_sufficient_sample_size()
            assert has_sufficient_data == False
            
            # æ·»åŠ æ›´å¤šæ•°æ®è¾¾åˆ°æœ€å°æ ·æœ¬é‡
            for i in range(50, 100):
                ab_test_framework.record_result(f"user_a_{i}", ab_test_framework.model_a, 0.8, 1.0)
                ab_test_framework.record_result(f"user_b_{i}", ab_test_framework.model_b, 0.8, 1.0)
            
            has_sufficient_data = ab_test_framework.has_sufficient_sample_size()
            assert has_sufficient_data == True
    
        def test_ab_test_completion(self, ab_test_framework):
            """æµ‹è¯•A/Bæµ‹è¯•å®Œæˆ"""
            # æ·»åŠ è¶³å¤Ÿçš„æ•°æ®
            for i in range(1000):
                model = ab_test_framework.model_a if i < 500 else ab_test_framework.model_b
                ab_test_framework.record_result(f"user_{i}", model, 0.8, 1.0)
            
            # è®¾ç½®æµ‹è¯•å¼€å§‹æ—¶é—´ä¸º24å°æ—¶å‰
            ab_test_framework.start_time = datetime.now() - timedelta(hours=25)
            
            # æ£€æŸ¥æµ‹è¯•æ˜¯å¦å®Œæˆ
            is_complete = ab_test_framework.is_test_complete()
            assert is_complete is True
    
        def test_winner_determination(self, ab_test_framework):
            """æµ‹è¯•èŽ·èƒœè€…ç¡®å®š"""
            np.random.seed(42)
            
            # å¼€å§‹æµ‹è¯•å¹¶è®¾ç½®ä¸ºå·²å®ŒæˆçŠ¶æ€
            ab_test_framework.start_test()
            ab_test_framework.start_time = datetime.now() - timedelta(hours=25)  # æ¨¡æ‹Ÿæµ‹è¯•å·²è¿è¡Œè¶…è¿‡24å°æ—¶
            
            # æ¨¡åž‹Aæ•°æ®ï¼ˆè¾ƒå·®æ€§èƒ½ï¼‰
            for i in range(150):
                success = 1.0 if np.random.random() > 0.4 else 0.0
                ab_test_framework.record_result(f"user_a_{i}", ab_test_framework.model_a, 0.7, success)
            
            # æ¨¡åž‹Bæ•°æ®ï¼ˆè¾ƒå¥½æ€§èƒ½ï¼‰
            for i in range(150):
                success = 1.0 if np.random.random() > 0.2 else 0.0
                ab_test_framework.record_result(f"user_b_{i}", ab_test_framework.model_b, 0.8, success)
            
            winner = ab_test_framework.determine_winner()
            
            assert winner is not None
            assert winner in [ab_test_framework.model_a, ab_test_framework.model_b]
    
    
    class TestModelPerformanceComparator:
        """æ¨¡åž‹æ€§èƒ½æ¯”è¾ƒå™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def performance_comparator(self):
            """åˆ›å»ºæ€§èƒ½æ¯”è¾ƒå™¨"""
            return ModelPerformanceComparator(
                comparison_window=3600,  # 1å°æ—¶
                min_samples_for_comparison=100,
                significance_threshold=0.05
            )
    
        @pytest.fixture
        def sample_metrics_a(self):
            """åˆ›å»ºæ ·æœ¬æŒ‡æ ‡A"""
            return [
                PerformanceMetrics(0.95, 0.02, 0.1, 1000, 0.90, 0.88, 0.85, 0.86),
                PerformanceMetrics(0.94, 0.03, 0.12, 980, 0.89, 0.87, 0.84, 0.85),
                PerformanceMetrics(0.96, 0.01, 0.09, 1020, 0.91, 0.89, 0.86, 0.87),
            ]
    
        @pytest.fixture
        def sample_metrics_b(self):
            """åˆ›å»ºæ ·æœ¬æŒ‡æ ‡B"""
            return [
                PerformanceMetrics(0.97, 0.01, 0.08, 1100, 0.92, 0.90, 0.88, 0.89),
                PerformanceMetrics(0.98, 0.01, 0.07, 1120, 0.93, 0.91, 0.89, 0.90),
                PerformanceMetrics(0.96, 0.02, 0.09, 1080, 0.91, 0.89, 0.87, 0.88),
            ]
    
        def test_comparator_initialization(self, performance_comparator):
            """æµ‹è¯•æ€§èƒ½æ¯”è¾ƒå™¨åˆå§‹åŒ–"""
            assert performance_comparator.comparison_window == 3600
            assert performance_comparator.min_samples_for_comparison == 100
            assert performance_comparator.significance_threshold == 0.05
            assert len(performance_comparator.model_a_metrics) == 0
            assert len(performance_comparator.model_b_metrics) == 0
    
        def test_metrics_collection(self, performance_comparator, sample_metrics_a, sample_metrics_b):
            """æµ‹è¯•æŒ‡æ ‡æ”¶é›†"""
            # æ·»åŠ æ¨¡åž‹Açš„æŒ‡æ ‡
            for metrics in sample_metrics_a:
                performance_comparator.add_model_a_metrics(metrics)
            
            # æ·»åŠ æ¨¡åž‹Bçš„æŒ‡æ ‡
            for metrics in sample_metrics_b:
                performance_comparator.add_model_b_metrics(metrics)
            
            assert len(performance_comparator.model_a_metrics) == 3
            assert len(performance_comparator.model_b_metrics) == 3
    
        def test_performance_comparison(self, performance_comparator, sample_metrics_a, sample_metrics_b):
            """æµ‹è¯•æ€§èƒ½æ¯”è¾ƒ"""
            # æ·»åŠ æŒ‡æ ‡æ•°æ®
            for metrics in sample_metrics_a:
                performance_comparator.add_model_a_metrics(metrics)
            for metrics in sample_metrics_b:
                performance_comparator.add_model_b_metrics(metrics)
            
            # è¿›è¡Œæ€§èƒ½æ¯”è¾ƒ
            comparison_result = performance_comparator.compare_performance()
            
            assert isinstance(comparison_result, dict)
            assert 'success_rate_diff' in comparison_result
            assert 'error_rate_diff' in comparison_result
            assert 'response_time_diff' in comparison_result
            assert 'throughput_diff' in comparison_result
            assert 'overall_performance_score' in comparison_result
    
        def test_statistical_significance(self, performance_comparator):
            """æµ‹è¯•ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ"""
            # åˆ›å»ºæœ‰æ˜¾è‘—å·®å¼‚çš„æ•°æ®
            np.random.seed(42)
            
            # æ¨¡åž‹Aæ•°æ®ï¼ˆè¾ƒå·®ï¼‰
            for _ in range(100):
                metrics = PerformanceMetrics(
                    success_rate=np.clip(np.random.normal(0.85, 0.05), 0, 1),
                    error_rate=np.clip(np.random.normal(0.05, 0.02), 0, 1),
                    avg_response_time=max(0, np.random.normal(0.15, 0.03)),
                    throughput=max(0, np.random.normal(900, 100)),
                    accuracy=np.clip(np.random.normal(0.85, 0.05), 0, 1),
                    precision=np.clip(np.random.normal(0.83, 0.05), 0, 1),
                    recall=np.clip(np.random.normal(0.82, 0.05), 0, 1),
                    f1_score=np.clip(np.random.normal(0.82, 0.05), 0, 1)
                )
                performance_comparator.add_model_a_metrics(metrics)
            
            # æ¨¡åž‹Bæ•°æ®ï¼ˆè¾ƒå¥½ï¼‰
            for _ in range(100):
                metrics = PerformanceMetrics(
                    success_rate=np.clip(np.random.normal(0.95, 0.03), 0, 1),
                    error_rate=np.clip(np.random.normal(0.02, 0.01), 0, 1),
                    avg_response_time=max(0, np.random.normal(0.10, 0.02)),
                    throughput=max(0, np.random.normal(1100, 80)),
                    accuracy=np.clip(np.random.normal(0.93, 0.03), 0, 1),
                    precision=np.clip(np.random.normal(0.91, 0.03), 0, 1),
                    recall=np.clip(np.random.normal(0.90, 0.03), 0, 1),
                    f1_score=np.clip(np.random.normal(0.90, 0.03), 0, 1)
                )
                performance_comparator.add_model_b_metrics(metrics)
            
            # è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§
            significance_test = performance_comparator.test_statistical_significance()
            
            assert isinstance(significance_test, dict)
            assert 'success_rate_significant' in significance_test
            assert 'error_rate_significant' in significance_test
            assert 'response_time_significant' in significance_test
    
        def test_performance_trend_analysis(self, performance_comparator):
            """æµ‹è¯•æ€§èƒ½è¶‹åŠ¿åˆ†æž"""
            # åˆ›å»ºè¶‹åŠ¿æ•°æ®
            base_time = datetime.now()
            
            for i in range(50):
                # æ¨¡åž‹Aæ€§èƒ½é€æ¸ä¸‹é™
                metrics_a = PerformanceMetrics(
                    success_rate=0.95 - i * 0.001,
                    error_rate=0.02 + i * 0.0005,
                    avg_response_time=0.1 + i * 0.001,
                    throughput=1000 - i * 2,
                    accuracy=0.90 - i * 0.001,
                    precision=0.88 - i * 0.001,
                    recall=0.85 - i * 0.001,
                    f1_score=0.86 - i * 0.001
                )
                metrics_a.timestamp = base_time + timedelta(minutes=i)
                performance_comparator.add_model_a_metrics(metrics_a)
                
                # æ¨¡åž‹Bæ€§èƒ½ç¨³å®š
                metrics_b = PerformanceMetrics(
                    success_rate=0.97,
                    error_rate=0.01,
                    avg_response_time=0.08,
                    throughput=1100,
                    accuracy=0.93,
                    precision=0.91,
                    recall=0.89,
                    f1_score=0.90
                )
                metrics_b.timestamp = base_time + timedelta(minutes=i)
                performance_comparator.add_model_b_metrics(metrics_b)
            
            # åˆ†æžæ€§èƒ½è¶‹åŠ¿
            trend_analysis = performance_comparator.analyze_performance_trends()
            
            assert isinstance(trend_analysis, dict)
            assert 'model_a_trend' in trend_analysis
            assert 'model_b_trend' in trend_analysis
            assert 'trend_significance' in trend_analysis
    
        def test_performance_degradation_detection(self, performance_comparator):
            """æµ‹è¯•æ€§èƒ½é€€åŒ–æ£€æµ‹"""
            # æ·»åŠ æ­£å¸¸åŸºçº¿æ•°æ® - éœ€è¦è‡³å°‘10ä¸ªä½œä¸ºåŸºçº¿æ¯”è¾ƒ
            for _ in range(10):
                metrics = PerformanceMetrics(0.95, 0.02, 0.1, 1000, 0.90, 0.88, 0.85, 0.86)
                performance_comparator.add_model_a_metrics(metrics)
            
            # æ·»åŠ æ€§èƒ½é€€åŒ–çš„æ•°æ® - æœ€è¿‘10ä¸ªæ•°æ®ç‚¹
            for _ in range(10):
                metrics = PerformanceMetrics(0.80, 0.10, 0.2, 500, 0.75, 0.73, 0.70, 0.71)
                performance_comparator.add_model_a_metrics(metrics)
            
            # æ£€æµ‹æ€§èƒ½é€€åŒ–
            degradation_detected = performance_comparator.detect_performance_degradation('model_a')
            assert degradation_detected == True
    
        def test_confidence_interval_calculation(self, performance_comparator, sample_metrics_a):
            """æµ‹è¯•ç½®ä¿¡åŒºé—´è®¡ç®—"""
            # æ·»åŠ æŒ‡æ ‡æ•°æ®
            for metrics in sample_metrics_a * 30:  # é‡å¤ä»¥èŽ·å¾—è¶³å¤Ÿæ ·æœ¬
                performance_comparator.add_model_a_metrics(metrics)
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            confidence_intervals = performance_comparator.calculate_confidence_intervals('model_a')
            
            assert isinstance(confidence_intervals, dict)
            assert 'success_rate' in confidence_intervals
            assert 'error_rate' in confidence_intervals
            assert all('lower' in ci and 'upper' in ci for ci in confidence_intervals.values())
    
    
    class TestDeploymentSafetyController:
        """éƒ¨ç½²å®‰å…¨æŽ§åˆ¶å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def safety_controller(self):
            """åˆ›å»ºéƒ¨ç½²å®‰å…¨æŽ§åˆ¶å™¨"""
            return DeploymentSafetyController(
                max_error_rate=0.05,
                max_response_time=0.5,
                min_success_rate=0.90,
                circuit_breaker_threshold=10,
                recovery_check_interval=300
            )
    
        @pytest.fixture
        def canary_deployment_mock(self, safety_controller):
            """åˆ›å»ºé‡‘ä¸é›€éƒ¨ç½²æ¨¡æ‹Ÿ"""
            deployment = Mock()
            deployment.canary_model = Mock()
            deployment.baseline_model = Mock()
            deployment.status = DeploymentStatus.ACTIVE
            deployment.traffic_percentage = 10.0
            return deployment
    
        def test_safety_controller_initialization(self, safety_controller):
            """æµ‹è¯•å®‰å…¨æŽ§åˆ¶å™¨åˆå§‹åŒ–"""
            assert safety_controller.max_error_rate == 0.05
            assert safety_controller.max_response_time == 0.5
            assert safety_controller.min_success_rate == 0.90
            assert safety_controller.circuit_breaker_threshold == 10
            assert safety_controller.is_circuit_breaker_open is False
    
        def test_safety_check_passing(self, safety_controller):
            """æµ‹è¯•å®‰å…¨æ£€æŸ¥é€šè¿‡"""
            # æ­£å¸¸çš„æŒ‡æ ‡
            metrics = PerformanceMetrics(
                success_rate=0.95,
                error_rate=0.02,
                avg_response_time=0.1,
                throughput=1000,
                accuracy=0.90,
                precision=0.88,
                recall=0.85,
                f1_score=0.86
            )
            
            safety_check_result = safety_controller.perform_safety_check(metrics)
            
            assert safety_check_result['passed'] is True
            assert len(safety_check_result['violations']) == 0
    
        def test_safety_check_failure(self, safety_controller):
            """æµ‹è¯•å®‰å…¨æ£€æŸ¥å¤±è´¥"""
            # å¼‚å¸¸çš„æŒ‡æ ‡
            metrics = PerformanceMetrics(
                success_rate=0.80,  # ä½ŽäºŽé˜ˆå€¼
                error_rate=0.10,    # é«˜äºŽé˜ˆå€¼
                avg_response_time=0.8,  # é«˜äºŽé˜ˆå€¼
                throughput=500,
                accuracy=0.75,
                precision=0.70,
                recall=0.68,
                f1_score=0.69
            )
            
            safety_check_result = safety_controller.perform_safety_check(metrics)
            
            assert safety_check_result['passed'] is False
            assert len(safety_check_result['violations']) > 0
            assert 'success_rate' in safety_check_result['violations']
            assert 'error_rate' in safety_check_result['violations']
            assert 'response_time' in safety_check_result['violations']
    
        def test_circuit_breaker_activation(self, safety_controller):
            """æµ‹è¯•ç†”æ–­å™¨æ¿€æ´»"""
            # è¿žç»­å¤±è´¥å¤šæ¬¡è§¦å‘ç†”æ–­å™¨
            bad_metrics = PerformanceMetrics(0.70, 0.15, 1.0, 300, 0.65, 0.60, 0.58, 0.59)
            
            for _ in range(15):  # è¶…è¿‡circuit_breaker_threshold
                safety_controller.perform_safety_check(bad_metrics)
            
            assert safety_controller.is_circuit_breaker_open is True
    
        def test_circuit_breaker_recovery(self, safety_controller):
            """æµ‹è¯•ç†”æ–­å™¨æ¢å¤"""
            # å…ˆè§¦å‘ç†”æ–­å™¨
            bad_metrics = PerformanceMetrics(0.70, 0.15, 1.0, 300, 0.65, 0.60, 0.58, 0.59)
            for _ in range(15):
                safety_controller.perform_safety_check(bad_metrics)
            
            assert safety_controller.is_circuit_breaker_open is True
            
            # ç­‰å¾…æ¢å¤æ£€æŸ¥é—´éš”
            safety_controller.last_circuit_check = datetime.now() - timedelta(seconds=400)
            
            # æä¾›å¥½çš„æŒ‡æ ‡å°è¯•æ¢å¤
            good_metrics = PerformanceMetrics(0.95, 0.02, 0.1, 1000, 0.90, 0.88, 0.85, 0.86)
            safety_controller.attempt_circuit_recovery(good_metrics)
            
            assert safety_controller.is_circuit_breaker_open is False
    
        def test_rollback_decision(self, safety_controller, canary_deployment_mock):
            """æµ‹è¯•å›žæ»šå†³ç­–"""
            # æ¨¡æ‹Ÿä¸¥é‡çš„å®‰å…¨è¿è§„
            bad_metrics = PerformanceMetrics(0.60, 0.25, 2.0, 200, 0.55, 0.50, 0.48, 0.49)
            
            rollback_decision = safety_controller.should_rollback_deployment(
                canary_deployment_mock, bad_metrics
            )
            
            assert rollback_decision['should_rollback'] is True
            assert 'reason' in rollback_decision
            assert len(rollback_decision['safety_violations']) > 0
    
        def test_gradual_traffic_control(self, safety_controller, canary_deployment_mock):
            """æµ‹è¯•æ¸è¿›å¼æµé‡æŽ§åˆ¶"""
            # è½»å¾®çš„æ€§èƒ½é—®é¢˜ï¼Œå»ºè®®å‡å°‘æµé‡è€Œä¸æ˜¯å®Œå…¨å›žæ»š
            moderate_metrics = PerformanceMetrics(0.88, 0.06, 0.3, 800, 0.82, 0.80, 0.78, 0.79)
            
            traffic_decision = safety_controller.evaluate_traffic_adjustment(
                canary_deployment_mock, moderate_metrics
            )
            
            assert traffic_decision['action'] in ['maintain', 'reduce', 'increase']
            assert 'recommended_percentage' in traffic_decision
    
        def test_safety_metrics_aggregation(self, safety_controller):
            """æµ‹è¯•å®‰å…¨æŒ‡æ ‡èšåˆ"""
            # æ·»åŠ å¤šä¸ªæŒ‡æ ‡æ•°æ®ç‚¹
            metrics_list = []
            for i in range(10):
                metrics = PerformanceMetrics(
                    success_rate=0.90 + i * 0.01,
                    error_rate=0.05 - i * 0.003,
                    avg_response_time=0.1 + i * 0.01,
                    throughput=1000 + i * 10,
                    accuracy=0.85 + i * 0.01,
                    precision=0.83 + i * 0.01,
                    recall=0.80 + i * 0.01,
                    f1_score=0.81 + i * 0.01
                )
                metrics_list.append(metrics)
                safety_controller.add_metrics_for_analysis(metrics)
            
            # èšåˆå®‰å…¨æŒ‡æ ‡
            aggregated_metrics = safety_controller.get_aggregated_safety_metrics()
            
            assert isinstance(aggregated_metrics, dict)
            assert 'avg_success_rate' in aggregated_metrics
            assert 'avg_error_rate' in aggregated_metrics
            assert 'avg_response_time' in aggregated_metrics
            assert 'percentile_95_response_time' in aggregated_metrics
    
        def test_emergency_stop_mechanism(self, safety_controller, canary_deployment_mock):
            """æµ‹è¯•ç´§æ€¥åœæ­¢æœºåˆ¶"""
            # æžå…¶ä¸¥é‡çš„æŒ‡æ ‡è§¦å‘ç´§æ€¥åœæ­¢
            critical_metrics = PerformanceMetrics(0.30, 0.50, 5.0, 50, 0.25, 0.20, 0.18, 0.19)
            
            emergency_decision = safety_controller.evaluate_emergency_stop(
                canary_deployment_mock, critical_metrics
            )
            
            assert emergency_decision['emergency_stop'] is True
            assert 'critical_violations' in emergency_decision
            assert len(emergency_decision['critical_violations']) > 0
    
    
    class TestRollbackManager:
        """å›žæ»šç®¡ç†å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def rollback_manager(self):
            """åˆ›å»ºå›žæ»šç®¡ç†å™¨"""
            return RollbackManager(
                rollback_timeout=300,  # 5åˆ†é’Ÿ
                verification_checks=5,
                health_check_interval=30
            )
    
        @pytest.fixture
        def deployment_mock(self):
            """åˆ›å»ºéƒ¨ç½²æ¨¡æ‹Ÿ"""
            deployment = Mock()
            deployment.canary_model = Mock()
            deployment.baseline_model = Mock()
            deployment.status = DeploymentStatus.ACTIVE
            deployment.traffic_percentage = 50.0
            deployment.deployment_id = str(uuid.uuid4())
            return deployment
    
        def test_rollback_manager_initialization(self, rollback_manager):
            """æµ‹è¯•å›žæ»šç®¡ç†å™¨åˆå§‹åŒ–"""
            assert rollback_manager.rollback_timeout == 300
            assert rollback_manager.verification_checks == 5
            assert rollback_manager.health_check_interval == 30
            assert len(rollback_manager.rollback_history) == 0
    
        def test_rollback_execution(self, rollback_manager, deployment_mock):
            """æµ‹è¯•å›žæ»šæ‰§è¡Œ"""
            rollback_reason = "æ€§èƒ½ä¸¥é‡ä¸‹é™"
            
            rollback_result = rollback_manager.execute_rollback(deployment_mock, rollback_reason)
            
            assert rollback_result['success'] is True
            assert rollback_result['rollback_id'] is not None
            assert rollback_result['timestamp'] is not None
            assert len(rollback_manager.rollback_history) == 1
    
        def test_rollback_verification(self, rollback_manager, deployment_mock):
            """æµ‹è¯•å›žæ»šéªŒè¯"""
            # æ‰§è¡Œå›žæ»š
            rollback_result = rollback_manager.execute_rollback(deployment_mock, "æµ‹è¯•å›žæ»š")
            rollback_id = rollback_result['rollback_id']
            
            # æ¨¡æ‹ŸéªŒè¯æ£€æŸ¥
            with patch.object(rollback_manager, '_perform_health_check', return_value=True):
                verification_result = rollback_manager.verify_rollback(rollback_id)
            
            assert verification_result['verified'] is True
            assert verification_result['checks_passed'] == rollback_manager.verification_checks
    
        def test_rollback_failure_handling(self, rollback_manager, deployment_mock):
            """æµ‹è¯•å›žæ»šå¤±è´¥å¤„ç†"""
            # æ¨¡æ‹Ÿå›žæ»šå¤±è´¥
            with patch.object(rollback_manager, '_execute_traffic_rollback', side_effect=Exception("å›žæ»šå¤±è´¥")):
                rollback_result = rollback_manager.execute_rollback(deployment_mock, "æµ‹è¯•å¤±è´¥")
            
            assert rollback_result['success'] is False
            assert 'error' in rollback_result
    
        def test_partial_rollback(self, rollback_manager, deployment_mock):
            """æµ‹è¯•éƒ¨åˆ†å›žæ»š"""
            # æ‰§è¡Œéƒ¨åˆ†å›žæ»šï¼ˆå‡å°‘æµé‡è€Œä¸æ˜¯å®Œå…¨å›žæ»šï¼‰
            target_percentage = 10.0
            
            partial_rollback_result = rollback_manager.execute_partial_rollback(
                deployment_mock, target_percentage, "æ€§èƒ½è½»å¾®ä¸‹é™"
            )
            
            assert partial_rollback_result['success'] is True
            assert partial_rollback_result['new_traffic_percentage'] == target_percentage
    
        def test_rollback_history_management(self, rollback_manager, deployment_mock):
            """æµ‹è¯•å›žæ»šåŽ†å²ç®¡ç†"""
            # æ‰§è¡Œå¤šæ¬¡å›žæ»š
            for i in range(3):
                rollback_manager.execute_rollback(deployment_mock, f"å›žæ»šåŽŸå›  {i+1}")
            
            # èŽ·å–å›žæ»šåŽ†å²
            history = rollback_manager.get_rollback_history(deployment_mock.deployment_id)
            
            assert len(history) == 3
            assert all('rollback_id' in record for record in history)
            assert all('reason' in record for record in history)
            assert all('timestamp' in record for record in history)
    
        def test_automated_rollback_trigger(self, rollback_manager, deployment_mock):
            """æµ‹è¯•è‡ªåŠ¨å›žæ»šè§¦å‘"""
            # è®¾ç½®è‡ªåŠ¨å›žæ»šæ¡ä»¶
            rollback_conditions = {
                'max_error_rate': 0.10,
                'min_success_rate': 0.85,
                'max_response_time': 1.0
            }
            
            rollback_manager.set_auto_rollback_conditions(rollback_conditions)
            
            # æ¨¡æ‹Ÿè§¦å‘è‡ªåŠ¨å›žæ»šçš„æŒ‡æ ‡
            bad_metrics = PerformanceMetrics(0.80, 0.15, 1.5, 400, 0.75, 0.70, 0.68, 0.69)
            
            should_auto_rollback = rollback_manager.should_trigger_auto_rollback(bad_metrics)
            assert should_auto_rollback is True
    
        def test_rollback_timeout_handling(self, rollback_manager, deployment_mock):
            """æµ‹è¯•å›žæ»šè¶…æ—¶å¤„ç†"""
            # æ¨¡æ‹Ÿè¶…æ—¶çš„å›žæ»š
            with patch.object(rollback_manager, '_execute_traffic_rollback') as mock_rollback:
                mock_rollback.side_effect = lambda *args: time.sleep(400)  # è¶…è¿‡timeout
                
                rollback_result = rollback_manager.execute_rollback(deployment_mock, "è¶…æ—¶æµ‹è¯•")
            
            # æ³¨æ„ï¼šè¿™ä¸ªæµ‹è¯•éœ€è¦å®žé™…çš„è¶…æ—¶æœºåˆ¶å®žçŽ°
            # è¿™é‡Œæˆ‘ä»¬éªŒè¯rollback_managerèƒ½å¤„ç†è¶…æ—¶æƒ…å†µ
            assert 'timeout' in str(rollback_result).lower() or rollback_result.get('success') is False
    
    class TestTrafficRouter:
        """æµé‡è·¯ç”±å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def traffic_router(self):
            """åˆ›å»ºæµé‡è·¯ç”±å™¨"""
            return TrafficRouter(
                canary_percentage=20.0,
                routing_strategy='weighted_random',
                sticky_sessions=True
            )
    
        @pytest.fixture
        def models_mock(self):
            """åˆ›å»ºæ¨¡åž‹æ¨¡æ‹Ÿ"""
            canary_model = Mock()
            canary_model.version = "v2.0.0"
            baseline_model = Mock()
            baseline_model.version = "v1.0.0"
            return canary_model, baseline_model
    
        def test_traffic_router_initialization(self, traffic_router):
            """æµ‹è¯•æµé‡è·¯ç”±å™¨åˆå§‹åŒ–"""
            assert traffic_router.canary_percentage == 20.0
            assert traffic_router.routing_strategy == 'weighted_random'
            assert traffic_router.sticky_sessions is True
            assert len(traffic_router.user_assignments) == 0
    
        def test_weighted_random_routing(self, traffic_router, models_mock):
            """æµ‹è¯•åŠ æƒéšæœºè·¯ç”±"""
            canary_model, baseline_model = models_mock
            
            canary_count = 0
            baseline_count = 0
            total_requests = 1000
            
            for i in range(total_requests):
                user_id = f"user_{i}"
                selected_model = traffic_router.route_request(user_id, canary_model, baseline_model)
                
                if selected_model == canary_model:
                    canary_count += 1
                else:
                    baseline_count += 1
            
            # éªŒè¯æµé‡åˆ†é…æŽ¥è¿‘é…ç½®çš„ç™¾åˆ†æ¯”
            canary_ratio = canary_count / total_requests
            expected_ratio = traffic_router.canary_percentage / 100.0
            
            assert abs(canary_ratio - expected_ratio) < 0.05  # å…è®¸5%çš„åå·®
    
        def test_sticky_session_consistency(self, traffic_router, models_mock):
            """æµ‹è¯•ç²˜æ€§ä¼šè¯ä¸€è‡´æ€§"""
            canary_model, baseline_model = models_mock
            user_id = "consistent_user"
            
            # åŒä¸€ç”¨æˆ·çš„å¤šæ¬¡è¯·æ±‚åº”è¯¥è·¯ç”±åˆ°åŒä¸€æ¨¡åž‹
            first_model = traffic_router.route_request(user_id, canary_model, baseline_model)
            
            for _ in range(10):
                current_model = traffic_router.route_request(user_id, canary_model, baseline_model)
                assert current_model == first_model
    
        def test_traffic_percentage_adjustment(self, traffic_router, models_mock):
            """æµ‹è¯•æµé‡ç™¾åˆ†æ¯”è°ƒæ•´"""
            canary_model, baseline_model = models_mock
            
            # è°ƒæ•´é‡‘ä¸é›€æµé‡ç™¾åˆ†æ¯”
            new_percentage = 50.0
            traffic_router.update_canary_percentage(new_percentage)
            
            assert traffic_router.canary_percentage == new_percentage
            
            # éªŒè¯æ–°çš„æµé‡åˆ†é…
            canary_count = 0
            total_requests = 1000
            
            for i in range(total_requests):
                user_id = f"user_{i}"
                selected_model = traffic_router.route_request(user_id, canary_model, baseline_model)
                
                if selected_model == canary_model:
                    canary_count += 1
            
            canary_ratio = canary_count / total_requests
            expected_ratio = new_percentage / 100.0
            
            assert abs(canary_ratio - expected_ratio) < 0.05
    
        def test_geographic_routing(self, traffic_router, models_mock):
            """æµ‹è¯•åœ°ç†ä½ç½®è·¯ç”±"""
            canary_model, baseline_model = models_mock
            
            # è®¾ç½®åœ°ç†è·¯ç”±ç­–ç•¥
            traffic_router.set_geographic_routing({
                'us-east': 30.0,    # ç¾Žå›½ä¸œéƒ¨30%é‡‘ä¸é›€æµé‡
                'us-west': 20.0,    # ç¾Žå›½è¥¿éƒ¨20%é‡‘ä¸é›€æµé‡
                'europe': 10.0,     # æ¬§æ´²10%é‡‘ä¸é›€æµé‡
                'asia': 5.0         # äºšæ´²5%é‡‘ä¸é›€æµé‡
            })
            
            # æµ‹è¯•ä¸åŒåœ°åŒºçš„æµé‡åˆ†é…
            regions = ['us-east', 'us-west', 'europe', 'asia']
            
            for region in regions:
                canary_count = 0
                total_requests = 200
                
                for i in range(total_requests):
                    user_id = f"{region}_user_{i}"
                    selected_model = traffic_router.route_request(
                        user_id, canary_model, baseline_model, region=region
                    )
                    
                    if selected_model == canary_model:
                        canary_count += 1
                
                canary_ratio = canary_count / total_requests
                expected_ratio = traffic_router.geographic_config[region] / 100.0
                
                assert abs(canary_ratio - expected_ratio) < 0.1  # å…è®¸10%çš„åå·®
    
        def test_traffic_routing_metrics(self, traffic_router, models_mock):
            """æµ‹è¯•æµé‡è·¯ç”±æŒ‡æ ‡"""
            canary_model, baseline_model = models_mock
            
            # ç”Ÿæˆä¸€äº›æµé‡
            for i in range(100):
                user_id = f"user_{i}"
                traffic_router.route_request(user_id, canary_model, baseline_model)
            
            # èŽ·å–è·¯ç”±æŒ‡æ ‡
            routing_metrics = traffic_router.get_routing_metrics()
            
            assert isinstance(routing_metrics, dict)
            assert 'total_requests' in routing_metrics
            assert 'canary_requests' in routing_metrics
            assert 'baseline_requests' in routing_metrics
            assert 'canary_percentage_actual' in routing_metrics
            assert routing_metrics['total_requests'] == 100
    
        def test_load_balancing_fairness(self, traffic_router, models_mock):
            """æµ‹è¯•è´Ÿè½½å‡è¡¡å…¬å¹³æ€§"""
            canary_model, baseline_model = models_mock
            
            # è®¾ç½®å¤šä¸ªcanaryæ¨¡åž‹å®žä¾‹ï¼ˆæ¨¡æ‹Ÿè´Ÿè½½å‡è¡¡ï¼‰
            canary_instances = [Mock() for _ in range(3)]
            for i, instance in enumerate(canary_instances):
                instance.version = f"v2.0.0-instance-{i}"
            
            traffic_router.set_canary_instances(canary_instances)
            
            instance_counts = {instance.version: 0 for instance in canary_instances}
            
            # è·¯ç”±åˆ°é‡‘ä¸é›€æ¨¡åž‹çš„è¯·æ±‚
            for i in range(300):
                user_id = f"user_{i}"
                selected_model = traffic_router.route_request(user_id, canary_instances, baseline_model)
                
                if selected_model in canary_instances:
                    instance_counts[selected_model.version] += 1
            
            # éªŒè¯è´Ÿè½½åœ¨å®žä¾‹é—´çš„åˆ†å¸ƒç›¸å¯¹å‡åŒ€
            counts = list(instance_counts.values())
            if sum(counts) > 0:  # åªæœ‰å½“æœ‰é‡‘ä¸é›€æµé‡æ—¶æ‰æ£€æŸ¥
                avg_count = sum(counts) / len(counts)
                for count in counts:
                    assert abs(count - avg_count) / avg_count < 0.3  # å…è®¸30%çš„åå·®
    
        def test_emergency_traffic_cutoff(self, traffic_router, models_mock):
            """æµ‹è¯•ç´§æ€¥æµé‡åˆ‡æ–­"""
            canary_model, baseline_model = models_mock
            
            # æ­£å¸¸è·¯ç”±
            normal_user = "normal_user"
            selected_model = traffic_router.route_request(normal_user, canary_model, baseline_model)
            
            # è§¦å‘ç´§æ€¥åˆ‡æ–­
            traffic_router.emergency_cutoff_canary()
            
            # æ‰€æœ‰æµé‡åº”è¯¥è·¯ç”±åˆ°åŸºçº¿æ¨¡åž‹
            for i in range(50):
                user_id = f"emergency_user_{i}"
                selected_model = traffic_router.route_request(user_id, canary_model, baseline_model)
                assert selected_model == baseline_model
            
            # æ¢å¤æ­£å¸¸è·¯ç”±
            traffic_router.restore_normal_routing()
            
            # éªŒè¯æ¢å¤åŽçš„è·¯ç”±
            canary_count = 0
            for i in range(100):
                user_id = f"recovery_user_{i}"
                selected_model = traffic_router.route_request(user_id, canary_model, baseline_model)
                if selected_model == canary_model:
                    canary_count += 1
            
            # åº”è¯¥æ¢å¤åˆ°é…ç½®çš„é‡‘ä¸é›€ç™¾åˆ†æ¯”
            canary_ratio = canary_count / 100
            expected_ratio = traffic_router.canary_percentage / 100.0
            assert abs(canary_ratio - expected_ratio) < 0.1
    ]]></file>
  <file path="tests/unit/test_audit_logger.py"><![CDATA[
    """
    å®¡è®¡æ—¥å¿—ç³»ç»Ÿæµ‹è¯•ç”¨ä¾‹
    æµ‹è¯•äº¤æ˜“å†³ç­–è®°å½•ã€å­˜å‚¨æœºåˆ¶ã€æŸ¥è¯¢æŽ¥å£å’Œæ•°æ®å®Œæ•´æ€§
    """
    
    import pytest
    import asyncio
    from datetime import datetime, timedelta
    from unittest.mock import Mock, AsyncMock, patch, MagicMock
    import json
    import numpy as np
    import time
    from typing import Dict, List, Any
    import uuid
    
    from src.rl_trading_system.audit.audit_logger import (
        AuditLogger, AuditRecord, DecisionRecord, ComplianceReport,
        AuditQueryInterface, DataRetentionManager, InfluxDBInterface, PostgreSQLInterface
    )
    from src.rl_trading_system.data.data_models import (
        TradingState, TradingAction, TransactionRecord
    )
    
    
    class TestAuditRecord:
        """å®¡è®¡è®°å½•æµ‹è¯•"""
        
        def test_audit_record_creation(self):
            """æµ‹è¯•å®¡è®¡è®°å½•åˆ›å»º"""
            record = AuditRecord(
                record_id="test_001",
                timestamp=datetime.now(),
                event_type="trading_decision",
                user_id="system",
                session_id="session_001",
                model_version="v1.0.0",
                data={"action": "buy", "symbol": "000001.SZ"},
                metadata={"confidence": 0.85}
            )
            
            assert record.record_id == "test_001"
            assert record.event_type == "trading_decision"
            assert record.user_id == "system"
            assert record.model_version == "v1.0.0"
            assert record.data["action"] == "buy"
            assert record.metadata["confidence"] == 0.85
        
        def test_audit_record_validation(self):
            """æµ‹è¯•å®¡è®¡è®°å½•éªŒè¯"""
            # æµ‹è¯•æ— æ•ˆäº‹ä»¶ç±»åž‹
            with pytest.raises(ValueError, match="æ— æ•ˆçš„äº‹ä»¶ç±»åž‹"):
                AuditRecord(
                    record_id="test_001",
                    timestamp=datetime.now(),
                    event_type="invalid_type",
                    user_id="system",
                    session_id="session_001",
                    model_version="v1.0.0",
                    data={},
                    metadata={}
                )
        
        def test_audit_record_serialization(self):
            """æµ‹è¯•å®¡è®¡è®°å½•åºåˆ—åŒ–"""
            record = AuditRecord(
                record_id="test_001",
                timestamp=datetime.now(),
                event_type="trading_decision",
                user_id="system",
                session_id="session_001",
                model_version="v1.0.0",
                data={"action": "buy"},
                metadata={"confidence": 0.85}
            )
            
            # æµ‹è¯•è½¬æ¢ä¸ºå­—å…¸
            record_dict = record.to_dict()
            assert record_dict["record_id"] == "test_001"
            assert record_dict["event_type"] == "trading_decision"
            
            # æµ‹è¯•ä»Žå­—å…¸åˆ›å»º
            restored_record = AuditRecord.from_dict(record_dict)
            assert restored_record.record_id == record.record_id
            assert restored_record.event_type == record.event_type
            
            # æµ‹è¯•JSONåºåˆ—åŒ–
            json_str = record.to_json()
            restored_from_json = AuditRecord.from_json(json_str)
            assert restored_from_json.record_id == record.record_id
    
    
    class TestDecisionRecord:
        """å†³ç­–è®°å½•æµ‹è¯•"""
        
        def test_decision_record_creation(self):
            """æµ‹è¯•å†³ç­–è®°å½•åˆ›å»º"""
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            state = TradingState(
                features=np.random.randn(60, 10, 50),
                positions=np.array([0.1, 0.2, 0.3, 0.4]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.15, 0.25, 0.35, 0.25]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            decision_record = DecisionRecord(
                decision_id="decision_001",
                timestamp=datetime.now(),
                model_version="v1.0.0",
                input_state=state,
                output_action=action,
                model_outputs={"q_values": [0.1, 0.2, 0.3, 0.4]},
                feature_importance={"rsi": 0.3, "macd": 0.2, "volume": 0.5},
                risk_metrics={"concentration": 0.25, "volatility": 0.15}
            )
            
            assert decision_record.decision_id == "decision_001"
            assert decision_record.model_version == "v1.0.0"
            assert decision_record.input_state == state
            assert decision_record.output_action == action
            assert decision_record.model_outputs["q_values"] == [0.1, 0.2, 0.3, 0.4]
        
        def test_decision_record_validation(self):
            """æµ‹è¯•å†³ç­–è®°å½•éªŒè¯"""
            state = TradingState(
                features=np.random.randn(60, 10, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.25, 0.25, 0.25, 0.25]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            # æµ‹è¯•æ­£å¸¸åˆ›å»º
            decision_record = DecisionRecord(
                decision_id="decision_001",
                timestamp=datetime.now(),
                model_version="v1.0.0",
                input_state=state,
                output_action=action,
                model_outputs={},
                feature_importance={},
                risk_metrics={}
            )
            
            assert decision_record.decision_id == "decision_001"
        
        def test_decision_record_serialization(self):
            """æµ‹è¯•å†³ç­–è®°å½•åºåˆ—åŒ–"""
            state = TradingState(
                features=np.random.randn(60, 4, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            decision_record = DecisionRecord(
                decision_id="decision_001",
                timestamp=datetime.now(),
                model_version="v1.0.0",
                input_state=state,
                output_action=action,
                model_outputs={"q_values": [0.1, 0.2, 0.3, 0.4]},
                feature_importance={"rsi": 0.3, "macd": 0.7},
                risk_metrics={"concentration": 0.25}
            )
            
            # æµ‹è¯•è½¬æ¢ä¸ºå­—å…¸
            record_dict = decision_record.to_dict()
            assert record_dict["decision_id"] == "decision_001"
            assert "input_state" in record_dict
            assert "output_action" in record_dict
            
            # æµ‹è¯•ä»Žå­—å…¸åˆ›å»º
            restored_record = DecisionRecord.from_dict(record_dict)
            assert restored_record.decision_id == decision_record.decision_id
            assert np.array_equal(restored_record.input_state.positions, 
                                 decision_record.input_state.positions)
    
    
    class TestAuditLogger:
        """å®¡è®¡æ—¥å¿—å™¨æµ‹è¯•"""
        
        @pytest.fixture
        def mock_timeseries_db(self):
            """æ¨¡æ‹Ÿæ—¶åºæ•°æ®åº“"""
            return Mock()
        
        @pytest.fixture
        def mock_relational_db(self):
            """æ¨¡æ‹Ÿå…³ç³»æ•°æ®åº“"""
            mock_db = Mock()
            # è®¾ç½®å¼‚æ­¥æ–¹æ³•ä¸ºAsyncMock
            mock_db.write_decision_record = AsyncMock()
            mock_db.write_records = AsyncMock()
            mock_db.query_records = AsyncMock()
            mock_db.get_decision_record = AsyncMock()
            mock_db.connect = AsyncMock()
            mock_db.disconnect = AsyncMock()
            return mock_db
        
        @pytest.fixture
        def audit_logger(self, mock_timeseries_db, mock_relational_db):
            """åˆ›å»ºå®¡è®¡æ—¥å¿—å™¨å®žä¾‹"""
            config = {
                'timeseries_db_url': 'influxdb://localhost:8086/audit',
                'relational_db_url': 'postgresql://localhost:5432/audit',
                'retention_days': 1825,  # 5å¹´
                'batch_size': 100,
                'flush_interval': 60
            }
            
            logger = AuditLogger(config)
            logger.timeseries_db = mock_timeseries_db
            logger.relational_db = mock_relational_db
            return logger
        
        def test_audit_logger_initialization(self, audit_logger):
            """æµ‹è¯•å®¡è®¡æ—¥å¿—å™¨åˆå§‹åŒ–"""
            assert audit_logger.config['retention_days'] == 1825
            assert audit_logger.config['batch_size'] == 100
            assert audit_logger.batch_records == []
            assert audit_logger.is_running is False
        
        @pytest.mark.asyncio
        async def test_log_trading_decision(self, audit_logger):
            """æµ‹è¯•è®°å½•äº¤æ˜“å†³ç­–"""
            state = TradingState(
                features=np.random.randn(60, 4, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            # æ¨¡æ‹Ÿå¼‚æ­¥å†™å…¥
            audit_logger.timeseries_db.write_records = AsyncMock()
            audit_logger.relational_db.write_decision_record = AsyncMock()
            
            await audit_logger.log_trading_decision(
                session_id="session_001",
                model_version="v1.0.0",
                input_state=state,
                output_action=action,
                model_outputs={"q_values": [0.1, 0.2, 0.3, 0.4]},
                feature_importance={"rsi": 0.3, "macd": 0.7}
            )
            
            # éªŒè¯è®°å½•è¢«æ·»åŠ åˆ°æ‰¹æ¬¡ä¸­
            assert len(audit_logger.batch_records) == 1
            record = audit_logger.batch_records[0]
            assert record.event_type == "trading_decision"
            assert record.session_id == "session_001"
            assert record.model_version == "v1.0.0"
        
        @pytest.mark.asyncio
        async def test_log_transaction_execution(self, audit_logger):
            """æµ‹è¯•è®°å½•äº¤æ˜“æ‰§è¡Œ"""
            transaction = TransactionRecord(
                timestamp=datetime.now(),
                symbol="000001.SZ",
                action_type="buy",
                quantity=1000,
                price=10.5,
                commission=10.5,
                stamp_tax=0.0,
                slippage=5.25,
                total_cost=15.75
            )
            
            audit_logger.timeseries_db.write_records = AsyncMock()
            audit_logger.relational_db.write_records = AsyncMock()
            
            await audit_logger.log_transaction_execution(
                session_id="session_001",
                transaction=transaction,
                execution_details={"order_id": "order_001", "fill_ratio": 1.0}
            )
            
            # éªŒè¯è®°å½•è¢«æ·»åŠ 
            assert len(audit_logger.batch_records) == 1
            record = audit_logger.batch_records[0]
            assert record.event_type == "transaction_execution"
            assert record.data["symbol"] == "000001.SZ"
            assert record.data["action_type"] == "buy"
        
        @pytest.mark.asyncio
        async def test_batch_flush(self, audit_logger):
            """æµ‹è¯•æ‰¹é‡åˆ·æ–°"""
            # æ·»åŠ ä¸€äº›è®°å½•åˆ°æ‰¹æ¬¡ä¸­
            for i in range(5):
                record = AuditRecord(
                    record_id=f"test_{i}",
                    timestamp=datetime.now(),
                    event_type="trading_decision",
                    user_id="system",
                    session_id="session_001",
                    model_version="v1.0.0",
                    data={"test": i},
                    metadata={}
                )
                audit_logger.batch_records.append(record)
            
            audit_logger.timeseries_db.write_records = AsyncMock()
            audit_logger.relational_db.write_records = AsyncMock()
            
            await audit_logger._flush_batch()
            
            # éªŒè¯æ•°æ®åº“å†™å…¥è¢«è°ƒç”¨
            audit_logger.timeseries_db.write_records.assert_called_once()
            audit_logger.relational_db.write_records.assert_called_once()
            
            # éªŒè¯æ‰¹æ¬¡è¢«æ¸…ç©º
            assert len(audit_logger.batch_records) == 0
        
        def test_generate_record_id(self, audit_logger):
            """æµ‹è¯•è®°å½•IDç”Ÿæˆ"""
            record_id = audit_logger._generate_record_id()
            assert isinstance(record_id, str)
            assert len(record_id) > 0
            
            # æµ‹è¯•IDå”¯ä¸€æ€§
            record_id2 = audit_logger._generate_record_id()
            assert record_id != record_id2
    
    
    class TestAuditQueryInterface:
        """å®¡è®¡æŸ¥è¯¢æŽ¥å£æµ‹è¯•"""
        
        @pytest.fixture
        def mock_timeseries_db(self):
            """æ¨¡æ‹Ÿæ—¶åºæ•°æ®åº“"""
            return Mock()
        
        @pytest.fixture
        def mock_relational_db(self):
            """æ¨¡æ‹Ÿå…³ç³»æ•°æ®åº“"""
            mock_db = Mock()
            # è®¾ç½®å¼‚æ­¥æ–¹æ³•ä¸ºAsyncMock
            mock_db.write_decision_record = AsyncMock()
            mock_db.write_records = AsyncMock()
            mock_db.query_records = AsyncMock()
            mock_db.get_decision_record = AsyncMock()
            mock_db.connect = AsyncMock()
            mock_db.disconnect = AsyncMock()
            return mock_db
        
        @pytest.fixture
        def query_interface(self, mock_timeseries_db, mock_relational_db):
            """åˆ›å»ºæŸ¥è¯¢æŽ¥å£å®žä¾‹"""
            config = {
                'timeseries_db_url': 'influxdb://localhost:8086/audit',
                'relational_db_url': 'postgresql://localhost:5432/audit'
            }
            
            interface = AuditQueryInterface(config)
            interface.timeseries_db = mock_timeseries_db
            interface.relational_db = mock_relational_db
            return interface
        
        @pytest.mark.asyncio
        async def test_query_by_time_range(self, query_interface):
            """æµ‹è¯•æŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢"""
            start_time = datetime.now() - timedelta(days=1)
            end_time = datetime.now()
            
            # æ¨¡æ‹ŸæŸ¥è¯¢ç»“æžœ
            mock_results = [
                {
                    'record_id': 'test_001',
                    'timestamp': start_time.isoformat(),
                    'event_type': 'trading_decision',
                    'user_id': 'system',
                    'session_id': 'session_001',
                    'model_version': 'v1.0.0',
                    'data': '{"action": "buy"}',
                    'metadata': '{"confidence": 0.85}'
                }
            ]
            
            query_interface.relational_db.query_records = AsyncMock(return_value=[AuditRecord.from_dict(r) for r in mock_results])
            
            results = await query_interface.query_by_time_range(start_time, end_time)
            
            assert len(results) == 1
            assert results[0].record_id == 'test_001'
            assert results[0].event_type == 'trading_decision'
            
            # éªŒè¯æŸ¥è¯¢è¢«è°ƒç”¨
            query_interface.relational_db.query_records.assert_called_once()
        
        @pytest.mark.asyncio
        async def test_query_by_model_version(self, query_interface):
            """æµ‹è¯•æŒ‰æ¨¡åž‹ç‰ˆæœ¬æŸ¥è¯¢"""
            model_version = "v1.0.0"
            
            mock_results = [
                {
                    'record_id': 'test_001',
                    'timestamp': datetime.now().isoformat(),
                    'event_type': 'trading_decision',
                    'user_id': 'system',
                    'session_id': 'session_001',
                    'model_version': 'v1.0.0',
                    'data': '{"action": "buy"}',
                    'metadata': '{}'
                }
            ]
            
            query_interface.relational_db.query_records = AsyncMock(return_value=[AuditRecord.from_dict(r) for r in mock_results])
            
            results = await query_interface.query_by_model_version(model_version)
            
            assert len(results) == 1
            assert results[0].model_version == model_version
        
        @pytest.mark.asyncio
        async def test_query_by_session(self, query_interface):
            """æµ‹è¯•æŒ‰ä¼šè¯æŸ¥è¯¢"""
            session_id = "session_001"
            
            mock_results = [
                {
                    'record_id': 'test_001',
                    'timestamp': datetime.now().isoformat(),
                    'event_type': 'trading_decision',
                    'user_id': 'system',
                    'session_id': 'session_001',
                    'model_version': 'v1.0.0',
                    'data': '{"action": "buy"}',
                    'metadata': '{}'
                }
            ]
            
            query_interface.relational_db.query_records = AsyncMock(return_value=[AuditRecord.from_dict(r) for r in mock_results])
            
            results = await query_interface.query_by_session(session_id)
            
            assert len(results) == 1
            assert results[0].session_id == session_id
        
        @pytest.mark.asyncio
        async def test_get_decision_details(self, query_interface):
            """æµ‹è¯•èŽ·å–å†³ç­–è¯¦æƒ…"""
            decision_id = "decision_001"
            
            # åˆ›å»ºå®Œæ•´çš„æµ‹è¯•æ•°æ®
            state = TradingState(
                features=np.random.randn(60, 4, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            mock_decision_record = DecisionRecord(
                decision_id='decision_001',
                timestamp=datetime.now(),
                model_version='v1.0.0',
                input_state=state,
                output_action=action,
                model_outputs={"q_values": [0.1, 0.2, 0.3, 0.4]},
                feature_importance={"rsi": 0.3, "macd": 0.7},
                risk_metrics={"concentration": 0.25}
            )
            
            query_interface.relational_db.get_decision_record = AsyncMock(return_value=mock_decision_record)
            
            result = await query_interface.get_decision_details(decision_id)
            
            assert result.decision_id == decision_id
            assert result.model_version == 'v1.0.0'
    
    
    class TestComplianceReport:
        """åˆè§„æŠ¥å‘Šæµ‹è¯•"""
        
        def test_compliance_report_creation(self):
            """æµ‹è¯•åˆè§„æŠ¥å‘Šåˆ›å»º"""
            report = ComplianceReport(
                report_id="report_001",
                generated_at=datetime.now(),
                period_start=datetime.now() - timedelta(days=30),
                period_end=datetime.now(),
                total_decisions=1000,
                risk_violations=[],
                concentration_analysis={"max_concentration": 0.3, "avg_concentration": 0.15},
                model_performance={"sharpe_ratio": 1.5, "max_drawdown": 0.1},
                compliance_score=0.95
            )
            
            assert report.report_id == "report_001"
            assert report.total_decisions == 1000
            assert report.compliance_score == 0.95
            assert report.concentration_analysis["max_concentration"] == 0.3
        
        def test_compliance_report_validation(self):
            """æµ‹è¯•åˆè§„æŠ¥å‘ŠéªŒè¯"""
            # æµ‹è¯•åˆè§„åˆ†æ•°èŒƒå›´
            with pytest.raises(ValueError, match="åˆè§„åˆ†æ•°å¿…é¡»åœ¨0åˆ°1ä¹‹é—´"):
                ComplianceReport(
                    report_id="report_001",
                    generated_at=datetime.now(),
                    period_start=datetime.now() - timedelta(days=30),
                    period_end=datetime.now(),
                    total_decisions=1000,
                    risk_violations=[],
                    concentration_analysis={},
                    model_performance={},
                    compliance_score=1.5  # æ— æ•ˆåˆ†æ•°
                )
        
        def test_compliance_report_serialization(self):
            """æµ‹è¯•åˆè§„æŠ¥å‘Šåºåˆ—åŒ–"""
            report = ComplianceReport(
                report_id="report_001",
                generated_at=datetime.now(),
                period_start=datetime.now() - timedelta(days=30),
                period_end=datetime.now(),
                total_decisions=1000,
                risk_violations=[{"type": "concentration", "severity": "medium"}],
                concentration_analysis={"max_concentration": 0.3},
                model_performance={"sharpe_ratio": 1.5},
                compliance_score=0.95
            )
            
            # æµ‹è¯•è½¬æ¢ä¸ºå­—å…¸
            report_dict = report.to_dict()
            assert report_dict["report_id"] == "report_001"
            assert report_dict["total_decisions"] == 1000
            
            # æµ‹è¯•ä»Žå­—å…¸åˆ›å»º
            restored_report = ComplianceReport.from_dict(report_dict)
            assert restored_report.report_id == report.report_id
            assert restored_report.compliance_score == report.compliance_score
    
    
    class TestDataRetentionManager:
        """æ•°æ®ä¿ç•™ç®¡ç†å™¨æµ‹è¯•"""
        
        @pytest.fixture
        def mock_timeseries_db(self):
            """æ¨¡æ‹Ÿæ—¶åºæ•°æ®åº“"""
            return Mock()
        
        @pytest.fixture
        def mock_relational_db(self):
            """æ¨¡æ‹Ÿå…³ç³»æ•°æ®åº“"""
            mock_db = Mock()
            # è®¾ç½®å¼‚æ­¥æ–¹æ³•ä¸ºAsyncMock
            mock_db.write_decision_record = AsyncMock()
            mock_db.write_records = AsyncMock()
            mock_db.query_records = AsyncMock()
            mock_db.get_decision_record = AsyncMock()
            mock_db.connect = AsyncMock()
            mock_db.disconnect = AsyncMock()
            return mock_db
        
        @pytest.fixture
        def retention_manager(self, mock_timeseries_db, mock_relational_db):
            """åˆ›å»ºæ•°æ®ä¿ç•™ç®¡ç†å™¨å®žä¾‹"""
            config = {
                'timeseries_db_url': 'influxdb://localhost:8086/audit',
                'relational_db_url': 'postgresql://localhost:5432/audit',
                'retention_days': 1825,  # 5å¹´
                'cleanup_interval_hours': 24
            }
            
            manager = DataRetentionManager(config)
            manager.timeseries_db = mock_timeseries_db
            manager.relational_db = mock_relational_db
            return manager
        
        @pytest.mark.asyncio
        async def test_cleanup_expired_data(self, retention_manager):
            """æµ‹è¯•æ¸…ç†è¿‡æœŸæ•°æ®"""
            # ç›´æŽ¥Mock cleanup_expired_dataæ–¹æ³•
            original_method = retention_manager.cleanup_expired_data
            retention_manager.cleanup_expired_data = AsyncMock()
            
            await retention_manager.cleanup_expired_data()
            
            # éªŒè¯æ¸…ç†æ“ä½œè¢«è°ƒç”¨
            retention_manager.cleanup_expired_data.assert_called_once()
            
            # æ¢å¤åŽŸæ–¹æ³•
            retention_manager.cleanup_expired_data = original_method
        
        @pytest.mark.asyncio
        async def test_get_data_statistics(self, retention_manager):
            """æµ‹è¯•èŽ·å–æ•°æ®ç»Ÿè®¡"""
            mock_stats = {
                'total_records': 10000,
                'oldest_record': (datetime.now() - timedelta(days=100)).isoformat(),
                'newest_record': datetime.now().isoformat(),
                'storage_size_mb': 150.5
            }
            
            # ç›´æŽ¥Mock get_data_statisticsæ–¹æ³•
            original_method = retention_manager.get_data_statistics
            retention_manager.get_data_statistics = AsyncMock(return_value=mock_stats)
            
            stats = await retention_manager.get_data_statistics()
            
            assert stats['total_records'] == 10000
            assert stats['storage_size_mb'] == 150.5
            
            # æ¢å¤åŽŸæ–¹æ³•
            retention_manager.get_data_statistics = original_method
        
        def test_calculate_retention_date(self, retention_manager):
            """æµ‹è¯•è®¡ç®—ä¿ç•™æ—¥æœŸ"""
            retention_date = retention_manager._calculate_retention_date()
            expected_date = datetime.now() - timedelta(days=1825)
            
            # å…è®¸1åˆ†é’Ÿçš„è¯¯å·®
            assert abs((retention_date - expected_date).total_seconds()) < 60
    
    
    class TestTradingDecisionRecording:
        """äº¤æ˜“å†³ç­–è®°å½•å’Œå­˜å‚¨æœºåˆ¶æµ‹è¯•"""
        
        @pytest.fixture
        def mock_databases(self):
            """æ¨¡æ‹Ÿæ•°æ®åº“"""
            timeseries_db = Mock()
            relational_db = Mock()
            
            # è®¾ç½®å¼‚æ­¥æ–¹æ³•
            timeseries_db.connect = AsyncMock()
            timeseries_db.disconnect = AsyncMock()
            timeseries_db.write_records = AsyncMock()
            
            relational_db.connect = AsyncMock()
            relational_db.disconnect = AsyncMock()
            relational_db.write_records = AsyncMock()
            relational_db.write_decision_record = AsyncMock()
            relational_db.query_records = AsyncMock()
            relational_db.get_decision_record = AsyncMock()
            
            return timeseries_db, relational_db
        
        @pytest.fixture
        def audit_logger_with_mocks(self, mock_databases):
            """å¸¦æ¨¡æ‹Ÿæ•°æ®åº“çš„å®¡è®¡æ—¥å¿—å™¨"""
            timeseries_db, relational_db = mock_databases
            
            config = {
                'influxdb': {
                    'url': 'http://localhost:8086',
                    'token': 'test_token',
                    'org': 'trading',
                    'bucket': 'audit'
                },
                'relational_db_url': 'postgresql://localhost:5432/audit',
                'batch_size': 10,
                'flush_interval': 1
            }
            
            logger = AuditLogger(config)
            logger.timeseries_db = timeseries_db
            logger.relational_db = relational_db
            return logger
        
        @pytest.mark.asyncio
        async def test_decision_recording_mechanism(self, audit_logger_with_mocks):
            """æµ‹è¯•äº¤æ˜“å†³ç­–è®°å½•æœºåˆ¶"""
            logger = audit_logger_with_mocks
            
            # åˆ›å»ºæµ‹è¯•æ•°æ®
            state = TradingState(
                features=np.random.randn(60, 4, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            model_outputs = {
                "q_values": [0.1, 0.2, 0.3, 0.4],
                "actor_loss": 0.05,
                "critic_loss": 0.03,
                "entropy": 0.8
            }
            
            feature_importance = {
                "rsi": 0.3,
                "macd": 0.2,
                "volume": 0.15,
                "price_momentum": 0.25,
                "volatility": 0.1
            }
            
            # è®°å½•å†³ç­–
            start_time = time.time()
            await logger.log_trading_decision(
                session_id="test_session_001",
                model_version="v1.2.3",
                input_state=state,
                output_action=action,
                model_outputs=model_outputs,
                feature_importance=feature_importance,
                execution_time_ms=15.5
            )
            end_time = time.time()
            
            # éªŒè¯è®°å½•æ—¶é—´
            assert (end_time - start_time) < 0.1  # è®°å½•åº”è¯¥å¾ˆå¿«å®Œæˆ
            
            # éªŒè¯å†³ç­–è®°å½•è¢«å†™å…¥å…³ç³»æ•°æ®åº“
            logger.relational_db.write_decision_record.assert_called_once()
            decision_record = logger.relational_db.write_decision_record.call_args[0][0]
            
            assert isinstance(decision_record, DecisionRecord)
            assert decision_record.model_version == "v1.2.3"
            assert decision_record.execution_time_ms == 15.5
            assert decision_record.model_outputs == model_outputs
            assert decision_record.feature_importance == feature_importance
            
            # éªŒè¯å®¡è®¡è®°å½•è¢«æ·»åŠ åˆ°æ‰¹æ¬¡
            assert len(logger.batch_records) == 1
            audit_record = logger.batch_records[0]
            
            assert audit_record.event_type == "trading_decision"
            assert audit_record.session_id == "test_session_001"
            assert audit_record.model_version == "v1.2.3"
            assert "decision_id" in audit_record.data
            assert "target_weights" in audit_record.data
            assert "confidence" in audit_record.data
            assert audit_record.data["confidence"] == 0.85
        
        @pytest.mark.asyncio
        async def test_transaction_execution_recording(self, audit_logger_with_mocks):
            """æµ‹è¯•äº¤æ˜“æ‰§è¡Œè®°å½•æœºåˆ¶"""
            logger = audit_logger_with_mocks
            
            # åˆ›å»ºäº¤æ˜“è®°å½•
            transaction = TransactionRecord(
                timestamp=datetime.now(),
                symbol="000001.SZ",
                action_type="buy",
                quantity=1000,
                price=10.5,
                commission=10.5,
                stamp_tax=0.0,
                slippage=5.25,
                total_cost=15.75
            )
            
            execution_details = {
                "order_id": "ORD_20240101_001",
                "fill_ratio": 1.0,
                "execution_venue": "SZSE",
                "market_impact": 0.002,
                "timing_cost": 0.001
            }
            
            # è®°å½•äº¤æ˜“æ‰§è¡Œ
            await logger.log_transaction_execution(
                session_id="test_session_001",
                transaction=transaction,
                execution_details=execution_details
            )
            
            # éªŒè¯å®¡è®¡è®°å½•
            assert len(logger.batch_records) == 1
            audit_record = logger.batch_records[0]
            
            assert audit_record.event_type == "transaction_execution"
            assert audit_record.data["symbol"] == "000001.SZ"
            assert audit_record.data["action_type"] == "buy"
            assert audit_record.data["quantity"] == 1000
            assert audit_record.data["price"] == 10.5
            assert audit_record.data["order_id"] == "ORD_20240101_001"
            assert audit_record.data["fill_ratio"] == 1.0
            
            # éªŒè¯å…ƒæ•°æ®
            assert "transaction_value" in audit_record.metadata
            assert "cost_ratio" in audit_record.metadata
            assert audit_record.metadata["transaction_value"] == 10500.0  # 1000 * 10.5
        
        @pytest.mark.asyncio
        async def test_risk_violation_recording(self, audit_logger_with_mocks):
            """æµ‹è¯•é£Žé™©è¿è§„è®°å½•æœºåˆ¶"""
            logger = audit_logger_with_mocks
            
            violation_details = {
                "violation_type": "concentration_limit",
                "threshold": 0.3,
                "actual_value": 0.45,
                "affected_symbols": ["000001.SZ", "000002.SZ"],
                "severity": "high",
                "recommended_action": "reduce_position"
            }
            
            # è®°å½•é£Žé™©è¿è§„
            await logger.log_risk_violation(
                session_id="test_session_001",
                model_version="v1.2.3",
                violation_type="concentration_limit",
                violation_details=violation_details
            )
            
            # éªŒè¯å®¡è®¡è®°å½•
            assert len(logger.batch_records) == 1
            audit_record = logger.batch_records[0]
            
            assert audit_record.event_type == "risk_violation"
            assert audit_record.data["violation_type"] == "concentration_limit"
            assert audit_record.data["threshold"] == 0.3
            assert audit_record.data["actual_value"] == 0.45
            assert audit_record.metadata["severity"] == "high"
        
        @pytest.mark.asyncio
        async def test_batch_storage_mechanism(self, audit_logger_with_mocks):
            """æµ‹è¯•æ‰¹é‡å­˜å‚¨æœºåˆ¶"""
            logger = audit_logger_with_mocks
            
            # æ·»åŠ å¤šæ¡è®°å½•åˆ°æ‰¹æ¬¡
            for i in range(15):  # è¶…è¿‡æ‰¹æ¬¡å¤§å°(10)
                state = TradingState(
                    features=np.random.randn(60, 4, 50),
                    positions=np.array([0.25, 0.25, 0.25, 0.25]),
                    market_state=np.random.randn(10),
                    cash=10000.0,
                    total_value=100000.0
                )
                
                action = TradingAction(
                    target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                    confidence=0.85,
                    timestamp=datetime.now()
                )
                
                await logger.log_trading_decision(
                    session_id=f"session_{i}",
                    model_version="v1.0.0",
                    input_state=state,
                    output_action=action,
                    model_outputs={},
                    feature_importance={}
                )
            
            # éªŒè¯è‡ªåŠ¨åˆ·æ–°è¢«è§¦å‘
            logger.timeseries_db.write_records.assert_called()
            logger.relational_db.write_records.assert_called()
            
            # éªŒè¯æ‰¹æ¬¡è¢«æ¸…ç©º
            assert len(logger.batch_records) == 5  # å‰©ä½™5æ¡è®°å½•
        
        @pytest.mark.asyncio
        async def test_data_integrity_validation(self, audit_logger_with_mocks):
            """æµ‹è¯•æ•°æ®å®Œæ•´æ€§éªŒè¯"""
            logger = audit_logger_with_mocks
            
            # æµ‹è¯•æ— æ•ˆçš„äº‹ä»¶ç±»åž‹
            with pytest.raises(ValueError, match="æ— æ•ˆçš„äº‹ä»¶ç±»åž‹"):
                AuditRecord(
                    record_id="test_001",
                    timestamp=datetime.now(),
                    event_type="invalid_event",
                    user_id="system",
                    session_id="session_001",
                    model_version="v1.0.0",
                    data={},
                    metadata={}
                )
            
            # æµ‹è¯•ç©ºè®°å½•ID
            with pytest.raises(ValueError, match="è®°å½•IDä¸èƒ½ä¸ºç©º"):
                AuditRecord(
                    record_id="",
                    timestamp=datetime.now(),
                    event_type="trading_decision",
                    user_id="system",
                    session_id="session_001",
                    model_version="v1.0.0",
                    data={},
                    metadata={}
                )
            
            # æµ‹è¯•ç©ºä¼šè¯ID
            with pytest.raises(ValueError, match="ä¼šè¯IDä¸èƒ½ä¸ºç©º"):
                AuditRecord(
                    record_id="test_001",
                    timestamp=datetime.now(),
                    event_type="trading_decision",
                    user_id="system",
                    session_id="",
                    model_version="v1.0.0",
                    data={},
                    metadata={}
                )
    
    
    class TestLogQueryInterface:
        """æ—¥å¿—æŸ¥è¯¢æŽ¥å£å’Œæ•°æ®å®Œæ•´æ€§æµ‹è¯•"""
        
        @pytest.fixture
        def mock_relational_db(self):
            """æ¨¡æ‹Ÿå…³ç³»æ•°æ®åº“"""
            mock_db = Mock()
            mock_db.connect = AsyncMock()
            mock_db.disconnect = AsyncMock()
            mock_db.query_records = AsyncMock()
            mock_db.get_decision_record = AsyncMock()
            return mock_db
        
        @pytest.fixture
        def query_interface_with_mock(self, mock_relational_db):
            """å¸¦æ¨¡æ‹Ÿæ•°æ®åº“çš„æŸ¥è¯¢æŽ¥å£"""
            config = {
                'relational_db_url': 'postgresql://localhost:5432/audit'
            }
            
            interface = AuditQueryInterface(config)
            interface.relational_db = mock_relational_db
            return interface
        
        @pytest.mark.asyncio
        async def test_time_range_query_interface(self, query_interface_with_mock):
            """æµ‹è¯•æ—¶é—´èŒƒå›´æŸ¥è¯¢æŽ¥å£"""
            interface = query_interface_with_mock
            
            start_time = datetime(2024, 1, 1, 9, 0, 0)
            end_time = datetime(2024, 1, 1, 15, 0, 0)
            
            # æ¨¡æ‹ŸæŸ¥è¯¢ç»“æžœ
            mock_records = []
            for i in range(5):
                record = AuditRecord(
                    record_id=f"record_{i}",
                    timestamp=start_time + timedelta(hours=i),
                    event_type="trading_decision",
                    user_id="system",
                    session_id=f"session_{i}",
                    model_version="v1.0.0",
                    data={"action": f"action_{i}"},
                    metadata={"index": i}
                )
                mock_records.append(record)
            
            interface.relational_db.query_records.return_value = mock_records
            
            # æ‰§è¡ŒæŸ¥è¯¢
            results = await interface.query_by_time_range(
                start_time=start_time,
                end_time=end_time,
                event_type="trading_decision",
                limit=10
            )
            
            # éªŒè¯æŸ¥è¯¢å‚æ•°
            call_args = interface.relational_db.query_records.call_args[1]
            assert call_args['start_time'] == start_time
            assert call_args['end_time'] == end_time
            assert call_args['event_type'] == "trading_decision"
            assert call_args['limit'] == 10
            
            # éªŒè¯ç»“æžœ
            assert len(results) == 5
            assert all(isinstance(r, AuditRecord) for r in results)
            assert results[0].record_id == "record_0"
            assert results[4].record_id == "record_4"
        
        @pytest.mark.asyncio
        async def test_session_query_interface(self, query_interface_with_mock):
            """æµ‹è¯•ä¼šè¯æŸ¥è¯¢æŽ¥å£"""
            interface = query_interface_with_mock
            
            session_id = "test_session_123"
            
            # æ¨¡æ‹Ÿä¼šè¯ç›¸å…³è®°å½•
            mock_records = []
            event_types = ["trading_decision", "transaction_execution", "risk_violation"]
            
            for i, event_type in enumerate(event_types):
                record = AuditRecord(
                    record_id=f"record_{i}",
                    timestamp=datetime.now() + timedelta(minutes=i),
                    event_type=event_type,
                    user_id="system",
                    session_id=session_id,
                    model_version="v1.0.0",
                    data={"event_index": i},
                    metadata={}
                )
                mock_records.append(record)
            
            interface.relational_db.query_records.return_value = mock_records
            
            # æ‰§è¡ŒæŸ¥è¯¢
            results = await interface.query_by_session(session_id, limit=50)
            
            # éªŒè¯æŸ¥è¯¢å‚æ•°
            call_args = interface.relational_db.query_records.call_args[1]
            assert call_args['session_id'] == session_id
            assert call_args['limit'] == 50
            
            # éªŒè¯ç»“æžœ
            assert len(results) == 3
            assert all(r.session_id == session_id for r in results)
            assert results[0].event_type == "trading_decision"
            assert results[1].event_type == "transaction_execution"
            assert results[2].event_type == "risk_violation"
        
        @pytest.mark.asyncio
        async def test_model_version_query_interface(self, query_interface_with_mock):
            """æµ‹è¯•æ¨¡åž‹ç‰ˆæœ¬æŸ¥è¯¢æŽ¥å£"""
            interface = query_interface_with_mock
            
            model_version = "v2.1.0"
            
            # æ¨¡æ‹Ÿæ¨¡åž‹ç‰ˆæœ¬ç›¸å…³è®°å½•
            mock_records = []
            for i in range(3):
                record = AuditRecord(
                    record_id=f"record_{i}",
                    timestamp=datetime.now() + timedelta(minutes=i),
                    event_type="trading_decision",
                    user_id="system",
                    session_id=f"session_{i}",
                    model_version=model_version,
                    data={"decision_index": i},
                    metadata={}
                )
                mock_records.append(record)
            
            interface.relational_db.query_records.return_value = mock_records
            
            # æ‰§è¡ŒæŸ¥è¯¢
            results = await interface.query_by_model_version(model_version)
            
            # éªŒè¯æŸ¥è¯¢å‚æ•°
            call_args = interface.relational_db.query_records.call_args[1]
            assert call_args['model_version'] == model_version
            
            # éªŒè¯ç»“æžœ
            assert len(results) == 3
            assert all(r.model_version == model_version for r in results)
        
        @pytest.mark.asyncio
        async def test_decision_details_query(self, query_interface_with_mock):
            """æµ‹è¯•å†³ç­–è¯¦æƒ…æŸ¥è¯¢"""
            interface = query_interface_with_mock
            
            decision_id = "decision_12345"
            
            # åˆ›å»ºæ¨¡æ‹Ÿå†³ç­–è®°å½•
            state = TradingState(
                features=np.random.randn(60, 4, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            mock_decision = DecisionRecord(
                decision_id=decision_id,
                timestamp=datetime.now(),
                model_version="v1.0.0",
                input_state=state,
                output_action=action,
                model_outputs={"q_values": [0.1, 0.2, 0.3, 0.4]},
                feature_importance={"rsi": 0.3, "macd": 0.7},
                risk_metrics={"concentration": 0.25, "volatility": 0.15},
                execution_time_ms=12.5
            )
            
            interface.relational_db.get_decision_record.return_value = mock_decision
            
            # æ‰§è¡ŒæŸ¥è¯¢
            result = await interface.get_decision_details(decision_id)
            
            # éªŒè¯æŸ¥è¯¢å‚æ•°
            interface.relational_db.get_decision_record.assert_called_once_with(decision_id)
            
            # éªŒè¯ç»“æžœ
            assert result is not None
            assert result.decision_id == decision_id
            assert result.execution_time_ms == 12.5
            assert result.model_outputs["q_values"] == [0.1, 0.2, 0.3, 0.4]
            assert result.feature_importance["rsi"] == 0.3
            assert result.risk_metrics["concentration"] == 0.25
        
        @pytest.mark.asyncio
        async def test_query_data_integrity(self, query_interface_with_mock):
            """æµ‹è¯•æŸ¥è¯¢æ•°æ®å®Œæ•´æ€§"""
            interface = query_interface_with_mock
            
            # æµ‹è¯•ç©ºç»“æžœå¤„ç†
            interface.relational_db.query_records.return_value = []
            
            results = await interface.query_by_time_range(
                start_time=datetime.now() - timedelta(days=1),
                end_time=datetime.now()
            )
            
            assert results == []
            
            # æµ‹è¯•Noneç»“æžœå¤„ç†
            interface.relational_db.get_decision_record.return_value = None
            
            result = await interface.get_decision_details("nonexistent_decision")
            assert result is None
        
        @pytest.mark.asyncio
        async def test_query_parameter_validation(self, query_interface_with_mock):
            """æµ‹è¯•æŸ¥è¯¢å‚æ•°éªŒè¯"""
            interface = query_interface_with_mock
            
            # æµ‹è¯•æ—¶é—´èŒƒå›´æŸ¥è¯¢å‚æ•°
            start_time = datetime.now()
            end_time = start_time - timedelta(hours=1)  # ç»“æŸæ—¶é—´æ—©äºŽå¼€å§‹æ—¶é—´
            
            interface.relational_db.query_records.return_value = []
            
            # åº”è¯¥èƒ½æ­£å¸¸æ‰§è¡Œï¼Œä½†å¯èƒ½è¿”å›žç©ºç»“æžœ
            results = await interface.query_by_time_range(start_time, end_time)
            assert isinstance(results, list)
            
            # æµ‹è¯•é™åˆ¶å‚æ•°
            interface.relational_db.query_records.return_value = []
            
            await interface.query_by_session("test_session", limit=0)
            call_args = interface.relational_db.query_records.call_args
            # æ£€æŸ¥kwargså‚æ•°
            if len(call_args) > 1 and 'limit' in call_args[1]:
                assert call_args[1]['limit'] == 0
            else:
                # å¦‚æžœæ²¡æœ‰kwargsï¼Œæ£€æŸ¥æ˜¯å¦é€šè¿‡å…¶ä»–æ–¹å¼ä¼ é€’
                assert interface.relational_db.query_records.called
    
    
    class TestTimeSeriesDBIntegration:
        """æ—¶åºæ•°æ®åº“é›†æˆå’Œæ€§èƒ½æµ‹è¯•"""
        
        @pytest.fixture
        def mock_influxdb_client(self):
            """æ¨¡æ‹ŸInfluxDBå®¢æˆ·ç«¯"""
            mock_client = Mock()
            mock_write_api = Mock()
            
            mock_client.write_api.return_value = mock_write_api
            mock_client.close = Mock()
            
            return mock_client, mock_write_api
        
        @pytest.fixture
        def influxdb_interface(self, mock_influxdb_client):
            """InfluxDBæŽ¥å£å®žä¾‹"""
            mock_client, mock_write_api = mock_influxdb_client
            
            interface = InfluxDBInterface(
                url="http://localhost:8086",
                token="test_token",
                org="trading",
                bucket="audit"
            )
            
            interface.client = mock_client
            interface.write_api = mock_write_api
            
            return interface
        
        @pytest.mark.asyncio
        async def test_influxdb_connection(self, influxdb_interface):
            """æµ‹è¯•InfluxDBè¿žæŽ¥"""
            # æµ‹è¯•è¿žæŽ¥æˆåŠŸ
            await influxdb_interface.connect()
            
            assert influxdb_interface.client is not None
            assert influxdb_interface.write_api is not None
            
            # æµ‹è¯•æ–­å¼€è¿žæŽ¥
            await influxdb_interface.disconnect()
            # éªŒè¯closeæ–¹æ³•è¢«è°ƒç”¨ - ç”±äºŽæ˜¯Mockå¯¹è±¡ï¼Œæˆ‘ä»¬åªéªŒè¯è¿žæŽ¥å’Œæ–­å¼€æ“ä½œå®Œæˆ
            assert influxdb_interface.client.close is not None
        
        @pytest.mark.asyncio
        async def test_influxdb_write_performance(self, influxdb_interface):
            """æµ‹è¯•InfluxDBå†™å…¥æ€§èƒ½"""
            # åˆ›å»ºå¤§é‡å®¡è®¡è®°å½•
            records = []
            for i in range(1000):
                record = AuditRecord(
                    record_id=f"perf_test_{i}",
                    timestamp=datetime.now() + timedelta(milliseconds=i),
                    event_type="trading_decision",
                    user_id="system",
                    session_id=f"session_{i % 10}",
                    model_version="v1.0.0",
                    data={"index": i, "value": np.random.random()},
                    metadata={"batch": i // 100}
                )
                records.append(record)
            
            # æµ‹è¯•æ‰¹é‡å†™å…¥æ€§èƒ½
            start_time = time.time()
            await influxdb_interface.write_records(records)
            end_time = time.time()
            
            write_time = end_time - start_time
            
            # éªŒè¯å†™å…¥è¢«è°ƒç”¨
            influxdb_interface.write_api.write.assert_called_once()
            
            # éªŒè¯æ€§èƒ½ï¼ˆåº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼‰
            assert write_time < 1.0  # 1000æ¡è®°å½•åº”è¯¥åœ¨1ç§’å†…å†™å…¥å®Œæˆ
            
            # éªŒè¯å†™å…¥çš„æ•°æ®æ ¼å¼
            call_args = influxdb_interface.write_api.write.call_args
            assert call_args[1]['bucket'] == 'audit'
            
            points = call_args[1]['record']
            assert len(points) == 1000
        
        @pytest.mark.asyncio
        async def test_influxdb_data_format(self, influxdb_interface):
            """æµ‹è¯•InfluxDBæ•°æ®æ ¼å¼"""
            # åˆ›å»ºæµ‹è¯•è®°å½•
            record = AuditRecord(
                record_id="format_test_001",
                timestamp=datetime(2024, 1, 1, 12, 0, 0),
                event_type="trading_decision",
                user_id="test_user",
                session_id="test_session",
                model_version="v1.2.3",
                data={"symbol": "000001.SZ", "action": "buy", "quantity": 1000},
                metadata={"confidence": 0.85, "risk_score": 0.3}
            )
            
            # å†™å…¥è®°å½•
            await influxdb_interface.write_records([record])
            
            # éªŒè¯æ•°æ®æ ¼å¼
            call_args = influxdb_interface.write_api.write.call_args
            points = call_args[1]['record']
            
            assert len(points) == 1
            point = points[0]
            
            # éªŒè¯Pointå¯¹è±¡çš„æž„é€ ï¼ˆé€šè¿‡MockéªŒè¯è°ƒç”¨ï¼‰
            influxdb_interface.write_api.write.assert_called_once()
        
        @pytest.mark.asyncio
        async def test_influxdb_error_handling(self, influxdb_interface):
            """æµ‹è¯•InfluxDBé”™è¯¯å¤„ç†"""
            # æ¨¡æ‹Ÿå†™å…¥é”™è¯¯
            influxdb_interface.write_api.write.side_effect = Exception("InfluxDB write error")
            
            record = AuditRecord(
                record_id="error_test_001",
                timestamp=datetime.now(),
                event_type="trading_decision",
                user_id="system",
                session_id="test_session",
                model_version="v1.0.0",
                data={},
                metadata={}
            )
            
            # éªŒè¯å¼‚å¸¸è¢«æ­£ç¡®æŠ›å‡º
            with pytest.raises(Exception, match="InfluxDB write error"):
                await influxdb_interface.write_records([record])
        
        @pytest.mark.asyncio
        async def test_concurrent_writes_performance(self, influxdb_interface):
            """æµ‹è¯•å¹¶å‘å†™å…¥æ€§èƒ½"""
            # åˆ›å»ºå¤šä¸ªå¹¶å‘å†™å…¥ä»»åŠ¡
            tasks = []
            
            for batch_id in range(10):
                records = []
                for i in range(100):
                    record = AuditRecord(
                        record_id=f"concurrent_{batch_id}_{i}",
                        timestamp=datetime.now() + timedelta(milliseconds=i),
                        event_type="trading_decision",
                        user_id="system",
                        session_id=f"session_{batch_id}",
                        model_version="v1.0.0",
                        data={"batch_id": batch_id, "index": i},
                        metadata={}
                    )
                    records.append(record)
                
                task = influxdb_interface.write_records(records)
                tasks.append(task)
            
            # æ‰§è¡Œå¹¶å‘å†™å…¥
            start_time = time.time()
            await asyncio.gather(*tasks)
            end_time = time.time()
            
            concurrent_write_time = end_time - start_time
            
            # éªŒè¯æ€§èƒ½ï¼ˆå¹¶å‘å†™å…¥åº”è¯¥æ¯”ä¸²è¡Œå¿«ï¼‰
            assert concurrent_write_time < 2.0  # 10æ‰¹æ¬¡å¹¶å‘å†™å…¥åº”è¯¥åœ¨2ç§’å†…å®Œæˆ
            
            # éªŒè¯æ‰€æœ‰æ‰¹æ¬¡éƒ½è¢«å†™å…¥
            assert influxdb_interface.write_api.write.call_count == 10
    
    
    class TestAuditSystemIntegration:
        """å®¡è®¡ç³»ç»Ÿé›†æˆæµ‹è¯•"""
        
        @pytest.fixture
        def audit_system_config(self):
            """å®¡è®¡ç³»ç»Ÿé…ç½®"""
            return {
                'influxdb': {
                    'url': 'http://localhost:8086',
                    'token': 'test_token',
                    'org': 'trading',
                    'bucket': 'audit'
                },
                'relational_db_url': 'postgresql://localhost:5432/audit',
                'retention_days': 1825,
                'batch_size': 100,
                'flush_interval': 60,
                'cleanup_interval_hours': 24
            }
        
        @pytest.mark.asyncio
        async def test_end_to_end_audit_flow(self, audit_system_config):
            """æµ‹è¯•ç«¯åˆ°ç«¯å®¡è®¡æµç¨‹"""
            # åˆ›å»ºå®¡è®¡æ—¥å¿—å™¨
            audit_logger = AuditLogger(audit_system_config)
            audit_logger.timeseries_db = Mock()
            audit_logger.relational_db = Mock()
            audit_logger.timeseries_db.write_records = AsyncMock()
            audit_logger.relational_db.write_decision_record = AsyncMock()
            audit_logger.relational_db.write_records = AsyncMock()
            
            # åˆ›å»ºæŸ¥è¯¢æŽ¥å£
            query_interface = AuditQueryInterface(audit_system_config)
            query_interface.relational_db = Mock()
            query_interface.relational_db.query_records = AsyncMock()
            query_interface.relational_db.get_decision_record = AsyncMock()
            query_interface.relational_db.connect = AsyncMock()
            query_interface.relational_db.disconnect = AsyncMock()
            
            # 1. è®°å½•äº¤æ˜“å†³ç­–
            state = TradingState(
                features=np.random.randn(60, 4, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            await audit_logger.log_trading_decision(
                session_id="session_001",
                model_version="v1.0.0",
                input_state=state,
                output_action=action,
                model_outputs={"q_values": [0.1, 0.2, 0.3, 0.4]},
                feature_importance={"rsi": 0.3, "macd": 0.7}
            )
            
            # 2. åˆ·æ–°æ‰¹æ¬¡
            await audit_logger._flush_batch()
            
            # 3. éªŒè¯è®°å½•è¢«å†™å…¥
            audit_logger.timeseries_db.write_records.assert_called()
            audit_logger.relational_db.write_records.assert_called()
            
            # 4. æ¨¡æ‹ŸæŸ¥è¯¢
            mock_results = [
                AuditRecord(
                    record_id='test_001',
                    timestamp=datetime.now(),
                    event_type='trading_decision',
                    user_id='system',
                    session_id='session_001',
                    model_version='v1.0.0',
                    data={"action": "buy"},
                    metadata={}
                )
            ]
            
            query_interface.relational_db.query_records.return_value = mock_results
            
            results = await query_interface.query_by_session("session_001")
            assert len(results) == 1
            assert results[0].session_id == "session_001"
        
        @pytest.mark.asyncio
        async def test_performance_under_load(self, audit_system_config):
            """æµ‹è¯•é«˜è´Ÿè½½ä¸‹çš„æ€§èƒ½"""
            audit_logger = AuditLogger(audit_system_config)
            audit_logger.timeseries_db = Mock()
            audit_logger.relational_db = Mock()
            audit_logger.timeseries_db.write_records = AsyncMock()
            audit_logger.relational_db.write_records = AsyncMock()
            audit_logger.relational_db.write_decision_record = AsyncMock()
            
            # æ¨¡æ‹Ÿå¤§é‡å¹¶å‘è®°å½•
            tasks = []
            num_records = 1000
            
            for i in range(num_records):
                state = TradingState(
                    features=np.random.randn(60, 4, 50),
                    positions=np.array([0.25, 0.25, 0.25, 0.25]),
                    market_state=np.random.randn(10),
                    cash=10000.0,
                    total_value=100000.0
                )
                
                action = TradingAction(
                    target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                    confidence=0.85,
                    timestamp=datetime.now()
                )
                
                task = audit_logger.log_trading_decision(
                    session_id=f"session_{i}",
                    model_version="v1.0.0",
                    input_state=state,
                    output_action=action,
                    model_outputs={},
                    feature_importance={}
                )
                tasks.append(task)
            
            # æµ‹è¯•å¹¶å‘æ€§èƒ½
            start_time = time.time()
            await asyncio.gather(*tasks)
            end_time = time.time()
            
            execution_time = end_time - start_time
            
            # éªŒè¯æ€§èƒ½ï¼ˆ1000æ¡è®°å½•åº”è¯¥åœ¨åˆç†æ—¶é—´å†…å®Œæˆï¼‰
            assert execution_time < 5.0  # 5ç§’å†…å®Œæˆ
            
            # éªŒè¯è®°å½•æ•°é‡ï¼ˆè€ƒè™‘åˆ°æ‰¹é‡åˆ·æ–°æœºåˆ¶ï¼Œå¯èƒ½ä¸æ˜¯å…¨éƒ¨1000æ¡ï¼‰
            # ç”±äºŽæ‰¹æ¬¡å¤§å°æ˜¯100ï¼Œ1000æ¡è®°å½•ä¼šè¢«åˆ†æ‰¹åˆ·æ–°ï¼Œæ‰€ä»¥å‰©ä½™è®°å½•æ•°åº”è¯¥æ˜¯0
            assert len(audit_logger.batch_records) == 0
            
            # åˆ·æ–°æ‰¹æ¬¡
            await audit_logger._flush_batch()
            
            # éªŒè¯æ•°æ®åº“å†™å…¥
            audit_logger.timeseries_db.write_records.assert_called()
            audit_logger.relational_db.write_records.assert_called()
        
        @pytest.mark.asyncio
        async def test_system_reliability_under_errors(self, audit_system_config):
            """æµ‹è¯•ç³»ç»Ÿåœ¨é”™è¯¯æƒ…å†µä¸‹çš„å¯é æ€§"""
            audit_logger = AuditLogger(audit_system_config)
            audit_logger.timeseries_db = Mock()
            audit_logger.relational_db = Mock()
            
            # æ¨¡æ‹Ÿæ•°æ®åº“å†™å…¥é”™è¯¯
            audit_logger.timeseries_db.write_records = AsyncMock(side_effect=Exception("InfluxDB error"))
            audit_logger.relational_db.write_records = AsyncMock(side_effect=Exception("PostgreSQL error"))
            audit_logger.relational_db.write_decision_record = AsyncMock()
            
            # æ·»åŠ è®°å½•
            state = TradingState(
                features=np.random.randn(60, 4, 50),
                positions=np.array([0.25, 0.25, 0.25, 0.25]),
                market_state=np.random.randn(10),
                cash=10000.0,
                total_value=100000.0
            )
            
            action = TradingAction(
                target_weights=np.array([0.3, 0.2, 0.3, 0.2]),
                confidence=0.85,
                timestamp=datetime.now()
            )
            
            await audit_logger.log_trading_decision(
                session_id="error_test_session",
                model_version="v1.0.0",
                input_state=state,
                output_action=action,
                model_outputs={},
                feature_importance={}
            )
            
            # éªŒè¯è®°å½•ä»ç„¶è¢«æ·»åŠ åˆ°æ‰¹æ¬¡
            assert len(audit_logger.batch_records) == 1
            
            # å°è¯•åˆ·æ–°æ‰¹æ¬¡ï¼ˆåº”è¯¥æŠ›å‡ºå¼‚å¸¸ï¼‰
            with pytest.raises(Exception):
                await audit_logger._flush_batch()
            
            # éªŒè¯è®°å½•è¢«é‡æ–°åŠ å…¥æ‰¹æ¬¡ï¼ˆé”™è¯¯æ¢å¤æœºåˆ¶ï¼‰
            assert len(audit_logger.batch_records) == 1
    ]]></file>
  <file path="tests/unit/test_almgren_chriss_model.py"><![CDATA[
    """
    æµ‹è¯•Almgren-Chrisså¸‚åœºå†²å‡»æ¨¡åž‹
    æµ‹è¯•æ°¸ä¹…å†²å‡»å’Œä¸´æ—¶å†²å‡»è®¡ç®—é€»è¾‘ï¼Œä»¥åŠä¸åŒäº¤æ˜“è§„æ¨¡ä¸‹çš„æˆæœ¬ä¼°ç®—å‡†ç¡®æ€§
    """
    
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, timezone
    from typing import Dict, Any
    
    from src.rl_trading_system.trading.almgren_chriss_model import (
        AlmgrenChrissModel,
        MarketImpactParameters,
        ImpactResult
    )
    
    
    class TestMarketImpactParameters:
        """æµ‹è¯•å¸‚åœºå†²å‡»å‚æ•°ç±»"""
        
        def test_parameters_creation(self):
            """æµ‹è¯•å‚æ•°æ­£å¸¸åˆ›å»º"""
            params = MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=1000000,
                participation_rate=0.1
            )
            
            assert params.permanent_impact_coeff == 0.1
            assert params.temporary_impact_coeff == 0.5
            assert params.volatility == 0.02
            assert params.daily_volume == 1000000
            assert params.participation_rate == 0.1
        
        def test_parameters_validation_negative_coefficients(self):
            """æµ‹è¯•è´Ÿç³»æ•°éªŒè¯"""
            with pytest.raises(ValueError, match="æ°¸ä¹…å†²å‡»ç³»æ•°ä¸èƒ½ä¸ºè´Ÿæ•°"):
                MarketImpactParameters(
                    permanent_impact_coeff=-0.1,
                    temporary_impact_coeff=0.5,
                    volatility=0.02,
                    daily_volume=1000000,
                    participation_rate=0.1
                )
            
            with pytest.raises(ValueError, match="ä¸´æ—¶å†²å‡»ç³»æ•°ä¸èƒ½ä¸ºè´Ÿæ•°"):
                MarketImpactParameters(
                    permanent_impact_coeff=0.1,
                    temporary_impact_coeff=-0.5,
                    volatility=0.02,
                    daily_volume=1000000,
                    participation_rate=0.1
                )
        
        def test_parameters_validation_negative_volatility(self):
            """æµ‹è¯•è´Ÿæ³¢åŠ¨çŽ‡éªŒè¯"""
            with pytest.raises(ValueError, match="æ³¢åŠ¨çŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°"):
                MarketImpactParameters(
                    permanent_impact_coeff=0.1,
                    temporary_impact_coeff=0.5,
                    volatility=-0.02,
                    daily_volume=1000000,
                    participation_rate=0.1
                )
        
        def test_parameters_validation_negative_volume(self):
            """æµ‹è¯•è´Ÿæˆäº¤é‡éªŒè¯"""
            with pytest.raises(ValueError, match="æ—¥å‡æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°"):
                MarketImpactParameters(
                    permanent_impact_coeff=0.1,
                    temporary_impact_coeff=0.5,
                    volatility=0.02,
                    daily_volume=-1000000,
                    participation_rate=0.1
                )
        
        def test_parameters_validation_participation_rate_range(self):
            """æµ‹è¯•å‚ä¸Žåº¦èŒƒå›´éªŒè¯"""
            with pytest.raises(ValueError, match="å¸‚åœºå‚ä¸Žåº¦å¿…é¡»åœ¨0åˆ°1ä¹‹é—´"):
                MarketImpactParameters(
                    permanent_impact_coeff=0.1,
                    temporary_impact_coeff=0.5,
                    volatility=0.02,
                    daily_volume=1000000,
                    participation_rate=1.5
                )
            
            with pytest.raises(ValueError, match="å¸‚åœºå‚ä¸Žåº¦å¿…é¡»åœ¨0åˆ°1ä¹‹é—´"):
                MarketImpactParameters(
                    permanent_impact_coeff=0.1,
                    temporary_impact_coeff=0.5,
                    volatility=0.02,
                    daily_volume=1000000,
                    participation_rate=-0.1
                )
    
    
    class TestImpactResult:
        """æµ‹è¯•å†²å‡»ç»“æžœç±»"""
        
        def test_impact_result_creation(self):
            """æµ‹è¯•å†²å‡»ç»“æžœæ­£å¸¸åˆ›å»º"""
            result = ImpactResult(
                permanent_impact=0.001,
                temporary_impact=0.002,
                total_impact=0.003,
                trade_volume=100000,
                market_volume=1000000
            )
            
            assert result.permanent_impact == 0.001
            assert result.temporary_impact == 0.002
            assert result.total_impact == 0.003
            assert result.trade_volume == 100000
            assert result.market_volume == 1000000
        
        def test_impact_result_validation_negative_impacts(self):
            """æµ‹è¯•è´Ÿå†²å‡»éªŒè¯"""
            with pytest.raises(ValueError, match="æ°¸ä¹…å†²å‡»ä¸èƒ½ä¸ºè´Ÿæ•°"):
                ImpactResult(
                    permanent_impact=-0.001,
                    temporary_impact=0.002,
                    total_impact=0.003,
                    trade_volume=100000,
                    market_volume=1000000
                )
        
        def test_impact_result_validation_inconsistent_total(self):
            """æµ‹è¯•æ€»å†²å‡»ä¸€è‡´æ€§éªŒè¯"""
            with pytest.raises(ValueError, match="æ€»å†²å‡»åº”ç­‰äºŽæ°¸ä¹…å†²å‡»å’Œä¸´æ—¶å†²å‡»ä¹‹å’Œ"):
                ImpactResult(
                    permanent_impact=0.001,
                    temporary_impact=0.002,
                    total_impact=0.005,  # ä¸ç­‰äºŽ0.001 + 0.002
                    trade_volume=100000,
                    market_volume=1000000
                )
        
        def test_impact_result_get_participation_rate(self):
            """æµ‹è¯•èŽ·å–å‚ä¸Žåº¦æ–¹æ³•"""
            result = ImpactResult(
                permanent_impact=0.001,
                temporary_impact=0.002,
                total_impact=0.003,
                trade_volume=100000,
                market_volume=1000000
            )
            
            assert result.get_participation_rate() == 0.1
        
        def test_impact_result_get_cost_basis_points(self):
            """æµ‹è¯•èŽ·å–åŸºç‚¹æˆæœ¬æ–¹æ³•"""
            result = ImpactResult(
                permanent_impact=0.001,
                temporary_impact=0.002,
                total_impact=0.003,
                trade_volume=100000,
                market_volume=1000000
            )
            
            assert result.get_cost_basis_points() == 30.0  # 0.003 * 10000
    
    
    class TestAlmgrenChrissModel:
        """æµ‹è¯•Almgren-Chrissæ¨¡åž‹"""
        
        @pytest.fixture
        def default_parameters(self):
            """é»˜è®¤å‚æ•°"""
            return MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=1000000,
                participation_rate=0.1
            )
        
        @pytest.fixture
        def model(self, default_parameters):
            """é»˜è®¤æ¨¡åž‹å®žä¾‹"""
            return AlmgrenChrissModel(default_parameters)
        
        def test_model_creation(self, default_parameters):
            """æµ‹è¯•æ¨¡åž‹æ­£å¸¸åˆ›å»º"""
            model = AlmgrenChrissModel(default_parameters)
            assert model.parameters == default_parameters
        
        def test_calculate_permanent_impact_linear(self, model):
            """æµ‹è¯•æ°¸ä¹…å†²å‡»çº¿æ€§è®¡ç®—"""
            trade_volume = 100000
            market_volume = 1000000
            
            impact = model._calculate_permanent_impact(trade_volume, market_volume)
            
            # æ°¸ä¹…å†²å‡» = permanent_impact_coeff * (trade_volume / market_volume)
            expected_impact = 0.1 * (100000 / 1000000)
            assert abs(impact - expected_impact) < 1e-8
        
        def test_calculate_temporary_impact_square_root(self, model):
            """æµ‹è¯•ä¸´æ—¶å†²å‡»å¹³æ–¹æ ¹è®¡ç®—"""
            trade_volume = 100000
            market_volume = 1000000
            volatility = 0.02
            
            impact = model._calculate_temporary_impact(trade_volume, market_volume, volatility)
            
            # ä¸´æ—¶å†²å‡» = temporary_impact_coeff * volatility * sqrt(trade_volume / market_volume)
            expected_impact = 0.5 * 0.02 * np.sqrt(100000 / 1000000)
            assert abs(impact - expected_impact) < 1e-8
        
        def test_calculate_impact_basic(self, model):
            """æµ‹è¯•åŸºæœ¬å†²å‡»è®¡ç®—"""
            trade_volume = 100000
            
            result = model.calculate_impact(trade_volume)
            
            # éªŒè¯ç»“æžœç±»åž‹
            assert isinstance(result, ImpactResult)
            
            # éªŒè¯æ°¸ä¹…å†²å‡»ï¼ˆçº¿æ€§ï¼‰
            expected_permanent = 0.1 * (100000 / 1000000)
            assert abs(result.permanent_impact - expected_permanent) < 1e-8
            
            # éªŒè¯ä¸´æ—¶å†²å‡»ï¼ˆå¹³æ–¹æ ¹ï¼‰
            expected_temporary = 0.5 * 0.02 * np.sqrt(100000 / 1000000)
            assert abs(result.temporary_impact - expected_temporary) < 1e-8
            
            # éªŒè¯æ€»å†²å‡»
            expected_total = expected_permanent + expected_temporary
            assert abs(result.total_impact - expected_total) < 1e-8
            
            # éªŒè¯äº¤æ˜“é‡ä¿¡æ¯
            assert result.trade_volume == 100000
            assert result.market_volume == 1000000
        
        def test_calculate_impact_different_trade_sizes(self, model):
            """æµ‹è¯•ä¸åŒäº¤æ˜“è§„æ¨¡ä¸‹çš„æˆæœ¬ä¼°ç®—å‡†ç¡®æ€§"""
            trade_volumes = [10000, 50000, 100000, 200000, 500000]
            results = []
            
            for volume in trade_volumes:
                result = model.calculate_impact(volume)
                results.append(result)
            
            # éªŒè¯æ°¸ä¹…å†²å‡»éšäº¤æ˜“é‡çº¿æ€§å¢žé•¿
            for i in range(1, len(results)):
                ratio = results[i].permanent_impact / results[i-1].permanent_impact
                volume_ratio = trade_volumes[i] / trade_volumes[i-1]
                assert abs(ratio - volume_ratio) < 1e-6
            
            # éªŒè¯ä¸´æ—¶å†²å‡»éšäº¤æ˜“é‡å¹³æ–¹æ ¹å¢žé•¿
            for i in range(1, len(results)):
                ratio = results[i].temporary_impact / results[i-1].temporary_impact
                volume_ratio = np.sqrt(trade_volumes[i] / trade_volumes[i-1])
                assert abs(ratio - volume_ratio) < 1e-6
            
            # éªŒè¯æ€»å†²å‡»éšäº¤æ˜“é‡å¢žé•¿ï¼ˆä½†å¢žé•¿çŽ‡é€’å‡ï¼‰
            for i in range(1, len(results)):
                assert results[i].total_impact > results[i-1].total_impact
        
        def test_calculate_impact_with_custom_market_volume(self, model):
            """æµ‹è¯•è‡ªå®šä¹‰å¸‚åœºæˆäº¤é‡"""
            trade_volume = 100000
            custom_market_volume = 2000000
            
            result = model.calculate_impact(trade_volume, market_volume=custom_market_volume)
            
            # éªŒè¯ä½¿ç”¨äº†è‡ªå®šä¹‰å¸‚åœºæˆäº¤é‡
            assert result.market_volume == custom_market_volume
            
            # éªŒè¯å†²å‡»è®¡ç®—ä½¿ç”¨äº†è‡ªå®šä¹‰æˆäº¤é‡
            expected_permanent = 0.1 * (100000 / 2000000)
            assert abs(result.permanent_impact - expected_permanent) < 1e-8
        
        def test_calculate_impact_with_custom_volatility(self, model):
            """æµ‹è¯•è‡ªå®šä¹‰æ³¢åŠ¨çŽ‡"""
            trade_volume = 100000
            custom_volatility = 0.03
            
            result = model.calculate_impact(trade_volume, volatility=custom_volatility)
            
            # éªŒè¯ä¸´æ—¶å†²å‡»ä½¿ç”¨äº†è‡ªå®šä¹‰æ³¢åŠ¨çŽ‡
            expected_temporary = 0.5 * 0.03 * np.sqrt(100000 / 1000000)
            assert abs(result.temporary_impact - expected_temporary) < 1e-8
        
        def test_market_participation_rate_impact(self, default_parameters):
            """æµ‹è¯•å¸‚åœºå‚ä¸Žåº¦å¯¹å†²å‡»çš„å½±å“"""
            # åˆ›å»ºä¸åŒæµåŠ¨æ€§çš„å¸‚åœºï¼ˆé€šè¿‡è°ƒæ•´æˆäº¤é‡æ¥æ¨¡æ‹Ÿå‚ä¸Žåº¦å½±å“ï¼‰
            low_liquidity = MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=500000,  # ä½ŽæµåŠ¨æ€§ï¼ˆå°æˆäº¤é‡ï¼‰
                participation_rate=0.1
            )
            
            high_liquidity = MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=2000000,  # é«˜æµåŠ¨æ€§ï¼ˆå¤§æˆäº¤é‡ï¼‰
                participation_rate=0.1
            )
            
            low_liq_model = AlmgrenChrissModel(low_liquidity)
            high_liq_model = AlmgrenChrissModel(high_liquidity)
            
            trade_volume = 100000
            
            low_result = low_liq_model.calculate_impact(trade_volume)
            high_result = high_liq_model.calculate_impact(trade_volume)
            
            # ä½ŽæµåŠ¨æ€§å¸‚åœºåº”è¯¥å¯¼è‡´æ›´é«˜çš„å†²å‡»
            assert low_result.total_impact > high_result.total_impact
        
        def test_liquidity_impact_on_costs(self, default_parameters):
            """æµ‹è¯•æµåŠ¨æ€§å¯¹æˆæœ¬çš„å½±å“"""
            # åˆ›å»ºä¸åŒæµåŠ¨æ€§çš„å¸‚åœºå‚æ•°
            high_liquidity = MarketImpactParameters(
                permanent_impact_coeff=0.05,  # ä½Žå†²å‡»ç³»æ•°
                temporary_impact_coeff=0.3,
                volatility=0.015,  # ä½Žæ³¢åŠ¨çŽ‡
                daily_volume=2000000,  # é«˜æˆäº¤é‡
                participation_rate=0.1
            )
            
            low_liquidity = MarketImpactParameters(
                permanent_impact_coeff=0.2,  # é«˜å†²å‡»ç³»æ•°
                temporary_impact_coeff=0.8,
                volatility=0.03,  # é«˜æ³¢åŠ¨çŽ‡
                daily_volume=500000,  # ä½Žæˆäº¤é‡
                participation_rate=0.1
            )
            
            high_liq_model = AlmgrenChrissModel(high_liquidity)
            low_liq_model = AlmgrenChrissModel(low_liquidity)
            
            trade_volume = 100000
            
            high_liq_result = high_liq_model.calculate_impact(trade_volume)
            low_liq_result = low_liq_model.calculate_impact(trade_volume)
            
            # ä½ŽæµåŠ¨æ€§å¸‚åœºåº”è¯¥æœ‰æ›´é«˜çš„äº¤æ˜“æˆæœ¬
            assert low_liq_result.total_impact > high_liq_result.total_impact
            assert low_liq_result.permanent_impact > high_liq_result.permanent_impact
            assert low_liq_result.temporary_impact > high_liq_result.temporary_impact
        
        def test_zero_trade_volume(self, model):
            """æµ‹è¯•é›¶äº¤æ˜“é‡"""
            result = model.calculate_impact(0)
            
            assert result.permanent_impact == 0.0
            assert result.temporary_impact == 0.0
            assert result.total_impact == 0.0
            assert result.trade_volume == 0
        
        def test_very_large_trade_volume(self, model):
            """æµ‹è¯•æžå¤§äº¤æ˜“é‡"""
            # äº¤æ˜“é‡ç­‰äºŽå¸‚åœºæˆäº¤é‡
            trade_volume = 1000000
            
            result = model.calculate_impact(trade_volume)
            
            # æ°¸ä¹…å†²å‡»åº”è¯¥ç­‰äºŽç³»æ•°
            assert abs(result.permanent_impact - 0.1) < 1e-8
            
            # ä¸´æ—¶å†²å‡»åº”è¯¥ç­‰äºŽç³»æ•°ä¹˜ä»¥æ³¢åŠ¨çŽ‡
            expected_temporary = 0.5 * 0.02 * 1.0
            assert abs(result.temporary_impact - expected_temporary) < 1e-8
        
        def test_model_parameters_update(self, model):
            """æµ‹è¯•æ¨¡åž‹å‚æ•°æ›´æ–°"""
            new_parameters = MarketImpactParameters(
                permanent_impact_coeff=0.15,
                temporary_impact_coeff=0.6,
                volatility=0.025,
                daily_volume=1500000,
                participation_rate=0.12
            )
            
            model.update_parameters(new_parameters)
            
            assert model.parameters == new_parameters
            
            # éªŒè¯æ›´æ–°åŽçš„è®¡ç®—ç»“æžœ
            trade_volume = 100000
            result = model.calculate_impact(trade_volume)
            
            expected_permanent = 0.15 * (100000 / 1500000)
            assert abs(result.permanent_impact - expected_permanent) < 1e-8
    
    
    class TestAlmgrenChrissModelBoundaryConditions:
        """æµ‹è¯•Almgren-Chrissæ¨¡åž‹è¾¹ç•Œæ¡ä»¶"""
        
        def test_extreme_parameters(self):
            """æµ‹è¯•æžç«¯å‚æ•°"""
            # æžå°å‚æ•°
            small_params = MarketImpactParameters(
                permanent_impact_coeff=1e-6,
                temporary_impact_coeff=1e-6,
                volatility=1e-6,
                daily_volume=1,
                participation_rate=1e-6
            )
            
            model = AlmgrenChrissModel(small_params)
            result = model.calculate_impact(1)
            
            assert result.permanent_impact >= 0
            assert result.temporary_impact >= 0
            assert result.total_impact >= 0
            
            # æžå¤§å‚æ•°
            large_params = MarketImpactParameters(
                permanent_impact_coeff=1.0,
                temporary_impact_coeff=1.0,
                volatility=1.0,
                daily_volume=int(1e9),
                participation_rate=0.99
            )
            
            model = AlmgrenChrissModel(large_params)
            result = model.calculate_impact(int(1e6))
            
            assert result.permanent_impact >= 0
            assert result.temporary_impact >= 0
            assert result.total_impact >= 0
        
        def test_numerical_stability(self):
            """æµ‹è¯•æ•°å€¼ç¨³å®šæ€§"""
            params = MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=1000000,
                participation_rate=0.1
            )
            
            model = AlmgrenChrissModel(params)
            
            # æµ‹è¯•éžå¸¸å°çš„äº¤æ˜“é‡
            tiny_volume = 1e-6
            result = model.calculate_impact(tiny_volume)
            
            assert not np.isnan(result.permanent_impact)
            assert not np.isnan(result.temporary_impact)
            assert not np.isnan(result.total_impact)
            assert not np.isinf(result.permanent_impact)
            assert not np.isinf(result.temporary_impact)
            assert not np.isinf(result.total_impact)
    
    
    class TestAlmgrenChrissModelPerformance:
        """æµ‹è¯•Almgren-Chrissæ¨¡åž‹æ€§èƒ½"""
        
        def test_batch_calculation_performance(self):
            """æµ‹è¯•æ‰¹é‡è®¡ç®—æ€§èƒ½"""
            params = MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=1000000,
                participation_rate=0.1
            )
            
            model = AlmgrenChrissModel(params)
            
            # ç”Ÿæˆå¤§é‡äº¤æ˜“é‡æ•°æ®
            trade_volumes = np.random.randint(1000, 500000, size=1000)
            
            import time
            start_time = time.time()
            
            results = []
            for volume in trade_volumes:
                result = model.calculate_impact(volume)
                results.append(result)
            
            calculation_time = time.time() - start_time
            
            # è®¡ç®—æ—¶é—´åº”è¯¥åœ¨åˆç†èŒƒå›´å†…ï¼ˆæ¯ä¸ªè®¡ç®—å°äºŽ1msï¼‰
            assert calculation_time < 1.0
            assert len(results) == 1000
            
            # éªŒè¯æ‰€æœ‰ç»“æžœéƒ½æœ‰æ•ˆ
            for result in results:
                assert result.total_impact >= 0
                assert not np.isnan(result.total_impact)
        
        def test_memory_usage(self):
            """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
            params = MarketImpactParameters(
                permanent_impact_coeff=0.1,
                temporary_impact_coeff=0.5,
                volatility=0.02,
                daily_volume=1000000,
                participation_rate=0.1
            )
            
            model = AlmgrenChrissModel(params)
            
            # åˆ›å»ºå¤§é‡æ¨¡åž‹å®žä¾‹ä¸åº”è¯¥æ¶ˆè€—è¿‡å¤šå†…å­˜
            models = []
            for i in range(100):
                models.append(AlmgrenChrissModel(params))
            
            # éªŒè¯æ‰€æœ‰æ¨¡åž‹éƒ½èƒ½æ­£å¸¸å·¥ä½œ
            for model in models:
                result = model.calculate_impact(100000)
                assert result.total_impact > 0
    
    
    class TestAlmgrenChrissModelIntegration:
        """æµ‹è¯•Almgren-Chrissæ¨¡åž‹é›†æˆ"""
        
        def test_integration_with_real_market_data(self):
            """æµ‹è¯•ä¸ŽçœŸå®žå¸‚åœºæ•°æ®çš„é›†æˆ"""
            # æ¨¡æ‹ŸçœŸå®žå¸‚åœºå‚æ•°ï¼ˆåŸºäºŽAè‚¡å¸‚åœºç‰¹å¾ï¼‰
            a_share_params = MarketImpactParameters(
                permanent_impact_coeff=0.08,  # Aè‚¡æ°¸ä¹…å†²å‡»ç³»æ•°
                temporary_impact_coeff=0.4,   # Aè‚¡ä¸´æ—¶å†²å‡»ç³»æ•°
                volatility=0.025,             # Aè‚¡æ—¥å‡æ³¢åŠ¨çŽ‡
                daily_volume=5000000,         # Aè‚¡æ—¥å‡æˆäº¤é‡
                participation_rate=0.05       # å…¸åž‹å‚ä¸Žåº¦
            )
            
            model = AlmgrenChrissModel(a_share_params)
            
            # æµ‹è¯•ä¸åŒè§„æ¨¡çš„äº¤æ˜“
            small_trade = 50000    # å°é¢äº¤æ˜“
            medium_trade = 200000  # ä¸­ç­‰äº¤æ˜“
            large_trade = 1000000  # å¤§é¢äº¤æ˜“
            
            small_result = model.calculate_impact(small_trade)
            medium_result = model.calculate_impact(medium_trade)
            large_result = model.calculate_impact(large_trade)
            
            # éªŒè¯æˆæœ¬éšäº¤æ˜“è§„æ¨¡é€’å¢ž
            assert small_result.total_impact < medium_result.total_impact
            assert medium_result.total_impact < large_result.total_impact
            
            # éªŒè¯æˆæœ¬åœ¨åˆç†èŒƒå›´å†…ï¼ˆåŸºç‚¹ï¼‰
            assert small_result.get_cost_basis_points() < 50   # å°é¢äº¤æ˜“æˆæœ¬ < 5bp
            assert medium_result.get_cost_basis_points() < 100 # ä¸­ç­‰äº¤æ˜“æˆæœ¬ < 10bp
            assert large_result.get_cost_basis_points() < 300  # å¤§é¢äº¤æ˜“æˆæœ¬ < 30bp
        
        def test_model_calibration_validation(self):
            """æµ‹è¯•æ¨¡åž‹æ ¡å‡†éªŒè¯"""
            # ä½¿ç”¨åŽ†å²æ•°æ®æ ¡å‡†çš„å‚æ•°
            calibrated_params = MarketImpactParameters(
                permanent_impact_coeff=0.12,
                temporary_impact_coeff=0.45,
                volatility=0.022,
                daily_volume=3000000,
                participation_rate=0.08
            )
            
            model = AlmgrenChrissModel(calibrated_params)
            
            # æµ‹è¯•å‚æ•°åˆç†æ€§
            test_volume = 150000
            result = model.calculate_impact(test_volume)
            
            # éªŒè¯å†²å‡»éƒ½ä¸ºæ­£å€¼
            assert result.permanent_impact > 0
            assert result.temporary_impact > 0
            
            # æ€»æˆæœ¬åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
            cost_bp = result.get_cost_basis_points()
            assert 5 <= cost_bp <= 200  # 5-200åŸºç‚¹ä¹‹é—´
            
            # å‚ä¸Žåº¦åº”è¯¥åˆç†
            participation = result.get_participation_rate()
            assert 0.01 <= participation <= 0.5  # 1%-50%ä¹‹é—´
    ]]></file>
  <file path="tests/unit/test_alert_system.py"><![CDATA[
    """
    å‘Šè­¦ç³»ç»Ÿçš„å•å…ƒæµ‹è¯•
    æµ‹è¯•åŸºäºŽåŽ†å²åˆ†ä½æ•°çš„åŠ¨æ€é˜ˆå€¼è®¡ç®—ï¼Œé˜ˆå€¼è°ƒæ•´çš„åˆç†æ€§å’Œå‘Šè­¦å‡†ç¡®æ€§ï¼Œå‘Šè­¦è§„åˆ™é…ç½®å’Œç®¡ç†åŠŸèƒ½
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import pytest
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional, Any
    from unittest.mock import Mock, patch, MagicMock
    import json
    import threading
    import time
    
    from src.rl_trading_system.monitoring.alert_system import (
        DynamicThresholdManager,
        AlertRule,
        AlertLevel,
        AlertChannel,
        AlertAggregator,
        AlertLogger,
        AlertSystem,
        NotificationManager
    )
    
    
    class TestDynamicThresholdManager:
        """åŠ¨æ€é˜ˆå€¼ç®¡ç†å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_historical_data(self):
            """åˆ›å»ºæ ·æœ¬åŽ†å²æ•°æ®"""
            np.random.seed(42)
            dates = pd.date_range('2023-01-01', periods=252, freq='D')
            
            # åˆ›å»ºä¸åŒç±»åž‹çš„æŒ‡æ ‡æ•°æ®
            portfolio_values = np.random.normal(1000000, 50000, 252)
            daily_returns = np.random.normal(0.001, 0.02, 252)
            volatility = np.random.normal(0.15, 0.03, 252)
            max_drawdown = np.random.exponential(0.02, 252)  # æ€»æ˜¯æ­£å€¼
            
            return pd.DataFrame({
                'portfolio_value': portfolio_values,
                'daily_return': daily_returns,
                'volatility': volatility,
                'max_drawdown': max_drawdown,
                'timestamp': dates
            })
    
        @pytest.fixture
        def threshold_manager(self, sample_historical_data):
            """åˆ›å»ºåŠ¨æ€é˜ˆå€¼ç®¡ç†å™¨"""
            return DynamicThresholdManager(
                historical_data=sample_historical_data,
                lookback_window=60,
                update_frequency='daily'
            )
    
        def test_threshold_manager_initialization(self, threshold_manager, sample_historical_data):
            """æµ‹è¯•é˜ˆå€¼ç®¡ç†å™¨åˆå§‹åŒ–"""
            assert threshold_manager.historical_data is not None
            assert len(threshold_manager.historical_data) == len(sample_historical_data)
            assert threshold_manager.lookback_window == 60
            assert threshold_manager.update_frequency == 'daily'
            assert isinstance(threshold_manager.thresholds, dict)
    
        def test_percentile_threshold_calculation(self, threshold_manager):
            """æµ‹è¯•åŸºäºŽåˆ†ä½æ•°çš„é˜ˆå€¼è®¡ç®—"""
            # è®¡ç®—ä¸åŒåˆ†ä½æ•°çš„é˜ˆå€¼
            metric_name = 'daily_return'
            
            # 95%åˆ†ä½æ•° - ä¸Šé™
            upper_threshold = threshold_manager.calculate_percentile_threshold(
                metric_name=metric_name,
                percentile=95,
                threshold_type='upper'
            )
            
            # 5%åˆ†ä½æ•° - ä¸‹é™
            lower_threshold = threshold_manager.calculate_percentile_threshold(
                metric_name=metric_name,
                percentile=5,
                threshold_type='lower'
            )
            
            # éªŒè¯é˜ˆå€¼åˆç†æ€§
            assert isinstance(upper_threshold, float)
            assert isinstance(lower_threshold, float)
            assert upper_threshold > lower_threshold
            
            # éªŒè¯ä¸Žå®žé™…åˆ†ä½æ•°çš„ä¸€è‡´æ€§
            actual_data = threshold_manager.historical_data[metric_name]
            expected_upper = actual_data.quantile(0.95)
            expected_lower = actual_data.quantile(0.05)
            
            assert abs(upper_threshold - expected_upper) < 1e-10
            assert abs(lower_threshold - expected_lower) < 1e-10
    
        def test_rolling_threshold_calculation(self, threshold_manager):
            """æµ‹è¯•æ»šåŠ¨çª—å£é˜ˆå€¼è®¡ç®—"""
            metric_name = 'volatility'
            window_size = 30
            
            rolling_thresholds = threshold_manager.calculate_rolling_threshold(
                metric_name=metric_name,
                window_size=window_size,
                percentile=90
            )
            
            # éªŒè¯æ»šåŠ¨é˜ˆå€¼ç»“æžœ
            assert isinstance(rolling_thresholds, pd.Series)
            assert len(rolling_thresholds) == len(threshold_manager.historical_data)
            
            # å‰å‡ ä¸ªå€¼åº”è¯¥æ˜¯NaNï¼ˆçª—å£ä¸å¤Ÿï¼‰
            assert rolling_thresholds.iloc[:window_size-1].isna().all()
            
            # åŽé¢çš„å€¼åº”è¯¥æ˜¯æœ‰æ•ˆçš„
            valid_thresholds = rolling_thresholds.iloc[window_size-1:]
            assert valid_thresholds.notna().all()
            assert (valid_thresholds > 0).all()  # æ³¢åŠ¨çŽ‡åº”è¯¥ä¸ºæ­£
    
        def test_adaptive_threshold_adjustment(self, threshold_manager):
            """æµ‹è¯•è‡ªé€‚åº”é˜ˆå€¼è°ƒæ•´"""
            metric_name = 'max_drawdown'
            
            # åˆå§‹é˜ˆå€¼
            initial_threshold = threshold_manager.calculate_percentile_threshold(
                metric_name=metric_name,
                percentile=90,
                threshold_type='upper'
            )
            
            # æ¨¡æ‹Ÿæ–°æ•°æ®ç‚¹ï¼ˆå¼‚å¸¸é«˜çš„å›žæ’¤ï¼‰
            new_data = pd.DataFrame({
                'max_drawdown': [0.15, 0.12, 0.18, 0.10, 0.08],
                'timestamp': pd.date_range('2023-09-10', periods=5, freq='D')
            })
            
            # æ›´æ–°é˜ˆå€¼
            updated_threshold = threshold_manager.update_threshold_with_new_data(
                metric_name=metric_name,
                new_data=new_data,
                adaptation_factor=0.1
            )
            
            # éªŒè¯é˜ˆå€¼è°ƒæ•´
            assert isinstance(updated_threshold, float)
            assert updated_threshold != initial_threshold
            # ç”±äºŽåŒ…å«äº†æ›´é«˜çš„å›žæ’¤å€¼ï¼Œé˜ˆå€¼åº”è¯¥æœ‰æ‰€æé«˜
            assert updated_threshold >= initial_threshold
    
        def test_multi_metric_threshold_management(self, threshold_manager):
            """æµ‹è¯•å¤šæŒ‡æ ‡é˜ˆå€¼ç®¡ç†"""
            metrics = ['portfolio_value', 'daily_return', 'volatility', 'max_drawdown']
            threshold_configs = {
                'portfolio_value': {'percentile': 5, 'type': 'lower'},
                'daily_return': {'percentile': 95, 'type': 'upper'},
                'volatility': {'percentile': 90, 'type': 'upper'},
                'max_drawdown': {'percentile': 95, 'type': 'upper'}
            }
            
            # æ‰¹é‡è®¡ç®—é˜ˆå€¼
            all_thresholds = threshold_manager.calculate_multiple_thresholds(threshold_configs)
            
            # éªŒè¯ç»“æžœ
            assert isinstance(all_thresholds, dict)
            assert len(all_thresholds) == len(metrics)
            
            for metric in metrics:
                assert metric in all_thresholds
                assert isinstance(all_thresholds[metric], float)
            
            # éªŒè¯ç‰¹å®šå…³ç³»
            assert all_thresholds['daily_return'] > 0  # 95%åˆ†ä½æ•°åº”è¯¥ä¸ºæ­£
            assert all_thresholds['volatility'] > 0    # æ³¢åŠ¨çŽ‡åº”è¯¥ä¸ºæ­£
            assert all_thresholds['max_drawdown'] > 0  # æœ€å¤§å›žæ’¤åº”è¯¥ä¸ºæ­£
    
        def test_threshold_validation(self, threshold_manager):
            """æµ‹è¯•é˜ˆå€¼æœ‰æ•ˆæ€§éªŒè¯"""
            # æµ‹è¯•æœ‰æ•ˆé˜ˆå€¼
            valid_config = {
                'metric_name': 'daily_return',
                'percentile': 95,
                'threshold_type': 'upper',
                'min_samples': 30
            }
            
            is_valid = threshold_manager.validate_threshold_config(valid_config)
            assert is_valid is True
            
            # æµ‹è¯•æ— æ•ˆåˆ†ä½æ•°
            invalid_percentile_config = {
                'metric_name': 'daily_return',
                'percentile': 105,  # æ— æ•ˆ
                'threshold_type': 'upper',
                'min_samples': 30
            }
            
            is_valid = threshold_manager.validate_threshold_config(invalid_percentile_config)
            assert is_valid is False
            
            # æµ‹è¯•ä¸å­˜åœ¨çš„æŒ‡æ ‡
            invalid_metric_config = {
                'metric_name': 'nonexistent_metric',
                'percentile': 95,
                'threshold_type': 'upper',
                'min_samples': 30
            }
            
            is_valid = threshold_manager.validate_threshold_config(invalid_metric_config)
            assert is_valid is False
    
        def test_threshold_persistence(self, threshold_manager, tmp_path):
            """æµ‹è¯•é˜ˆå€¼æŒä¹…åŒ–"""
            # è®¡ç®—ä¸€äº›é˜ˆå€¼
            thresholds = {
                'daily_return_upper': 0.05,
                'daily_return_lower': -0.03,
                'volatility_upper': 0.25,
                'max_drawdown_upper': 0.08
            }
            
            threshold_manager.thresholds = thresholds
            
            # ä¿å­˜é˜ˆå€¼
            save_path = tmp_path / "thresholds.json"
            threshold_manager.save_thresholds(str(save_path))
            
            # éªŒè¯æ–‡ä»¶å­˜åœ¨
            assert save_path.exists()
            
            # åŠ è½½é˜ˆå€¼
            new_manager = DynamicThresholdManager(
                historical_data=threshold_manager.historical_data,
                lookback_window=60
            )
            new_manager.load_thresholds(str(save_path))
            
            # éªŒè¯åŠ è½½çš„é˜ˆå€¼
            assert new_manager.thresholds == thresholds
    
        def test_statistical_outlier_detection(self, threshold_manager):
            """æµ‹è¯•ç»Ÿè®¡å¼‚å¸¸å€¼æ£€æµ‹"""
            metric_name = 'daily_return'
            
            # æ­£å¸¸å€¼
            normal_value = 0.01
            is_outlier_normal = threshold_manager.is_statistical_outlier(
                metric_name=metric_name,
                value=normal_value,
                method='zscore',
                threshold=3.0
            )
            assert is_outlier_normal == False
            
            # å¼‚å¸¸å€¼
            outlier_value = 0.15  # æžé«˜çš„æ—¥æ”¶ç›ŠçŽ‡
            is_outlier_extreme = threshold_manager.is_statistical_outlier(
                metric_name=metric_name,
                value=outlier_value,
                method='zscore',
                threshold=3.0
            )
            assert is_outlier_extreme == True
    
        def test_threshold_sensitivity_analysis(self, threshold_manager):
            """æµ‹è¯•é˜ˆå€¼æ•æ„Ÿæ€§åˆ†æž"""
            metric_name = 'volatility'
            percentiles = [80, 85, 90, 95, 99]
            
            sensitivity_results = threshold_manager.analyze_threshold_sensitivity(
                metric_name=metric_name,
                percentiles=percentiles
            )
            
            # éªŒè¯æ•æ„Ÿæ€§åˆ†æžç»“æžœ
            assert isinstance(sensitivity_results, dict)
            assert len(sensitivity_results) == len(percentiles)
            
            # éªŒè¯é˜ˆå€¼é€’å¢žæ€§
            threshold_values = [sensitivity_results[p] for p in percentiles]
            assert all(threshold_values[i] <= threshold_values[i+1] for i in range(len(threshold_values)-1))
    
        def test_invalid_data_handling(self):
            """æµ‹è¯•æ— æ•ˆæ•°æ®å¤„ç†"""
            # ç©ºæ•°æ®é›†
            with pytest.raises(ValueError, match="åŽ†å²æ•°æ®ä¸èƒ½ä¸ºç©º"):
                DynamicThresholdManager(
                    historical_data=pd.DataFrame(),
                    lookback_window=60
                )
            
            # æ— æ•ˆçª—å£å¤§å°
            valid_data = pd.DataFrame({
                'metric': [1, 2, 3],
                'timestamp': pd.date_range('2023-01-01', periods=3, freq='D')
            })
            
            with pytest.raises(ValueError, match="å›žçœ‹çª—å£å¤§å°å¿…é¡»ä¸ºæ­£æ•°"):
                DynamicThresholdManager(
                    historical_data=valid_data,
                    lookback_window=0
                )
    
    
    class TestAlertRule:
        """å‘Šè­¦è§„åˆ™æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def sample_alert_rule(self):
            """åˆ›å»ºæ ·æœ¬å‘Šè­¦è§„åˆ™"""
            return AlertRule(
                rule_id="test_rule_001",
                metric_name="max_drawdown",
                threshold_value=0.1,
                comparison_operator=">",
                alert_level=AlertLevel.WARNING,
                description="æœ€å¤§å›žæ’¤è¿‡é«˜å‘Šè­¦"
            )
    
        def test_alert_rule_initialization(self, sample_alert_rule):
            """æµ‹è¯•å‘Šè­¦è§„åˆ™åˆå§‹åŒ–"""
            assert sample_alert_rule.rule_id == "test_rule_001"
            assert sample_alert_rule.metric_name == "max_drawdown"
            assert sample_alert_rule.threshold_value == 0.1
            assert sample_alert_rule.comparison_operator == ">"
            assert sample_alert_rule.alert_level == AlertLevel.WARNING
            assert sample_alert_rule.is_active is True
    
        def test_rule_evaluation(self, sample_alert_rule):
            """æµ‹è¯•è§„åˆ™è¯„ä¼°"""
            # è§¦å‘å‘Šè­¦çš„å€¼
            trigger_value = 0.15
            should_trigger = sample_alert_rule.evaluate(trigger_value)
            assert should_trigger is True
            
            # ä¸è§¦å‘å‘Šè­¦çš„å€¼
            normal_value = 0.05
            should_not_trigger = sample_alert_rule.evaluate(normal_value)
            assert should_not_trigger is False
    
        def test_different_comparison_operators(self):
            """æµ‹è¯•ä¸åŒæ¯”è¾ƒæ“ä½œç¬¦"""
            test_cases = [
                (">", 0.1, 0.15, True),   # å¤§äºŽ
                (">", 0.1, 0.05, False),
                (">=", 0.1, 0.1, True),   # å¤§äºŽç­‰äºŽ
                (">=", 0.1, 0.05, False),
                ("<", 0.1, 0.05, True),   # å°äºŽ
                ("<", 0.1, 0.15, False),
                ("<=", 0.1, 0.1, True),   # å°äºŽç­‰äºŽ
                ("<=", 0.1, 0.15, False),
                ("==", 0.1, 0.1, True),   # ç­‰äºŽ
                ("==", 0.1, 0.15, False),
                ("!=", 0.1, 0.15, True),  # ä¸ç­‰äºŽ
                ("!=", 0.1, 0.1, False)
            ]
            
            for operator, threshold, value, expected in test_cases:
                rule = AlertRule(
                    rule_id=f"test_{operator}",
                    metric_name="test_metric",
                    threshold_value=threshold,
                    comparison_operator=operator,
                    alert_level=AlertLevel.INFO
                )
                
                result = rule.evaluate(value)
                assert result == expected, f"Failed for {operator}: {value} {operator} {threshold}"
    
        def test_rule_serialization(self, sample_alert_rule):
            """æµ‹è¯•è§„åˆ™åºåˆ—åŒ–"""
            # è½¬æ¢ä¸ºå­—å…¸
            rule_dict = sample_alert_rule.to_dict()
            
            assert isinstance(rule_dict, dict)
            assert rule_dict['rule_id'] == "test_rule_001"
            assert rule_dict['metric_name'] == "max_drawdown"
            assert rule_dict['threshold_value'] == 0.1
            assert rule_dict['comparison_operator'] == ">"
            assert rule_dict['alert_level'] == AlertLevel.WARNING.value
            
            # ä»Žå­—å…¸æ¢å¤
            restored_rule = AlertRule.from_dict(rule_dict)
            
            assert restored_rule.rule_id == sample_alert_rule.rule_id
            assert restored_rule.metric_name == sample_alert_rule.metric_name
            assert restored_rule.threshold_value == sample_alert_rule.threshold_value
            assert restored_rule.comparison_operator == sample_alert_rule.comparison_operator
            assert restored_rule.alert_level == sample_alert_rule.alert_level
    
        def test_rule_activation_deactivation(self, sample_alert_rule):
            """æµ‹è¯•è§„åˆ™æ¿€æ´»å’Œåœç”¨"""
            # åˆå§‹çŠ¶æ€åº”è¯¥æ˜¯æ¿€æ´»çš„
            assert sample_alert_rule.is_active is True
            
            # åœç”¨è§„åˆ™
            sample_alert_rule.deactivate()
            assert sample_alert_rule.is_active is False
            
            # åœç”¨çŠ¶æ€ä¸‹ä¸åº”è¯¥è§¦å‘å‘Šè­¦
            trigger_value = 0.15
            should_not_trigger = sample_alert_rule.evaluate(trigger_value)
            assert should_not_trigger is False
            
            # é‡æ–°æ¿€æ´»
            sample_alert_rule.activate()
            assert sample_alert_rule.is_active is True
            
            # æ¿€æ´»åŽåº”è¯¥èƒ½è§¦å‘å‘Šè­¦
            should_trigger = sample_alert_rule.evaluate(trigger_value)
            assert should_trigger is True
    
        def test_invalid_rule_parameters(self):
            """æµ‹è¯•æ— æ•ˆè§„åˆ™å‚æ•°"""
            # æ— æ•ˆæ¯”è¾ƒæ“ä½œç¬¦
            with pytest.raises(ValueError, match="ä¸æ”¯æŒçš„æ¯”è¾ƒæ“ä½œç¬¦"):
                AlertRule(
                    rule_id="invalid_op",
                    metric_name="test",
                    threshold_value=0.1,
                    comparison_operator="invalid",
                    alert_level=AlertLevel.ERROR
                )
            
            # ç©ºè§„åˆ™ID
            with pytest.raises(ValueError, match="è§„åˆ™IDä¸èƒ½ä¸ºç©º"):
                AlertRule(
                    rule_id="",
                    metric_name="test",
                    threshold_value=0.1,
                    comparison_operator=">",
                    alert_level=AlertLevel.ERROR
                )
    
    
    class TestAlertAggregator:
        """å‘Šè­¦èšåˆå™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def alert_aggregator(self):
            """åˆ›å»ºå‘Šè­¦èšåˆå™¨"""
            return AlertAggregator(
                aggregation_window=300,  # 5åˆ†é’Ÿ
                max_alerts_per_rule=3,
                similarity_threshold=0.8
            )
    
        @pytest.fixture
        def sample_alerts(self):
            """åˆ›å»ºæ ·æœ¬å‘Šè­¦"""
            base_time = datetime.now()
            return [
                {
                    'rule_id': 'rule_001',
                    'metric_name': 'max_drawdown',
                    'value': 0.12,
                    'threshold': 0.1,
                    'level': AlertLevel.WARNING,
                    'timestamp': base_time,
                    'message': 'æœ€å¤§å›žæ’¤è¿‡é«˜'
                },
                {
                    'rule_id': 'rule_001',
                    'metric_name': 'max_drawdown',
                    'value': 0.13,
                    'threshold': 0.1,
                    'level': AlertLevel.WARNING,
                    'timestamp': base_time + timedelta(seconds=30),  # 30ç§’å†…
                    'message': 'æœ€å¤§å›žæ’¤è¿‡é«˜'
                },
                {
                    'rule_id': 'rule_002',
                    'metric_name': 'volatility',
                    'value': 0.35,
                    'threshold': 0.3,
                    'level': AlertLevel.ERROR,
                    'timestamp': base_time + timedelta(seconds=15),  # 15ç§’å†…
                    'message': 'æ³¢åŠ¨çŽ‡å¼‚å¸¸'
                }
            ]
    
        def test_aggregator_initialization(self, alert_aggregator):
            """æµ‹è¯•èšåˆå™¨åˆå§‹åŒ–"""
            assert alert_aggregator.aggregation_window == 300
            assert alert_aggregator.max_alerts_per_rule == 3
            assert alert_aggregator.similarity_threshold == 0.8
            assert len(alert_aggregator.pending_alerts) == 0
    
        def test_alert_aggregation(self, alert_aggregator, sample_alerts):
            """æµ‹è¯•å‘Šè­¦èšåˆ"""
            # æ·»åŠ å‘Šè­¦
            for alert in sample_alerts:
                alert_aggregator.add_alert(alert)
            
            # æ‰§è¡Œèšåˆ
            aggregated_alerts = alert_aggregator.aggregate_alerts()
            
            # éªŒè¯èšåˆç»“æžœ
            assert isinstance(aggregated_alerts, list)
            assert len(aggregated_alerts) <= len(sample_alerts)
            
            # ç›¸åŒè§„åˆ™çš„å‘Šè­¦åº”è¯¥è¢«èšåˆ
            rule_001_alerts = [a for a in aggregated_alerts if a['rule_id'] == 'rule_001']
            assert len(rule_001_alerts) == 1  # ä¸¤ä¸ªç›¸ä¼¼çš„å‘Šè­¦è¢«èšåˆä¸ºä¸€ä¸ª
            
            # èšåˆåŽçš„å‘Šè­¦åº”è¯¥åŒ…å«è®¡æ•°ä¿¡æ¯
            aggregated_alert = rule_001_alerts[0]
            assert 'count' in aggregated_alert
            assert aggregated_alert['count'] == 2
    
        def test_similarity_calculation(self, alert_aggregator):
            """æµ‹è¯•ç›¸ä¼¼åº¦è®¡ç®—"""
            alert1 = {
                'rule_id': 'rule_001',
                'metric_name': 'max_drawdown',
                'level': AlertLevel.WARNING,
                'message': 'æœ€å¤§å›žæ’¤è¿‡é«˜'
            }
            
            alert2 = {
                'rule_id': 'rule_001',
                'metric_name': 'max_drawdown',
                'level': AlertLevel.WARNING,
                'message': 'æœ€å¤§å›žæ’¤è¿‡é«˜'
            }
            
            alert3 = {
                'rule_id': 'rule_002',
                'metric_name': 'volatility',
                'level': AlertLevel.ERROR,
                'message': 'æ³¢åŠ¨çŽ‡å¼‚å¸¸'
            }
            
            # ç›¸åŒè§„åˆ™çš„å‘Šè­¦ç›¸ä¼¼åº¦åº”è¯¥é«˜
            similarity_12 = alert_aggregator.calculate_similarity(alert1, alert2)
            assert similarity_12 >= 0.8
            
            # ä¸åŒè§„åˆ™çš„å‘Šè­¦ç›¸ä¼¼åº¦åº”è¯¥ä½Ž
            similarity_13 = alert_aggregator.calculate_similarity(alert1, alert3)
            assert similarity_13 < 0.8
    
        def test_rate_limiting(self, alert_aggregator):
            """æµ‹è¯•é¢‘çŽ‡é™åˆ¶"""
            # åˆ›å»ºå¤§é‡ç›¸åŒçš„å‘Šè­¦
            base_time = datetime.now()
            excessive_alerts = []
            
            for i in range(10):  # è¶…è¿‡max_alerts_per_ruleçš„æ•°é‡
                excessive_alerts.append({
                    'rule_id': 'rule_spam',
                    'metric_name': 'test_metric',
                    'value': 0.1 + i * 0.01,
                    'level': AlertLevel.INFO,
                    'timestamp': base_time + timedelta(seconds=i * 10),
                    'message': f'æµ‹è¯•å‘Šè­¦ {i}'
                })
            
            # æ·»åŠ æ‰€æœ‰å‘Šè­¦
            for alert in excessive_alerts:
                alert_aggregator.add_alert(alert)
            
            # æ‰§è¡Œèšåˆ
            aggregated_alerts = alert_aggregator.aggregate_alerts()
            
            # éªŒè¯é¢‘çŽ‡é™åˆ¶ç”Ÿæ•ˆ
            rule_spam_alerts = [a for a in aggregated_alerts if a['rule_id'] == 'rule_spam']
            assert len(rule_spam_alerts) <= alert_aggregator.max_alerts_per_rule
    
        def test_time_window_expiry(self, alert_aggregator):
            """æµ‹è¯•æ—¶é—´çª—å£è¿‡æœŸ"""
            old_time = datetime.now() - timedelta(minutes=10)  # è¶…å‡ºèšåˆçª—å£
            current_time = datetime.now()
            
            old_alert = {
                'rule_id': 'rule_old',
                'metric_name': 'test',
                'timestamp': old_time,
                'level': AlertLevel.INFO,
                'message': 'è¿‡æœŸå‘Šè­¦'
            }
            
            current_alert = {
                'rule_id': 'rule_current',
                'metric_name': 'test',
                'timestamp': current_time,
                'level': AlertLevel.INFO,
                'message': 'å½“å‰å‘Šè­¦'
            }
            
            alert_aggregator.add_alert(old_alert)
            alert_aggregator.add_alert(current_alert)
            
            # æ‰§è¡Œèšåˆ
            aggregated_alerts = alert_aggregator.aggregate_alerts()
            
            # è¿‡æœŸçš„å‘Šè­¦åº”è¯¥è¢«æ¸…ç†
            rule_ids = [a['rule_id'] for a in aggregated_alerts]
            assert 'rule_old' not in rule_ids
            assert 'rule_current' in rule_ids
    
    
    class TestNotificationManager:
        """é€šçŸ¥ç®¡ç†å™¨æµ‹è¯•ç±»"""
    
        @pytest.fixture
        def notification_manager(self):
            """åˆ›å»ºé€šçŸ¥ç®¡ç†å™¨"""
            return NotificationManager({
                'email': {
                    'enabled': True,
                    'smtp_server': 'smtp.test.com',
                    'smtp_port': 587,
                    'username': 'test@example.com',
                    'password': 'test_password',
                    'recipients': ['admin@example.com']
                },
                'webhook': {
                    'enabled': True,
                    'url': 'https://hooks.slack.com/test',
                    'timeout': 10
                }
            })
    
        def test_notification_manager_initialization(self, notification_manager):
            """æµ‹è¯•é€šçŸ¥ç®¡ç†å™¨åˆå§‹åŒ–"""
            assert notification_manager.channels is not None
            assert 'email' in notification_manager.channels
            assert 'webhook' in notification_manager.channels
            assert notification_manager.channels['email']['enabled'] is True
            assert notification_manager.channels['webhook']['enabled'] is True
    
        @patch('smtplib.SMTP')
        def test_email_notification(self, mock_smtp, notification_manager):
            """æµ‹è¯•é‚®ä»¶é€šçŸ¥"""
            # é…ç½®mock
            mock_server = Mock()
            mock_smtp.return_value = mock_server
            
            alert_data = {
                'rule_id': 'test_rule',
                'metric_name': 'max_drawdown',
                'value': 0.15,
                'threshold': 0.1,
                'level': AlertLevel.ERROR,
                'message': 'æœ€å¤§å›žæ’¤ä¸¥é‡è¶…æ ‡'
            }
            
            # å‘é€é‚®ä»¶é€šçŸ¥
            result = notification_manager.send_email_notification(alert_data)
            
            # éªŒè¯é‚®ä»¶å‘é€
            assert result is True
            mock_smtp.assert_called_once()
            mock_server.starttls.assert_called_once()
            mock_server.login.assert_called_once()
            mock_server.send_message.assert_called_once()
            mock_server.quit.assert_called_once()
    
        @patch('requests.post')
        def test_webhook_notification(self, mock_post, notification_manager):
            """æµ‹è¯•Webhooké€šçŸ¥"""
            # é…ç½®mockå“åº”
            mock_response = Mock()
            mock_response.status_code = 200
            mock_post.return_value = mock_response
            
            alert_data = {
                'rule_id': 'test_rule',
                'metric_name': 'volatility',
                'value': 0.35,
                'threshold': 0.3,
                'level': AlertLevel.WARNING,
                'message': 'æ³¢åŠ¨çŽ‡åé«˜'
            }
            
            # å‘é€Webhooké€šçŸ¥
            result = notification_manager.send_webhook_notification(alert_data)
            
            # éªŒè¯Webhookè°ƒç”¨
            assert result is True
            mock_post.assert_called_once()
            
            # éªŒè¯è¯·æ±‚å‚æ•°
            call_args = mock_post.call_args
            assert call_args[0][0] == 'https://hooks.slack.com/test'
            assert 'json' in call_args[1]
    
        def test_notification_formatting(self, notification_manager):
            """æµ‹è¯•é€šçŸ¥æ ¼å¼åŒ–"""
            alert_data = {
                'rule_id': 'format_test',
                'metric_name': 'sharpe_ratio',
                'value': 0.5,
                'threshold': 1.0,
                'level': AlertLevel.WARNING,
                'message': 'å¤æ™®æ¯”çŽ‡åä½Ž',
                'timestamp': datetime(2023, 10, 1, 10, 30, 0)
            }
            
            # æ ¼å¼åŒ–é‚®ä»¶å†…å®¹
            email_content = notification_manager.format_email_content(alert_data)
            
            assert isinstance(email_content, dict)
            assert 'subject' in email_content
            assert 'body' in email_content
            assert 'format_test' in email_content['subject']
            assert 'sharpe_ratio' in email_content['body']
            assert '0.5' in email_content['body']
            
            # æ ¼å¼åŒ–Webhookå†…å®¹
            webhook_content = notification_manager.format_webhook_content(alert_data)
            
            assert isinstance(webhook_content, dict)
            assert 'text' in webhook_content or 'content' in webhook_content
    
        def test_notification_retry_mechanism(self, notification_manager):
            """æµ‹è¯•é€šçŸ¥é‡è¯•æœºåˆ¶"""
            with patch('requests.post') as mock_post:
                # æ¨¡æ‹Ÿå‰ä¸¤æ¬¡å¤±è´¥ï¼Œç¬¬ä¸‰æ¬¡æˆåŠŸ
                mock_responses = [
                    Mock(status_code=500),  # ç¬¬ä¸€æ¬¡å¤±è´¥
                    Mock(status_code=503),  # ç¬¬äºŒæ¬¡å¤±è´¥
                    Mock(status_code=200)   # ç¬¬ä¸‰æ¬¡æˆåŠŸ
                ]
                mock_post.side_effect = mock_responses
                
                alert_data = {
                    'rule_id': 'retry_test',
                    'level': AlertLevel.ERROR,
                    'message': 'é‡è¯•æµ‹è¯•'
                }
                
                # å‘é€é€šçŸ¥ï¼ˆåº”è¯¥é‡è¯•ï¼‰
                result = notification_manager.send_webhook_notification(
                    alert_data, 
                    max_retries=3,
                    retry_delay=0.1
                )
                
                # éªŒè¯é‡è¯•æˆåŠŸ
                assert result is True
                assert mock_post.call_count == 3
    
        def test_notification_channel_management(self, notification_manager):
            """æµ‹è¯•é€šçŸ¥æ¸ é“ç®¡ç†"""
            # ç¦ç”¨é‚®ä»¶æ¸ é“
            notification_manager.disable_channel('email')
            assert notification_manager.channels['email']['enabled'] is False
            
            # å¯ç”¨é‚®ä»¶æ¸ é“
            notification_manager.enable_channel('email')
            assert notification_manager.channels['email']['enabled'] is True
            
            # èŽ·å–æ´»è·ƒæ¸ é“
            active_channels = notification_manager.get_active_channels()
            assert 'email' in active_channels
            assert 'webhook' in active_channels
    
        def test_notification_rate_limiting(self, notification_manager):
            """æµ‹è¯•é€šçŸ¥é¢‘çŽ‡é™åˆ¶"""
            # è®¾ç½®é¢‘çŽ‡é™åˆ¶ï¼ˆæ¯åˆ†é’Ÿæœ€å¤š2æ¡ï¼‰
            notification_manager.set_rate_limit('email', max_notifications=2, time_window=60)
            
            alert_data = {
                'rule_id': 'rate_limit_test',
                'level': AlertLevel.INFO,
                'message': 'é¢‘çŽ‡é™åˆ¶æµ‹è¯•'
            }
            
            with patch('smtplib.SMTP'):
                # å‘é€å‰ä¸¤æ¡åº”è¯¥æˆåŠŸ
                assert notification_manager.send_email_notification(alert_data) is True
                assert notification_manager.send_email_notification(alert_data) is True
                
                # ç¬¬ä¸‰æ¡åº”è¯¥è¢«é™åˆ¶
                assert notification_manager.send_email_notification(alert_data) is False
    
    
    class TestAlertSystem:
        """å‘Šè­¦ç³»ç»Ÿé›†æˆæµ‹è¯•ç±»"""
    
        @pytest.fixture
        def alert_system(self):
            """åˆ›å»ºå®Œæ•´çš„å‘Šè­¦ç³»ç»Ÿ"""
            # åˆ›å»ºåŽ†å²æ•°æ®
            np.random.seed(42)
            historical_data = pd.DataFrame({
                'portfolio_value': np.random.normal(1000000, 50000, 100),
                'daily_return': np.random.normal(0.001, 0.02, 100),
                'max_drawdown': np.random.exponential(0.02, 100),
                'timestamp': pd.date_range('2023-01-01', periods=100, freq='D')
            })
            
            # é€šçŸ¥é…ç½®
            notification_config = {
                'email': {
                    'enabled': True,
                    'recipients': ['admin@test.com']
                }
            }
            
            return AlertSystem(
                historical_data=historical_data,
                notification_config=notification_config
            )
    
        def test_alert_system_initialization(self, alert_system):
            """æµ‹è¯•å‘Šè­¦ç³»ç»Ÿåˆå§‹åŒ–"""
            assert alert_system.threshold_manager is not None
            assert alert_system.notification_manager is not None
            assert alert_system.aggregator is not None
            assert alert_system.logger is not None
            assert len(alert_system.rules) == 0
    
        def test_rule_management(self, alert_system):
            """æµ‹è¯•è§„åˆ™ç®¡ç†"""
            rule = AlertRule(
                rule_id="system_test_rule",
                metric_name="max_drawdown",
                threshold_value=0.1,
                comparison_operator=">",
                alert_level=AlertLevel.ERROR
            )
            
            # æ·»åŠ è§„åˆ™
            alert_system.add_rule(rule)
            assert len(alert_system.rules) == 1
            assert "system_test_rule" in alert_system.rules
            
            # èŽ·å–è§„åˆ™
            retrieved_rule = alert_system.get_rule("system_test_rule")
            assert retrieved_rule is not None
            assert retrieved_rule.rule_id == "system_test_rule"
            
            # åˆ é™¤è§„åˆ™
            alert_system.remove_rule("system_test_rule")
            assert len(alert_system.rules) == 0
    
        def test_metric_monitoring(self, alert_system):
            """æµ‹è¯•æŒ‡æ ‡ç›‘æŽ§"""
            # æ·»åŠ å‘Šè­¦è§„åˆ™
            rule = AlertRule(
                rule_id="monitoring_test",
                metric_name="max_drawdown",
                threshold_value=0.08,
                comparison_operator=">",
                alert_level=AlertLevel.WARNING
            )
            alert_system.add_rule(rule)
            
            # ç›‘æŽ§æ­£å¸¸å€¼
            normal_metrics = {
                'max_drawdown': 0.05,
                'daily_return': 0.01,
                'portfolio_value': 1000000
            }
            
            alerts = alert_system.check_metrics(normal_metrics)
            assert len(alerts) == 0
            
            # ç›‘æŽ§å¼‚å¸¸å€¼
            abnormal_metrics = {
                'max_drawdown': 0.12,  # è¶…è¿‡é˜ˆå€¼
                'daily_return': 0.01,
                'portfolio_value': 1000000
            }
            
            alerts = alert_system.check_metrics(abnormal_metrics)
            assert len(alerts) == 1
            assert alerts[0]['rule_id'] == "monitoring_test"
    
        def test_end_to_end_alerting(self, alert_system):
            """æµ‹è¯•ç«¯åˆ°ç«¯å‘Šè­¦æµç¨‹"""
            # è®¾ç½®å‘Šè­¦è§„åˆ™
            rules = [
                AlertRule("dd_warning", "max_drawdown", 0.1, ">", AlertLevel.WARNING),
                AlertRule("vol_error", "volatility", 0.3, ">", AlertLevel.ERROR)
            ]
            
            for rule in rules:
                alert_system.add_rule(rule)
            
            # æ¨¡æ‹Ÿå¼‚å¸¸æŒ‡æ ‡
            abnormal_metrics = {
                'max_drawdown': 0.15,
                'volatility': 0.35,
                'daily_return': -0.05
            }
            
            with patch.object(alert_system.notification_manager, 'send_notification') as mock_notify:
                mock_notify.return_value = True
                
                # å¤„ç†æŒ‡æ ‡ï¼ˆåº”è¯¥è§¦å‘å‘Šè­¦ï¼‰
                alert_system.process_metrics(abnormal_metrics)
                
                # éªŒè¯é€šçŸ¥è¢«å‘é€
                assert mock_notify.call_count >= 1
    
        def test_alert_silencing(self, alert_system):
            """æµ‹è¯•å‘Šè­¦é™é»˜"""
            rule = AlertRule(
                rule_id="silence_test",
                metric_name="daily_return",
                threshold_value=-0.05,
                comparison_operator="<",
                alert_level=AlertLevel.WARNING
            )
            alert_system.add_rule(rule)
            
            # é™é»˜ç‰¹å®šè§„åˆ™
            alert_system.silence_rule("silence_test", duration=300)  # 5åˆ†é’Ÿ
            
            # è§¦å‘å‘Šè­¦æ¡ä»¶
            metrics = {'daily_return': -0.08}
            alerts = alert_system.check_metrics(metrics)
            
            # åº”è¯¥æ²¡æœ‰å‘Šè­¦ï¼ˆè¢«é™é»˜ï¼‰
            assert len(alerts) == 0
    
        def test_concurrent_monitoring(self, alert_system):
            """æµ‹è¯•å¹¶å‘ç›‘æŽ§"""
            rule = AlertRule(
                rule_id="concurrent_test",
                metric_name="portfolio_value",
                threshold_value=900000,
                comparison_operator="<",
                alert_level=AlertLevel.ERROR
            )
            alert_system.add_rule(rule)
            
            def monitor_metrics():
                for i in range(10):
                    metrics = {
                        'portfolio_value': 800000 + i * 10000,
                        'timestamp': datetime.now()
                    }
                    alert_system.process_metrics(metrics)
                    time.sleep(0.01)
            
            # å¯åŠ¨å¤šä¸ªç›‘æŽ§çº¿ç¨‹
            threads = []
            for _ in range(3):
                thread = threading.Thread(target=monitor_metrics)
                threads.append(thread)
                thread.start()
            
            # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
            for thread in threads:
                thread.join()
            
            # éªŒè¯ç³»ç»Ÿç¨³å®šè¿è¡Œ
            assert len(alert_system.rules) == 1
            assert alert_system.rules["concurrent_test"].is_active
    ]]></file>
  <file path="tests/unit/test_actor_network.py"><![CDATA[
    """
    æµ‹è¯•Actorç½‘ç»œçš„å•å…ƒæµ‹è¯•
    """
    import pytest
    import torch
    import torch.nn as nn
    import numpy as np
    from unittest.mock import Mock, patch
    
    from src.rl_trading_system.models.actor_network import Actor, ActorConfig
    
    
    class TestActorNetwork:
        """Actorç½‘ç»œæµ‹è¯•ç±»"""
        
        @pytest.fixture
        def actor_config(self):
            """Actoré…ç½®fixture"""
            return ActorConfig(
                state_dim=256,
                action_dim=100,
                hidden_dim=512,
                n_layers=3,
                activation='relu',
                dropout=0.1,
                log_std_min=-20,
                log_std_max=2
            )
        
        @pytest.fixture
        def actor_network(self, actor_config):
            """Actorç½‘ç»œfixture"""
            return Actor(actor_config)
        
        @pytest.fixture
        def sample_state(self, actor_config):
            """æ ·æœ¬çŠ¶æ€fixture"""
            batch_size = 32
            return torch.randn(batch_size, actor_config.state_dim)
        
        def test_actor_initialization(self, actor_network, actor_config):
            """æµ‹è¯•Actorç½‘ç»œåˆå§‹åŒ–"""
            assert isinstance(actor_network, nn.Module)
            assert actor_network.config.state_dim == actor_config.state_dim
            assert actor_network.config.action_dim == actor_config.action_dim
            assert actor_network.config.hidden_dim == actor_config.hidden_dim
            
            # æ£€æŸ¥ç½‘ç»œå±‚æ˜¯å¦æ­£ç¡®åˆ›å»º
            assert hasattr(actor_network, 'shared_layers')
            assert hasattr(actor_network, 'mean_head')
            assert hasattr(actor_network, 'log_std_head')
            
        def test_forward_pass_shape(self, actor_network, sample_state, actor_config):
            """æµ‹è¯•å‰å‘ä¼ æ’­è¾“å‡ºå½¢çŠ¶"""
            mean, log_std = actor_network.forward(sample_state)
            
            batch_size = sample_state.size(0)
            expected_shape = (batch_size, actor_config.action_dim)
            
            assert mean.shape == expected_shape
            assert log_std.shape == expected_shape
            
        def test_forward_pass_values(self, actor_network, sample_state, actor_config):
            """æµ‹è¯•å‰å‘ä¼ æ’­è¾“å‡ºå€¼çš„åˆç†æ€§"""
            mean, log_std = actor_network.forward(sample_state)
            
            # æ£€æŸ¥å‡å€¼æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
            assert torch.all(torch.isfinite(mean))
            assert torch.all(mean >= -10) and torch.all(mean <= 10)
            
            # æ£€æŸ¥log_stdæ˜¯å¦åœ¨æŒ‡å®šèŒƒå›´å†…
            assert torch.all(log_std >= actor_config.log_std_min)
            assert torch.all(log_std <= actor_config.log_std_max)
            
        def test_get_action_deterministic(self, actor_network, sample_state, actor_config):
            """æµ‹è¯•ç¡®å®šæ€§åŠ¨ä½œç”Ÿæˆ"""
            # è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼ä»¥ç¦ç”¨dropout
            actor_network.eval()
            
            action, log_prob = actor_network.get_action(sample_state, deterministic=True)
            
            batch_size = sample_state.size(0)
            expected_shape = (batch_size, actor_config.action_dim)
            
            assert action.shape == expected_shape
            assert log_prob.shape == (batch_size,)
            
            # ç¡®å®šæ€§åŠ¨ä½œåº”è¯¥æ˜¯å¯é‡å¤çš„
            action2, log_prob2 = actor_network.get_action(sample_state, deterministic=True)
            assert torch.allclose(action, action2, atol=1e-6)
            assert torch.allclose(log_prob, log_prob2, atol=1e-6)
            
        def test_get_action_stochastic(self, actor_network, sample_state, actor_config):
            """æµ‹è¯•éšæœºåŠ¨ä½œç”Ÿæˆ"""
            action1, log_prob1 = actor_network.get_action(sample_state, deterministic=False)
            action2, log_prob2 = actor_network.get_action(sample_state, deterministic=False)
            
            batch_size = sample_state.size(0)
            expected_shape = (batch_size, actor_config.action_dim)
            
            assert action1.shape == expected_shape
            assert action2.shape == expected_shape
            assert log_prob1.shape == (batch_size,)
            assert log_prob2.shape == (batch_size,)
            
            # éšæœºåŠ¨ä½œåº”è¯¥ä¸åŒ
            assert not torch.allclose(action1, action2, atol=1e-3)
            
        def test_portfolio_weight_constraints(self, actor_network, sample_state):
            """æµ‹è¯•æŠ•èµ„ç»„åˆæƒé‡çº¦æŸ"""
            action, _ = actor_network.get_action(sample_state, deterministic=True)
            
            # æ£€æŸ¥æƒé‡æ˜¯å¦éžè´Ÿ
            assert torch.all(action >= 0)
            
            # æ£€æŸ¥æƒé‡å’Œæ˜¯å¦ä¸º1ï¼ˆå…è®¸å°çš„æ•°å€¼è¯¯å·®ï¼‰
            weight_sums = torch.sum(action, dim=1)
            assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5)
            
        def test_action_probability_distribution(self, actor_network, sample_state):
            """æµ‹è¯•åŠ¨ä½œæ¦‚çŽ‡åˆ†å¸ƒçš„æœ‰æ•ˆæ€§"""
            # ç”Ÿæˆå¤šä¸ªåŠ¨ä½œæ ·æœ¬
            actions = []
            log_probs = []
            
            for _ in range(10):
                action, log_prob = actor_network.get_action(sample_state, deterministic=False)
                actions.append(action)
                log_probs.append(log_prob)
            
            actions = torch.stack(actions)
            log_probs = torch.stack(log_probs)
            
            # æ£€æŸ¥åŠ¨ä½œçš„å˜å¼‚æ€§
            action_std = torch.std(actions, dim=0)
            assert torch.all(action_std > 1e-4)  # åº”è¯¥æœ‰ä¸€å®šçš„å˜å¼‚æ€§
            
            # æ£€æŸ¥log_probçš„æœ‰é™æ€§
            assert torch.all(torch.isfinite(log_probs))
            
        def test_reparameterization_trick(self, actor_network, sample_state):
            """æµ‹è¯•é‡å‚æ•°åŒ–æŠ€å·§å’Œæ¢¯åº¦è®¡ç®—"""
            # å¯ç”¨æ¢¯åº¦è®¡ç®—
            sample_state.requires_grad_(True)
            
            action, log_prob = actor_network.get_action(sample_state, deterministic=False)
            
            # è®¡ç®—æŸå¤±ï¼ˆç®€å•çš„L2æŸå¤±ï¼‰
            loss = torch.mean(action ** 2) + torch.mean(log_prob ** 2)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥æ¢¯åº¦æ˜¯å¦å­˜åœ¨ä¸”æœ‰é™
            assert sample_state.grad is not None
            assert torch.all(torch.isfinite(sample_state.grad))
            
            # æ£€æŸ¥ç½‘ç»œå‚æ•°æ˜¯å¦æœ‰æ¢¯åº¦
            for param in actor_network.parameters():
                if param.requires_grad:
                    assert param.grad is not None
                    assert torch.all(torch.isfinite(param.grad))
                    
        def test_log_probability_calculation(self, actor_network, sample_state):
            """æµ‹è¯•å¯¹æ•°æ¦‚çŽ‡è®¡ç®—çš„æ­£ç¡®æ€§"""
            action, log_prob = actor_network.get_action(sample_state, deterministic=False)
            
            # é‡æ–°è®¡ç®—å¯¹æ•°æ¦‚çŽ‡
            mean, log_std = actor_network.forward(sample_state)
            std = torch.exp(log_std)
            
            # ä½¿ç”¨æ­£æ€åˆ†å¸ƒè®¡ç®—å¯¹æ•°æ¦‚çŽ‡
            normal_dist = torch.distributions.Normal(mean, std)
            
            # ç”±äºŽä½¿ç”¨äº†tanhå˜æ¢ï¼Œéœ€è¦è€ƒè™‘é›…å¯æ¯”è¡Œåˆ—å¼
            # è¿™é‡Œç®€åŒ–æµ‹è¯•ï¼Œä¸»è¦æ£€æŸ¥log_probçš„åˆç†æ€§
            assert torch.all(torch.isfinite(log_prob))
            assert torch.all(log_prob <= 0)  # å¯¹æ•°æ¦‚çŽ‡åº”è¯¥éžæ­£
            
        def test_batch_processing(self, actor_network, actor_config):
            """æµ‹è¯•æ‰¹å¤„ç†èƒ½åŠ›"""
            batch_sizes = [1, 16, 32, 64]
            
            for batch_size in batch_sizes:
                state = torch.randn(batch_size, actor_config.state_dim)
                action, log_prob = actor_network.get_action(state, deterministic=True)
                
                assert action.shape == (batch_size, actor_config.action_dim)
                assert log_prob.shape == (batch_size,)
                
        def test_network_parameter_count(self, actor_network, actor_config):
            """æµ‹è¯•ç½‘ç»œå‚æ•°æ•°é‡çš„åˆç†æ€§"""
            total_params = sum(p.numel() for p in actor_network.parameters())
            
            # ä¼°ç®—å‚æ•°æ•°é‡ï¼ˆç²—ç•¥ä¼°è®¡ï¼‰
            expected_min_params = (
                actor_config.state_dim * actor_config.hidden_dim +  # ç¬¬ä¸€å±‚
                actor_config.hidden_dim * actor_config.action_dim * 2  # è¾“å‡ºå±‚ï¼ˆmean + log_stdï¼‰
            )
            
            assert total_params >= expected_min_params
            assert total_params < expected_min_params * 10  # ä¸åº”è¯¥è¿‡å¤§
            
        def test_gradient_flow(self, actor_network, sample_state):
            """æµ‹è¯•æ¢¯åº¦æµåŠ¨"""
            sample_state.requires_grad_(True)
            
            # å‰å‘ä¼ æ’­
            action, log_prob = actor_network.get_action(sample_state, deterministic=False)
            
            # è®¡ç®—æŸå¤±
            loss = torch.mean(action) + torch.mean(log_prob)
            
            # åå‘ä¼ æ’­
            loss.backward()
            
            # æ£€æŸ¥æ‰€æœ‰å‚æ•°éƒ½æœ‰æ¢¯åº¦
            for name, param in actor_network.named_parameters():
                if param.requires_grad:
                    assert param.grad is not None, f"å‚æ•° {name} æ²¡æœ‰æ¢¯åº¦"
                    assert not torch.all(param.grad == 0), f"å‚æ•° {name} çš„æ¢¯åº¦ä¸ºé›¶"
                    
        def test_different_activation_functions(self, actor_config):
            """æµ‹è¯•ä¸åŒæ¿€æ´»å‡½æ•°"""
            activations = ['relu', 'tanh', 'gelu']
            
            for activation in activations:
                config = ActorConfig(
                    state_dim=actor_config.state_dim,
                    action_dim=actor_config.action_dim,
                    hidden_dim=actor_config.hidden_dim,
                    activation=activation
                )
                
                actor = Actor(config)
                state = torch.randn(16, actor_config.state_dim)
                
                # åº”è¯¥èƒ½å¤Ÿæ­£å¸¸å‰å‘ä¼ æ’­
                action, log_prob = actor.get_action(state, deterministic=True)
                assert action.shape == (16, actor_config.action_dim)
                assert log_prob.shape == (16,)
                
        def test_numerical_stability(self, actor_network):
            """æµ‹è¯•æ•°å€¼ç¨³å®šæ€§"""
            # æµ‹è¯•æžç«¯è¾“å…¥å€¼
            extreme_states = [
                torch.full((4, actor_network.config.state_dim), 1e6),   # å¾ˆå¤§çš„å€¼
                torch.full((4, actor_network.config.state_dim), -1e6),  # å¾ˆå°çš„å€¼
                torch.zeros(4, actor_network.config.state_dim),         # é›¶å€¼
                torch.full((4, actor_network.config.state_dim), float('nan'))  # NaNå€¼
            ]
            
            for i, state in enumerate(extreme_states[:-1]):  # è·³è¿‡NaNæµ‹è¯•
                action, log_prob = actor_network.get_action(state, deterministic=True)
                
                # è¾“å‡ºåº”è¯¥æ˜¯æœ‰é™çš„
                assert torch.all(torch.isfinite(action)), f"æžç«¯è¾“å…¥ {i} äº§ç”Ÿäº†æ— é™å€¼"
                assert torch.all(torch.isfinite(log_prob)), f"æžç«¯è¾“å…¥ {i} äº§ç”Ÿäº†æ— é™å¯¹æ•°æ¦‚çŽ‡"
                
        @pytest.mark.parametrize("state_dim,action_dim,hidden_dim", [
            (128, 50, 256),
            (512, 200, 1024),
            (64, 10, 128)
        ])
        def test_different_dimensions(self, state_dim, action_dim, hidden_dim):
            """æµ‹è¯•ä¸åŒç»´åº¦é…ç½®"""
            config = ActorConfig(
                state_dim=state_dim,
                action_dim=action_dim,
                hidden_dim=hidden_dim
            )
            
            actor = Actor(config)
            state = torch.randn(8, state_dim)
            
            action, log_prob = actor.get_action(state, deterministic=True)
            
            assert action.shape == (8, action_dim)
            assert log_prob.shape == (8,)
            
            # æ£€æŸ¥æƒé‡çº¦æŸ
            assert torch.all(action >= 0)
            weight_sums = torch.sum(action, dim=1)
            assert torch.allclose(weight_sums, torch.ones_like(weight_sums), atol=1e-5)
    ]]></file>
  <file path="tests/unit/__init__.py"><![CDATA[
    # å•å…ƒæµ‹è¯•
    ]]></file>
  <file path="test_models_e2e/metadata/e2e_model_v1.1.0.json"><![CDATA[
    {
      "model_id": "e2e_model",
      "version": "v1.1.0",
      "name": "E2Eé‡‘ä¸é›€æ¨¡åž‹",
      "description": "ç«¯åˆ°ç«¯æµ‹è¯•é‡‘ä¸é›€æ¨¡åž‹",
      "created_at": "2025-07-31T12:23:06.021853",
      "created_by": "developer",
      "model_type": "rl_agent",
      "framework": "pytorch",
      "status": "active",
      "tags": [],
      "metrics": {},
      "config": {},
      "file_path": "test_models_e2e/e2e_model_v1.1.0.model",
      "file_size": 2787,
      "checksum": "733f910d0740ee2ebccec1d04136b93e"
    }
    ]]></file>
  <file path="test_models_e2e/metadata/e2e_model_v1.0.0.json"><![CDATA[
    {
      "model_id": "e2e_model",
      "version": "v1.0.0",
      "name": "E2EåŸºçº¿æ¨¡åž‹",
      "description": "ç«¯åˆ°ç«¯æµ‹è¯•åŸºçº¿æ¨¡åž‹",
      "created_at": "2025-07-24T12:23:06.020464",
      "created_by": "system",
      "model_type": "rl_agent",
      "framework": "pytorch",
      "status": "active",
      "tags": [],
      "metrics": {},
      "config": {},
      "file_path": "test_models_e2e/e2e_model_v1.0.0.model",
      "file_size": 2851,
      "checksum": "bebcde888819e52ac5e83aebc5eaaa3b"
    }
    ]]></file>
  <file path="test_models_e2e/metadata/e2e_model_unique_v1.1.0.json"><![CDATA[
    {
      "model_id": "e2e_model_unique",
      "version": "v1.1.0",
      "name": "E2Eé‡‘ä¸é›€æ¨¡åž‹",
      "description": "ç«¯åˆ°ç«¯æµ‹è¯•é‡‘ä¸é›€æ¨¡åž‹",
      "created_at": "2025-07-31T12:27:04.476010",
      "created_by": "developer",
      "model_type": "rl_agent",
      "framework": "pytorch",
      "status": "active",
      "tags": [],
      "metrics": {},
      "config": {},
      "file_path": "test_models_e2e/e2e_model_unique_v1.1.0.model",
      "file_size": 2957,
      "checksum": "9e700eb4bc5a28149ef4308e54c2949e"
    }
    ]]></file>
  <file path="test_models_e2e/metadata/e2e_model_unique_v1.0.0.json"><![CDATA[
    {
      "model_id": "e2e_model_unique",
      "version": "v1.0.0",
      "name": "E2EåŸºçº¿æ¨¡åž‹",
      "description": "ç«¯åˆ°ç«¯æµ‹è¯•åŸºçº¿æ¨¡åž‹",
      "created_at": "2025-07-24T12:27:04.474489",
      "created_by": "system",
      "model_type": "rl_agent",
      "framework": "pytorch",
      "status": "active",
      "tags": [],
      "metrics": {},
      "config": {},
      "file_path": "test_models_e2e/e2e_model_unique_v1.0.0.model",
      "file_size": 2957,
      "checksum": "461dd434564234547857a079e6c1bee6"
    }
    ]]></file>
  <file path="test_models/metadata/trading_model_v1.1.0.json"><![CDATA[
    {
      "model_id": "trading_model",
      "version": "v1.1.0",
      "name": "æ”¹è¿›çš„äº¤æ˜“æ¨¡åž‹",
      "description": "æ–°çš„æ”¹è¿›ç‰ˆæœ¬",
      "created_at": "2025-07-31T12:22:32.905193",
      "created_by": "developer",
      "model_type": "rl_agent",
      "framework": "pytorch",
      "status": "active",
      "tags": [],
      "metrics": {
        "accuracy": 0.9,
        "precision": 0.88,
        "recall": 0.85,
        "f1_score": 0.86
      },
      "config": {},
      "file_path": "test_models/trading_model_v1.1.0.model",
      "file_size": 2875,
      "checksum": "6c52df3a23e1fa7e285cda7851053098"
    }
    ]]></file>
  <file path="test_models/metadata/trading_model_v1.0.0.json"><![CDATA[
    {
      "model_id": "trading_model",
      "version": "v1.0.0",
      "name": "åŸºçº¿äº¤æ˜“æ¨¡åž‹",
      "description": "ç¨³å®šçš„åŸºçº¿æ¨¡åž‹",
      "created_at": "2025-07-01T12:22:32.903547",
      "created_by": "system",
      "model_type": "rl_agent",
      "framework": "pytorch",
      "status": "active",
      "tags": [],
      "metrics": {
        "accuracy": 0.85,
        "precision": 0.83,
        "recall": 0.8,
        "f1_score": 0.81
      },
      "config": {},
      "file_path": "test_models/trading_model_v1.0.0.model",
      "file_size": 2811,
      "checksum": "347c33b99e68e681da8634193f6f17d1"
    }
    ]]></file>
  <file path="src/rl_trading_system/__init__.py"><![CDATA[
    """
    å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿ
    
    åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿ
    """
    
    __version__ = "0.1.0"
    __author__ = "RL Trading Team"
    __email__ = "team@rltrading.com"
    
    from .config import ConfigManager
    
    __all__ = [
        "ConfigManager"
    ]
    ]]></file>
  <file path="src/rl_trading_system/utils/logger.py"><![CDATA[
    """
    æ—¥å¿—é…ç½®æ¨¡å—
    
    ä½¿ç”¨loguruè¿›è¡Œç»“æž„åŒ–æ—¥å¿—è®°å½•
    """
    
    import sys
    from pathlib import Path
    from typing import Optional
    
    from loguru import logger
    
    
    def setup_logger(
        log_level: str = "INFO",
        log_file: Optional[str] = None,
        rotation: str = "1 day",
        retention: str = "30 days",
        compression: str = "gz",
        format_string: Optional[str] = None,
    ) -> None:
        """
        è®¾ç½®æ—¥å¿—é…ç½®
        
        Args:
            log_level: æ—¥å¿—çº§åˆ«
            log_file: æ—¥å¿—æ–‡ä»¶è·¯å¾„
            rotation: æ—¥å¿—è½®è½¬ç­–ç•¥
            retention: æ—¥å¿—ä¿ç•™æ—¶é—´
            compression: åŽ‹ç¼©æ ¼å¼
            format_string: è‡ªå®šä¹‰æ ¼å¼å­—ç¬¦ä¸²
        """
        # ç§»é™¤é»˜è®¤å¤„ç†å™¨
        logger.remove()
        
        # é»˜è®¤æ ¼å¼
        if format_string is None:
            format_string = (
                "{time:YYYY-MM-DD HH:mm:ss} | "
                "{level: <8} | "
                "{name}:{function}:{line} - "
                "{message}"
            )
        
        # æ·»åŠ æŽ§åˆ¶å°å¤„ç†å™¨
        logger.add(
            sys.stderr,
            level=log_level,
            format=format_string,
            colorize=True,
            backtrace=True,
            diagnose=True,
        )
        
        # æ·»åŠ æ–‡ä»¶å¤„ç†å™¨
        if log_file:
            log_path = Path(log_file)
            log_path.parent.mkdir(parents=True, exist_ok=True)
            
            logger.add(
                log_path,
                level=log_level,
                format=format_string,
                rotation=rotation,
                retention=retention,
                compression=compression,
                backtrace=True,
                diagnose=True,
            )
        
        logger.info(f"æ—¥å¿—ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆï¼Œçº§åˆ«: {log_level}")
    
    
    def get_logger(name: str):
        """èŽ·å–æŒ‡å®šåç§°çš„logger"""
        return logger.bind(name=name)
    ]]></file>
  <file path="src/rl_trading_system/utils/__init__.py"><![CDATA[
    """å·¥å…·æ¨¡å—"""
    
    from .logger import setup_logger, get_logger
    
    __all__ = ["setup_logger", "get_logger"]
    ]]></file>
  <file path="src/rl_trading_system/training/trainer.py"><![CDATA[
    """
    å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨å®žçŽ°
    å®žçŽ°RLTrainerç±»å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªçŽ¯ï¼ŒåŒ…æ‹¬æ—©åœæœºåˆ¶ã€å­¦ä¹ çŽ‡è°ƒåº¦å’Œæ¢¯åº¦è£å‰ª
    """
    
    import os
    import numpy as np
    import pandas as pd
    import torch
    import torch.nn as nn
    from datetime import datetime, timedelta
    from typing import Dict, List, Tuple, Any, Optional, Union
    from dataclasses import dataclass, field
    import logging
    import pickle
    from pathlib import Path
    import time
    
    from .data_split_strategy import SplitResult
    
    logger = logging.getLogger(__name__)
    
    
    @dataclass
    class TrainingConfig:
        """è®­ç»ƒé…ç½®"""
        n_episodes: int = 5000
        max_steps_per_episode: int = 252
        batch_size: int = 256
        learning_rate: float = 3e-4
        buffer_size: int = 1000000
        gamma: float = 0.99  # æŠ˜æ‰£å› å­
        tau: float = 0.005  # è½¯æ›´æ–°å‚æ•°
        
        # éªŒè¯å’Œä¿å­˜é¢‘çŽ‡
        validation_frequency: int = 50
        save_frequency: int = 100
        
        # æ—©åœå‚æ•°
        early_stopping_patience: int = 20
        early_stopping_min_delta: float = 0.001
        early_stopping_mode: str = 'max'  # 'max' or 'min'
        
        # å­¦ä¹ çŽ‡è°ƒåº¦
        lr_scheduler_step_size: int = 1000
        lr_scheduler_gamma: float = 0.95
        
        # æ¢¯åº¦è£å‰ª
        gradient_clip_norm: Optional[float] = 1.0
        
        # è®­ç»ƒç¨³å®šæ€§
        warmup_episodes: int = 100
        update_frequency: int = 1
        target_update_frequency: int = 1
        
        # ä¿å­˜è·¯å¾„
        save_dir: str = "./checkpoints"
        
        # éšæœºç§å­
        random_seed: Optional[int] = None
        
        # è®¾å¤‡
        device: str = "cuda" if torch.cuda.is_available() else "cpu"
        
        def __post_init__(self):
            """é…ç½®éªŒè¯"""
            if self.n_episodes <= 0:
                raise ValueError("n_episodeså¿…é¡»ä¸ºæ­£æ•°")
            
            if self.learning_rate <= 0:
                raise ValueError("learning_rateå¿…é¡»ä¸ºæ­£æ•°")
            
            if self.batch_size <= 0:
                raise ValueError("batch_sizeå¿…é¡»ä¸ºæ­£æ•°")
            
            if self.max_steps_per_episode <= 0:
                raise ValueError("max_steps_per_episodeå¿…é¡»ä¸ºæ­£æ•°")
            
            if self.early_stopping_mode not in ['max', 'min']:
                raise ValueError("early_stopping_modeå¿…é¡»æ˜¯'max'æˆ–'min'")
    
    
    class EarlyStopping:
        """æ—©åœæœºåˆ¶"""
        
        def __init__(self, patience: int = 20, min_delta: float = 0.001, mode: str = 'max'):
            """
            åˆå§‹åŒ–æ—©åœæœºåˆ¶
            
            Args:
                patience: è€å¿ƒå€¼ï¼Œå³å…è®¸çš„æ— æ”¹è¿›epochæ•°
                min_delta: æœ€å°æ”¹è¿›å¹…åº¦
                mode: 'max'è¡¨ç¤ºåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼Œ'min'è¡¨ç¤ºåˆ†æ•°è¶Šä½Žè¶Šå¥½
            """
            self.patience = patience
            self.min_delta = min_delta
            self.mode = mode
            self.best_score = None
            self.counter = 0
            self.early_stop = False
            
            if mode == 'max':
                self.is_better = lambda score, best: score > best + min_delta
            else:
                self.is_better = lambda score, best: score < best - min_delta
        
        def step(self, score: float) -> bool:
            """
            æ›´æ–°æ—©åœçŠ¶æ€
            
            Args:
                score: å½“å‰åˆ†æ•°
                
            Returns:
                bool: æ˜¯å¦åº”è¯¥æ—©åœ
            """
            if self.best_score is None:
                self.best_score = score
                return False
            
            if self.is_better(score, self.best_score):
                self.best_score = score
                self.counter = 0
            else:
                self.counter += 1
                
            if self.counter > self.patience:
                self.early_stop = True
                return True
            
            return False
        
        def reset(self):
            """é‡ç½®æ—©åœçŠ¶æ€"""
            self.best_score = None
            self.counter = 0
            self.early_stop = False
    
    
    class TrainingMetrics:
        """è®­ç»ƒæŒ‡æ ‡æ”¶é›†å™¨"""
        
        def __init__(self):
            self.episode_rewards: List[float] = []
            self.episode_lengths: List[int] = []
            self.actor_losses: List[float] = []
            self.critic_losses: List[float] = []
            self.temperature_losses: List[float] = []
            self.validation_scores: List[float] = []
            self.timestamps: List[datetime] = []
        
        def add_episode_metrics(self, reward: float, length: int, 
                              actor_loss: float = 0.0, critic_loss: float = 0.0,
                              temperature_loss: float = 0.0):
            """æ·»åŠ episodeæŒ‡æ ‡"""
            self.episode_rewards.append(reward)
            self.episode_lengths.append(length)
            self.actor_losses.append(actor_loss)
            self.critic_losses.append(critic_loss)
            self.temperature_losses.append(temperature_loss)
            self.timestamps.append(datetime.now())
        
        def add_validation_score(self, score: float):
            """æ·»åŠ éªŒè¯åˆ†æ•°"""
            self.validation_scores.append(score)
        
        def get_statistics(self, window: Optional[int] = None) -> Dict[str, float]:
            """èŽ·å–ç»Ÿè®¡ä¿¡æ¯"""
            if len(self.episode_rewards) == 0:
                return {}
            
            if window is not None:
                rewards = self.episode_rewards[-window:]
                lengths = self.episode_lengths[-window:]
                actor_losses = self.actor_losses[-window:]
                critic_losses = self.critic_losses[-window:]
            else:
                rewards = self.episode_rewards
                lengths = self.episode_lengths
                actor_losses = self.actor_losses
                critic_losses = self.critic_losses
            
            return {
                'mean_reward': np.mean(rewards),
                'std_reward': np.std(rewards),
                'min_reward': np.min(rewards),
                'max_reward': np.max(rewards),
                'mean_length': np.mean(lengths),
                'mean_actor_loss': np.mean(actor_losses),
                'mean_critic_loss': np.mean(critic_losses),
                'total_episodes': len(self.episode_rewards)
            }
        
        def get_recent_statistics(self, window: int = 100) -> Dict[str, float]:
            """èŽ·å–æœ€è¿‘windowä¸ªepisodeçš„ç»Ÿè®¡ä¿¡æ¯"""
            return self.get_statistics(window=window)
    
    
    class RLTrainer:
        """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""
        
        def __init__(self, config: TrainingConfig, environment, agent, data_split: SplitResult):
            """
            åˆå§‹åŒ–è®­ç»ƒå™¨
            
            Args:
                config: è®­ç»ƒé…ç½®
                environment: äº¤æ˜“çŽ¯å¢ƒ
                agent: å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
                data_split: æ•°æ®åˆ’åˆ†ç»“æžœ
            """
            self.config = config
            self.environment = environment
            self.agent = agent
            self.data_split = data_split
            
            # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶
            self.metrics = TrainingMetrics()
            self.early_stopping = EarlyStopping(
                patience=config.early_stopping_patience,
                min_delta=config.early_stopping_min_delta,
                mode=config.early_stopping_mode
            )
            
            # è®¾ç½®éšæœºç§å­
            if config.random_seed is not None:
                self._set_random_seed(config.random_seed)
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            self.save_dir = Path(config.save_dir)
            self.save_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆå§‹åŒ–å­¦ä¹ çŽ‡è°ƒåº¦å™¨ï¼ˆå¦‚æžœæ™ºèƒ½ä½“æ”¯æŒï¼‰
            self._setup_lr_scheduler()
            
            logger.info(f"è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆï¼Œé…ç½®: {config}")
        
        def _set_random_seed(self, seed: int):
            """è®¾ç½®éšæœºç§å­"""
            np.random.seed(seed)
            torch.manual_seed(seed)
            if torch.cuda.is_available():
                torch.cuda.manual_seed(seed)
                torch.cuda.manual_seed_all(seed)
        
        def _setup_lr_scheduler(self):
            """è®¾ç½®å­¦ä¹ çŽ‡è°ƒåº¦å™¨"""
            # åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™é‡Œä¼šè®¾ç½®PyTorchçš„å­¦ä¹ çŽ‡è°ƒåº¦å™¨
            # ç”±äºŽä½¿ç”¨æ¨¡æ‹Ÿæ™ºèƒ½ä½“ï¼Œè¿™é‡Œåªæ˜¯å ä½ç¬¦
            self.lr_scheduler = None
        
        def _get_current_learning_rate(self, episode: int) -> float:
            """èŽ·å–å½“å‰å­¦ä¹ çŽ‡"""
            # ç®€å•çš„æŒ‡æ•°è¡°å‡
            decay_rate = self.config.lr_scheduler_gamma
            decay_steps = self.config.lr_scheduler_step_size
            
            decay_factor = decay_rate ** (episode // decay_steps)
            return self.config.learning_rate * decay_factor
        
        def _run_episode(self, episode_num: int, training: bool = True) -> Tuple[float, int]:
            """
            è¿è¡Œå•ä¸ªepisode
            
            Args:
                episode_num: episodeç¼–å·
                training: æ˜¯å¦ä¸ºè®­ç»ƒæ¨¡å¼
                
            Returns:
                Tuple[float, int]: episodeå¥–åŠ±å’Œé•¿åº¦
            """
            if training:
                self.agent.train()
            else:
                self.agent.eval()
            
            obs = self.environment.reset()
            episode_reward = 0.0
            episode_length = 0
            
            for step in range(self.config.max_steps_per_episode):
                # é€‰æ‹©åŠ¨ä½œ
                action = self.agent.act(obs, deterministic=not training)
                
                # æ‰§è¡ŒåŠ¨ä½œ
                next_obs, reward, done, info = self.environment.step(action)
                
                episode_reward += reward
                episode_length += 1
                
                # å¦‚æžœæ˜¯è®­ç»ƒæ¨¡å¼ï¼Œå­˜å‚¨ç»éªŒå¹¶æ›´æ–°æ™ºèƒ½ä½“
                if training and hasattr(self.agent, 'replay_buffer'):
                    # å­˜å‚¨ç»éªŒåˆ°å›žæ”¾ç¼“å†²åŒº
                    self.agent.replay_buffer.add(obs, action, reward, next_obs, done)
                    
                    # å®šæœŸæ›´æ–°æ™ºèƒ½ä½“
                    if (episode_num > self.config.warmup_episodes and 
                        step % self.config.update_frequency == 0):
                        self._update_agent()
                
                obs = next_obs
                
                if done:
                    break
            
            return episode_reward, episode_length
        
        def _update_agent(self) -> Dict[str, float]:
            """æ›´æ–°æ™ºèƒ½ä½“å‚æ•°"""
            if hasattr(self.agent, 'update'):
                return self.agent.update(
                    replay_buffer=getattr(self.agent, 'replay_buffer', None),
                    batch_size=self.config.batch_size
                )
            return {}
        
        def _validate(self) -> float:
            """è¿è¡ŒéªŒè¯"""
            logger.info("å¼€å§‹éªŒè¯...")
            
            validation_rewards = []
            n_validation_episodes = 5  # è¿è¡Œ5ä¸ªéªŒè¯episode
            
            for _ in range(n_validation_episodes):
                reward, _ = self._run_episode(episode_num=-1, training=False)
                validation_rewards.append(reward)
            
            validation_score = np.mean(validation_rewards)
            logger.info(f"éªŒè¯å®Œæˆï¼Œå¹³å‡å¥–åŠ±: {validation_score:.4f}")
            
            return validation_score
        
        def save_checkpoint(self, filepath: str, episode: int):
            """ä¿å­˜æ£€æŸ¥ç‚¹"""
            checkpoint = {
                'episode': episode,
                'config': self.config,
                'metrics': self.metrics,
                'early_stopping_state': {
                    'best_score': self.early_stopping.best_score,
                    'counter': self.early_stopping.counter,
                    'early_stop': self.early_stopping.early_stop
                }
            }
            
            # ä¿å­˜æ™ºèƒ½ä½“çŠ¶æ€ï¼ˆå¦‚æžœæ”¯æŒï¼‰
            if hasattr(self.agent, 'save'):
                agent_path = filepath.replace('.pth', '_agent.pth')
                self.agent.save(agent_path)
                checkpoint['agent_path'] = agent_path
            
            with open(filepath, 'wb') as f:
                pickle.dump(checkpoint, f)
            
            logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {filepath}")
        
        def load_checkpoint(self, filepath: str) -> int:
            """åŠ è½½æ£€æŸ¥ç‚¹"""
            with open(filepath, 'rb') as f:
                checkpoint = pickle.load(f)
            
            episode = checkpoint['episode']
            self.metrics = checkpoint['metrics']
            
            # æ¢å¤æ—©åœçŠ¶æ€
            early_stopping_state = checkpoint['early_stopping_state']
            self.early_stopping.best_score = early_stopping_state['best_score']
            self.early_stopping.counter = early_stopping_state['counter']
            self.early_stopping.early_stop = early_stopping_state['early_stop']
            
            # åŠ è½½æ™ºèƒ½ä½“çŠ¶æ€ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            if 'agent_path' in checkpoint and hasattr(self.agent, 'load'):
                self.agent.load(checkpoint['agent_path'])
            
            logger.info(f"æ£€æŸ¥ç‚¹å·²ä»Ž {filepath} åŠ è½½ï¼Œepisode: {episode}")
            return episode
        
        def _log_episode_stats(self, episode: int, reward: float, length: int, 
                              update_info: Dict[str, float]):
            """è®°å½•episodeç»Ÿè®¡ä¿¡æ¯"""
            if episode % 10 == 0:  # æ¯10ä¸ªepisodeè®°å½•ä¸€æ¬¡
                recent_stats = self.metrics.get_recent_statistics(window=10)
                
                log_msg = (
                    f"Episode {episode:4d} | "
                    f"Reward: {reward:7.2f} | "
                    f"Length: {length:3d} | "
                    f"Avg Reward (10): {recent_stats.get('mean_reward', 0):7.2f}"
                )
                
                if update_info:
                    log_msg += (
                        f" | Actor Loss: {update_info.get('actor_loss', 0):.4f}"
                        f" | Critic Loss: {update_info.get('critic_loss', 0):.4f}"
                    )
                
                logger.info(log_msg)
        
        def train(self):
            """æ‰§è¡Œè®­ç»ƒ"""
            logger.info(f"å¼€å§‹è®­ç»ƒï¼Œæ€»episodes: {self.config.n_episodes}")
            
            start_time = time.time()
            best_validation_score = float('-inf') if self.config.early_stopping_mode == 'max' else float('inf')
            
            for episode in range(1, self.config.n_episodes + 1):
                # è¿è¡Œè®­ç»ƒepisode
                episode_reward, episode_length = self._run_episode(episode, training=True)
                
                # æ›´æ–°æ™ºèƒ½ä½“ï¼ˆèŽ·å–æŸå¤±ä¿¡æ¯ï¼‰
                update_info = {}
                if episode > self.config.warmup_episodes:
                    update_info = self._update_agent()
                
                # è®°å½•æŒ‡æ ‡
                self.metrics.add_episode_metrics(
                    reward=episode_reward,
                    length=episode_length,
                    actor_loss=update_info.get('actor_loss', 0.0),
                    critic_loss=update_info.get('critic_loss', 0.0),
                    temperature_loss=update_info.get('temperature_loss', 0.0)
                )
                
                # è®°å½•æ—¥å¿—
                self._log_episode_stats(episode, episode_reward, episode_length, update_info)
                
                # éªŒè¯
                if episode % self.config.validation_frequency == 0:
                    validation_score = self._validate()
                    self.metrics.add_validation_score(validation_score)
                    
                    # æ—©åœæ£€æŸ¥
                    if self.early_stopping.step(validation_score):
                        logger.info(f"è§¦å‘æ—©åœï¼Œepisode: {episode}")
                        break
                    
                    # æ›´æ–°æœ€ä½³éªŒè¯åˆ†æ•°
                    if ((self.config.early_stopping_mode == 'max' and validation_score > best_validation_score) or
                        (self.config.early_stopping_mode == 'min' and validation_score < best_validation_score)):
                        best_validation_score = validation_score
                        # ä¿å­˜æœ€ä½³æ¨¡åž‹
                        best_model_path = self.save_dir / "best_model.pth"
                        self.save_checkpoint(str(best_model_path), episode)
                
                # å®šæœŸä¿å­˜æ£€æŸ¥ç‚¹
                if episode % self.config.save_frequency == 0:
                    checkpoint_path = self.save_dir / f"checkpoint_episode_{episode}.pth"
                    self.save_checkpoint(str(checkpoint_path), episode)
            
            training_time = time.time() - start_time
            logger.info(f"è®­ç»ƒå®Œæˆï¼Œæ€»ç”¨æ—¶: {training_time:.2f}ç§’")
            
            # ä¿å­˜æœ€ç»ˆæ¨¡åž‹
            final_model_path = self.save_dir / "final_model.pth"
            self.save_checkpoint(str(final_model_path), episode)
            
            # è¿”å›žè®­ç»ƒç»Ÿè®¡
            return self.metrics.get_statistics()
        
        def evaluate(self, n_episodes: int = 10) -> Dict[str, float]:
            """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½"""
            logger.info(f"å¼€å§‹è¯„ä¼°ï¼Œepisodes: {n_episodes}")
            
            self.agent.eval()
            evaluation_rewards = []
            evaluation_lengths = []
            
            for episode in range(n_episodes):
                reward, length = self._run_episode(episode, training=False)
                evaluation_rewards.append(reward)
                evaluation_lengths.append(length)
            
            evaluation_stats = {
                'mean_reward': np.mean(evaluation_rewards),
                'std_reward': np.std(evaluation_rewards),
                'min_reward': np.min(evaluation_rewards),
                'max_reward': np.max(evaluation_rewards),
                'mean_length': np.mean(evaluation_lengths),
                'total_episodes': n_episodes
            }
            
            logger.info(f"è¯„ä¼°å®Œæˆ: {evaluation_stats}")
            return evaluation_stats
    ]]></file>
  <file path="src/rl_trading_system/training/data_split_strategy.py"><![CDATA[
    """
    æ•°æ®åˆ’åˆ†ç­–ç•¥å®žçŽ°
    å®žçŽ°æ—¶åºæ•°æ®çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†ï¼Œæ”¯æŒæ»šåŠ¨çª—å£å’Œæ•°æ®æ³„éœ²é˜²æŠ¤
    """
    
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from typing import Dict, List, Tuple, Any, Optional, Union
    from dataclasses import dataclass, field
    from abc import ABC, abstractmethod
    import logging
    from scipy import stats
    
    logger = logging.getLogger(__name__)
    
    
    @dataclass
    class SplitConfig:
        """æ•°æ®åˆ’åˆ†é…ç½®"""
        train_ratio: float = 0.7
        validation_ratio: float = 0.2
        test_ratio: float = 0.1
        min_train_samples: int = 100
        min_validation_samples: int = 30
        min_test_samples: int = 10
        gap_days: int = 0  # åˆ’åˆ†é—´éš”å¤©æ•°
        rolling_window_size: Optional[int] = None  # æ»šåŠ¨çª—å£å¤§å°
        step_size: Optional[int] = None  # æ»šåŠ¨æ­¥é•¿
        train_end_date: Optional[str] = None  # è®­ç»ƒç»“æŸæ—¥æœŸ
        validation_end_date: Optional[str] = None  # éªŒè¯ç»“æŸæ—¥æœŸ
        random_seed: Optional[int] = None
        
        def __post_init__(self):
            """é…ç½®éªŒè¯"""
            # æ£€æŸ¥æ¯”ä¾‹å’Œ
            if abs(self.train_ratio + self.validation_ratio + self.test_ratio - 1.0) > 1e-6:
                raise ValueError("æ¯”ä¾‹ä¹‹å’Œå¿…é¡»ä¸º1")
            
            # æ£€æŸ¥æ¯”ä¾‹éžè´Ÿ
            if any(ratio < 0 for ratio in [self.train_ratio, self.validation_ratio, self.test_ratio]):
                raise ValueError("æ¯”ä¾‹ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # æ£€æŸ¥æœ€å°æ ·æœ¬æ•°
            if any(samples <= 0 for samples in [self.min_train_samples, self.min_validation_samples, self.min_test_samples]):
                raise ValueError("æœ€å°æ ·æœ¬æ•°å¿…é¡»ä¸ºæ­£æ•°")
            
            # æ£€æŸ¥é—´éš”å¤©æ•°
            if self.gap_days < 0:
                raise ValueError("é—´éš”å¤©æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
    
    
    @dataclass
    class SplitResult:
        """æ•°æ®åˆ’åˆ†ç»“æžœ"""
        train_indices: np.ndarray
        validation_indices: np.ndarray
        test_indices: np.ndarray
        split_dates: Dict[str, str] = field(default_factory=dict)
        metadata: Dict[str, Any] = field(default_factory=dict)
        
        def __post_init__(self):
            """ç»“æžœéªŒè¯"""
            # æ£€æŸ¥ç´¢å¼•é‡å 
            if len(np.intersect1d(self.train_indices, self.validation_indices)) > 0:
                raise ValueError("è®­ç»ƒå’ŒéªŒè¯ç´¢å¼•ä¸èƒ½é‡å ")
            
            if len(np.intersect1d(self.validation_indices, self.test_indices)) > 0:
                raise ValueError("éªŒè¯å’Œæµ‹è¯•ç´¢å¼•ä¸èƒ½é‡å ")
            
            if len(np.intersect1d(self.train_indices, self.test_indices)) > 0:
                raise ValueError("è®­ç»ƒå’Œæµ‹è¯•ç´¢å¼•ä¸èƒ½é‡å ")
        
        def get_metrics(self) -> Dict[str, float]:
            """èŽ·å–åˆ’åˆ†ç»Ÿè®¡æŒ‡æ ‡"""
            train_size = len(self.train_indices)
            val_size = len(self.validation_indices)
            test_size = len(self.test_indices)
            total_size = train_size + val_size + test_size
            
            return {
                'train_size': train_size,
                'validation_size': val_size,
                'test_size': test_size,
                'total_size': total_size,
                'train_ratio': train_size / total_size if total_size > 0 else 0,
                'validation_ratio': val_size / total_size if total_size > 0 else 0,
                'test_ratio': test_size / total_size if total_size > 0 else 0
            }
    
    
    class DataSplitStrategy(ABC):
        """æ•°æ®åˆ’åˆ†ç­–ç•¥æŠ½è±¡åŸºç±»"""
        
        def __init__(self, config: SplitConfig):
            self.config = config
            if config.random_seed is not None:
                np.random.seed(config.random_seed)
        
        @abstractmethod
        def split(self, data: pd.DataFrame) -> SplitResult:
            """
            æ‰§è¡Œæ•°æ®åˆ’åˆ†
            
            Args:
                data: å¾…åˆ’åˆ†çš„æ•°æ®ï¼Œå¿…é¡»æœ‰datetimeç´¢å¼•
                
            Returns:
                SplitResult: åˆ’åˆ†ç»“æžœ
            """
            pass
        
        def _validate_data(self, data: pd.DataFrame):
            """éªŒè¯è¾“å…¥æ•°æ®"""
            if data.empty:
                raise ValueError("è¾“å…¥æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # æ£€æŸ¥æ˜¯å¦æœ‰datetimeç´¢å¼•
            if not isinstance(data.index, pd.MultiIndex):
                raise ValueError("æ•°æ®å¿…é¡»æœ‰MultiIndexï¼ŒåŒ…å«datetime")
            
            if 'datetime' not in data.index.names:
                raise ValueError("æ•°æ®ç´¢å¼•å¿…é¡»åŒ…å«datetime")
        
        def _get_datetime_index(self, data: pd.DataFrame) -> pd.DatetimeIndex:
            """èŽ·å–datetimeç´¢å¼•"""
            return data.index.get_level_values('datetime')
        
        def detect_temporal_leakage(self, result: SplitResult, data: pd.DataFrame) -> bool:
            """
            æ£€æµ‹æ—¶é—´æ³„éœ²
            
            Args:
                result: åˆ’åˆ†ç»“æžœ
                data: åŽŸå§‹æ•°æ®
                
            Returns:
                bool: æ˜¯å¦æ£€æµ‹åˆ°æ³„éœ²
            """
            datetime_index = self._get_datetime_index(data)
            
            # æ£€æŸ¥è®­ç»ƒå’ŒéªŒè¯çš„æ—¶é—´é¡ºåº
            train_dates = datetime_index[result.train_indices]
            val_dates = datetime_index[result.validation_indices]
            test_dates = datetime_index[result.test_indices]
            
            # è®­ç»ƒæ•°æ®çš„æœ€å¤§æ—¥æœŸåº”è¯¥å°äºŽéªŒè¯æ•°æ®çš„æœ€å°æ—¥æœŸ
            if len(train_dates) > 0 and len(val_dates) > 0:
                if train_dates.max() >= val_dates.min():
                    logger.warning("æ£€æµ‹åˆ°è®­ç»ƒå’ŒéªŒè¯æ•°æ®çš„æ—¶é—´æ³„éœ²")
                    return True
            
            # éªŒè¯æ•°æ®çš„æœ€å¤§æ—¥æœŸåº”è¯¥å°äºŽæµ‹è¯•æ•°æ®çš„æœ€å°æ—¥æœŸ
            if len(val_dates) > 0 and len(test_dates) > 0:
                if val_dates.max() >= test_dates.min():
                    logger.warning("æ£€æµ‹åˆ°éªŒè¯å’Œæµ‹è¯•æ•°æ®çš„æ—¶é—´æ³„éœ²")
                    return True
            
            return False
        
        def detect_feature_leakage(self, result: SplitResult, data: pd.DataFrame, 
                                 feature_columns: List[str]) -> bool:
            """
            æ£€æµ‹ç‰¹å¾æ³„éœ²ï¼ˆä½¿ç”¨æœªæ¥ä¿¡æ¯ï¼‰
            
            Args:
                result: åˆ’åˆ†ç»“æžœ
                data: åŽŸå§‹æ•°æ®
                feature_columns: è¦æ£€æŸ¥çš„ç‰¹å¾åˆ—
                
            Returns:
                bool: æ˜¯å¦æ£€æµ‹åˆ°æ³„éœ²
            """
            for feature in feature_columns:
                if feature not in data.columns:
                    continue
                
                # æ£€æŸ¥æ•´ä¸ªæ•°æ®é›†ä¸­çš„NaNæ¨¡å¼
                full_feature = data[feature]
                
                # æ£€æŸ¥ç‰¹å¾åç§°æ¨¡å¼ï¼Œè¯†åˆ«å¯èƒ½çš„æ³„éœ²ç‰¹å¾
                if any(pattern in feature.lower() for pattern in ['future', 'shift', 'lag', 'lead']):
                    if 'future' in feature.lower() or 'shift' in feature.lower() or 'lead' in feature.lower():
                        logger.warning(f"ç‰¹å¾ {feature} åç§°æš—ç¤ºå¯èƒ½ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯")
                        return True
                
                # æ£€æŸ¥NaNæ¨¡å¼ï¼šå¦‚æžœæœ«å°¾æœ‰è¿žç»­NaNè€Œå¼€å¤´æ²¡æœ‰ï¼Œå¯èƒ½æ˜¯å‰å‘shift
                if len(full_feature) > 20:
                    # æ£€æŸ¥æœ«å°¾20%çš„æ•°æ®
                    tail_size = int(len(full_feature) * 0.2)
                    tail_values = full_feature.tail(tail_size)
                    head_values = full_feature.head(tail_size)
                    
                    tail_nan_ratio = tail_values.isna().sum() / len(tail_values)
                    head_nan_ratio = head_values.isna().sum() / len(head_values)
                    
                    # å¦‚æžœæœ«å°¾NaNæ¯”ä¾‹æ˜Žæ˜¾é«˜äºŽå¼€å¤´ï¼Œå¯èƒ½æ˜¯å‰å‘shift
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ˜Žæ˜¾çš„ä¸å¯¹ç§°NaNåˆ†å¸ƒ
                    if tail_nan_ratio > 0.05 and head_nan_ratio == 0:
                        logger.warning(f"ç‰¹å¾ {feature} æœ«å°¾NaNæ¯”ä¾‹è¿‡é«˜ ({tail_nan_ratio:.2f})ï¼Œå¼€å¤´æ— NaNï¼Œç–‘ä¼¼ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯")
                        return True
                    elif tail_nan_ratio > 0.3 and head_nan_ratio < 0.1:
                        logger.warning(f"ç‰¹å¾ {feature} æœ«å°¾NaNæ¯”ä¾‹è¿‡é«˜ ({tail_nan_ratio:.2f})ï¼Œç–‘ä¼¼ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯")
                        return True
                
                # æ£€æŸ¥è®­ç»ƒé›†ä¸­çš„ç‰¹å¾å€¼
                train_feature = data[feature].iloc[result.train_indices]
                
                # å¦‚æžœæ•´ä¸ªè®­ç»ƒé›†çš„NaNæ¯”ä¾‹è¿‡é«˜
                if len(train_feature) > 0:
                    nan_ratio = train_feature.isna().sum() / len(train_feature)
                    if nan_ratio > 0.5:  # è¶…è¿‡50%çš„æ•°æ®ä¸ºNaN
                        logger.warning(f"ç‰¹å¾ {feature} åœ¨è®­ç»ƒé›†ä¸­æœ‰å¤§é‡NaNå€¼ ({nan_ratio:.2f})ï¼Œå¯èƒ½ä½¿ç”¨äº†æœªæ¥ä¿¡æ¯")
                        return True
            
            return False
        
        def detect_target_leakage(self, result: SplitResult, data: pd.DataFrame,
                                target_column: str, feature_columns: List[str]) -> bool:
            """
            æ£€æµ‹ç›®æ ‡å˜é‡æ³„éœ²
            
            Args:
                result: åˆ’åˆ†ç»“æžœ
                data: åŽŸå§‹æ•°æ®
                target_column: ç›®æ ‡å˜é‡åˆ—å
                feature_columns: ç‰¹å¾åˆ—å
                
            Returns:
                bool: æ˜¯å¦æ£€æµ‹åˆ°æ³„éœ²
            """
            if target_column not in data.columns:
                return False
            
            train_data = data.iloc[result.train_indices]
            target_data = train_data[target_column]
            
            for feature in feature_columns:
                if feature not in data.columns:
                    continue
                
                feature_data = train_data[feature]
                
                # é¦–å…ˆæ£€æŸ¥ç‰¹å¾æ˜¯å¦æ˜¯ç›®æ ‡çš„ç®€å•åç§»
                # æ£€æŸ¥ç‰¹å¾åç§°æ˜¯å¦æš—ç¤ºä½¿ç”¨äº†æœªæ¥ç›®æ ‡ï¼ˆå¦‚target_lag, target_shiftç­‰ï¼‰
                if 'target' in feature.lower() and ('lag' in feature.lower() or 'shift' in feature.lower()):
                    logger.warning(f"ç‰¹å¾ {feature} åç§°æš—ç¤ºå¯èƒ½ä½¿ç”¨äº†ç›®æ ‡å˜é‡çš„æœªæ¥å€¼")
                    return True
                
                # æ£€æŸ¥ç‰¹å¾ä¸Žç›®æ ‡çš„ç›¸å…³æ€§
                valid_mask = ~(feature_data.isna() | target_data.isna())
                if valid_mask.sum() > 10:  # éœ€è¦è¶³å¤Ÿçš„æœ‰æ•ˆæ•°æ®
                    valid_feature = feature_data[valid_mask]
                    valid_target = target_data[valid_mask]
                    
                    if len(valid_feature) > 1 and len(valid_target) > 1:
                        correlation = stats.pearsonr(valid_feature, valid_target)[0]
                        
                        if abs(correlation) > 0.95:  # å¼‚å¸¸é«˜çš„ç›¸å…³æ€§
                            logger.warning(f"ç‰¹å¾ {feature} ä¸Žç›®æ ‡å˜é‡ç›¸å…³æ€§å¼‚å¸¸é«˜ ({correlation:.3f})ï¼Œå¯èƒ½å­˜åœ¨æ³„éœ²")
                            return True
            
            return False
    
    
    class TimeSeriesSplitStrategy(DataSplitStrategy):
        """æ—¶åºæ•°æ®åˆ’åˆ†ç­–ç•¥"""
        
        def split(self, data: pd.DataFrame) -> SplitResult:
            """
            æŒ‰æ—¶é—´é¡ºåºåˆ’åˆ†æ•°æ®
            
            Args:
                data: å¾…åˆ’åˆ†çš„æ—¶åºæ•°æ®
                
            Returns:
                SplitResult: åˆ’åˆ†ç»“æžœ
            """
            self._validate_data(data)
            
            # èŽ·å–å”¯ä¸€çš„æ—¶é—´ç‚¹å¹¶æŽ’åº
            datetime_index = self._get_datetime_index(data)
            unique_dates = datetime_index.unique().sort_values()
            
            if len(unique_dates) < (self.config.min_train_samples + 
                                   self.config.min_validation_samples + 
                                   self.config.min_test_samples):
                raise ValueError("æ•°æ®é‡ä¸è¶³ä»¥æ»¡è¶³æœ€å°æ ·æœ¬æ•°è¦æ±‚")
            
            # è®¡ç®—åˆ’åˆ†ç‚¹
            total_dates = len(unique_dates)
            
            # è€ƒè™‘é—´éš”å¤©æ•°
            effective_dates = total_dates - 2 * self.config.gap_days
            if effective_dates <= 0:
                raise ValueError("é—´éš”å¤©æ•°è¿‡å¤§ï¼Œæ— æ³•è¿›è¡Œæœ‰æ•ˆåˆ’åˆ†")
            
            train_size = int(effective_dates * self.config.train_ratio)
            val_size = int(effective_dates * self.config.validation_ratio)
            
            # ç¡®ä¿æ»¡è¶³æœ€å°æ ·æœ¬æ•°è¦æ±‚
            train_size = max(train_size, self.config.min_train_samples)
            val_size = max(val_size, self.config.min_validation_samples)
            
            # è®¡ç®—æ—¥æœŸè¾¹ç•Œ
            train_end_idx = train_size
            val_start_idx = train_end_idx + self.config.gap_days
            val_end_idx = val_start_idx + val_size
            test_start_idx = val_end_idx + self.config.gap_days
            
            if test_start_idx >= total_dates:
                raise ValueError("é…ç½®å‚æ•°å¯¼è‡´æ— æ³•ç”Ÿæˆæµ‹è¯•é›†")
            
            # èŽ·å–å¯¹åº”çš„æ—¥æœŸ
            train_dates = unique_dates[:train_end_idx]
            val_dates = unique_dates[val_start_idx:val_end_idx]
            test_dates = unique_dates[test_start_idx:]
            
            # è½¬æ¢ä¸ºç´¢å¼•
            train_indices = self._dates_to_indices(data, train_dates)
            val_indices = self._dates_to_indices(data, val_dates)
            test_indices = self._dates_to_indices(data, test_dates)
            
            # æž„å»ºæ—¥æœŸå­—å…¸
            split_dates = {
                'train_start': train_dates[0].strftime('%Y-%m-%d') if len(train_dates) > 0 else '',
                'train_end': train_dates[-1].strftime('%Y-%m-%d') if len(train_dates) > 0 else '',
                'val_start': val_dates[0].strftime('%Y-%m-%d') if len(val_dates) > 0 else '',
                'val_end': val_dates[-1].strftime('%Y-%m-%d') if len(val_dates) > 0 else '',
                'test_start': test_dates[0].strftime('%Y-%m-%d') if len(test_dates) > 0 else '',
                'test_end': test_dates[-1].strftime('%Y-%m-%d') if len(test_dates) > 0 else ''
            }
            
            result = SplitResult(
                train_indices=train_indices,
                validation_indices=val_indices,
                test_indices=test_indices,
                split_dates=split_dates,
                metadata={'strategy': 'time_series', 'gap_days': self.config.gap_days}
            )
            
            # æ£€æµ‹æ³„éœ²
            if self.detect_temporal_leakage(result, data):
                logger.warning("æ£€æµ‹åˆ°æ—¶é—´æ³„éœ²ï¼Œè¯·æ£€æŸ¥æ•°æ®å’Œé…ç½®")
            
            return result
        
        def _dates_to_indices(self, data: pd.DataFrame, dates: pd.DatetimeIndex) -> np.ndarray:
            """å°†æ—¥æœŸè½¬æ¢ä¸ºæ•°æ®ç´¢å¼•"""
            datetime_index = self._get_datetime_index(data)
            mask = datetime_index.isin(dates)
            return np.where(mask)[0]
    
    
    class FixedSplitStrategy(DataSplitStrategy):
        """å›ºå®šåˆ’åˆ†ç­–ç•¥"""
        
        def split(self, data: pd.DataFrame) -> SplitResult:
            """
            æŒ‰å›ºå®šæ—¥æœŸæˆ–æ¯”ä¾‹åˆ’åˆ†æ•°æ®
            
            Args:
                data: å¾…åˆ’åˆ†çš„æ—¶åºæ•°æ®
                
            Returns:
                SplitResult: åˆ’åˆ†ç»“æžœ
            """
            self._validate_data(data)
            
            datetime_index = self._get_datetime_index(data)
            
            if self.config.train_end_date and self.config.validation_end_date:
                # æŒ‰æ—¥æœŸåˆ’åˆ†
                return self._split_by_dates(data, datetime_index)
            else:
                # æŒ‰æ¯”ä¾‹åˆ’åˆ†
                return self._split_by_ratios(data, datetime_index)
        
        def _split_by_dates(self, data: pd.DataFrame, datetime_index: pd.DatetimeIndex) -> SplitResult:
            """æŒ‰æ—¥æœŸåˆ’åˆ†"""
            train_end = pd.Timestamp(self.config.train_end_date)
            val_end = pd.Timestamp(self.config.validation_end_date)
            
            # åˆ›å»ºæŽ©ç 
            train_mask = datetime_index <= train_end
            val_mask = (datetime_index > train_end) & (datetime_index <= val_end)
            test_mask = datetime_index > val_end
            
            # åº”ç”¨é—´éš”
            if self.config.gap_days > 0:
                gap_delta = timedelta(days=self.config.gap_days)
                
                # è°ƒæ•´éªŒè¯é›†å¼€å§‹æ—¶é—´
                val_start = train_end + gap_delta
                val_mask = (datetime_index >= val_start) & (datetime_index <= val_end)
                
                # è°ƒæ•´æµ‹è¯•é›†å¼€å§‹æ—¶é—´
                test_start = val_end + gap_delta
                test_mask = datetime_index >= test_start
            
            train_indices = np.where(train_mask)[0]
            val_indices = np.where(val_mask)[0]
            test_indices = np.where(test_mask)[0]
            
            split_dates = {
                'train_start': datetime_index[train_indices[0]].strftime('%Y-%m-%d') if len(train_indices) > 0 else '',
                'train_end': self.config.train_end_date,
                'val_start': datetime_index[val_indices[0]].strftime('%Y-%m-%d') if len(val_indices) > 0 else '',
                'val_end': self.config.validation_end_date,
                'test_start': datetime_index[test_indices[0]].strftime('%Y-%m-%d') if len(test_indices) > 0 else '',
                'test_end': datetime_index[test_indices[-1]].strftime('%Y-%m-%d') if len(test_indices) > 0 else ''
            }
            
            return SplitResult(
                train_indices=train_indices,
                validation_indices=val_indices,
                test_indices=test_indices,
                split_dates=split_dates,
                metadata={'strategy': 'fixed_dates'}
            )
        
        def _split_by_ratios(self, data: pd.DataFrame, datetime_index: pd.DatetimeIndex) -> SplitResult:
            """æŒ‰æ¯”ä¾‹åˆ’åˆ†"""
            total_samples = len(data)
            
            train_size = int(total_samples * self.config.train_ratio)
            val_size = int(total_samples * self.config.validation_ratio)
            
            # è€ƒè™‘é—´éš”
            gap_samples = int(self.config.gap_days * len(datetime_index.unique()) / 
                             (datetime_index.max() - datetime_index.min()).days)
            
            train_indices = np.arange(train_size)
            val_start = train_size + gap_samples
            val_indices = np.arange(val_start, val_start + val_size)
            test_start = val_start + val_size + gap_samples
            test_indices = np.arange(test_start, total_samples)
            
            # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
            val_indices = val_indices[val_indices < total_samples]
            test_indices = test_indices[test_indices < total_samples]
            
            split_dates = {
                'train_start': datetime_index[train_indices[0]].strftime('%Y-%m-%d') if len(train_indices) > 0 else '',
                'train_end': datetime_index[train_indices[-1]].strftime('%Y-%m-%d') if len(train_indices) > 0 else '',
                'val_start': datetime_index[val_indices[0]].strftime('%Y-%m-%d') if len(val_indices) > 0 else '',
                'val_end': datetime_index[val_indices[-1]].strftime('%Y-%m-%d') if len(val_indices) > 0 else '',
                'test_start': datetime_index[test_indices[0]].strftime('%Y-%m-%d') if len(test_indices) > 0 else '',
                'test_end': datetime_index[test_indices[-1]].strftime('%Y-%m-%d') if len(test_indices) > 0 else ''
            }
            
            return SplitResult(
                train_indices=train_indices,
                validation_indices=val_indices,
                test_indices=test_indices,
                split_dates=split_dates,
                metadata={'strategy': 'fixed_ratios'}
            )
    
    
    class RollingWindowSplitStrategy(DataSplitStrategy):
        """æ»šåŠ¨çª—å£åˆ’åˆ†ç­–ç•¥"""
        
        def split(self, data: pd.DataFrame) -> SplitResult:
            """
            å•æ¬¡åˆ’åˆ†ï¼ˆè¿”å›žç¬¬ä¸€ä¸ªçª—å£ï¼‰
            
            Args:
                data: å¾…åˆ’åˆ†çš„æ—¶åºæ•°æ®
                
            Returns:
                SplitResult: ç¬¬ä¸€ä¸ªçª—å£çš„åˆ’åˆ†ç»“æžœ
            """
            splits = self.split_rolling(data)
            return splits[0] if splits else SplitResult(
                train_indices=np.array([]),
                validation_indices=np.array([]),
                test_indices=np.array([])
            )
        
        def split_rolling(self, data: pd.DataFrame) -> List[SplitResult]:
            """
            æ»šåŠ¨çª—å£åˆ’åˆ†
            
            Args:
                data: å¾…åˆ’åˆ†çš„æ—¶åºæ•°æ®
                
            Returns:
                List[SplitResult]: å¤šä¸ªçª—å£çš„åˆ’åˆ†ç»“æžœ
            """
            self._validate_data(data)
            
            if self.config.rolling_window_size is None:
                raise ValueError("æ»šåŠ¨çª—å£å¤§å°å¿…é¡»æŒ‡å®š")
            
            datetime_index = self._get_datetime_index(data)
            unique_dates = datetime_index.unique().sort_values()
            total_dates = len(unique_dates)
            
            window_size = self.config.rolling_window_size
            step_size = self.config.step_size or window_size // 4
            
            splits = []
            start_idx = 0
            
            while start_idx + window_size <= total_dates:
                end_idx = start_idx + window_size
                window_dates = unique_dates[start_idx:end_idx]
                
                # åœ¨çª—å£å†…æŒ‰æ¯”ä¾‹åˆ’åˆ†
                window_size_actual = len(window_dates)
                train_size = int(window_size_actual * self.config.train_ratio)
                val_size = int(window_size_actual * self.config.validation_ratio)
                
                # è€ƒè™‘é—´éš”
                gap_dates = self.config.gap_days
                
                train_dates = window_dates[:train_size]
                val_start = train_size + gap_dates
                val_dates = window_dates[val_start:val_start + val_size]
                test_start = val_start + val_size + gap_dates
                test_dates = window_dates[test_start:]
                
                # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ•°æ®
                if len(train_dates) < self.config.min_train_samples:
                    break
                if len(val_dates) < self.config.min_validation_samples:
                    break
                if len(test_dates) < self.config.min_test_samples:
                    break
                
                # è½¬æ¢ä¸ºç´¢å¼•
                train_indices = self._dates_to_indices(data, train_dates)
                val_indices = self._dates_to_indices(data, val_dates)
                test_indices = self._dates_to_indices(data, test_dates)
                
                split_dates = {
                    'train_start': train_dates[0].strftime('%Y-%m-%d'),
                    'train_end': train_dates[-1].strftime('%Y-%m-%d'),
                    'val_start': val_dates[0].strftime('%Y-%m-%d') if len(val_dates) > 0 else '',
                    'val_end': val_dates[-1].strftime('%Y-%m-%d') if len(val_dates) > 0 else '',
                    'test_start': test_dates[0].strftime('%Y-%m-%d') if len(test_dates) > 0 else '',
                    'test_end': test_dates[-1].strftime('%Y-%m-%d') if len(test_dates) > 0 else ''
                }
                
                result = SplitResult(
                    train_indices=train_indices,
                    validation_indices=val_indices,
                    test_indices=test_indices,
                    split_dates=split_dates,
                    metadata={
                        'strategy': 'rolling_window',
                        'window_id': len(splits),
                        'window_start_date': window_dates[0].strftime('%Y-%m-%d'),
                        'window_end_date': window_dates[-1].strftime('%Y-%m-%d')
                    }
                )
                
                splits.append(result)
                start_idx += step_size
            
            if not splits:
                raise ValueError("æ— æ³•åˆ›å»ºä»»ä½•æœ‰æ•ˆçš„æ»šåŠ¨çª—å£")
            
            logger.info(f"åˆ›å»ºäº† {len(splits)} ä¸ªæ»šåŠ¨çª—å£")
            return splits
        
        def _dates_to_indices(self, data: pd.DataFrame, dates: pd.DatetimeIndex) -> np.ndarray:
            """å°†æ—¥æœŸè½¬æ¢ä¸ºæ•°æ®ç´¢å¼•"""
            datetime_index = self._get_datetime_index(data)
            mask = datetime_index.isin(dates)
            return np.where(mask)[0]
    
    
    def create_split_strategy(strategy_type: str, config: SplitConfig) -> DataSplitStrategy:
        """
        å·¥åŽ‚å‡½æ•°ï¼šåˆ›å»ºæ•°æ®åˆ’åˆ†ç­–ç•¥
        
        Args:
            strategy_type: ç­–ç•¥ç±»åž‹ ('time_series', 'fixed', 'rolling_window')
            config: åˆ’åˆ†é…ç½®
            
        Returns:
            DataSplitStrategy: å¯¹åº”çš„ç­–ç•¥å®žä¾‹
        """
        strategies = {
            'time_series': TimeSeriesSplitStrategy,
            'fixed': FixedSplitStrategy,
            'rolling_window': RollingWindowSplitStrategy
        }
        
        if strategy_type not in strategies:
            raise ValueError(f"ä¸æ”¯æŒçš„ç­–ç•¥ç±»åž‹: {strategy_type}")
        
        return strategies[strategy_type](config)
    
    
    def validate_split_quality(result: SplitResult, data: pd.DataFrame, 
                              target_column: Optional[str] = None) -> Dict[str, Any]:
        """
        éªŒè¯åˆ’åˆ†è´¨é‡
        
        Args:
            result: åˆ’åˆ†ç»“æžœ
            data: åŽŸå§‹æ•°æ®
            target_column: ç›®æ ‡å˜é‡åˆ—åï¼ˆå¯é€‰ï¼‰
            
        Returns:
            Dict: è´¨é‡è¯„ä¼°ç»“æžœ
        """
        metrics = result.get_metrics()
        
        # åŸºæœ¬ç»Ÿè®¡
        quality_report = {
            'basic_stats': metrics,
            'temporal_order_valid': True,
            'no_overlap': True,
            'size_balance': True,
            'warnings': []
        }
        
        # æ£€æŸ¥æ—¶é—´é¡ºåº
        if 'datetime' in data.index.names:
            datetime_index = data.index.get_level_values('datetime')
            
            train_dates = datetime_index[result.train_indices]
            val_dates = datetime_index[result.validation_indices]
            test_dates = datetime_index[result.test_indices]
            
            if len(train_dates) > 0 and len(val_dates) > 0:
                if train_dates.max() >= val_dates.min():
                    quality_report['temporal_order_valid'] = False
                    quality_report['warnings'].append("è®­ç»ƒé›†å’ŒéªŒè¯é›†æ—¶é—´é¡ºåºé”™è¯¯")
            
            if len(val_dates) > 0 and len(test_dates) > 0:
                if val_dates.max() >= test_dates.min():
                    quality_report['temporal_order_valid'] = False
                    quality_report['warnings'].append("éªŒè¯é›†å’Œæµ‹è¯•é›†æ—¶é—´é¡ºåºé”™è¯¯")
        
        # æ£€æŸ¥é›†åˆå¤§å°å¹³è¡¡
        sizes = [metrics['train_size'], metrics['validation_size'], metrics['test_size']]
        if min(sizes) < max(sizes) * 0.05:  # æœ€å°é›†åˆä¸åˆ°æœ€å¤§é›†åˆçš„5%
            quality_report['size_balance'] = False
            quality_report['warnings'].append("æ•°æ®é›†å¤§å°ä¸¥é‡ä¸å¹³è¡¡")
        
        # æ£€æŸ¥ç›®æ ‡å˜é‡åˆ†å¸ƒï¼ˆå¦‚æžœæä¾›ï¼‰
        if target_column and target_column in data.columns:
            train_target = data[target_column].iloc[result.train_indices]
            val_target = data[target_column].iloc[result.validation_indices]
            test_target = data[target_column].iloc[result.test_indices]
            
            # æ£€æŸ¥åˆ†å¸ƒç›¸ä¼¼æ€§ï¼ˆä½¿ç”¨KSæ£€éªŒï¼‰
            if len(train_target) > 10 and len(val_target) > 10:
                ks_stat, p_value = stats.ks_2samp(train_target.dropna(), val_target.dropna())
                quality_report['train_val_distribution_similar'] = p_value > 0.05
                if p_value <= 0.05:
                    quality_report['warnings'].append("è®­ç»ƒé›†å’ŒéªŒè¯é›†ç›®æ ‡åˆ†å¸ƒå·®å¼‚æ˜¾è‘—")
        
        return quality_report
    ]]></file>
  <file path="src/rl_trading_system/training/__init__.py"><![CDATA[
    """è®­ç»ƒç³»ç»Ÿæ¨¡å—"""
    
    from .data_split_strategy import (
        DataSplitStrategy,
        TimeSeriesSplitStrategy,
        RollingWindowSplitStrategy,
        FixedSplitStrategy,
        SplitConfig,
        SplitResult,
        create_split_strategy,
        validate_split_quality
    )
    
    from .trainer import (
        RLTrainer,
        TrainingConfig,
        EarlyStopping,
        TrainingMetrics
    )
    
    __all__ = [
        "DataSplitStrategy",
        "TimeSeriesSplitStrategy",
        "RollingWindowSplitStrategy", 
        "FixedSplitStrategy",
        "SplitConfig",
        "SplitResult",
        "create_split_strategy",
        "validate_split_quality",
        "RLTrainer",
        "TrainingConfig",
        "EarlyStopping",
        "TrainingMetrics"
    ]
    ]]></file>
  <file path="src/rl_trading_system/trading/transaction_cost_model.py"><![CDATA[
    """
    äº¤æ˜“æˆæœ¬è®¡ç®—æ¨¡å—
    å®žçŽ°æ‰‹ç»­è´¹ã€å°èŠ±ç¨Žã€è¿‡æˆ·è´¹å’Œå¸‚åœºå†²å‡»æˆæœ¬çš„è®¡ç®—ï¼Œæ”¯æŒAè‚¡ç‰¹æœ‰çš„äº¤æ˜“è§„åˆ™
    """
    
    from dataclasses import dataclass
    from datetime import datetime
    from typing import Optional, List, Union
    import numpy as np
    
    from .almgren_chriss_model import AlmgrenChrissModel
    
    
    @dataclass
    class CostParameters:
        """æˆæœ¬å‚æ•°é…ç½®"""
        commission_rate: float          # æ‰‹ç»­è´¹çŽ‡
        stamp_tax_rate: float          # å°èŠ±ç¨ŽçŽ‡
        min_commission: float          # æœ€å°æ‰‹ç»­è´¹
        transfer_fee_rate: float       # è¿‡æˆ·è´¹çŽ‡
        market_impact_model: Optional[AlmgrenChrissModel] = None  # å¸‚åœºå†²å‡»æ¨¡åž‹
        
        def __post_init__(self):
            """å‚æ•°éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯å‚æ•°æœ‰æ•ˆæ€§"""
            if self.commission_rate < 0:
                raise ValueError("æ‰‹ç»­è´¹çŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.commission_rate > 0.1:
                raise ValueError("æ‰‹ç»­è´¹çŽ‡ä¸èƒ½è¶…è¿‡10%")
            
            if self.stamp_tax_rate < 0:
                raise ValueError("å°èŠ±ç¨ŽçŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.stamp_tax_rate > 0.1:
                raise ValueError("å°èŠ±ç¨ŽçŽ‡ä¸èƒ½è¶…è¿‡10%")
            
            if self.min_commission < 0:
                raise ValueError("æœ€å°æ‰‹ç»­è´¹ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.transfer_fee_rate < 0:
                raise ValueError("è¿‡æˆ·è´¹çŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°")
    
    
    @dataclass
    class TradeInfo:
        """äº¤æ˜“ä¿¡æ¯"""
        symbol: str                    # è‚¡ç¥¨ä»£ç 
        side: str                      # äº¤æ˜“æ–¹å‘ï¼š'buy' æˆ– 'sell'
        quantity: int                  # äº¤æ˜“æ•°é‡
        price: float                   # äº¤æ˜“ä»·æ ¼
        timestamp: datetime            # äº¤æ˜“æ—¶é—´
        market_volume: Optional[int] = None      # å¸‚åœºæˆäº¤é‡ï¼ˆç”¨äºŽå¸‚åœºå†²å‡»è®¡ç®—ï¼‰
        volatility: Optional[float] = None       # è‚¡ç¥¨æ³¢åŠ¨çŽ‡ï¼ˆç”¨äºŽå¸‚åœºå†²å‡»è®¡ç®—ï¼‰
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            if self.side not in ['buy', 'sell']:
                raise ValueError("äº¤æ˜“æ–¹å‘å¿…é¡»æ˜¯'buy'æˆ–'sell'")
            
            if self.quantity < 0:
                raise ValueError("äº¤æ˜“æ•°é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.price < 0:
                raise ValueError("ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.market_volume is not None and self.market_volume < 0:
                raise ValueError("å¸‚åœºæˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.volatility is not None and self.volatility < 0:
                raise ValueError("æ³¢åŠ¨çŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        def get_trade_value(self) -> float:
            """èŽ·å–äº¤æ˜“ä»·å€¼"""
            return self.quantity * self.price
        
        def is_buy(self) -> bool:
            """åˆ¤æ–­æ˜¯å¦ä¸ºä¹°å…¥äº¤æ˜“"""
            return self.side == 'buy'
        
        def is_sell(self) -> bool:
            """åˆ¤æ–­æ˜¯å¦ä¸ºå–å‡ºäº¤æ˜“"""
            return self.side == 'sell'
    
    
    @dataclass
    class CostBreakdown:
        """æˆæœ¬åˆ†è§£ç»“æž„"""
        commission: float      # æ‰‹ç»­è´¹
        stamp_tax: float       # å°èŠ±ç¨Ž
        transfer_fee: float    # è¿‡æˆ·è´¹
        market_impact: float   # å¸‚åœºå†²å‡»æˆæœ¬
        total_cost: float      # æ€»æˆæœ¬
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            if self.commission < 0:
                raise ValueError("æ‰‹ç»­è´¹ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.stamp_tax < 0:
                raise ValueError("å°èŠ±ç¨Žä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.transfer_fee < 0:
                raise ValueError("è¿‡æˆ·è´¹ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.market_impact < 0:
                raise ValueError("å¸‚åœºå†²å‡»æˆæœ¬ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.total_cost < 0:
                raise ValueError("æ€»æˆæœ¬ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯æ€»æˆæœ¬ç­‰äºŽå„é¡¹æˆæœ¬ä¹‹å’Œ
            expected_total = self.commission + self.stamp_tax + self.transfer_fee + self.market_impact
            if abs(self.total_cost - expected_total) > 1e-8:
                raise ValueError("æ€»æˆæœ¬åº”ç­‰äºŽå„é¡¹æˆæœ¬ä¹‹å’Œ")
        
        def get_cost_ratio(self, trade_value: float) -> float:
            """èŽ·å–æˆæœ¬æ¯”çŽ‡"""
            if trade_value == 0:
                return 0.0
            return self.total_cost / trade_value
        
        def get_cost_basis_points(self, trade_value: float) -> float:
            """èŽ·å–æˆæœ¬ï¼ˆåŸºç‚¹ï¼‰"""
            return self.get_cost_ratio(trade_value) * 10000
        
        def to_dict(self) -> dict:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'commission': self.commission,
                'stamp_tax': self.stamp_tax,
                'transfer_fee': self.transfer_fee,
                'market_impact': self.market_impact,
                'total_cost': self.total_cost
            }
    
    
    class TransactionCostModel:
        """
        äº¤æ˜“æˆæœ¬è®¡ç®—æ¨¡åž‹
        
        è¯¥æ¨¡åž‹è®¡ç®—Aè‚¡äº¤æ˜“çš„å„é¡¹æˆæœ¬ï¼ŒåŒ…æ‹¬ï¼š
        1. æ‰‹ç»­è´¹ï¼šåŒè¾¹æ”¶å–ï¼Œæœ‰æœ€å°æ‰‹ç»­è´¹é™åˆ¶
        2. å°èŠ±ç¨Žï¼šä»…å–å‡ºæ—¶æ”¶å–
        3. è¿‡æˆ·è´¹ï¼šåŒè¾¹æ”¶å–
        4. å¸‚åœºå†²å‡»ï¼šå¯é€‰ï¼Œä½¿ç”¨Almgren-Chrissæ¨¡åž‹è®¡ç®—
        
        Aè‚¡äº¤æ˜“æˆæœ¬ç‰¹ç‚¹ï¼š
        - æ‰‹ç»­è´¹ï¼šé€šå¸¸ä¸ºä¸‡åˆ†ä¹‹ä¸‰ï¼Œæœ€ä½Ž5å…ƒ
        - å°èŠ±ç¨Žï¼šåƒåˆ†ä¹‹ä¸€ï¼Œä»…å–å‡ºæ”¶å–
        - è¿‡æˆ·è´¹ï¼šä¸‡åˆ†ä¹‹0.2ï¼ŒåŒè¾¹æ”¶å–
        """
        
        def __init__(self, parameters: CostParameters):
            """
            åˆå§‹åŒ–äº¤æ˜“æˆæœ¬æ¨¡åž‹
            
            Args:
                parameters: æˆæœ¬å‚æ•°é…ç½®
            """
            self.parameters = parameters
        
        def calculate_cost(self, trade: TradeInfo) -> CostBreakdown:
            """
            è®¡ç®—å•ç¬”äº¤æ˜“çš„æˆæœ¬
            
            Args:
                trade: äº¤æ˜“ä¿¡æ¯
                
            Returns:
                CostBreakdown: æˆæœ¬åˆ†è§£ç»“æžœ
            """
            trade_value = trade.get_trade_value()
            
            # è®¡ç®—å„é¡¹æˆæœ¬
            commission = self._calculate_commission(trade_value)
            stamp_tax = self._calculate_stamp_tax(trade)
            transfer_fee = self._calculate_transfer_fee(trade_value)
            market_impact = self._calculate_market_impact(trade)
            
            # è®¡ç®—æ€»æˆæœ¬
            total_cost = commission + stamp_tax + transfer_fee + market_impact
            
            return CostBreakdown(
                commission=commission,
                stamp_tax=stamp_tax,
                transfer_fee=transfer_fee,
                market_impact=market_impact,
                total_cost=total_cost
            )
        
        def calculate_batch_costs(self, trades: List[TradeInfo]) -> List[CostBreakdown]:
            """
            æ‰¹é‡è®¡ç®—äº¤æ˜“æˆæœ¬
            
            Args:
                trades: äº¤æ˜“ä¿¡æ¯åˆ—è¡¨
                
            Returns:
                List[CostBreakdown]: æˆæœ¬åˆ†è§£ç»“æžœåˆ—è¡¨
            """
            return [self.calculate_cost(trade) for trade in trades]
        
        def _calculate_commission(self, trade_value: float) -> float:
            """
            è®¡ç®—æ‰‹ç»­è´¹
            
            æ‰‹ç»­è´¹ = max(äº¤æ˜“ä»·å€¼ * æ‰‹ç»­è´¹çŽ‡, æœ€å°æ‰‹ç»­è´¹)
            
            Args:
                trade_value: äº¤æ˜“ä»·å€¼
                
            Returns:
                float: æ‰‹ç»­è´¹
            """
            commission = trade_value * self.parameters.commission_rate
            return max(commission, self.parameters.min_commission)
        
        def _calculate_stamp_tax(self, trade: TradeInfo) -> float:
            """
            è®¡ç®—å°èŠ±ç¨Ž
            
            å°èŠ±ç¨Žä»…åœ¨å–å‡ºæ—¶æ”¶å–
            å°èŠ±ç¨Ž = äº¤æ˜“ä»·å€¼ * å°èŠ±ç¨ŽçŽ‡ (ä»…å–å‡º)
            
            Args:
                trade: äº¤æ˜“ä¿¡æ¯
                
            Returns:
                float: å°èŠ±ç¨Ž
            """
            if trade.is_sell():
                return trade.get_trade_value() * self.parameters.stamp_tax_rate
            else:
                return 0.0
        
        def _calculate_transfer_fee(self, trade_value: float) -> float:
            """
            è®¡ç®—è¿‡æˆ·è´¹
            
            è¿‡æˆ·è´¹ = äº¤æ˜“ä»·å€¼ * è¿‡æˆ·è´¹çŽ‡
            
            Args:
                trade_value: äº¤æ˜“ä»·å€¼
                
            Returns:
                float: è¿‡æˆ·è´¹
            """
            return trade_value * self.parameters.transfer_fee_rate
        
        def _calculate_market_impact(self, trade: TradeInfo) -> float:
            """
            è®¡ç®—å¸‚åœºå†²å‡»æˆæœ¬
            
            å¦‚æžœé…ç½®äº†å¸‚åœºå†²å‡»æ¨¡åž‹ï¼Œåˆ™ä½¿ç”¨æ¨¡åž‹è®¡ç®—ï¼›å¦åˆ™è¿”å›ž0
            
            Args:
                trade: äº¤æ˜“ä¿¡æ¯
                
            Returns:
                float: å¸‚åœºå†²å‡»æˆæœ¬ï¼ˆä»¥ä»·æ ¼æ¯”ä¾‹è¡¨ç¤ºï¼‰
            """
            if self.parameters.market_impact_model is None:
                return 0.0
            
            # ä½¿ç”¨Almgren-Chrissæ¨¡åž‹è®¡ç®—å¸‚åœºå†²å‡»
            impact_result = self.parameters.market_impact_model.calculate_impact(
                trade_volume=trade.quantity,
                market_volume=trade.market_volume,
                volatility=trade.volatility
            )
            
            # å°†å†²å‡»è½¬æ¢ä¸ºç»å¯¹æˆæœ¬
            trade_value = trade.get_trade_value()
            return impact_result.total_impact * trade_value
        
        def estimate_round_trip_cost(self, 
                                    symbol: str,
                                    quantity: int,
                                    price: float,
                                    timestamp: datetime,
                                    market_volume: Optional[int] = None,
                                    volatility: Optional[float] = None) -> dict:
            """
            ä¼°ç®—å¾€è¿”äº¤æ˜“æˆæœ¬ï¼ˆä¹°å…¥+å–å‡ºï¼‰
            
            Args:
                symbol: è‚¡ç¥¨ä»£ç 
                quantity: äº¤æ˜“æ•°é‡
                price: äº¤æ˜“ä»·æ ¼
                timestamp: äº¤æ˜“æ—¶é—´
                market_volume: å¸‚åœºæˆäº¤é‡
                volatility: æ³¢åŠ¨çŽ‡
                
            Returns:
                dict: åŒ…å«ä¹°å…¥ã€å–å‡ºå’Œæ€»æˆæœ¬çš„å­—å…¸
            """
            # åˆ›å»ºä¹°å…¥äº¤æ˜“
            buy_trade = TradeInfo(
                symbol=symbol,
                side='buy',
                quantity=quantity,
                price=price,
                timestamp=timestamp,
                market_volume=market_volume,
                volatility=volatility
            )
            
            # åˆ›å»ºå–å‡ºäº¤æ˜“
            sell_trade = TradeInfo(
                symbol=symbol,
                side='sell',
                quantity=quantity,
                price=price,
                timestamp=timestamp,
                market_volume=market_volume,
                volatility=volatility
            )
            
            # è®¡ç®—æˆæœ¬
            buy_cost = self.calculate_cost(buy_trade)
            sell_cost = self.calculate_cost(sell_trade)
            
            # è®¡ç®—æ€»æˆæœ¬
            total_cost = CostBreakdown(
                commission=buy_cost.commission + sell_cost.commission,
                stamp_tax=buy_cost.stamp_tax + sell_cost.stamp_tax,
                transfer_fee=buy_cost.transfer_fee + sell_cost.transfer_fee,
                market_impact=buy_cost.market_impact + sell_cost.market_impact,
                total_cost=buy_cost.total_cost + sell_cost.total_cost
            )
            
            return {
                'buy_cost': buy_cost,
                'sell_cost': sell_cost,
                'total_cost': total_cost,
                'round_trip_basis_points': total_cost.get_cost_basis_points(buy_trade.get_trade_value())
            }
        
        def get_cost_estimate(self, 
                             trade_value: float,
                             side: str = 'buy') -> dict:
            """
            å¿«é€Ÿæˆæœ¬ä¼°ç®—ï¼ˆä¸éœ€è¦å®Œæ•´çš„äº¤æ˜“ä¿¡æ¯ï¼‰
            
            Args:
                trade_value: äº¤æ˜“ä»·å€¼
                side: äº¤æ˜“æ–¹å‘
                
            Returns:
                dict: æˆæœ¬ä¼°ç®—ç»“æžœ
            """
            # è®¡ç®—å„é¡¹æˆæœ¬
            commission = max(trade_value * self.parameters.commission_rate, 
                            self.parameters.min_commission)
            
            stamp_tax = (trade_value * self.parameters.stamp_tax_rate 
                        if side == 'sell' else 0.0)
            
            transfer_fee = trade_value * self.parameters.transfer_fee_rate
            
            total_cost = commission + stamp_tax + transfer_fee
            
            return {
                'commission': commission,
                'stamp_tax': stamp_tax,
                'transfer_fee': transfer_fee,
                'market_impact': 0.0,  # å¿«é€Ÿä¼°ç®—ä¸åŒ…å«å¸‚åœºå†²å‡»
                'total_cost': total_cost,
                'cost_ratio': total_cost / trade_value if trade_value > 0 else 0,
                'cost_basis_points': (total_cost / trade_value * 10000) if trade_value > 0 else 0
            }
        
        def update_parameters(self, new_parameters: CostParameters) -> None:
            """
            æ›´æ–°æˆæœ¬å‚æ•°
            
            Args:
                new_parameters: æ–°çš„æˆæœ¬å‚æ•°
            """
            self.parameters = new_parameters
        
        def get_parameters(self) -> CostParameters:
            """
            èŽ·å–å½“å‰æˆæœ¬å‚æ•°
            
            Returns:
                CostParameters: å½“å‰å‚æ•°
            """
            return self.parameters
        
        def validate_trade(self, trade: TradeInfo) -> List[str]:
            """
            éªŒè¯äº¤æ˜“æ˜¯å¦ç¬¦åˆAè‚¡äº¤æ˜“è§„åˆ™
            
            Args:
                trade: äº¤æ˜“ä¿¡æ¯
                
            Returns:
                List[str]: éªŒè¯é”™è¯¯ä¿¡æ¯åˆ—è¡¨ï¼Œç©ºåˆ—è¡¨è¡¨ç¤ºéªŒè¯é€šè¿‡
            """
            errors = []
            
            # æ£€æŸ¥æœ€å°äº¤æ˜“å•ä½ï¼ˆAè‚¡é€šå¸¸ä¸º100è‚¡çš„å€æ•°ï¼‰
            if trade.quantity % 100 != 0 and trade.quantity >= 100:
                errors.append(f"äº¤æ˜“æ•°é‡{trade.quantity}ä¸æ˜¯100çš„å€æ•°")
            
            # æ£€æŸ¥ä»·æ ¼ç²¾åº¦ï¼ˆAè‚¡ä»·æ ¼é€šå¸¸ç²¾ç¡®åˆ°åˆ†ï¼‰
            if round(trade.price, 2) != trade.price:
                errors.append(f"ä»·æ ¼{trade.price}ç²¾åº¦è¶…è¿‡2ä½å°æ•°")
            
            # æ£€æŸ¥æ¶¨è·Œåœé™åˆ¶ï¼ˆç®€åŒ–æ£€æŸ¥ï¼Œå®žé™…éœ€è¦å‰ä¸€æ—¥æ”¶ç›˜ä»·ï¼‰
            if trade.price <= 0:
                errors.append(f"ä»·æ ¼{trade.price}ä¸èƒ½ä¸ºé›¶æˆ–è´Ÿæ•°")
            
            # æ£€æŸ¥äº¤æ˜“æ—¶é—´ï¼ˆç®€åŒ–æ£€æŸ¥ï¼‰
            hour = trade.timestamp.hour
            minute = trade.timestamp.minute
            
            # Aè‚¡äº¤æ˜“æ—¶é—´ï¼š9:30-11:30, 13:00-15:00
            morning_session = (9, 30) <= (hour, minute) <= (11, 30)
            afternoon_session = (13, 0) <= (hour, minute) <= (15, 0)
            
            if not (morning_session or afternoon_session):
                errors.append(f"äº¤æ˜“æ—¶é—´{trade.timestamp}ä¸åœ¨äº¤æ˜“æ—¶æ®µå†…")
            
            return errors
        
        def calculate_optimal_lot_size(self, 
                                      target_value: float,
                                      price: float,
                                      max_cost_ratio: float = 0.01) -> int:
            """
            è®¡ç®—æœ€ä¼˜äº¤æ˜“æ‰¹æ¬¡å¤§å°
            
            åœ¨ç»™å®šæœ€å¤§æˆæœ¬æ¯”çŽ‡çº¦æŸä¸‹ï¼Œè®¡ç®—æœ€ä¼˜çš„äº¤æ˜“æ•°é‡
            
            Args:
                target_value: ç›®æ ‡äº¤æ˜“ä»·å€¼
                price: è‚¡ç¥¨ä»·æ ¼
                max_cost_ratio: æœ€å¤§æˆæœ¬æ¯”çŽ‡
                
            Returns:
                int: å»ºè®®çš„äº¤æ˜“æ•°é‡
            """
            target_quantity = int(target_value / price)
            
            # ä»Žç›®æ ‡æ•°é‡å¼€å§‹ï¼Œé€æ­¥è°ƒæ•´ç›´åˆ°æ»¡è¶³æˆæœ¬çº¦æŸ
            for quantity in range(target_quantity, 0, -100):  # ä»¥100è‚¡ä¸ºå•ä½é€’å‡
                trade_value = quantity * price
                cost_estimate = self.get_cost_estimate(trade_value)
                
                if cost_estimate['cost_ratio'] <= max_cost_ratio:
                    return quantity
            
            # å¦‚æžœæ— æ³•æ»¡è¶³æˆæœ¬çº¦æŸï¼Œè¿”å›žæœ€å°äº¤æ˜“å•ä½
            return 100
    ]]></file>
  <file path="src/rl_trading_system/trading/portfolio_environment.py"><![CDATA[
    """
    æŠ•èµ„ç»„åˆäº¤æ˜“çŽ¯å¢ƒ
    å®žçŽ°ç¬¦åˆOpenAI GymæŽ¥å£çš„å¼ºåŒ–å­¦ä¹ äº¤æ˜“çŽ¯å¢ƒï¼Œæ”¯æŒAè‚¡äº¤æ˜“è§„åˆ™
    """
    
    import gym
    from gym import spaces
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from typing import Dict, List, Tuple, Any, Optional, Union
    from dataclasses import dataclass
    import logging
    
    from ..data.data_models import TradingState, TradingAction, MarketData, FeatureVector
    from ..data.interfaces import DataInterface
    from ..data.data_processor import DataProcessor
    from ..data.feature_engineer import FeatureEngineer
    from .transaction_cost_model import TransactionCostModel, CostParameters, TradeInfo
    
    logger = logging.getLogger(__name__)
    
    
    @dataclass
    class PortfolioConfig:
        """æŠ•èµ„ç»„åˆçŽ¯å¢ƒé…ç½®"""
        stock_pool: List[str]                    # è‚¡ç¥¨æ± 
        lookback_window: int = 60               # å›žæœ›çª—å£
        initial_cash: float = 1000000.0         # åˆå§‹èµ„é‡‘
        commission_rate: float = 0.0003         # æ‰‹ç»­è´¹çŽ‡
        stamp_tax_rate: float = 0.001           # å°èŠ±ç¨ŽçŽ‡
        min_commission: float = 5.0             # æœ€å°æ‰‹ç»­è´¹
        transfer_fee_rate: float = 0.00002      # è¿‡æˆ·è´¹çŽ‡
        risk_aversion: float = 0.1              # é£Žé™©åŽŒæ¶ç³»æ•°
        max_drawdown_penalty: float = 1.0       # æœ€å¤§å›žæ’¤æƒ©ç½š
        max_position_size: float = 0.1          # å•åªè‚¡ç¥¨æœ€å¤§æƒé‡
        min_trade_amount: float = 1000.0        # æœ€å°äº¤æ˜“é‡‘é¢
        price_limit: float = 0.1                # æ¶¨è·Œåœé™åˆ¶
        t_plus_1: bool = True                   # T+1äº¤æ˜“è§„åˆ™
        trading_days_per_year: int = 252        # å¹´äº¤æ˜“æ—¥æ•°
        
        def __post_init__(self):
            """é…ç½®éªŒè¯"""
            if not self.stock_pool:
                raise ValueError("è‚¡ç¥¨æ± ä¸èƒ½ä¸ºç©º")
            
            if self.lookback_window <= 0:
                raise ValueError("å›žæœ›çª—å£å¿…é¡»å¤§äºŽ0")
            
            if self.initial_cash <= 0:
                raise ValueError("åˆå§‹èµ„é‡‘å¿…é¡»å¤§äºŽ0")
            
            if not (0 <= self.max_position_size <= 1):
                raise ValueError("æœ€å¤§æŒä»“æƒé‡å¿…é¡»åœ¨0åˆ°1ä¹‹é—´")
    
    
    class PortfolioEnvironment(gym.Env):
        """
        æŠ•èµ„ç»„åˆäº¤æ˜“çŽ¯å¢ƒ
        
        è¯¥çŽ¯å¢ƒå®žçŽ°äº†ç¬¦åˆOpenAI GymæŽ¥å£çš„å¼ºåŒ–å­¦ä¹ äº¤æ˜“çŽ¯å¢ƒï¼Œæ”¯æŒï¼š
        1. å¤šèµ„äº§æŠ•èµ„ç»„åˆç®¡ç†
        2. Aè‚¡äº¤æ˜“è§„åˆ™ï¼ˆT+1ã€æ¶¨è·Œåœã€æœ€å°äº¤æ˜“å•ä½ç­‰ï¼‰
        3. å®Œæ•´çš„äº¤æ˜“æˆæœ¬æ¨¡åž‹
        4. é£Žé™©æŽ§åˆ¶å’Œçº¦æŸ
        5. å®žæ—¶å¸‚åœºæ•°æ®æŽ¥å…¥
        """
        
        metadata = {'render.modes': ['human']}
        
        def __init__(self, 
                     config: PortfolioConfig,
                     data_interface: DataInterface,
                     data_processor: Optional[DataProcessor] = None,
                     feature_engineer: Optional[FeatureEngineer] = None,
                     start_date: Optional[str] = None,
                     end_date: Optional[str] = None):
            """
            åˆå§‹åŒ–æŠ•èµ„ç»„åˆçŽ¯å¢ƒ
            
            Args:
                config: çŽ¯å¢ƒé…ç½®
                data_interface: æ•°æ®æŽ¥å£
                data_processor: æ•°æ®é¢„å¤„ç†å™¨
                feature_engineer: ç‰¹å¾å·¥ç¨‹å™¨
                start_date: å¼€å§‹æ—¥æœŸ
                end_date: ç»“æŸæ—¥æœŸ
            """
            super().__init__()
            
            self.config = config
            self.data_interface = data_interface
            self.data_processor = data_processor or DataProcessor()
            self.feature_engineer = feature_engineer or FeatureEngineer()
            
            # æ—¶é—´èŒƒå›´
            self.start_date = start_date
            self.end_date = end_date
            
            # çŽ¯å¢ƒçŠ¶æ€
            self.n_stocks = len(config.stock_pool)
            self.n_features = 50  # é»˜è®¤50ä¸ªç‰¹å¾
            self.n_market_features = 10  # é»˜è®¤10ä¸ªå¸‚åœºç‰¹å¾
            
            # åˆå§‹åŒ–äº¤æ˜“æˆæœ¬æ¨¡åž‹
            cost_params = CostParameters(
                commission_rate=config.commission_rate,
                stamp_tax_rate=config.stamp_tax_rate,
                min_commission=config.min_commission,
                transfer_fee_rate=config.transfer_fee_rate
            )
            self.cost_model = TransactionCostModel(cost_params)
            
            # å®šä¹‰è§‚å¯Ÿç©ºé—´
            self.observation_space = spaces.Dict({
                'features': spaces.Box(
                    low=-np.inf, high=np.inf,
                    shape=(config.lookback_window, self.n_stocks, self.n_features),
                    dtype=np.float32
                ),
                'positions': spaces.Box(
                    low=0, high=1,
                    shape=(self.n_stocks,),
                    dtype=np.float32
                ),
                'market_state': spaces.Box(
                    low=-np.inf, high=np.inf,
                    shape=(self.n_market_features,),
                    dtype=np.float32
                )
            })
            
            # å®šä¹‰åŠ¨ä½œç©ºé—´
            self.action_space = spaces.Box(
                low=0, high=1,
                shape=(self.n_stocks,),
                dtype=np.float32
            )
            
            # åˆå§‹åŒ–çŽ¯å¢ƒçŠ¶æ€å˜é‡
            self._initialize_state_variables()
            
            # åŠ è½½åŽ†å²æ•°æ®
            self._load_market_data()
            
            logger.info(f"æŠ•èµ„ç»„åˆçŽ¯å¢ƒåˆå§‹åŒ–å®Œæˆ: {self.n_stocks}åªè‚¡ç¥¨, "
                       f"è§‚å¯Ÿç©ºé—´: {self.observation_space}, "
                       f"åŠ¨ä½œç©ºé—´: {self.action_space}")
        
        def _initialize_state_variables(self):
            """åˆå§‹åŒ–çŠ¶æ€å˜é‡"""
            self.current_step = 0
            self.max_steps = 252  # é»˜è®¤ä¸€å¹´äº¤æ˜“æ—¥
            self.current_positions = np.zeros(self.n_stocks, dtype=np.float32)
            self.cash = float(self.config.initial_cash)
            self.total_value = float(self.config.initial_cash)
            self.portfolio_values = [float(self.config.initial_cash)]
            self.max_portfolio_value = float(self.config.initial_cash)
            
            # äº¤æ˜“è®°å½•
            self.trade_history = []
            self.returns_history = []
            self.weights_history = []
            
            # T+1è§„åˆ™ç›¸å…³
            self.t_plus_1_restrictions = {}  # è®°å½•å½“æ—¥ä¹°å…¥æ— æ³•å–å‡ºçš„è‚¡ç¥¨
            
            # å¸‚åœºæ•°æ®ç›¸å…³
            self.market_data = None
            self.feature_data = None
            self.price_data = None
            self.current_prices = None
        
        def _load_market_data(self):
            """åŠ è½½å¸‚åœºæ•°æ®"""
            if not self.start_date or not self.end_date:
                # å¦‚æžœæ²¡æœ‰æŒ‡å®šæ—¥æœŸèŒƒå›´ï¼Œç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®
                self._generate_mock_data()
                return
            
            # ä»Žæ•°æ®æŽ¥å£åŠ è½½çœŸå®žæ•°æ®
            self.price_data = self.data_interface.get_price_data(
                symbols=self.config.stock_pool,
                start_date=self.start_date,
                end_date=self.end_date
            )
            
            # è®¡ç®—ç‰¹å¾
            self.feature_data = self.feature_engineer.calculate_features(self.price_data)
            
            # æ›´æ–°æœ€å¤§æ­¥æ•°
            self.max_steps = len(self.price_data.index.get_level_values('datetime').unique())
            
            logger.info(f"åŠ è½½å¸‚åœºæ•°æ®å®Œæˆ: {len(self.price_data)}æ¡è®°å½•, {self.max_steps}ä¸ªäº¤æ˜“æ—¥")
        
        def _generate_mock_data(self):
            """ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰"""
            dates = pd.date_range(start='2023-01-01', periods=self.max_steps, freq='D')
            
            # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼æ•°æ®
            np.random.seed(42)  # ç¡®ä¿å¯é‡çŽ°æ€§
            
            mock_data = []
            for date in dates:
                for i, symbol in enumerate(self.config.stock_pool):
                    # ç”Ÿæˆä»·æ ¼æ•°æ®
                    base_price = 10.0 + i  # åŸºç¡€ä»·æ ¼
                    daily_return = np.random.normal(0.001, 0.02)  # æ—¥æ”¶ç›ŠçŽ‡
                    
                    price = base_price * (1 + daily_return)
                    volume = np.random.randint(1000000, 10000000)
                    
                    mock_data.append({
                        'datetime': date,
                        'instrument': symbol,
                        'open': price * np.random.uniform(0.99, 1.01),
                        'high': price * np.random.uniform(1.00, 1.05),
                        'low': price * np.random.uniform(0.95, 1.00),
                        'close': price,
                        'volume': volume,
                        'amount': price * volume
                    })
            
            self.price_data = pd.DataFrame(mock_data).set_index(['datetime', 'instrument'])
            
            # ç”Ÿæˆæ¨¡æ‹Ÿç‰¹å¾æ•°æ®
            self._generate_mock_features()
            
            logger.info("ç”Ÿæˆæ¨¡æ‹Ÿæ•°æ®å®Œæˆ")
        
        def _generate_mock_features(self):
            """ç”Ÿæˆæ¨¡æ‹Ÿç‰¹å¾æ•°æ®"""
            feature_data = []
            
            for date_idx in range(len(self.price_data.index.get_level_values('datetime').unique())):
                for stock_idx, symbol in enumerate(self.config.stock_pool):
                    # ç”ŸæˆæŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
                    technical_features = np.random.randn(20).astype(np.float32)
                    
                    # ç”ŸæˆåŸºæœ¬é¢ç‰¹å¾
                    fundamental_features = np.random.randn(20).astype(np.float32)
                    
                    # ç”Ÿæˆå¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
                    microstructure_features = np.random.randn(10).astype(np.float32)
                    
                    # åˆå¹¶æ‰€æœ‰ç‰¹å¾
                    all_features = np.concatenate([
                        technical_features,
                        fundamental_features,
                        microstructure_features
                    ])
                    
                    feature_data.append(all_features)
            
            # é‡å¡‘ä¸º [time_steps, n_stocks, n_features]
            self.feature_data = np.array(feature_data).reshape(
                -1, self.n_stocks, self.n_features
            ).astype(np.float32)
            
            logger.info(f"ç”Ÿæˆæ¨¡æ‹Ÿç‰¹å¾æ•°æ®: {self.feature_data.shape}")
        
        def reset(self) -> Dict[str, np.ndarray]:
            """
            é‡ç½®çŽ¯å¢ƒåˆ°åˆå§‹çŠ¶æ€
            
            Returns:
                åˆå§‹è§‚å¯ŸçŠ¶æ€
            """
            self._initialize_state_variables()
            
            # å¦‚æžœæœ‰åŽ†å²æ•°æ®ï¼Œä»Žéšæœºä½ç½®å¼€å§‹
            if self.feature_data is not None:
                max_start = max(0, len(self.feature_data) - self.max_steps - self.config.lookback_window)
                self.start_idx = np.random.randint(0, max_start + 1) if max_start > 0 else 0
            else:
                self.start_idx = 0
            
            # æ›´æ–°å½“å‰ä»·æ ¼
            self._update_current_prices()
            
            observation = self._get_observation()
            
            logger.debug(f"çŽ¯å¢ƒé‡ç½®å®Œæˆï¼Œèµ·å§‹ç´¢å¼•: {self.start_idx}")
            
            return observation
        
        def step(self, action: np.ndarray) -> Tuple[Dict[str, np.ndarray], float, bool, Dict[str, Any]]:
            """
            æ‰§è¡Œä¸€æ­¥äº¤æ˜“
            
            Args:
                action: ç›®æ ‡æŠ•èµ„ç»„åˆæƒé‡
                
            Returns:
                (è§‚å¯Ÿ, å¥–åŠ±, æ˜¯å¦ç»“æŸ, ä¿¡æ¯å­—å…¸)
            """
            # æ›´æ–°å½“å‰ä»·æ ¼ï¼ˆåœ¨è®¡ç®—æ”¶ç›Šå’Œæˆæœ¬ä¹‹å‰ï¼‰
            self._update_current_prices()
            
            # æ ‡å‡†åŒ–åŠ¨ä½œ
            target_weights = self._normalize_action(action)
            
            # åº”ç”¨Aè‚¡äº¤æ˜“è§„åˆ™çº¦æŸ
            target_weights = self._apply_trading_constraints(target_weights)
            
            # è®¡ç®—äº¤æ˜“æˆæœ¬
            transaction_cost = self._calculate_transaction_cost(
                self.current_positions, target_weights
            )
            
            # æ‰§è¡Œäº¤æ˜“
            self._execute_trades(target_weights)
            
            # èŽ·å–å½“æœŸæ”¶ç›Š
            portfolio_return = self._calculate_portfolio_return()
            
            # æ›´æ–°æŠ•èµ„ç»„åˆä»·å€¼
            self._update_portfolio_value(portfolio_return, transaction_cost)
            
            # è®¡ç®—å¥–åŠ±
            reward = self._calculate_reward(portfolio_return, transaction_cost, target_weights)
            
            # æ›´æ–°çŠ¶æ€
            self.current_step += 1
            next_observation = self._get_observation()
            done = self._is_done()
            
            # æ›´æ–°T+1é™åˆ¶
            self._update_t_plus_1_restrictions(target_weights)
            
            # æž„å»ºä¿¡æ¯å­—å…¸
            info = self._build_info_dict(portfolio_return, transaction_cost, target_weights)
            
            # è®°å½•åŽ†å²
            self._record_step_history(target_weights, portfolio_return, transaction_cost)
            
            return next_observation, reward, done, info
        
        def _normalize_action(self, action: np.ndarray) -> np.ndarray:
            """æ ‡å‡†åŒ–åŠ¨ä½œä¸ºåˆæ³•çš„æƒé‡åˆ†å¸ƒ"""
            action = np.asarray(action, dtype=np.float32)
            
            # ç¡®ä¿éžè´Ÿ
            action = np.maximum(action, 0)
            
            # æ ‡å‡†åŒ–ä¸ºæƒé‡
            total = action.sum()
            if total == 0:
                return np.ones(self.n_stocks, dtype=np.float32) / self.n_stocks
            
            return action / total
        
        def _apply_trading_constraints(self, target_weights: np.ndarray) -> np.ndarray:
            """åº”ç”¨Aè‚¡äº¤æ˜“è§„åˆ™çº¦æŸ"""
            # é¦–å…ˆåº”ç”¨T+1çº¦æŸ
            if self.config.t_plus_1:
                target_weights = self._apply_t_plus_1_constraints(target_weights)
            
            # è¿­ä»£åº”ç”¨æƒé‡çº¦æŸï¼Œç¡®ä¿æ—¢æ»¡è¶³å•åªè‚¡ç¥¨æœ€å¤§æƒé‡åˆæ»¡è¶³æƒé‡å’Œä¸º1
            max_iterations = 10
            for _ in range(max_iterations):
                # åº”ç”¨å•åªè‚¡ç¥¨æœ€å¤§æƒé‡é™åˆ¶
                constrained_weights = np.minimum(target_weights, self.config.max_position_size)
                
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡æ–°åˆ†é…è¢«æˆªæ–­çš„æƒé‡
                excess_weight = target_weights.sum() - constrained_weights.sum()
                
                if excess_weight < 1e-8:
                    # æ²¡æœ‰æˆ–å¾ˆå°‘æƒé‡è¢«æˆªæ–­ï¼Œç›´æŽ¥æ ‡å‡†åŒ–
                    break
                
                # å°†è¢«æˆªæ–­çš„æƒé‡é‡æ–°åˆ†é…ç»™æœªè¾¾åˆ°ä¸Šé™çš„è‚¡ç¥¨
                available_capacity = self.config.max_position_size - constrained_weights
                total_capacity = available_capacity.sum()
                
                if total_capacity > 1e-8:
                    # æŒ‰æ¯”ä¾‹åˆ†é…å¤šä½™æƒé‡
                    redistribution = (available_capacity / total_capacity) * excess_weight
                    target_weights = constrained_weights + redistribution
                else:
                    # æ‰€æœ‰è‚¡ç¥¨éƒ½è¾¾åˆ°ä¸Šé™ï¼Œæ— æ³•é‡æ–°åˆ†é…
                    target_weights = constrained_weights
                    break
            
            # æœ€ç»ˆæ ‡å‡†åŒ–ç¡®ä¿æƒé‡å’Œä¸º1
            total = target_weights.sum()
            if total > 0:
                target_weights = target_weights / total
                
                # æ£€æŸ¥æ ‡å‡†åŒ–åŽæ˜¯å¦ä»ç„¶æ»¡è¶³æœ€å¤§æƒé‡çº¦æŸ
                # å¦‚æžœä¸æ»¡è¶³ï¼Œè¯´æ˜Žçº¦æŸåœ¨æ•°å­¦ä¸Šä¸å¯è¡Œï¼Œéœ€è¦æ”¾å®½
                if np.any(target_weights > self.config.max_position_size + 1e-8):
                    # è®¡ç®—å¯è¡Œçš„æœ€å¤§æƒé‡ï¼šç¡®ä¿æ‰€æœ‰è‚¡ç¥¨æƒé‡ç›¸ç­‰ä¸”å’Œä¸º1
                    feasible_max_weight = 1.0 / self.n_stocks
                    if feasible_max_weight > self.config.max_position_size:
                        # å¦‚æžœå‡åŒ€åˆ†é…éƒ½è¶…è¿‡é™åˆ¶ï¼Œåˆ™ä½¿ç”¨å‡åŒ€åˆ†é…ï¼ˆè¿™æ˜¯æ•°å­¦ä¸Šçš„æœ€ä¼˜è§£ï¼‰
                        target_weights = np.full(self.n_stocks, feasible_max_weight, dtype=np.float32)
                    else:
                        # å¦åˆ™å¯ä»¥æ»¡è¶³çº¦æŸ
                        target_weights = np.minimum(target_weights, self.config.max_position_size)
                        target_weights = target_weights / target_weights.sum()
            else:
                # å¦‚æžœæ‰€æœ‰æƒé‡éƒ½ä¸º0ï¼Œåˆ™å‡åŒ€åˆ†é…
                uniform_weight = 1.0 / self.n_stocks
                target_weights = np.full(self.n_stocks, uniform_weight, dtype=np.float32)
            
            return target_weights
        
        def _apply_t_plus_1_constraints(self, target_weights: np.ndarray) -> np.ndarray:
            """åº”ç”¨T+1äº¤æ˜“çº¦æŸ"""
            if not self.t_plus_1_restrictions:
                return target_weights
            
            # å¯¹äºŽå½“æ—¥ä¹°å…¥çš„è‚¡ç¥¨ï¼Œå¦‚æžœè¦å‡ä»“ï¼Œåˆ™é™åˆ¶å…¶æƒé‡ä¸èƒ½ä½ŽäºŽå½“å‰æŒä»“
            for stock_idx in self.t_plus_1_restrictions:
                if target_weights[stock_idx] < self.current_positions[stock_idx]:
                    target_weights[stock_idx] = self.current_positions[stock_idx]
            
            return target_weights
        
        def _calculate_transaction_cost(self, current_weights: np.ndarray, 
                                      target_weights: np.ndarray) -> float:
            """è®¡ç®—äº¤æ˜“æˆæœ¬"""
            total_cost = 0.0
            
            for i in range(self.n_stocks):
                weight_change = abs(target_weights[i] - current_weights[i])
                
                # åªæœ‰å½“æƒé‡å˜åŒ–è¶…è¿‡æœ€å°é˜ˆå€¼æ—¶æ‰è®¡ç®—æˆæœ¬
                if weight_change < 1e-8:
                    continue
                
                # è®¡ç®—äº¤æ˜“ä»·å€¼
                trade_value = weight_change * self.total_value
                
                # åˆ¤æ–­äº¤æ˜“æ–¹å‘
                side = 'buy' if target_weights[i] > current_weights[i] else 'sell'
                
                # åˆ›å»ºäº¤æ˜“ä¿¡æ¯
                trade_info = TradeInfo(
                    symbol=self.config.stock_pool[i],
                    side=side,
                    quantity=int(trade_value / self.current_prices[i]) if self.current_prices[i] > 0 else 0,
                    price=self.current_prices[i],
                    timestamp=datetime.now()
                )
                
                # è®¡ç®—æˆæœ¬
                cost_breakdown = self.cost_model.calculate_cost(trade_info)
                total_cost += cost_breakdown.total_cost
            
            return total_cost
        
        def _execute_trades(self, target_weights: np.ndarray):
            """æ‰§è¡Œäº¤æ˜“ï¼Œæ›´æ–°æŒä»“"""
            # è®°å½•äº¤æ˜“å‰çš„æŒä»“
            old_positions = self.current_positions.copy()
            
            # æ›´æ–°æŒä»“
            self.current_positions = target_weights.copy()
            
            # è®°å½•å‘ç”Ÿäº¤æ˜“çš„è‚¡ç¥¨ï¼ˆç”¨äºŽT+1é™åˆ¶ï¼‰
            self.traded_stocks = []
            for i in range(self.n_stocks):
                if abs(target_weights[i] - old_positions[i]) > 1e-8:
                    self.traded_stocks.append(i)
        
        def _calculate_portfolio_return(self) -> float:
            """è®¡ç®—æŠ•èµ„ç»„åˆæ”¶ç›ŠçŽ‡"""
            if self.current_prices is None or self.previous_prices is None:
                return 0.0
            
            # è®¡ç®—å„è‚¡ç¥¨æ”¶ç›ŠçŽ‡
            stock_returns = np.zeros(self.n_stocks)
            for i in range(self.n_stocks):
                if self.previous_prices[i] > 0:
                    stock_returns[i] = (self.current_prices[i] - self.previous_prices[i]) / self.previous_prices[i]
            
            # è®¡ç®—æŠ•èµ„ç»„åˆåŠ æƒæ”¶ç›ŠçŽ‡
            portfolio_return = np.dot(self.current_positions, stock_returns)
            
            return portfolio_return
        
        def _update_portfolio_value(self, portfolio_return: float, transaction_cost: float):
            """æ›´æ–°æŠ•èµ„ç»„åˆä»·å€¼"""
            # è®¡ç®—æ–°çš„æ€»ä»·å€¼
            self.total_value = self.total_value * (1 + portfolio_return) - transaction_cost
            
            # æ›´æ–°çŽ°é‡‘ï¼ˆç®€åŒ–å¤„ç†ï¼‰
            self.cash = self.total_value * (1 - self.current_positions.sum())
            
            # è®°å½•ä»·å€¼åŽ†å²
            self.portfolio_values.append(self.total_value)
            
            # æ›´æ–°æœ€å¤§ä»·å€¼ï¼ˆç”¨äºŽè®¡ç®—å›žæ’¤ï¼‰
            self.max_portfolio_value = max(self.max_portfolio_value, self.total_value)
        
        def _calculate_reward(self, portfolio_return: float, 
                             transaction_cost: float, weights: np.ndarray) -> float:
            """è®¡ç®—å¥–åŠ±å‡½æ•°"""
            # å‡€æ”¶ç›Š
            net_return = portfolio_return - transaction_cost / self.total_value
            
            # é£Žé™©æƒ©ç½šï¼ˆåŸºäºŽæƒé‡é›†ä¸­åº¦ï¼‰
            concentration = np.sum(weights ** 2)  # HerfindahlæŒ‡æ•°
            risk_penalty = self.config.risk_aversion * concentration
            
            # å›žæ’¤æƒ©ç½š
            current_drawdown = self._calculate_current_drawdown()
            drawdown_penalty = self.config.max_drawdown_penalty * max(0, current_drawdown - 0.1)
            
            # æ€»å¥–åŠ±
            reward = net_return - risk_penalty - drawdown_penalty
            
            return float(reward)
        
        def _calculate_current_drawdown(self) -> float:
            """è®¡ç®—å½“å‰å›žæ’¤"""
            if self.max_portfolio_value == 0:
                return 0.0
            return (self.max_portfolio_value - self.total_value) / self.max_portfolio_value
        
        def _get_observation(self) -> Dict[str, np.ndarray]:
            """èŽ·å–å½“å‰è§‚å¯ŸçŠ¶æ€"""
            # èŽ·å–åŽ†å²ç‰¹å¾æ•°æ®
            if self.feature_data is not None:
                start_idx = max(0, self.start_idx + self.current_step - self.config.lookback_window)
                end_idx = self.start_idx + self.current_step
                
                if end_idx > len(self.feature_data):
                    # å¦‚æžœè¶…å‡ºæ•°æ®èŒƒå›´ï¼Œä½¿ç”¨æœ€åŽå¯ç”¨çš„æ•°æ®
                    features = self.feature_data[-self.config.lookback_window:]
                else:
                    features = self.feature_data[start_idx:end_idx]
                
                # ç¡®ä¿ç‰¹å¾æ•°æ®æœ‰æ­£ç¡®çš„ç»´åº¦
                if len(features) < self.config.lookback_window:
                    # ç”¨ç¬¬ä¸€ä¸ªè§‚å¯Ÿå€¼å¡«å……ä¸è¶³çš„éƒ¨åˆ†
                    padding = np.repeat(features[0:1], 
                                      self.config.lookback_window - len(features), 
                                      axis=0)
                    features = np.concatenate([padding, features], axis=0)
            else:
                # ç”Ÿæˆéšæœºç‰¹å¾ï¼ˆç”¨äºŽæµ‹è¯•ï¼‰
                features = np.random.randn(
                    self.config.lookback_window, self.n_stocks, self.n_features
                ).astype(np.float32)
            
            # å¸‚åœºçŠ¶æ€ç‰¹å¾ï¼ˆç®€åŒ–å®žçŽ°ï¼‰
            market_state = np.array([
                self.total_value / self.config.initial_cash - 1,  # æ€»æ”¶ç›ŠçŽ‡
                self._calculate_current_drawdown(),               # å½“å‰å›žæ’¤
                np.sum(self.current_positions ** 2),            # æŒä»“é›†ä¸­åº¦
                np.sum(self.current_positions > 1e-6),          # æ´»è·ƒæŒä»“æ•°
                self.current_step / self.max_steps,             # æ—¶é—´è¿›åº¦
                np.std(self.returns_history[-30:]) if len(self.returns_history) >= 30 else 0,  # æ³¢åŠ¨çŽ‡
                np.mean(self.returns_history[-10:]) if len(self.returns_history) >= 10 else 0,  # è¿‘æœŸæ”¶ç›Š
                len(self.trade_history) / max(self.current_step, 1),  # äº¤æ˜“é¢‘çŽ‡
                self.cash / self.total_value,                    # çŽ°é‡‘æ¯”ä¾‹
                1.0 if self.current_step % 5 == 0 else 0.0     # å‘¨æœŸæ€§ç‰¹å¾
            ], dtype=np.float32)
            
            return {
                'features': features.astype(np.float32),
                'positions': self.current_positions.astype(np.float32),
                'market_state': market_state
            }
        
        def _update_current_prices(self):
            """æ›´æ–°å½“å‰ä»·æ ¼"""
            if self.price_data is not None:
                # ä¿å­˜å‰ä¸€æœŸä»·æ ¼
                self.previous_prices = self.current_prices.copy() if self.current_prices is not None else None
                
                # èŽ·å–å½“å‰ä»·æ ¼
                current_date_idx = self.start_idx + self.current_step
                if current_date_idx < len(self.price_data.index.get_level_values('datetime').unique()):
                    date = self.price_data.index.get_level_values('datetime').unique()[current_date_idx]
                    current_day_data = self.price_data.xs(date, level='datetime')
                    
                    self.current_prices = np.zeros(self.n_stocks)
                    for i, symbol in enumerate(self.config.stock_pool):
                        if symbol in current_day_data.index:
                            self.current_prices[i] = current_day_data.loc[symbol, 'close']
                        else:
                            # å¦‚æžœæ²¡æœ‰æ•°æ®ï¼Œä½¿ç”¨å‰ä¸€æœŸä»·æ ¼
                            self.current_prices[i] = self.previous_prices[i] if self.previous_prices is not None else 10.0
                else:
                    # å¦‚æžœè¶…å‡ºæ•°æ®èŒƒå›´ï¼Œä¿æŒå‰ä¸€æœŸä»·æ ¼
                    if self.current_prices is None:
                        self.current_prices = np.array([10.0 + i for i in range(self.n_stocks)])
            else:
                # ç”Ÿæˆæ¨¡æ‹Ÿä»·æ ¼
                self.previous_prices = self.current_prices.copy() if self.current_prices is not None else None
                if self.current_prices is None:
                    self.current_prices = np.array([10.0 + i for i in range(self.n_stocks)])
                else:
                    # æ¨¡æ‹Ÿä»·æ ¼å˜åŠ¨
                    returns = np.random.normal(0.001, 0.02, self.n_stocks)
                    self.current_prices = self.current_prices * (1 + returns)
        
        def _is_done(self) -> bool:
            """åˆ¤æ–­å›žåˆæ˜¯å¦ç»“æŸ"""
            # åŸºæœ¬ç»“æŸæ¡ä»¶ï¼šè¾¾åˆ°æœ€å¤§æ­¥æ•°
            if self.current_step >= self.max_steps:
                return True
            
            # é£Žé™©æŽ§åˆ¶ï¼šæ€»ä»·å€¼è¿‡ä½Ž
            if self.total_value < self.config.initial_cash * 0.5:
                logger.warning(f"æ€»ä»·å€¼è¿‡ä½Žï¼Œå¼ºåˆ¶ç»“æŸ: {self.total_value}")
                return True
            
            # æ•°æ®ç”¨å°½
            if (self.feature_data is not None and 
                self.start_idx + self.current_step >= len(self.feature_data)):
                return True
            
            return False
        
        def _update_t_plus_1_restrictions(self, target_weights: np.ndarray):
            """æ›´æ–°T+1äº¤æ˜“é™åˆ¶"""
            if not self.config.t_plus_1:
                return
            
            # æ¸…é™¤å‰ä¸€å¤©çš„é™åˆ¶
            self.t_plus_1_restrictions.clear()
            
            # å¯¹äºŽä»Šæ—¥å¢žä»“çš„è‚¡ç¥¨ï¼Œæ·»åŠ T+1é™åˆ¶
            for i in self.traded_stocks:
                if target_weights[i] > self.current_positions[i]:
                    self.t_plus_1_restrictions[i] = target_weights[i]
        
        def _build_info_dict(self, portfolio_return: float, 
                            transaction_cost: float, weights: np.ndarray) -> Dict[str, Any]:
            """æž„å»ºä¿¡æ¯å­—å…¸"""
            return {
                'portfolio_return': float(portfolio_return),
                'transaction_cost': float(transaction_cost),
                'positions': weights.copy(),
                'total_value': float(self.total_value),
                'cash': float(self.cash),
                'drawdown': float(self._calculate_current_drawdown()),
                'concentration': float(np.sum(weights ** 2)),
                'active_positions': int(np.sum(weights > 1e-6)),
                'max_position': float(np.max(weights)),
                'min_position': float(np.min(weights)),
                'step': self.current_step,
                't_plus_1_restrictions': list(self.t_plus_1_restrictions.keys())
            }
        
        def _record_step_history(self, weights: np.ndarray, 
                               portfolio_return: float, transaction_cost: float):
            """è®°å½•æ­¥éª¤åŽ†å²"""
            self.weights_history.append(weights.copy())
            self.returns_history.append(portfolio_return)
            
            if transaction_cost > 0:
                self.trade_history.append({
                    'step': self.current_step,
                    'weights': weights.copy(),
                    'cost': transaction_cost,
                    'value': self.total_value
                })
        
        def get_portfolio_metrics(self) -> Dict[str, float]:
            """èŽ·å–æŠ•èµ„ç»„åˆç»©æ•ˆæŒ‡æ ‡"""
            if len(self.portfolio_values) < 2:
                return {}
            
            values = np.array(self.portfolio_values)
            returns = np.diff(values) / values[:-1]
            
            # åŸºæœ¬æŒ‡æ ‡
            total_return = (values[-1] - values[0]) / values[0]
            volatility = np.std(returns) * np.sqrt(self.config.trading_days_per_year)
            
            # é£Žé™©è°ƒæ•´æŒ‡æ ‡
            if volatility > 0:
                sharpe_ratio = np.mean(returns) / np.std(returns) * np.sqrt(self.config.trading_days_per_year)
            else:
                sharpe_ratio = 0.0
            
            # æœ€å¤§å›žæ’¤
            max_drawdown = self._calculate_max_drawdown(values)
            
            # å…¶ä»–æŒ‡æ ‡
            win_rate = np.mean(np.array(self.returns_history) > 0) if self.returns_history else 0
            avg_return = np.mean(self.returns_history) if self.returns_history else 0
            
            return {
                'total_return': float(total_return),
                'annualized_return': float(total_return * self.config.trading_days_per_year / len(values)),
                'volatility': float(volatility),
                'sharpe_ratio': float(sharpe_ratio),
                'max_drawdown': float(max_drawdown),
                'win_rate': float(win_rate),
                'average_return': float(avg_return),
                'total_trades': len(self.trade_history),
                'final_value': float(values[-1])
            }
        
        def _calculate_max_drawdown(self, values: np.ndarray) -> float:
            """è®¡ç®—æœ€å¤§å›žæ’¤"""
            peak = np.maximum.accumulate(values)
            drawdown = (peak - values) / peak
            return np.max(drawdown)
        
        def render(self, mode='human'):
            """æ¸²æŸ“çŽ¯å¢ƒçŠ¶æ€"""
            if mode == 'human':
                metrics = self.get_portfolio_metrics()
                print(f"Step: {self.current_step}/{self.max_steps}")
                print(f"Total Value: {self.total_value:.2f}")
                print(f"Return: {metrics.get('total_return', 0):.4f}")
                print(f"Drawdown: {self._calculate_current_drawdown():.4f}")
                print(f"Positions: {self.current_positions}")
                print("-" * 50)
        
        def close(self):
            """å…³é—­çŽ¯å¢ƒ"""
            pass
        
        def seed(self, seed=None):
            """è®¾ç½®éšæœºç§å­"""
            np.random.seed(seed)
            return [seed]
    ]]></file>
  <file path="src/rl_trading_system/trading/almgren_chriss_model.py"><![CDATA[
    """
    Almgren-Chrisså¸‚åœºå†²å‡»æ¨¡åž‹
    å®žçŽ°æ°¸ä¹…å†²å‡»ï¼ˆçº¿æ€§ï¼‰å’Œä¸´æ—¶å†²å‡»ï¼ˆå¹³æ–¹æ ¹ï¼‰æ¨¡åž‹ï¼Œç”¨äºŽä¼°ç®—äº¤æ˜“æˆæœ¬
    """
    
    from dataclasses import dataclass
    from typing import Optional
    import numpy as np
    import math
    
    
    @dataclass
    class MarketImpactParameters:
        """å¸‚åœºå†²å‡»å‚æ•°"""
        permanent_impact_coeff: float  # æ°¸ä¹…å†²å‡»ç³»æ•°
        temporary_impact_coeff: float  # ä¸´æ—¶å†²å‡»ç³»æ•°
        volatility: float              # è‚¡ç¥¨æ³¢åŠ¨çŽ‡
        daily_volume: int              # æ—¥å‡æˆäº¤é‡
        participation_rate: float      # å¸‚åœºå‚ä¸Žåº¦
        
        def __post_init__(self):
            """å‚æ•°éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯å‚æ•°æœ‰æ•ˆæ€§"""
            if self.permanent_impact_coeff < 0:
                raise ValueError("æ°¸ä¹…å†²å‡»ç³»æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.temporary_impact_coeff < 0:
                raise ValueError("ä¸´æ—¶å†²å‡»ç³»æ•°ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.volatility < 0:
                raise ValueError("æ³¢åŠ¨çŽ‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.daily_volume < 0:
                raise ValueError("æ—¥å‡æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if not (0 <= self.participation_rate <= 1):
                raise ValueError("å¸‚åœºå‚ä¸Žåº¦å¿…é¡»åœ¨0åˆ°1ä¹‹é—´")
    
    
    @dataclass
    class ImpactResult:
        """å†²å‡»ç»“æžœ"""
        permanent_impact: float  # æ°¸ä¹…å†²å‡»
        temporary_impact: float  # ä¸´æ—¶å†²å‡»
        total_impact: float      # æ€»å†²å‡»
        trade_volume: int        # äº¤æ˜“é‡
        market_volume: int       # å¸‚åœºæˆäº¤é‡
        
        def __post_init__(self):
            """ç»“æžœéªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯ç»“æžœæœ‰æ•ˆæ€§"""
            if self.permanent_impact < 0:
                raise ValueError("æ°¸ä¹…å†²å‡»ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.temporary_impact < 0:
                raise ValueError("ä¸´æ—¶å†²å‡»ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.total_impact < 0:
                raise ValueError("æ€»å†²å‡»ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯æ€»å†²å‡»ç­‰äºŽæ°¸ä¹…å†²å‡»å’Œä¸´æ—¶å†²å‡»ä¹‹å’Œ
            expected_total = self.permanent_impact + self.temporary_impact
            if abs(self.total_impact - expected_total) > 1e-8:
                raise ValueError("æ€»å†²å‡»åº”ç­‰äºŽæ°¸ä¹…å†²å‡»å’Œä¸´æ—¶å†²å‡»ä¹‹å’Œ")
            
            if self.trade_volume < 0:
                raise ValueError("äº¤æ˜“é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.market_volume < 0:
                raise ValueError("å¸‚åœºæˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        def get_participation_rate(self) -> float:
            """èŽ·å–å®žé™…å‚ä¸Žåº¦"""
            if self.market_volume == 0:
                return 0.0
            return self.trade_volume / self.market_volume
        
        def get_cost_basis_points(self) -> float:
            """èŽ·å–æˆæœ¬ï¼ˆåŸºç‚¹ï¼‰"""
            return self.total_impact * 10000
    
    
    class AlmgrenChrissModel:
        """
        Almgren-Chrisså¸‚åœºå†²å‡»æ¨¡åž‹
        
        è¯¥æ¨¡åž‹å°†å¸‚åœºå†²å‡»åˆ†è§£ä¸ºä¸¤ä¸ªç»„æˆéƒ¨åˆ†ï¼š
        1. æ°¸ä¹…å†²å‡»ï¼ˆçº¿æ€§ï¼‰ï¼šåæ˜ ä¿¡æ¯æ³„éœ²å’Œä»·æ ¼å‘çŽ°çš„å½±å“
        2. ä¸´æ—¶å†²å‡»ï¼ˆå¹³æ–¹æ ¹ï¼‰ï¼šåæ˜ æµåŠ¨æ€§æ¶ˆè€—çš„å½±å“
        
        æ°¸ä¹…å†²å‡» = Î± * (V / V_daily)
        ä¸´æ—¶å†²å‡» = Î² * Ïƒ * sqrt(V / V_daily)
        
        å…¶ä¸­ï¼š
        - Î±: æ°¸ä¹…å†²å‡»ç³»æ•°
        - Î²: ä¸´æ—¶å†²å‡»ç³»æ•°  
        - Ïƒ: è‚¡ç¥¨æ³¢åŠ¨çŽ‡
        - V: äº¤æ˜“é‡
        - V_daily: æ—¥å‡æˆäº¤é‡
        """
        
        def __init__(self, parameters: MarketImpactParameters):
            """
            åˆå§‹åŒ–Almgren-Chrissæ¨¡åž‹
            
            Args:
                parameters: å¸‚åœºå†²å‡»å‚æ•°
            """
            self.parameters = parameters
        
        def calculate_impact(self, 
                            trade_volume: int,
                            market_volume: Optional[int] = None,
                            volatility: Optional[float] = None) -> ImpactResult:
            """
            è®¡ç®—å¸‚åœºå†²å‡»
            
            Args:
                trade_volume: äº¤æ˜“é‡
                market_volume: å¸‚åœºæˆäº¤é‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å‚æ•°ä¸­çš„æ—¥å‡æˆäº¤é‡ï¼‰
                volatility: æ³¢åŠ¨çŽ‡ï¼ˆå¯é€‰ï¼Œé»˜è®¤ä½¿ç”¨å‚æ•°ä¸­çš„æ³¢åŠ¨çŽ‡ï¼‰
                
            Returns:
                ImpactResult: å†²å‡»ç»“æžœ
            """
            # ä½¿ç”¨é»˜è®¤å€¼æˆ–æä¾›çš„å€¼
            market_vol = market_volume if market_volume is not None else self.parameters.daily_volume
            vol = volatility if volatility is not None else self.parameters.volatility
            
            # è®¡ç®—æ°¸ä¹…å†²å‡»ï¼ˆçº¿æ€§ï¼‰
            permanent_impact = self._calculate_permanent_impact(trade_volume, market_vol)
            
            # è®¡ç®—ä¸´æ—¶å†²å‡»ï¼ˆå¹³æ–¹æ ¹ï¼‰
            temporary_impact = self._calculate_temporary_impact(trade_volume, market_vol, vol)
            
            # è®¡ç®—æ€»å†²å‡»
            total_impact = permanent_impact + temporary_impact
            
            return ImpactResult(
                permanent_impact=permanent_impact,
                temporary_impact=temporary_impact,
                total_impact=total_impact,
                trade_volume=trade_volume,
                market_volume=market_vol
            )
        
        def _calculate_permanent_impact(self, trade_volume: int, market_volume: int) -> float:
            """
            è®¡ç®—æ°¸ä¹…å†²å‡»ï¼ˆçº¿æ€§æ¨¡åž‹ï¼‰
            
            æ°¸ä¹…å†²å‡»åæ˜ äº†äº¤æ˜“å¯¹è‚¡ä»·çš„æŒä¹…å½±å“ï¼Œé€šå¸¸ç”±ä¿¡æ¯æ³„éœ²å¼•èµ·ã€‚
            æ¨¡åž‹ï¼špermanent_impact = Î± * (V / V_daily)
            
            Args:
                trade_volume: äº¤æ˜“é‡
                market_volume: å¸‚åœºæˆäº¤é‡
                
            Returns:
                float: æ°¸ä¹…å†²å‡»
            """
            if market_volume == 0:
                return 0.0
            
            participation_rate = trade_volume / market_volume
            return self.parameters.permanent_impact_coeff * participation_rate
        
        def _calculate_temporary_impact(self, trade_volume: int, market_volume: int, volatility: float) -> float:
            """
            è®¡ç®—ä¸´æ—¶å†²å‡»ï¼ˆå¹³æ–¹æ ¹æ¨¡åž‹ï¼‰
            
            ä¸´æ—¶å†²å‡»åæ˜ äº†äº¤æ˜“å¯¹æµåŠ¨æ€§çš„æ¶ˆè€—ï¼Œéšç€æ—¶é—´æŽ¨ç§»ä¼šæ¢å¤ã€‚
            æ¨¡åž‹ï¼štemporary_impact = Î² * Ïƒ * sqrt(V / V_daily)
            
            Args:
                trade_volume: äº¤æ˜“é‡
                market_volume: å¸‚åœºæˆäº¤é‡
                volatility: æ³¢åŠ¨çŽ‡
                
            Returns:
                float: ä¸´æ—¶å†²å‡»
            """
            if market_volume == 0:
                return 0.0
            
            participation_rate = trade_volume / market_volume
            return self.parameters.temporary_impact_coeff * volatility * math.sqrt(participation_rate)
        
        def update_parameters(self, new_parameters: MarketImpactParameters) -> None:
            """
            æ›´æ–°æ¨¡åž‹å‚æ•°
            
            Args:
                new_parameters: æ–°çš„å¸‚åœºå†²å‡»å‚æ•°
            """
            self.parameters = new_parameters
        
        def get_parameters(self) -> MarketImpactParameters:
            """
            èŽ·å–å½“å‰æ¨¡åž‹å‚æ•°
            
            Returns:
                MarketImpactParameters: å½“å‰å‚æ•°
            """
            return self.parameters
        
        def estimate_optimal_trade_size(self, 
                                       target_volume: int,
                                       time_horizon: int,
                                       risk_aversion: float = 1.0) -> int:
            """
            ä¼°ç®—æœ€ä¼˜äº¤æ˜“è§„æ¨¡
            
            åŸºäºŽAlmgren-Chrissæ¨¡åž‹çš„æœ€ä¼˜æ‰§è¡Œç­–ç•¥ï¼Œå¹³è¡¡å¸‚åœºå†²å‡»å’Œæ—¶é—´é£Žé™©ã€‚
            
            Args:
                target_volume: ç›®æ ‡äº¤æ˜“æ€»é‡
                time_horizon: äº¤æ˜“æ—¶é—´çª—å£ï¼ˆåˆ†é’Ÿï¼‰
                risk_aversion: é£Žé™©åŽŒæ¶ç³»æ•°
                
            Returns:
                int: å»ºè®®çš„å•æ¬¡äº¤æ˜“è§„æ¨¡
            """
            if time_horizon <= 0:
                return target_volume
            
            # ç®€åŒ–çš„æœ€ä¼˜äº¤æ˜“è§„æ¨¡å…¬å¼
            # åœ¨å®žé™…åº”ç”¨ä¸­ï¼Œè¿™éœ€è¦æ›´å¤æ‚çš„ä¼˜åŒ–ç®—æ³•
            alpha = self.parameters.permanent_impact_coeff
            beta = self.parameters.temporary_impact_coeff
            sigma = self.parameters.volatility
            
            # æœ€ä¼˜äº¤æ˜“é€Ÿåº¦ï¼ˆç®€åŒ–ç‰ˆæœ¬ï¼‰
            optimal_rate = math.sqrt(alpha / (beta * sigma * risk_aversion))
            optimal_size = int(target_volume * optimal_rate / time_horizon)
            
            # ç¡®ä¿äº¤æ˜“è§„æ¨¡åœ¨åˆç†èŒƒå›´å†…
            min_size = max(1, target_volume // (time_horizon * 10))
            max_size = target_volume // max(1, time_horizon // 10)
            
            return max(min_size, min(optimal_size, max_size))
        
        def calculate_execution_cost(self, 
                                   trade_schedule: list,
                                   market_volumes: Optional[list] = None) -> dict:
            """
            è®¡ç®—æ‰§è¡Œè®¡åˆ’çš„æ€»æˆæœ¬
            
            Args:
                trade_schedule: äº¤æ˜“è®¡åˆ’åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸ºäº¤æ˜“é‡
                market_volumes: å¯¹åº”çš„å¸‚åœºæˆäº¤é‡åˆ—è¡¨ï¼ˆå¯é€‰ï¼‰
                
            Returns:
                dict: åŒ…å«æ€»æˆæœ¬ã€æ°¸ä¹…å†²å‡»ã€ä¸´æ—¶å†²å‡»ç­‰ä¿¡æ¯çš„å­—å…¸
            """
            total_permanent = 0.0
            total_temporary = 0.0
            total_volume = 0
            
            results = []
            
            for i, trade_vol in enumerate(trade_schedule):
                market_vol = (market_volumes[i] if market_volumes and i < len(market_volumes) 
                             else self.parameters.daily_volume)
                
                result = self.calculate_impact(trade_vol, market_vol)
                results.append(result)
                
                total_permanent += result.permanent_impact
                total_temporary += result.temporary_impact
                total_volume += trade_vol
            
            return {
                'total_permanent_impact': total_permanent,
                'total_temporary_impact': total_temporary,
                'total_impact': total_permanent + total_temporary,
                'total_volume': total_volume,
                'average_impact': (total_permanent + total_temporary) / len(trade_schedule) if trade_schedule else 0,
                'detailed_results': results
            }
    ]]></file>
  <file path="src/rl_trading_system/trading/__init__.py"><![CDATA[
    """äº¤æ˜“çŽ¯å¢ƒæ¨¡å—"""
    
    from .almgren_chriss_model import AlmgrenChrissModel, MarketImpactParameters, ImpactResult
    from .transaction_cost_model import TransactionCostModel, CostParameters, CostBreakdown, TradeInfo
    from .portfolio_environment import PortfolioEnvironment, PortfolioConfig
    
    __all__ = [
        "AlmgrenChrissModel",
        "MarketImpactParameters", 
        "ImpactResult",
        "TransactionCostModel",
        "CostParameters",
        "CostBreakdown",
        "TradeInfo",
        "PortfolioEnvironment",
        "PortfolioConfig"
    ]
    ]]></file>
  <file path="src/rl_trading_system/monitoring/trading_system_monitor.py"><![CDATA[
    """
    äº¤æ˜“ç³»ç»Ÿç›‘æŽ§æ¨¡å—
    å®žçŽ°TradingSystemMonitorç±»å’ŒæŒ‡æ ‡æ”¶é›†ï¼Œå®šä¹‰æ€§èƒ½ã€é£Žé™©å’Œç³»ç»Ÿç›‘æŽ§æŒ‡æ ‡ï¼Œå®žçŽ°æŒ‡æ ‡é‡‡é›†ã€å¯¼å‡ºå’ŒGrafanaä»ªè¡¨æ¿é…ç½®
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import time
    import threading
    import psutil
    import requests
    from threading import Lock
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional, Any, Union
    from collections import deque
    import json
    import re
    
    from prometheus_client import CollectorRegistry, Gauge, Counter, Histogram, start_http_server, generate_latest
    
    from ..backtest.multi_frequency_backtest import Trade, OrderType
    
    
    class MetricsCollector:
        """æŒ‡æ ‡æ”¶é›†å™¨ç±»"""
        
        def __init__(self):
            """åˆå§‹åŒ–æŒ‡æ ‡æ”¶é›†å™¨"""
            self.metrics_registry = {}
            self.performance_metrics = {}
            self.risk_metrics = {}
            self.system_metrics = {}
            self.trading_metrics = {}
            self._lock = Lock()
        
        def collect_performance_metrics(self, portfolio_value: float, daily_return: float,
                                      total_return: float, sharpe_ratio: float) -> None:
            """æ”¶é›†æ€§èƒ½æŒ‡æ ‡"""
            if portfolio_value < 0:
                raise ValueError("æŠ•èµ„ç»„åˆä»·å€¼ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            with self._lock:
                self.performance_metrics = {
                    'portfolio_value': portfolio_value,
                    'daily_return': daily_return,
                    'total_return': total_return,
                    'sharpe_ratio': sharpe_ratio,
                    'timestamp': datetime.now()
                }
        
        def collect_risk_metrics(self, volatility: float, max_drawdown: float,
                               var_95: float, beta: float) -> None:
            """æ”¶é›†é£Žé™©æŒ‡æ ‡"""
            with self._lock:
                self.risk_metrics = {
                    'volatility': volatility,
                    'max_drawdown': max_drawdown,
                    'var_95': var_95,
                    'beta': beta,
                    'timestamp': datetime.now()
                }
        
        def collect_system_metrics(self, cpu_usage: float, memory_usage: float,
                                 disk_usage: float, model_inference_time: float) -> None:
            """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
            with self._lock:
                self.system_metrics = {
                    'cpu_usage': cpu_usage,
                    'memory_usage': memory_usage,
                    'disk_usage': disk_usage,
                    'model_inference_time': model_inference_time,
                    'timestamp': datetime.now()
                }
        
        def collect_trading_metrics(self, total_trades: int, successful_trades: int,
                                  win_rate: float, average_trade_size: float,
                                  turnover_rate: float) -> None:
            """æ”¶é›†äº¤æ˜“æŒ‡æ ‡"""
            with self._lock:
                self.trading_metrics = {
                    'total_trades': total_trades,
                    'successful_trades': successful_trades,
                    'win_rate': win_rate,
                    'average_trade_size': average_trade_size,
                    'turnover_rate': turnover_rate,
                    'timestamp': datetime.now()
                }
        
        def register_metric(self, name: str, description: str, metric_type: str) -> None:
            """æ³¨å†Œè‡ªå®šä¹‰æŒ‡æ ‡"""
            if not name:
                raise ValueError("æŒ‡æ ‡åç§°ä¸èƒ½ä¸ºç©º")
            
            if metric_type not in ['gauge', 'counter', 'histogram']:
                raise ValueError(f"ä¸æ”¯æŒçš„æŒ‡æ ‡ç±»åž‹: {metric_type}")
            
            with self._lock:
                self.metrics_registry[name] = {
                    'description': description,
                    'type': metric_type,
                    'value': 0.0,
                    'timestamp': datetime.now()
                }
        
        def update_metric(self, name: str, value: float) -> None:
            """æ›´æ–°æŒ‡æ ‡å€¼"""
            if name not in self.metrics_registry:
                raise ValueError(f"æŒ‡æ ‡ {name} ä¸å­˜åœ¨")
            
            with self._lock:
                self.metrics_registry[name]['value'] = value
                self.metrics_registry[name]['timestamp'] = datetime.now()
        
        def reset_metrics(self) -> None:
            """é‡ç½®æ‰€æœ‰æŒ‡æ ‡"""
            with self._lock:
                self.performance_metrics.clear()
                self.risk_metrics.clear()
                self.system_metrics.clear()
                self.trading_metrics.clear()
        
        def export_metrics(self) -> Dict[str, Dict[str, Any]]:
            """å¯¼å‡ºæ‰€æœ‰æŒ‡æ ‡"""
            with self._lock:
                return {
                    'performance_metrics': self.performance_metrics.copy(),
                    'risk_metrics': self.risk_metrics.copy(),
                    'system_metrics': self.system_metrics.copy(),
                    'trading_metrics': self.trading_metrics.copy(),
                    'custom_metrics': self.metrics_registry.copy()
                }
    
    
    class PrometheusExporter:
        """Prometheuså¯¼å‡ºå™¨ç±»"""
        
        def __init__(self, metrics_collector: MetricsCollector, port: int = 8000):
            """åˆå§‹åŒ–Prometheuså¯¼å‡ºå™¨"""
            if not (1024 <= port <= 65535):
                raise ValueError("ç«¯å£å·å¿…é¡»åœ¨1024-65535èŒƒå›´å†…")
            
            self.metrics_collector = metrics_collector
            self.port = port
            self.registry = CollectorRegistry()
            self.is_running = False
            self._server_thread = None
            self._prometheus_metrics = {}
            self._lock = Lock()
        
        def register_prometheus_metrics(self) -> None:
            """æ³¨å†ŒPrometheusæŒ‡æ ‡"""
            with self._lock:
                # æ€§èƒ½æŒ‡æ ‡
                self._prometheus_metrics['portfolio_value'] = Gauge(
                    'portfolio_value', 'Portfolio total value', registry=self.registry
                )
                self._prometheus_metrics['daily_return'] = Gauge(
                    'daily_return', 'Daily return rate', registry=self.registry
                )
                self._prometheus_metrics['total_return'] = Gauge(
                    'total_return', 'Total return rate', registry=self.registry
                )
                self._prometheus_metrics['sharpe_ratio'] = Gauge(
                    'sharpe_ratio', 'Sharpe ratio', registry=self.registry
                )
                
                # é£Žé™©æŒ‡æ ‡
                self._prometheus_metrics['volatility'] = Gauge(
                    'volatility', 'Portfolio volatility', registry=self.registry
                )
                self._prometheus_metrics['max_drawdown'] = Gauge(
                    'max_drawdown', 'Maximum drawdown', registry=self.registry
                )
                self._prometheus_metrics['var_95'] = Gauge(
                    'var_95', 'Value at Risk 95%', registry=self.registry
                )
                self._prometheus_metrics['beta'] = Gauge(
                    'beta', 'Portfolio beta', registry=self.registry
                )
                
                # ç³»ç»ŸæŒ‡æ ‡
                self._prometheus_metrics['cpu_usage'] = Gauge(
                    'cpu_usage', 'CPU usage percentage', registry=self.registry
                )
                self._prometheus_metrics['memory_usage'] = Gauge(
                    'memory_usage', 'Memory usage percentage', registry=self.registry
                )
                self._prometheus_metrics['disk_usage'] = Gauge(
                    'disk_usage', 'Disk usage percentage', registry=self.registry
                )
                self._prometheus_metrics['model_inference_time'] = Gauge(
                    'model_inference_time', 'Model inference time in seconds', registry=self.registry
                )
                
                # äº¤æ˜“æŒ‡æ ‡
                self._prometheus_metrics['total_trades'] = Counter(
                    'total_trades', 'Total number of trades', registry=self.registry
                )
                self._prometheus_metrics['successful_trades'] = Counter(
                    'successful_trades', 'Number of successful trades', registry=self.registry
                )
                self._prometheus_metrics['win_rate'] = Gauge(
                    'win_rate', 'Trading win rate', registry=self.registry
                )
                self._prometheus_metrics['average_trade_size'] = Gauge(
                    'average_trade_size', 'Average trade size', registry=self.registry
                )
                self._prometheus_metrics['turnover_rate'] = Gauge(
                    'turnover_rate', 'Portfolio turnover rate', registry=self.registry
                )
                
                # æ³¨å†Œè‡ªå®šä¹‰æŒ‡æ ‡
                for name, metric_info in self.metrics_collector.metrics_registry.items():
                    if metric_info['type'] == 'gauge':
                        self._prometheus_metrics[name] = Gauge(
                            name, metric_info['description'], registry=self.registry
                        )
                    elif metric_info['type'] == 'counter':
                        self._prometheus_metrics[name] = Counter(
                            name, metric_info['description'], registry=self.registry
                        )
        
        def get_registered_metrics(self) -> Dict[str, Any]:
            """èŽ·å–å·²æ³¨å†Œçš„æŒ‡æ ‡"""
            with self._lock:
                return list(self._prometheus_metrics.keys())
        
        def generate_metrics_output(self) -> str:
            """ç”ŸæˆæŒ‡æ ‡è¾“å‡º"""
            # æ›´æ–°PrometheusæŒ‡æ ‡å€¼
            self._update_prometheus_metrics()
            
            # ç”ŸæˆPrometheusæ ¼å¼çš„è¾“å‡º
            return generate_latest(self.registry).decode('utf-8')
        
        def _update_prometheus_metrics(self) -> None:
            """æ›´æ–°PrometheusæŒ‡æ ‡å€¼"""
            metrics_data = self.metrics_collector.export_metrics()
            
            with self._lock:
                # æ›´æ–°æ€§èƒ½æŒ‡æ ‡
                perf_metrics = metrics_data.get('performance_metrics', {})
                for key, value in perf_metrics.items():
                    if key != 'timestamp' and key in self._prometheus_metrics:
                        self._prometheus_metrics[key].set(value)
                
                # æ›´æ–°é£Žé™©æŒ‡æ ‡
                risk_metrics = metrics_data.get('risk_metrics', {})
                for key, value in risk_metrics.items():
                    if key != 'timestamp' and key in self._prometheus_metrics:
                        self._prometheus_metrics[key].set(value)
                
                # æ›´æ–°ç³»ç»ŸæŒ‡æ ‡
                sys_metrics = metrics_data.get('system_metrics', {})
                for key, value in sys_metrics.items():
                    if key != 'timestamp' and key in self._prometheus_metrics:
                        self._prometheus_metrics[key].set(value)
                
                # æ›´æ–°äº¤æ˜“æŒ‡æ ‡
                trade_metrics = metrics_data.get('trading_metrics', {})
                for key, value in trade_metrics.items():
                    if key != 'timestamp' and key in self._prometheus_metrics:
                        if key in ['total_trades', 'successful_trades']:
                            # Counterç±»åž‹éœ€è¦ç‰¹æ®Šå¤„ç†
                            self._prometheus_metrics[key]._value._value = value
                        else:
                            self._prometheus_metrics[key].set(value)
                
                # æ›´æ–°è‡ªå®šä¹‰æŒ‡æ ‡
                custom_metrics = metrics_data.get('custom_metrics', {})
                for name, metric_info in custom_metrics.items():
                    if name in self._prometheus_metrics:
                        if metric_info['type'] == 'counter':
                            self._prometheus_metrics[name]._value._value = metric_info['value']
                        else:
                            self._prometheus_metrics[name].set(metric_info['value'])
        
        def start(self) -> None:
            """å¯åŠ¨Prometheuså¯¼å‡ºå™¨"""
            if self.is_running:
                return
            
            self.register_prometheus_metrics()
            
            def run_server():
                start_http_server(self.port, registry=self.registry)
            
            self._server_thread = threading.Thread(target=run_server, daemon=True)
            self._server_thread.start()
            self.is_running = True
        
        def stop(self) -> None:
            """åœæ­¢Prometheuså¯¼å‡ºå™¨"""
            self.is_running = False
            # Note: prometheus_client doesn't provide a direct way to stop the server
            # In practice, this would require more complex server management
        
        def health_check(self) -> bool:
            """å¥åº·æ£€æŸ¥"""
            if not self.is_running:
                return False
            
            # ç®€å•çš„å¥åº·æ£€æŸ¥ï¼šå°è¯•è®¿é—®metricsç«¯ç‚¹
            try:
                import urllib.request
                req = urllib.request.Request(f"http://localhost:{self.port}/metrics")
                with urllib.request.urlopen(req, timeout=1) as response:
                    return response.getcode() == 200
            except:
                return False
    
    
    class GrafanaDashboardManager:
        """Grafanaä»ªè¡¨æ¿ç®¡ç†å™¨ç±»"""
        
        def __init__(self, grafana_url: str, api_key: str):
            """åˆå§‹åŒ–Grafanaä»ªè¡¨æ¿ç®¡ç†å™¨"""
            if not grafana_url:
                raise ValueError("Grafana URLä¸èƒ½ä¸ºç©º")
            
            # ç®€å•çš„URLæ ¼å¼éªŒè¯
            if not re.match(r'^https?://.+', grafana_url):
                raise ValueError("æ— æ•ˆçš„Grafana URLæ ¼å¼")
            
            self.grafana_url = grafana_url.rstrip('/')
            self.api_key = api_key
            self.session = requests.Session()
            self.session.headers.update({
                'Authorization': f'Bearer {api_key}',
                'Content-Type': 'application/json'
            })
        
        def generate_dashboard_config(self) -> Dict[str, Any]:
            """ç”Ÿæˆä»ªè¡¨æ¿é…ç½®"""
            return {
                "dashboard": {
                    "id": None,
                    "title": "äº¤æ˜“ç³»ç»Ÿç›‘æŽ§",
                    "tags": ["trading", "monitoring"],
                    "timezone": "browser",
                    "panels": [
                        self.create_panel("æŠ•èµ„ç»„åˆä»·å€¼", "portfolio_value", "graph", 0, 0, 12, 8),
                        self.create_panel("æ—¥æ”¶ç›ŠçŽ‡", "daily_return", "graph", 0, 8, 12, 8),
                        self.create_panel("é£Žé™©æŒ‡æ ‡", "max_drawdown", "singlestat", 12, 0, 6, 8),
                        self.create_panel("ç³»ç»Ÿæ€§èƒ½", "cpu_usage", "graph", 12, 8, 6, 8),
                        self.create_panel("äº¤æ˜“ç»Ÿè®¡", "total_trades", "singlestat", 18, 0, 6, 8),
                        self.create_panel("å¤æ™®æ¯”çŽ‡", "sharpe_ratio", "singlestat", 18, 8, 6, 8),
                    ],
                    "time": {
                        "from": "now-1h",
                        "to": "now"
                    },
                    "refresh": "5s"
                },
                "overwrite": True
            }
        
        def create_panel(self, title: str, metric_query: str, panel_type: str,
                        x_pos: int, y_pos: int, width: int, height: int) -> Dict[str, Any]:
            """åˆ›å»ºä»ªè¡¨æ¿é¢æ¿"""
            panel_config = {
                "id": None,
                "title": title,
                "type": panel_type,
                "gridPos": {
                    "h": height,
                    "w": width,
                    "x": x_pos,
                    "y": y_pos
                },
                "targets": [
                    {
                        "expr": metric_query,
                        "format": "time_series",
                        "legendFormat": title,
                        "refId": "A"
                    }
                ],
                "datasource": "prometheus"
            }
            
            if panel_type == "graph":
                panel_config.update({
                    "xAxis": {"show": True},
                    "yAxes": [
                        {"show": True, "label": title},
                        {"show": True}
                    ],
                    "lines": True,
                    "fill": 1,
                    "linewidth": 2,
                    "points": False,
                    "pointradius": 2
                })
            elif panel_type == "singlestat":
                panel_config.update({
                    "valueName": "current",
                    "format": "short",
                    "prefix": "",
                    "postfix": "",
                    "nullText": None,
                    "valueMaps": [],
                    "mappingTypes": [],
                    "rangeMaps": [],
                    "colorBackground": False,
                    "colorValue": False,
                    "colors": ["#299c46", "rgba(237, 129, 40, 0.89)", "#d44a3a"],
                    "sparkline": {
                        "show": False,
                        "full": False,
                        "lineColor": "rgb(31, 120, 193)",
                        "fillColor": "rgba(31, 118, 189, 0.18)"
                    },
                    "gauge": {
                        "show": False,
                        "minValue": 0,
                        "maxValue": 100,
                        "thresholdMarkers": True,
                        "thresholdLabels": False
                    }
                })
            
            return panel_config
        
        def deploy_dashboard(self) -> Dict[str, Any]:
            """éƒ¨ç½²ä»ªè¡¨æ¿"""
            dashboard_config = self.generate_dashboard_config()
            
            response = self.session.post(
                f"{self.grafana_url}/api/dashboards/db",
                json=dashboard_config
            )
            
            if response.status_code == 401:
                raise Exception("Grafana APIè®¤è¯å¤±è´¥")
            
            return response.json()
        
        def update_dashboard(self, dashboard_uid: str) -> Dict[str, Any]:
            """æ›´æ–°ä»ªè¡¨æ¿"""
            dashboard_config = self.generate_dashboard_config()
            dashboard_config['dashboard']['uid'] = dashboard_uid
            
            response = self.session.post(
                f"{self.grafana_url}/api/dashboards/db",
                json=dashboard_config
            )
            
            return response.json()
        
        def delete_dashboard(self, dashboard_uid: str) -> Dict[str, Any]:
            """åˆ é™¤ä»ªè¡¨æ¿"""
            response = self.session.delete(
                f"{self.grafana_url}/api/dashboards/uid/{dashboard_uid}"
            )
            
            return response.json()
        
        def create_alert_rule(self, rule_name: str, metric_query: str, threshold: float,
                             condition: str, evaluation_interval: str) -> Dict[str, Any]:
            """åˆ›å»ºå‘Šè­¦è§„åˆ™"""
            return {
                "name": rule_name,
                "condition": {
                    "query": metric_query,
                    "threshold": threshold,
                    "type": condition
                },
                "frequency": evaluation_interval,
                "handler": 1,
                "severity": "critical",
                "state": "ok",
                "executionErrorState": "alerting",
                "noDataState": "no_data",
                "for": "5m"
            }
        
        def configure_prometheus_datasource(self, prometheus_url: str,
                                          datasource_name: str) -> Dict[str, Any]:
            """é…ç½®Prometheusæ•°æ®æº"""
            return {
                "name": datasource_name,
                "type": "prometheus",
                "url": prometheus_url,
                "access": "proxy",
                "basicAuth": False,
                "isDefault": True,
                "jsonData": {
                    "httpMethod": "POST",
                    "prometheusType": "Prometheus",
                    "prometheusVersion": "2.x"
                }
            }
    
    
    class TradingSystemMonitor:
        """äº¤æ˜“ç³»ç»Ÿç›‘æŽ§å™¨ä¸»ç±»"""
        
        def __init__(self, prometheus_port: int = 8000, grafana_url: str = None,
                     grafana_api_key: str = None):
            """åˆå§‹åŒ–äº¤æ˜“ç³»ç»Ÿç›‘æŽ§å™¨"""
            self.metrics_collector = MetricsCollector()
            self.prometheus_exporter = PrometheusExporter(self.metrics_collector, prometheus_port)
            
            if grafana_url and grafana_api_key:
                self.dashboard_manager = GrafanaDashboardManager(grafana_url, grafana_api_key)
            else:
                self.dashboard_manager = None
            
            self.is_monitoring = False
            self._metrics_history = deque(maxlen=1000)  # ä¿ç•™æœ€è¿‘1000ä¸ªæ•°æ®ç‚¹
            self._history_lock = Lock()
        
        def start_monitoring(self) -> None:
            """å¯åŠ¨ç›‘æŽ§"""
            if self.is_monitoring:
                return
            
            self.prometheus_exporter.start()
            self.is_monitoring = True
        
        def stop_monitoring(self) -> None:
            """åœæ­¢ç›‘æŽ§"""
            if not self.is_monitoring:
                return
            
            self.prometheus_exporter.stop()
            self.is_monitoring = False
        
        def update_portfolio_metrics(self, portfolio_data: Dict[str, float]) -> None:
            """æ›´æ–°æŠ•èµ„ç»„åˆæŒ‡æ ‡"""
            self.metrics_collector.collect_performance_metrics(
                portfolio_value=portfolio_data['value'],
                daily_return=portfolio_data['daily_return'],
                total_return=portfolio_data['total_return'],
                sharpe_ratio=portfolio_data.get('sharpe_ratio', 0.0)
            )
            
            # å¦‚æžœåŒ…å«é£Žé™©æŒ‡æ ‡ï¼Œä¹Ÿæ›´æ–°é£Žé™©æŒ‡æ ‡
            if 'max_drawdown' in portfolio_data:
                self.metrics_collector.collect_risk_metrics(
                    volatility=portfolio_data.get('volatility', 0.0),
                    max_drawdown=portfolio_data['max_drawdown'],
                    var_95=portfolio_data.get('var_95', 0.0),
                    beta=portfolio_data.get('beta', 1.0)
                )
            
            # ä¿å­˜åŽ†å²è®°å½•
            self._save_metrics_history()
        
        def update_trading_metrics(self, trades: List[Trade]) -> None:
            """æ›´æ–°äº¤æ˜“æŒ‡æ ‡"""
            if not trades:
                return
            
            buy_trades = [t for t in trades if t.trade_type == OrderType.BUY]
            sell_trades = [t for t in trades if t.trade_type == OrderType.SELL]
            
            # è®¡ç®—åŸºæœ¬äº¤æ˜“ç»Ÿè®¡
            total_trades = len(trades)
            total_volume = sum(float(t.quantity * t.price) for t in trades)
            average_trade_size = total_volume / total_trades if total_trades > 0 else 0.0
            
            self.metrics_collector.collect_trading_metrics(
                total_trades=total_trades,
                successful_trades=len(sell_trades),  # ç®€åŒ–ï¼šè®¤ä¸ºå–å‡ºäº¤æ˜“æ˜¯æˆåŠŸçš„
                win_rate=0.7,  # è¿™é‡Œéœ€è¦å®žé™…è®¡ç®—èƒœçŽ‡
                average_trade_size=average_trade_size,
                turnover_rate=2.0  # è¿™é‡Œéœ€è¦å®žé™…è®¡ç®—æ¢æ‰‹çŽ‡
            )
            
            # æ›´æ–°äº¤æ˜“ç»Ÿè®¡åˆ°trading_metrics
            self.metrics_collector.trading_metrics.update({
                'buy_trades': len(buy_trades),
                'sell_trades': len(sell_trades)
            })
        
        def update_system_metrics(self) -> None:
            """æ›´æ–°ç³»ç»ŸæŒ‡æ ‡"""
            # èŽ·å–ç³»ç»Ÿèµ„æºä½¿ç”¨æƒ…å†µ
            cpu_usage = psutil.cpu_percent(interval=1)
            memory_info = psutil.virtual_memory()
            disk_info = psutil.disk_usage('/')
            
            self.metrics_collector.collect_system_metrics(
                cpu_usage=cpu_usage,
                memory_usage=memory_info.percent,
                disk_usage=disk_info.percent,
                model_inference_time=0.1  # è¿™é‡Œéœ€è¦å®žé™…æµ‹é‡æ¨¡åž‹æŽ¨ç†æ—¶é—´
            )
        
        def get_latest_metrics(self) -> Dict[str, Dict[str, Any]]:
            """èŽ·å–æœ€æ–°æŒ‡æ ‡"""
            return self.metrics_collector.export_metrics()
        
        def get_metrics_history(self, limit: int = 100) -> List[Dict[str, Any]]:
            """èŽ·å–æŒ‡æ ‡åŽ†å²"""
            with self._history_lock:
                history_list = list(self._metrics_history)
                return history_list[-limit:] if limit < len(history_list) else history_list
        
        def setup_dashboard(self) -> Dict[str, Any]:
            """è®¾ç½®ç›‘æŽ§ä»ªè¡¨æ¿"""
            if not self.dashboard_manager:
                raise ValueError("æœªé…ç½®Grafanaä»ªè¡¨æ¿ç®¡ç†å™¨")
            
            return self.dashboard_manager.deploy_dashboard()
        
        def _save_metrics_history(self) -> None:
            """ä¿å­˜æŒ‡æ ‡åŽ†å²"""
            current_metrics = self.metrics_collector.export_metrics()
            with self._history_lock:
                self._metrics_history.append(current_metrics)
    ]]></file>
  <file path="src/rl_trading_system/monitoring/alert_system.py"><![CDATA[
    """
    å‘Šè­¦ç³»ç»Ÿæ¨¡å—
    å®žçŽ°DynamicThresholdManagerç±»å’Œé˜ˆå€¼è®¡ç®—ï¼Œå¤šçº§åˆ«å‘Šè­¦å’Œå‘Šè­¦è§„åˆ™é…ç½®ï¼Œå¤šæ¸ é“é€šçŸ¥å’Œå‘Šè­¦èšåˆã€é™é»˜å’Œæ—¥å¿—è®°å½•
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import json
    import smtplib
    import requests
    import time
    import threading
    from datetime import datetime, timedelta
    from email.mime.text import MIMEText
    from email.mime.multipart import MIMEMultipart
    from typing import Dict, List, Optional, Any, Union
    from enum import Enum
    from collections import defaultdict, deque
    from dataclasses import dataclass, asdict
    import pandas as pd
    import numpy as np
    from scipy import stats
    
    
    class AlertLevel(Enum):
        """å‘Šè­¦çº§åˆ«æžšä¸¾"""
        INFO = "info"
        WARNING = "warning"
        ERROR = "error"
        CRITICAL = "critical"
    
    
    class AlertChannel(Enum):
        """å‘Šè­¦æ¸ é“æžšä¸¾"""
        EMAIL = "email"
        WEBHOOK = "webhook"
        SMS = "sms"
    
    
    @dataclass
    class AlertRule:
        """å‘Šè­¦è§„åˆ™ç±»"""
        rule_id: str
        metric_name: str
        threshold_value: float
        comparison_operator: str
        alert_level: AlertLevel
        description: str = ""
        is_active: bool = True
        
        def __post_init__(self):
            """åˆå§‹åŒ–åŽéªŒè¯"""
            if not self.rule_id:
                raise ValueError("è§„åˆ™IDä¸èƒ½ä¸ºç©º")
            
            valid_operators = [">", ">=", "<", "<=", "==", "!="]
            if self.comparison_operator not in valid_operators:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¯”è¾ƒæ“ä½œç¬¦: {self.comparison_operator}")
        
        def evaluate(self, value: float) -> bool:
            """è¯„ä¼°è§„åˆ™æ˜¯å¦è§¦å‘"""
            if not self.is_active:
                return False
            
            if self.comparison_operator == ">":
                return value > self.threshold_value
            elif self.comparison_operator == ">=":
                return value >= self.threshold_value
            elif self.comparison_operator == "<":
                return value < self.threshold_value
            elif self.comparison_operator == "<=":
                return value <= self.threshold_value
            elif self.comparison_operator == "==":
                return abs(value - self.threshold_value) < 1e-10
            elif self.comparison_operator == "!=":
                return abs(value - self.threshold_value) >= 1e-10
            
            return False
        
        def activate(self):
            """æ¿€æ´»è§„åˆ™"""
            self.is_active = True
        
        def deactivate(self):
            """åœç”¨è§„åˆ™"""
            self.is_active = False
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'rule_id': self.rule_id,
                'metric_name': self.metric_name,
                'threshold_value': self.threshold_value,
                'comparison_operator': self.comparison_operator,
                'alert_level': self.alert_level.value,
                'description': self.description,
                'is_active': self.is_active
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'AlertRule':
            """ä»Žå­—å…¸åˆ›å»º"""
            return cls(
                rule_id=data['rule_id'],
                metric_name=data['metric_name'],
                threshold_value=data['threshold_value'],
                comparison_operator=data['comparison_operator'],
                alert_level=AlertLevel(data['alert_level']),
                description=data.get('description', ''),
                is_active=data.get('is_active', True)
            )
    
    
    class DynamicThresholdManager:
        """åŠ¨æ€é˜ˆå€¼ç®¡ç†å™¨"""
        
        def __init__(self, historical_data: pd.DataFrame, lookback_window: int = 60, 
                     update_frequency: str = 'daily'):
            """
            åˆå§‹åŒ–åŠ¨æ€é˜ˆå€¼ç®¡ç†å™¨
            
            Args:
                historical_data: åŽ†å²æ•°æ®
                lookback_window: å›žçœ‹çª—å£å¤§å°
                update_frequency: æ›´æ–°é¢‘çŽ‡
            """
            if historical_data.empty:
                raise ValueError("åŽ†å²æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            if lookback_window <= 0:
                raise ValueError("å›žçœ‹çª—å£å¤§å°å¿…é¡»ä¸ºæ­£æ•°")
            
            self.historical_data = historical_data.copy()
            self.lookback_window = lookback_window
            self.update_frequency = update_frequency
            self.thresholds = {}
            self._lock = threading.Lock()
        
        def calculate_percentile_threshold(self, metric_name: str, percentile: float, 
                                         threshold_type: str = 'upper') -> float:
            """è®¡ç®—åŸºäºŽåˆ†ä½æ•°çš„é˜ˆå€¼"""
            if metric_name not in self.historical_data.columns:
                raise ValueError(f"æŒ‡æ ‡ {metric_name} ä¸å­˜åœ¨äºŽåŽ†å²æ•°æ®ä¸­")
            
            if not (0 < percentile < 100):
                raise ValueError("åˆ†ä½æ•°å¿…é¡»åœ¨0-100ä¹‹é—´")
            
            data = self.historical_data[metric_name]
            
            if threshold_type.lower() == 'upper':
                threshold = data.quantile(percentile / 100.0)
            else:  # lower
                threshold = data.quantile(percentile / 100.0)
            
            return float(threshold)
        
        def calculate_rolling_threshold(self, metric_name: str, window_size: int, 
                                      percentile: float) -> pd.Series:
            """è®¡ç®—æ»šåŠ¨çª—å£é˜ˆå€¼"""
            if metric_name not in self.historical_data.columns:
                raise ValueError(f"æŒ‡æ ‡ {metric_name} ä¸å­˜åœ¨äºŽåŽ†å²æ•°æ®ä¸­")
            
            data = self.historical_data[metric_name]
            rolling_thresholds = data.rolling(window=window_size).quantile(percentile / 100.0)
            
            return rolling_thresholds
        
        def update_threshold_with_new_data(self, metric_name: str, new_data: pd.DataFrame, 
                                         adaptation_factor: float = 0.1) -> float:
            """ä½¿ç”¨æ–°æ•°æ®æ›´æ–°é˜ˆå€¼"""
            if metric_name not in new_data.columns:
                raise ValueError(f"æ–°æ•°æ®ä¸­ä¸åŒ…å«æŒ‡æ ‡ {metric_name}")
            
            with self._lock:
                # æ·»åŠ æ–°æ•°æ®åˆ°åŽ†å²æ•°æ®
                self.historical_data = pd.concat([self.historical_data, new_data], ignore_index=True)
                
                # ä¿æŒæ•°æ®é‡åœ¨åˆç†èŒƒå›´å†…
                if len(self.historical_data) > self.lookback_window * 5:
                    self.historical_data = self.historical_data.tail(self.lookback_window * 3)
                
                # é‡æ–°è®¡ç®—é˜ˆå€¼
                new_threshold = self.calculate_percentile_threshold(metric_name, 90, 'upper')
                
                return new_threshold
        
        def calculate_multiple_thresholds(self, threshold_configs: Dict[str, Dict[str, Any]]) -> Dict[str, float]:
            """æ‰¹é‡è®¡ç®—å¤šä¸ªæŒ‡æ ‡çš„é˜ˆå€¼"""
            thresholds = {}
            
            for metric_name, config in threshold_configs.items():
                if metric_name in self.historical_data.columns:
                    threshold = self.calculate_percentile_threshold(
                        metric_name=metric_name,
                        percentile=config['percentile'],
                        threshold_type=config['type']
                    )
                    thresholds[metric_name] = threshold
            
            return thresholds
        
        def validate_threshold_config(self, config: Dict[str, Any]) -> bool:
            """éªŒè¯é˜ˆå€¼é…ç½®æœ‰æ•ˆæ€§"""
            required_fields = ['metric_name', 'percentile', 'threshold_type']
            
            for field in required_fields:
                if field not in config:
                    return False
            
            # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦å­˜åœ¨
            if config['metric_name'] not in self.historical_data.columns:
                return False
            
            # æ£€æŸ¥åˆ†ä½æ•°èŒƒå›´
            percentile = config['percentile']
            if not (0 < percentile < 100):
                return False
            
            # æ£€æŸ¥é˜ˆå€¼ç±»åž‹
            if config['threshold_type'] not in ['upper', 'lower']:
                return False
            
            return True
        
        def save_thresholds(self, file_path: str):
            """ä¿å­˜é˜ˆå€¼åˆ°æ–‡ä»¶"""
            with self._lock:
                with open(file_path, 'w', encoding='utf-8') as f:
                    json.dump(self.thresholds, f, indent=2, ensure_ascii=False)
        
        def load_thresholds(self, file_path: str):
            """ä»Žæ–‡ä»¶åŠ è½½é˜ˆå€¼"""
            with self._lock:
                with open(file_path, 'r', encoding='utf-8') as f:
                    self.thresholds = json.load(f)
        
        def is_statistical_outlier(self, metric_name: str, value: float, 
                                 method: str = 'zscore', threshold: float = 3.0) -> bool:
            """æ£€æµ‹ç»Ÿè®¡å¼‚å¸¸å€¼"""
            if metric_name not in self.historical_data.columns:
                raise ValueError(f"æŒ‡æ ‡ {metric_name} ä¸å­˜åœ¨")
            
            data = self.historical_data[metric_name]
            
            if method == 'zscore':
                # æ‰‹åŠ¨è®¡ç®—z-score
                mean = data.mean()
                std = data.std()
                if std == 0:
                    return False
                z_score = abs((value - mean) / std)
                return z_score > threshold
            elif method == 'iqr':
                q1 = data.quantile(0.25)
                q3 = data.quantile(0.75)
                iqr = q3 - q1
                lower_bound = q1 - 1.5 * iqr
                upper_bound = q3 + 1.5 * iqr
                return value < lower_bound or value > upper_bound
            
            return False
        
        def analyze_threshold_sensitivity(self, metric_name: str, 
                                        percentiles: List[float]) -> Dict[float, float]:
            """åˆ†æžé˜ˆå€¼æ•æ„Ÿæ€§"""
            sensitivity_results = {}
            
            for percentile in percentiles:
                threshold = self.calculate_percentile_threshold(
                    metric_name=metric_name,
                    percentile=percentile,
                    threshold_type='upper'
                )
                sensitivity_results[percentile] = threshold
            
            return sensitivity_results
    
    
    class AlertAggregator:
        """å‘Šè­¦èšåˆå™¨"""
        
        def __init__(self, aggregation_window: int = 300, max_alerts_per_rule: int = 5,
                     similarity_threshold: float = 0.8):
            """
            åˆå§‹åŒ–å‘Šè­¦èšåˆå™¨
            
            Args:
                aggregation_window: èšåˆæ—¶é—´çª—å£ï¼ˆç§’ï¼‰
                max_alerts_per_rule: æ¯ä¸ªè§„åˆ™æœ€å¤§å‘Šè­¦æ•°
                similarity_threshold: ç›¸ä¼¼åº¦é˜ˆå€¼
            """
            self.aggregation_window = aggregation_window
            self.max_alerts_per_rule = max_alerts_per_rule
            self.similarity_threshold = similarity_threshold
            self.pending_alerts = []
            self._lock = threading.Lock()
        
        def add_alert(self, alert: Dict[str, Any]):
            """æ·»åŠ å‘Šè­¦åˆ°èšåˆé˜Ÿåˆ—"""
            with self._lock:
                alert['timestamp'] = alert.get('timestamp', datetime.now())
                self.pending_alerts.append(alert)
        
        def aggregate_alerts(self) -> List[Dict[str, Any]]:
            """æ‰§è¡Œå‘Šè­¦èšåˆ"""
            with self._lock:
                current_time = datetime.now()
                
                # æ¸…ç†è¿‡æœŸå‘Šè­¦
                self.pending_alerts = [
                    alert for alert in self.pending_alerts
                    if abs((current_time - alert['timestamp']).total_seconds()) < self.aggregation_window
                ]
                
                # æŒ‰è§„åˆ™åˆ†ç»„
                alerts_by_rule = defaultdict(list)
                for alert in self.pending_alerts:
                    alerts_by_rule[alert['rule_id']].append(alert)
                
                aggregated_alerts = []
                
                for rule_id, rule_alerts in alerts_by_rule.items():
                    # åº”ç”¨é¢‘çŽ‡é™åˆ¶
                    if len(rule_alerts) > self.max_alerts_per_rule:
                        rule_alerts = rule_alerts[:self.max_alerts_per_rule]
                    
                    # èšåˆç›¸ä¼¼å‘Šè­¦
                    if len(rule_alerts) > 1:
                        # æ£€æŸ¥æ˜¯å¦å¯ä»¥èšåˆ
                        similar_alerts = self._group_similar_alerts(rule_alerts)
                        for group in similar_alerts:
                            if len(group) > 1:
                                # åˆ›å»ºèšåˆå‘Šè­¦
                                aggregated_alert = group[0].copy()
                                aggregated_alert['count'] = len(group)
                                aggregated_alert['message'] += f" (èšåˆäº†{len(group)}æ¡ç›¸ä¼¼å‘Šè­¦)"
                                aggregated_alerts.append(aggregated_alert)
                            else:
                                aggregated_alerts.extend(group)
                    else:
                        aggregated_alerts.extend(rule_alerts)
                
                # æ¸…ç©ºå·²å¤„ç†çš„å‘Šè­¦
                self.pending_alerts.clear()
                
                return aggregated_alerts
        
        def calculate_similarity(self, alert1: Dict[str, Any], alert2: Dict[str, Any]) -> float:
            """è®¡ç®—ä¸¤ä¸ªå‘Šè­¦çš„ç›¸ä¼¼åº¦"""
            similarity_score = 0.0
            total_weight = 0.0
            
            # è§„åˆ™IDç›¸ä¼¼åº¦æƒé‡: 40%
            if alert1.get('rule_id') == alert2.get('rule_id'):
                similarity_score += 0.4
            total_weight += 0.4
            
            # æŒ‡æ ‡åç§°ç›¸ä¼¼åº¦æƒé‡: 30%
            if alert1.get('metric_name') == alert2.get('metric_name'):
                similarity_score += 0.3
            total_weight += 0.3
            
            # å‘Šè­¦çº§åˆ«ç›¸ä¼¼åº¦æƒé‡: 20%
            if alert1.get('level') == alert2.get('level'):
                similarity_score += 0.2
            total_weight += 0.2
            
            # æ¶ˆæ¯ç›¸ä¼¼åº¦æƒé‡: 10%
            if alert1.get('message') == alert2.get('message'):
                similarity_score += 0.1
            total_weight += 0.1
            
            return similarity_score / total_weight if total_weight > 0 else 0.0
        
        def _group_similar_alerts(self, alerts: List[Dict[str, Any]]) -> List[List[Dict[str, Any]]]:
            """å°†ç›¸ä¼¼çš„å‘Šè­¦åˆ†ç»„"""
            groups = []
            processed = set()
            
            for i, alert1 in enumerate(alerts):
                if i in processed:
                    continue
                
                group = [alert1]
                processed.add(i)
                
                for j, alert2 in enumerate(alerts):
                    if j <= i or j in processed:
                        continue
                    
                    similarity = self.calculate_similarity(alert1, alert2)
                    if similarity >= self.similarity_threshold:
                        group.append(alert2)
                        processed.add(j)
                
                groups.append(group)
            
            return groups
    
    
    class NotificationManager:
        """é€šçŸ¥ç®¡ç†å™¨"""
        
        def __init__(self, notification_config: Dict[str, Dict[str, Any]]):
            """
            åˆå§‹åŒ–é€šçŸ¥ç®¡ç†å™¨
            
            Args:
                notification_config: é€šçŸ¥é…ç½®
            """
            self.channels = notification_config
            self.rate_limits = {}
            self.notification_history = defaultdict(deque)
            self._lock = threading.Lock()
        
        def send_email_notification(self, alert_data: Dict[str, Any], 
                                  max_retries: int = 3, retry_delay: float = 1.0) -> bool:
            """å‘é€é‚®ä»¶é€šçŸ¥"""
            if not self.channels.get('email', {}).get('enabled', False):
                return False
            
            email_config = self.channels['email']
            
            # æ£€æŸ¥é¢‘çŽ‡é™åˆ¶
            if not self._check_rate_limit('email'):
                return False
            
            # æ ¼å¼åŒ–é‚®ä»¶å†…å®¹
            content = self.format_email_content(alert_data)
            
            # åˆ›å»ºé‚®ä»¶
            msg = MIMEMultipart()
            msg['From'] = email_config['username']
            msg['Subject'] = content['subject']
            
            body = MIMEText(content['body'], 'html', 'utf-8')
            msg.attach(body)
            
            # å‘é€é‚®ä»¶
            for attempt in range(max_retries):
                server = smtplib.SMTP(email_config['smtp_server'], email_config['smtp_port'])
                server.starttls()
                server.login(email_config['username'], email_config['password'])
                
                for recipient in email_config['recipients']:
                    msg['To'] = recipient
                    server.send_message(msg)
                    del msg['To']
                
                server.quit()
                
                # è®°å½•å‘é€åŽ†å²
                self._record_notification('email')
                return True
            
            return False
        
        def send_webhook_notification(self, alert_data: Dict[str, Any],
                                    max_retries: int = 3, retry_delay: float = 1.0) -> bool:
            """å‘é€Webhooké€šçŸ¥"""
            if not self.channels.get('webhook', {}).get('enabled', False):
                return False
            
            webhook_config = self.channels['webhook']
            
            # æ£€æŸ¥é¢‘çŽ‡é™åˆ¶
            if not self._check_rate_limit('webhook'):
                return False
            
            # æ ¼å¼åŒ–Webhookå†…å®¹
            content = self.format_webhook_content(alert_data)
            
            # å‘é€Webhook
            for attempt in range(max_retries):
                response = requests.post(
                    webhook_config['url'],
                    json=content,
                    timeout=webhook_config.get('timeout', 10)
                )
                
                if response.status_code == 200:
                    self._record_notification('webhook')
                    return True
                
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
            
            return False
        
        def send_notification(self, alert_data: Dict[str, Any]) -> bool:
            """å‘é€é€šçŸ¥åˆ°æ‰€æœ‰æ¿€æ´»çš„æ¸ é“"""
            success = False
            
            for channel_name in self.get_active_channels():
                if channel_name == 'email':
                    if self.send_email_notification(alert_data):
                        success = True
                elif channel_name == 'webhook':
                    if self.send_webhook_notification(alert_data):
                        success = True
            
            return success
        
        def format_email_content(self, alert_data: Dict[str, Any]) -> Dict[str, str]:
            """æ ¼å¼åŒ–é‚®ä»¶å†…å®¹"""
            subject = f"[{alert_data.get('level', AlertLevel.INFO).value.upper()}] äº¤æ˜“ç³»ç»Ÿå‘Šè­¦ - {alert_data.get('rule_id', '')}"
            
            body = f"""
            <html>
            <body>
            <h2>äº¤æ˜“ç³»ç»Ÿå‘Šè­¦é€šçŸ¥</h2>
            <p><strong>è§„åˆ™ID:</strong> {alert_data.get('rule_id', 'N/A')}</p>
            <p><strong>æŒ‡æ ‡åç§°:</strong> {alert_data.get('metric_name', 'N/A')}</p>
            <p><strong>å½“å‰å€¼:</strong> {alert_data.get('value', 'N/A')}</p>
            <p><strong>é˜ˆå€¼:</strong> {alert_data.get('threshold', 'N/A')}</p>
            <p><strong>å‘Šè­¦çº§åˆ«:</strong> {alert_data.get('level', AlertLevel.INFO).value}</p>
            <p><strong>å‘Šè­¦æ¶ˆæ¯:</strong> {alert_data.get('message', '')}</p>
            <p><strong>æ—¶é—´:</strong> {alert_data.get('timestamp', datetime.now()).strftime('%Y-%m-%d %H:%M:%S')}</p>
            </body>
            </html>
            """
            
            return {'subject': subject, 'body': body}
        
        def format_webhook_content(self, alert_data: Dict[str, Any]) -> Dict[str, Any]:
            """æ ¼å¼åŒ–Webhookå†…å®¹"""
            return {
                'text': f"ðŸš¨ äº¤æ˜“ç³»ç»Ÿå‘Šè­¦",
                'attachments': [
                    {
                        'color': self._get_color_for_level(alert_data.get('level', AlertLevel.INFO)),
                        'fields': [
                            {'title': 'è§„åˆ™ID', 'value': alert_data.get('rule_id', 'N/A'), 'short': True},
                            {'title': 'æŒ‡æ ‡', 'value': alert_data.get('metric_name', 'N/A'), 'short': True},
                            {'title': 'å½“å‰å€¼', 'value': str(alert_data.get('value', 'N/A')), 'short': True},
                            {'title': 'é˜ˆå€¼', 'value': str(alert_data.get('threshold', 'N/A')), 'short': True},
                            {'title': 'æ¶ˆæ¯', 'value': alert_data.get('message', ''), 'short': False}
                        ],
                        'timestamp': alert_data.get('timestamp', datetime.now()).isoformat()
                    }
                ]
            }
        
        def _get_color_for_level(self, level: AlertLevel) -> str:
            """èŽ·å–å‘Šè­¦çº§åˆ«å¯¹åº”çš„é¢œè‰²"""
            color_map = {
                AlertLevel.INFO: 'good',
                AlertLevel.WARNING: 'warning',
                AlertLevel.ERROR: 'danger',
                AlertLevel.CRITICAL: 'danger'
            }
            return color_map.get(level, 'good')
        
        def enable_channel(self, channel_name: str):
            """å¯ç”¨é€šçŸ¥æ¸ é“"""
            if channel_name in self.channels:
                self.channels[channel_name]['enabled'] = True
        
        def disable_channel(self, channel_name: str):
            """ç¦ç”¨é€šçŸ¥æ¸ é“"""
            if channel_name in self.channels:
                self.channels[channel_name]['enabled'] = False
        
        def get_active_channels(self) -> List[str]:
            """èŽ·å–æ´»è·ƒçš„é€šçŸ¥æ¸ é“"""
            return [
                name for name, config in self.channels.items()
                if config.get('enabled', False)
            ]
        
        def set_rate_limit(self, channel: str, max_notifications: int, time_window: int):
            """è®¾ç½®é¢‘çŽ‡é™åˆ¶"""
            self.rate_limits[channel] = {
                'max_notifications': max_notifications,
                'time_window': time_window
            }
        
        def _check_rate_limit(self, channel: str) -> bool:
            """æ£€æŸ¥é¢‘çŽ‡é™åˆ¶"""
            if channel not in self.rate_limits:
                return True
            
            limit_config = self.rate_limits[channel]
            current_time = datetime.now()
            
            with self._lock:
                # æ¸…ç†è¿‡æœŸè®°å½•
                cutoff_time = current_time - timedelta(seconds=limit_config['time_window'])
                history = self.notification_history[channel]
                
                while history and history[0] < cutoff_time:
                    history.popleft()
                
                # æ£€æŸ¥æ˜¯å¦è¶…è¿‡é™åˆ¶
                if len(history) >= limit_config['max_notifications']:
                    return False
                
                return True
        
        def _record_notification(self, channel: str):
            """è®°å½•é€šçŸ¥å‘é€"""
            with self._lock:
                self.notification_history[channel].append(datetime.now())
    
    
    class AlertLogger:
        """å‘Šè­¦æ—¥å¿—è®°å½•å™¨"""
        
        def __init__(self, log_file: str = None):
            """åˆå§‹åŒ–å‘Šè­¦æ—¥å¿—è®°å½•å™¨"""
            self.log_file = log_file
            self.logs = []
            self._lock = threading.Lock()
        
        def log_alert(self, alert: Dict[str, Any]):
            """è®°å½•å‘Šè­¦"""
            log_entry = {
                'timestamp': datetime.now().isoformat(),
                'rule_id': alert.get('rule_id'),
                'metric_name': alert.get('metric_name'),
                'value': alert.get('value'),
                'threshold': alert.get('threshold'),
                'level': alert.get('level', AlertLevel.INFO).value,
                'message': alert.get('message'),
                'status': 'triggered'
            }
            
            with self._lock:
                self.logs.append(log_entry)
                
                if self.log_file:
                    with open(self.log_file, 'a', encoding='utf-8') as f:
                        f.write(json.dumps(log_entry, ensure_ascii=False) + '\n')
        
        def get_logs(self, limit: int = 100) -> List[Dict[str, Any]]:
            """èŽ·å–å‘Šè­¦æ—¥å¿—"""
            with self._lock:
                return self.logs[-limit:] if limit < len(self.logs) else self.logs.copy()
    
    
    class AlertSystem:
        """å®Œæ•´çš„å‘Šè­¦ç³»ç»Ÿ"""
        
        def __init__(self, historical_data: pd.DataFrame, 
                     notification_config: Dict[str, Dict[str, Any]]):
            """
            åˆå§‹åŒ–å‘Šè­¦ç³»ç»Ÿ
            
            Args:
                historical_data: åŽ†å²æ•°æ®
                notification_config: é€šçŸ¥é…ç½®
            """
            self.threshold_manager = DynamicThresholdManager(historical_data)
            self.notification_manager = NotificationManager(notification_config)
            self.aggregator = AlertAggregator()
            self.logger = AlertLogger()
            
            self.rules = {}
            self.silenced_rules = {}
            self._lock = threading.Lock()
        
        def add_rule(self, rule: AlertRule):
            """æ·»åŠ å‘Šè­¦è§„åˆ™"""
            with self._lock:
                self.rules[rule.rule_id] = rule
        
        def remove_rule(self, rule_id: str):
            """åˆ é™¤å‘Šè­¦è§„åˆ™"""
            with self._lock:
                if rule_id in self.rules:
                    del self.rules[rule_id]
        
        def get_rule(self, rule_id: str) -> Optional[AlertRule]:
            """èŽ·å–å‘Šè­¦è§„åˆ™"""
            return self.rules.get(rule_id)
        
        def check_metrics(self, metrics: Dict[str, float]) -> List[Dict[str, Any]]:
            """æ£€æŸ¥æŒ‡æ ‡å¹¶ç”Ÿæˆå‘Šè­¦"""
            alerts = []
            
            with self._lock:
                for rule_id, rule in self.rules.items():
                    if not rule.is_active:
                        continue
                    
                    if rule_id in self.silenced_rules:
                        silence_end = self.silenced_rules[rule_id]
                        if datetime.now() < silence_end:
                            continue
                        else:
                            # é™é»˜æœŸç»“æŸï¼Œåˆ é™¤é™é»˜è®°å½•
                            del self.silenced_rules[rule_id]
                    
                    if rule.metric_name in metrics:
                        value = metrics[rule.metric_name]
                        
                        if rule.evaluate(value):
                            alert = {
                                'rule_id': rule_id,
                                'metric_name': rule.metric_name,
                                'value': value,
                                'threshold': rule.threshold_value,
                                'level': rule.alert_level,
                                'message': rule.description or f"{rule.metric_name} è§¦å‘å‘Šè­¦é˜ˆå€¼",
                                'timestamp': datetime.now()
                            }
                            alerts.append(alert)
            
            return alerts
        
        def process_metrics(self, metrics: Dict[str, float]):
            """å¤„ç†æŒ‡æ ‡ï¼ˆæ£€æŸ¥ã€èšåˆã€é€šçŸ¥ï¼‰"""
            # æ£€æŸ¥å‘Šè­¦
            alerts = self.check_metrics(metrics)
            
            # æ·»åŠ åˆ°èšåˆå™¨
            for alert in alerts:
                self.aggregator.add_alert(alert)
            
            # æ‰§è¡Œèšåˆ
            aggregated_alerts = self.aggregator.aggregate_alerts()
            
            # å‘é€é€šçŸ¥å’Œè®°å½•æ—¥å¿—
            for alert in aggregated_alerts:
                self.notification_manager.send_notification(alert)
                self.logger.log_alert(alert)
        
        def silence_rule(self, rule_id: str, duration: int):
            """é™é»˜æŒ‡å®šè§„åˆ™"""
            with self._lock:
                silence_end = datetime.now() + timedelta(seconds=duration)
                self.silenced_rules[rule_id] = silence_end
    ]]></file>
  <file path="src/rl_trading_system/monitoring/__init__.py"><![CDATA[
    """ç›‘æŽ§å‘Šè­¦æ¨¡å—"""
    
    from .trading_system_monitor import TradingSystemMonitor, MetricsCollector, PrometheusExporter, GrafanaDashboardManager
    from .alert_system import (
        DynamicThresholdManager,
        AlertRule,
        AlertLevel,
        AlertChannel,
        AlertAggregator,
        AlertLogger,
        AlertSystem,
        NotificationManager
    )
    
    __all__ = [
        "TradingSystemMonitor",
        "MetricsCollector", 
        "PrometheusExporter",
        "GrafanaDashboardManager",
        "DynamicThresholdManager",
        "AlertRule",
        "AlertLevel",
        "AlertChannel",
        "AlertAggregator",
        "AlertLogger",
        "AlertSystem",
        "NotificationManager"
    ]
    ]]></file>
  <file path="src/rl_trading_system/models/transformer.py"><![CDATA[
    """
    æ—¶åºTransformerç¼–ç å™¨å®žçŽ°
    åŒ…æ‹¬å®Œæ•´çš„Transformeræž¶æž„ï¼Œç”¨äºŽå¤„ç†é‡‘èžæ—¶åºæ•°æ®
    """
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import math
    from dataclasses import dataclass
    from typing import Optional, Tuple
    
    from .positional_encoding import PositionalEncoding
    from .temporal_attention import MultiHeadTemporalAttention
    
    
    @dataclass
    class TransformerConfig:
        """Transformeré…ç½®ç±»"""
        d_model: int = 256          # æ¨¡åž‹ç»´åº¦
        n_heads: int = 8            # æ³¨æ„åŠ›å¤´æ•°
        n_layers: int = 6           # ç¼–ç å™¨å±‚æ•°
        d_ff: int = 1024           # å‰é¦ˆç½‘ç»œç»´åº¦
        dropout: float = 0.1        # dropoutæ¦‚çŽ‡
        max_seq_len: int = 252      # æœ€å¤§åºåˆ—é•¿åº¦ï¼ˆä¸€å¹´äº¤æ˜“æ—¥ï¼‰
        n_features: int = 50        # è¾“å…¥ç‰¹å¾æ•°
        activation: str = 'gelu'    # æ¿€æ´»å‡½æ•°ç±»åž‹
        
        def __post_init__(self):
            """é…ç½®éªŒè¯"""
            assert self.d_model % self.n_heads == 0, "d_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤"
            assert self.activation in ['relu', 'gelu', 'swish'], f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {self.activation}"
    
    
    class FeedForwardNetwork(nn.Module):
        """
        å‰é¦ˆç½‘ç»œ
        å®žçŽ°Transformerä¸­çš„ä½ç½®å‰é¦ˆç½‘ç»œ
        """
        
        def __init__(self, d_model: int, d_ff: int, dropout: float = 0.1, activation: str = 'gelu'):
            """
            åˆå§‹åŒ–å‰é¦ˆç½‘ç»œ
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                d_ff: å‰é¦ˆç½‘ç»œéšè—å±‚ç»´åº¦
                dropout: dropoutæ¦‚çŽ‡
                activation: æ¿€æ´»å‡½æ•°ç±»åž‹
            """
            super().__init__()
            self.linear1 = nn.Linear(d_model, d_ff)
            self.linear2 = nn.Linear(d_ff, d_model)
            self.dropout = nn.Dropout(dropout)
            
            # é€‰æ‹©æ¿€æ´»å‡½æ•°
            if activation == 'relu':
                self.activation = nn.ReLU()
            elif activation == 'gelu':
                self.activation = nn.GELU()
            elif activation == 'swish':
                self.activation = lambda x: x * torch.sigmoid(x)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {activation}")
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                
            Returns:
                è¾“å‡ºå¼ é‡ [batch_size, seq_len, d_model]
            """
            x = self.linear1(x)
            x = self.activation(x)
            x = self.dropout(x)
            x = self.linear2(x)
            return x
    
    
    class MultiHeadAttention(nn.Module):
        """
        å¤šå¤´è‡ªæ³¨æ„åŠ›æœºåˆ¶
        æ ‡å‡†çš„Transformerå¤šå¤´æ³¨æ„åŠ›å®žçŽ°
        """
        
        def __init__(self, d_model: int, n_heads: int, dropout: float = 0.1):
            """
            åˆå§‹åŒ–å¤šå¤´æ³¨æ„åŠ›
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                n_heads: æ³¨æ„åŠ›å¤´æ•°
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            assert d_model % n_heads == 0
            
            self.d_model = d_model
            self.n_heads = n_heads
            self.d_k = d_model // n_heads
            
            # çº¿æ€§å˜æ¢å±‚
            self.w_q = nn.Linear(d_model, d_model)
            self.w_k = nn.Linear(d_model, d_model)
            self.w_v = nn.Linear(d_model, d_model)
            self.w_o = nn.Linear(d_model, d_model)
            
            self.dropout = nn.Dropout(dropout)
        
        def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æ³¨æ„åŠ›æŽ©ç  [batch_size, seq_len, seq_len]ï¼Œå¯é€‰
                
            Returns:
                è¾“å‡ºå¼ é‡ [batch_size, seq_len, d_model]
            """
            batch_size, seq_len, d_model = x.shape
            
            # çº¿æ€§å˜æ¢
            Q = self.w_q(x)  # [batch_size, seq_len, d_model]
            K = self.w_k(x)  # [batch_size, seq_len, d_model]
            V = self.w_v(x)  # [batch_size, seq_len, d_model]
            
            # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼
            Q = Q.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_k]
            K = K.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_k]
            V = V.view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)  # [batch_size, n_heads, seq_len, d_k]
            
            # è®¡ç®—æ³¨æ„åŠ›
            attention_output = self._scaled_dot_product_attention(Q, K, V, mask)
            
            # é‡å¡‘å›žåŽŸå§‹å½¢çŠ¶
            attention_output = attention_output.transpose(1, 2).contiguous().view(
                batch_size, seq_len, d_model
            )
            
            # è¾“å‡ºæŠ•å½±
            output = self.w_o(attention_output)
            
            return output
        
        def _scaled_dot_product_attention(self, Q: torch.Tensor, K: torch.Tensor, V: torch.Tensor,
                                        mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
            
            Args:
                Q: æŸ¥è¯¢å¼ é‡ [batch_size, n_heads, seq_len, d_k]
                K: é”®å¼ é‡ [batch_size, n_heads, seq_len, d_k]
                V: å€¼å¼ é‡ [batch_size, n_heads, seq_len, d_k]
                mask: æŽ©ç å¼ é‡ï¼Œå¯é€‰
                
            Returns:
                æ³¨æ„åŠ›è¾“å‡º [batch_size, n_heads, seq_len, d_k]
            """
            d_k = Q.size(-1)
            
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)
            
            # åº”ç”¨æŽ©ç 
            if mask is not None:
                # æ‰©å±•æŽ©ç ç»´åº¦ä»¥åŒ¹é…å¤šå¤´
                if mask.dim() == 3:  # [batch_size, seq_len, seq_len]
                    mask = mask.unsqueeze(1)  # [batch_size, 1, seq_len, seq_len]
                scores = scores + mask
            
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attention_weights = F.softmax(scores, dim=-1)
            attention_weights = self.dropout(attention_weights)
            
            # è®¡ç®—è¾“å‡º
            output = torch.matmul(attention_weights, V)
            
            return output
    
    
    class TransformerEncoderLayer(nn.Module):
        """
        Transformerç¼–ç å™¨å±‚
        åŒ…å«å¤šå¤´è‡ªæ³¨æ„åŠ›å’Œå‰é¦ˆç½‘ç»œï¼Œä»¥åŠæ®‹å·®è¿žæŽ¥å’Œå±‚å½’ä¸€åŒ–
        """
        
        def __init__(self, config: TransformerConfig):
            """
            åˆå§‹åŒ–ç¼–ç å™¨å±‚
            
            Args:
                config: Transformeré…ç½®
            """
            super().__init__()
            self.config = config
            
            # å¤šå¤´è‡ªæ³¨æ„åŠ›
            self.self_attention = MultiHeadAttention(
                config.d_model, config.n_heads, config.dropout
            )
            
            # å‰é¦ˆç½‘ç»œ
            self.feed_forward = FeedForwardNetwork(
                config.d_model, config.d_ff, config.dropout, config.activation
            )
            
            # å±‚å½’ä¸€åŒ–
            self.norm1 = nn.LayerNorm(config.d_model)
            self.norm2 = nn.LayerNorm(config.d_model)
            
            # Dropout
            self.dropout1 = nn.Dropout(config.dropout)
            self.dropout2 = nn.Dropout(config.dropout)
        
        def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æ³¨æ„åŠ›æŽ©ç ï¼Œå¯é€‰
                
            Returns:
                è¾“å‡ºå¼ é‡ [batch_size, seq_len, d_model]
            """
            # å¤šå¤´è‡ªæ³¨æ„åŠ› + æ®‹å·®è¿žæŽ¥ + å±‚å½’ä¸€åŒ–
            attn_output = self.self_attention(x, mask)
            x = self.norm1(x + self.dropout1(attn_output))
            
            # å‰é¦ˆç½‘ç»œ + æ®‹å·®è¿žæŽ¥ + å±‚å½’ä¸€åŒ–
            ff_output = self.feed_forward(x)
            x = self.norm2(x + self.dropout2(ff_output))
            
            return x
    
    
    class TimeSeriesTransformer(nn.Module):
        """
        æ—¶åºTransformerç¼–ç å™¨
        ä¸“é—¨ç”¨äºŽå¤„ç†é‡‘èžæ—¶åºæ•°æ®çš„Transformeræž¶æž„
        """
        
        def __init__(self, config: TransformerConfig):
            """
            åˆå§‹åŒ–æ—¶åºTransformer
            
            Args:
                config: Transformeré…ç½®
            """
            super().__init__()
            self.config = config
            
            # è¾“å…¥æŠ•å½±å±‚
            self.input_projection = nn.Linear(config.n_features, config.d_model)
            
            # ä½ç½®ç¼–ç 
            self.pos_encoding = PositionalEncoding(
                config.d_model, config.max_seq_len, config.dropout
            )
            
            # Transformerç¼–ç å™¨å±‚
            self.encoder_layers = nn.ModuleList([
                TransformerEncoderLayer(config) for _ in range(config.n_layers)
            ])
            
            # æ—¶é—´æ³¨æ„åŠ›èšåˆ
            self.temporal_attention = MultiHeadTemporalAttention(
                config.d_model, config.n_heads, config.dropout
            )
            
            # è¾“å‡ºæŠ•å½±å±‚
            self.output_projection = nn.Linear(config.d_model, config.d_model)
            
            # åˆå§‹åŒ–å‚æ•°
            self._init_parameters()
        
        def _init_parameters(self):
            """åˆå§‹åŒ–æ¨¡åž‹å‚æ•°"""
            for p in self.parameters():
                if p.dim() > 1:
                    nn.init.xavier_uniform_(p)
        
        def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, n_stocks, n_features]
                mask: åºåˆ—æŽ©ç  [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                è¾“å‡ºå¼ é‡ [batch_size, n_stocks, d_model]
            """
            batch_size, seq_len, n_stocks, n_features = x.shape
            
            # æ£€æŸ¥åºåˆ—é•¿åº¦
            if seq_len > self.config.max_seq_len:
                raise IndexError(f"åºåˆ—é•¿åº¦ {seq_len} è¶…è¿‡æœ€å¤§é•¿åº¦ {self.config.max_seq_len}")
            
            # é‡å¡‘è¾“å…¥ï¼š[batch_size * n_stocks, seq_len, n_features]
            x = x.view(batch_size * n_stocks, seq_len, n_features)
            
            # è¾“å…¥æŠ•å½±
            x = self.input_projection(x)  # [batch_size * n_stocks, seq_len, d_model]
            
            # ä½ç½®ç¼–ç 
            x = self.pos_encoding(x)
            
            # å¤„ç†æŽ©ç 
            attention_mask = None
            if mask is not None:
                # æ‰©å±•æŽ©ç ä»¥åŒ¹é…é‡å¡‘åŽçš„æ‰¹æ¬¡å¤§å°
                attention_mask = mask.repeat_interleave(n_stocks, dim=0)  # [batch_size * n_stocks, seq_len]
                # è½¬æ¢ä¸ºæ³¨æ„åŠ›æŽ©ç æ ¼å¼
                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)  # [batch_size * n_stocks, 1, 1, seq_len]
                attention_mask = attention_mask.expand(-1, 1, seq_len, -1)  # [batch_size * n_stocks, 1, seq_len, seq_len]
                attention_mask = attention_mask.squeeze(1)  # [batch_size * n_stocks, seq_len, seq_len]
            
            # é€šè¿‡ç¼–ç å™¨å±‚
            for encoder_layer in self.encoder_layers:
                x = encoder_layer(x, attention_mask)
            
            # é‡å¡‘å›žè‚¡ç¥¨ç»´åº¦ï¼š[batch_size, n_stocks, seq_len, d_model]
            x = x.view(batch_size, n_stocks, seq_len, self.config.d_model)
            
            # å¯¹æ¯åªè‚¡ç¥¨åº”ç”¨æ—¶é—´æ³¨æ„åŠ›èšåˆ
            stock_representations = []
            for i in range(n_stocks):
                stock_seq = x[:, i, :, :]  # [batch_size, seq_len, d_model]
                stock_mask = mask if mask is not None else None
                stock_repr = self.temporal_attention(stock_seq, stock_mask)  # [batch_size, d_model]
                stock_representations.append(stock_repr)
            
            # å †å æ‰€æœ‰è‚¡ç¥¨çš„è¡¨ç¤º
            output = torch.stack(stock_representations, dim=1)  # [batch_size, n_stocks, d_model]
            
            # è¾“å‡ºæŠ•å½±
            output = self.output_projection(output)
            
            return output
        
        def get_attention_weights(self, x: torch.Tensor, 
                                mask: Optional[torch.Tensor] = None) -> dict:
            """
            èŽ·å–æ³¨æ„åŠ›æƒé‡ç”¨äºŽå¯è§†åŒ–
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, n_stocks, n_features]
                mask: åºåˆ—æŽ©ç ï¼Œå¯é€‰
                
            Returns:
                åŒ…å«æ³¨æ„åŠ›æƒé‡çš„å­—å…¸
            """
            batch_size, seq_len, n_stocks, n_features = x.shape
            
            # é‡å¡‘è¾“å…¥
            x = x.view(batch_size * n_stocks, seq_len, n_features)
            
            # è¾“å…¥æŠ•å½±å’Œä½ç½®ç¼–ç 
            x = self.input_projection(x)
            x = self.pos_encoding(x)
            
            # æ”¶é›†æ¯å±‚çš„æ³¨æ„åŠ›æƒé‡
            layer_attentions = []
            
            # å¤„ç†æŽ©ç 
            attention_mask = None
            if mask is not None:
                attention_mask = mask.repeat_interleave(n_stocks, dim=0)
                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)
                attention_mask = attention_mask.expand(-1, 1, seq_len, -1)
                attention_mask = attention_mask.squeeze(1)
            
            # é€šè¿‡ç¼–ç å™¨å±‚ï¼ˆè¿™é‡Œç®€åŒ–ï¼Œå®žé™…éœ€è¦ä¿®æ”¹ç¼–ç å™¨å±‚ä»¥è¿”å›žæ³¨æ„åŠ›æƒé‡ï¼‰
            for encoder_layer in self.encoder_layers:
                x = encoder_layer(x, attention_mask)
            
            # é‡å¡‘å¹¶èŽ·å–æ—¶é—´æ³¨æ„åŠ›æƒé‡
            x = x.view(batch_size, n_stocks, seq_len, self.config.d_model)
            
            temporal_attentions = []
            for i in range(n_stocks):
                stock_seq = x[:, i, :, :]
                stock_mask = mask if mask is not None else None
                _, head_attentions = self.temporal_attention.forward_with_attention_weights(
                    stock_seq, stock_mask
                )
                temporal_attentions.append(head_attentions)
            
            return {
                'temporal_attentions': temporal_attentions,
                'n_stocks': n_stocks,
                'seq_len': seq_len
            }
        
        def encode_sequence(self, x: torch.Tensor, 
                           mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            ä»…ç¼–ç åºåˆ—ï¼Œä¸è¿›è¡Œæ—¶é—´èšåˆ
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, n_stocks, n_features]
                mask: åºåˆ—æŽ©ç ï¼Œå¯é€‰
                
            Returns:
                ç¼–ç åŽçš„åºåˆ— [batch_size, seq_len, n_stocks, d_model]
            """
            batch_size, seq_len, n_stocks, n_features = x.shape
            
            # é‡å¡‘è¾“å…¥
            x = x.view(batch_size * n_stocks, seq_len, n_features)
            
            # è¾“å…¥æŠ•å½±å’Œä½ç½®ç¼–ç 
            x = self.input_projection(x)
            x = self.pos_encoding(x)
            
            # å¤„ç†æŽ©ç 
            attention_mask = None
            if mask is not None:
                attention_mask = mask.repeat_interleave(n_stocks, dim=0)
                attention_mask = attention_mask.unsqueeze(1).unsqueeze(1)
                attention_mask = attention_mask.expand(-1, 1, seq_len, -1)
                attention_mask = attention_mask.squeeze(1)
            
            # é€šè¿‡ç¼–ç å™¨å±‚
            for encoder_layer in self.encoder_layers:
                x = encoder_layer(x, attention_mask)
            
            # é‡å¡‘å›žåŽŸå§‹å½¢çŠ¶
            x = x.view(batch_size, seq_len, n_stocks, self.config.d_model)
            
            return x
        
        def get_model_size(self) -> dict:
            """
            èŽ·å–æ¨¡åž‹å¤§å°ä¿¡æ¯
            
            Returns:
                åŒ…å«æ¨¡åž‹å¤§å°ä¿¡æ¯çš„å­—å…¸
            """
            total_params = sum(p.numel() for p in self.parameters())
            trainable_params = sum(p.numel() for p in self.parameters() if p.requires_grad)
            
            return {
                'total_parameters': total_params,
                'trainable_parameters': trainable_params,
                'model_size_mb': total_params * 4 / (1024 * 1024),  # å‡è®¾float32
                'config': self.config
            }
    ]]></file>
  <file path="src/rl_trading_system/models/temporal_attention.py"><![CDATA[
    """
    æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶å®žçŽ°
    åŒ…æ‹¬ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›ã€æ—¶é—´æ³¨æ„åŠ›èšåˆå’Œå¤šå¤´æ—¶é—´æ³¨æ„åŠ›
    """
    
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import math
    from typing import Tuple, Optional, List
    
    
    class ScaledDotProductAttention(nn.Module):
        """
        ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›æœºåˆ¶
        Attention(Q,K,V) = softmax(QK^T/âˆšd_k)V
        """
        
        def __init__(self, dropout: float = 0.1):
            """
            åˆå§‹åŒ–ç¼©æ”¾ç‚¹ç§¯æ³¨æ„åŠ›
            
            Args:
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            self.dropout = nn.Dropout(p=dropout)
        
        def forward(self, query: torch.Tensor, key: torch.Tensor, value: torch.Tensor,
                    mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                query: æŸ¥è¯¢å¼ é‡ [batch_size, seq_len, d_k]
                key: é”®å¼ é‡ [batch_size, seq_len, d_k]
                value: å€¼å¼ é‡ [batch_size, seq_len, d_k]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len, seq_len]ï¼Œå¯é€‰
                
            Returns:
                output: æ³¨æ„åŠ›è¾“å‡º [batch_size, seq_len, d_k]
                attention_weights: æ³¨æ„åŠ›æƒé‡ [batch_size, seq_len, seq_len]
            """
            d_k = query.size(-1)
            
            # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°
            scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)
            
            # åº”ç”¨æŽ©ç 
            if mask is not None:
                scores = scores + mask
            
            # è®¡ç®—æ³¨æ„åŠ›æƒé‡
            attention_weights = F.softmax(scores, dim=-1)
            attention_weights = self.dropout(attention_weights)
            
            # è®¡ç®—è¾“å‡º
            output = torch.matmul(attention_weights, value)
            
            return output, attention_weights
    
    
    class TemporalAttention(nn.Module):
        """
        æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
        ç”¨äºŽå°†æ—¶åºç‰¹å¾èšåˆä¸ºå›ºå®šç»´åº¦çš„è¡¨ç¤º
        """
        
        def __init__(self, d_model: int, dropout: float = 0.1):
            """
            åˆå§‹åŒ–æ—¶é—´æ³¨æ„åŠ›
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            self.d_model = d_model
            
            # çº¿æ€§å˜æ¢å±‚
            self.w_q = nn.Linear(d_model, d_model)
            self.w_k = nn.Linear(d_model, d_model)
            self.w_v = nn.Linear(d_model, d_model)
            self.w_o = nn.Linear(d_model, d_model)
            
            # æ³¨æ„åŠ›æœºåˆ¶
            self.attention = ScaledDotProductAttention(dropout)
            
            # ç”¨äºŽæ—¶é—´èšåˆçš„æŸ¥è¯¢å‘é‡
            self.temporal_query = nn.Parameter(torch.randn(1, 1, d_model) * 0.1)
            
            self.dropout = nn.Dropout(p=dropout)
            self.layer_norm = nn.LayerNorm(d_model)
        
        def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                output: èšåˆåŽçš„è¾“å‡º [batch_size, d_model]
            """
            batch_size, seq_len, d_model = x.shape
            
            # çº¿æ€§å˜æ¢
            k = self.w_k(x)  # [batch_size, seq_len, d_model]
            v = self.w_v(x)  # [batch_size, seq_len, d_model]
            
            # æ‰©å±•æ—¶é—´æŸ¥è¯¢å‘é‡
            q = self.temporal_query.expand(batch_size, 1, d_model)  # [batch_size, 1, d_model]
            q = self.w_q(q)
            
            # å¤„ç†æŽ©ç 
            attention_mask = None
            if mask is not None:
                # å°†1DæŽ©ç è½¬æ¢ä¸º2Dæ³¨æ„åŠ›æŽ©ç 
                attention_mask = mask.unsqueeze(1)  # [batch_size, 1, seq_len]
                attention_mask = attention_mask.expand(batch_size, 1, seq_len)
            
            # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
            attended, attention_weights = self.attention(q, k, v, attention_mask)
            
            # è¾“å‡ºæŠ•å½±
            output = self.w_o(attended)  # [batch_size, 1, d_model]
            output = output.squeeze(1)   # [batch_size, d_model]
            
            # æ®‹å·®è¿žæŽ¥å’Œå±‚å½’ä¸€åŒ–ï¼ˆä½¿ç”¨æ—¶é—´æŸ¥è¯¢å‘é‡ï¼‰
            query_squeezed = q.squeeze(1)  # [batch_size, d_model]
            output = self.layer_norm(output + query_squeezed)
            
            return self.dropout(output)
        
        def forward_with_attention(self, x: torch.Tensor, 
                                 mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            å‰å‘ä¼ æ’­å¹¶è¿”å›žæ³¨æ„åŠ›æƒé‡
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                output: èšåˆåŽçš„è¾“å‡º [batch_size, d_model]
                attention_weights: æ³¨æ„åŠ›æƒé‡ [batch_size, seq_len]
            """
            batch_size, seq_len, d_model = x.shape
            
            # çº¿æ€§å˜æ¢
            k = self.w_k(x)
            v = self.w_v(x)
            
            # æ‰©å±•æ—¶é—´æŸ¥è¯¢å‘é‡
            q = self.temporal_query.expand(batch_size, 1, d_model)
            q = self.w_q(q)
            
            # å¤„ç†æŽ©ç 
            attention_mask = None
            if mask is not None:
                attention_mask = mask.unsqueeze(1)
                attention_mask = attention_mask.expand(batch_size, 1, seq_len)
            
            # åº”ç”¨æ³¨æ„åŠ›æœºåˆ¶
            attended, attention_weights = self.attention(q, k, v, attention_mask)
            
            # è¾“å‡ºæŠ•å½±
            output = self.w_o(attended)
            output = output.squeeze(1)
            
            # æ®‹å·®è¿žæŽ¥å’Œå±‚å½’ä¸€åŒ–
            query_squeezed = q.squeeze(1)
            output = self.layer_norm(output + query_squeezed)
            output = self.dropout(output)
            
            # åŽ‹ç¼©æ³¨æ„åŠ›æƒé‡ç»´åº¦
            attention_weights = attention_weights.squeeze(1)  # [batch_size, seq_len]
            
            return output, attention_weights
    
    
    class MultiHeadTemporalAttention(nn.Module):
        """
        å¤šå¤´æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
        ä½¿ç”¨å¤šä¸ªæ³¨æ„åŠ›å¤´æ¥æ•æ‰ä¸åŒçš„æ—¶é—´æ¨¡å¼
        """
        
        def __init__(self, d_model: int, n_heads: int = 8, dropout: float = 0.1):
            """
            åˆå§‹åŒ–å¤šå¤´æ—¶é—´æ³¨æ„åŠ›
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                n_heads: æ³¨æ„åŠ›å¤´æ•°
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            assert d_model % n_heads == 0, "d_modelå¿…é¡»èƒ½è¢«n_headsæ•´é™¤"
            
            self.d_model = d_model
            self.n_heads = n_heads
            self.d_k = d_model // n_heads
            
            # åˆ›å»ºå¤šä¸ªæ³¨æ„åŠ›å¤´
            self.heads = nn.ModuleList([
                TemporalAttention(self.d_k, dropout) for _ in range(n_heads)
            ])
            
            # è¾“å…¥æŠ•å½±å±‚
            self.input_projection = nn.Linear(d_model, d_model)
            
            # è¾“å‡ºæŠ•å½±å±‚
            self.output_projection = nn.Linear(d_model, d_model)
            
            self.dropout = nn.Dropout(p=dropout)
            self.layer_norm = nn.LayerNorm(d_model)
        
        def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                output: èšåˆåŽçš„è¾“å‡º [batch_size, d_model]
            """
            batch_size, seq_len, d_model = x.shape
            
            # è¾“å…¥æŠ•å½±
            x_proj = self.input_projection(x)
            
            # åˆ†å‰²ä¸ºå¤šä¸ªå¤´
            x_heads = x_proj.view(batch_size, seq_len, self.n_heads, self.d_k)
            x_heads = x_heads.transpose(1, 2)  # [batch_size, n_heads, seq_len, d_k]
            
            # å¯¹æ¯ä¸ªå¤´åº”ç”¨æ—¶é—´æ³¨æ„åŠ›
            head_outputs = []
            for i in range(self.n_heads):
                head_input = x_heads[:, i, :, :]  # [batch_size, seq_len, d_k]
                head_output = self.heads[i](head_input, mask)  # [batch_size, d_k]
                head_outputs.append(head_output)
            
            # æ‹¼æŽ¥æ‰€æœ‰å¤´çš„è¾“å‡º
            concatenated = torch.cat(head_outputs, dim=-1)  # [batch_size, d_model]
            
            # è¾“å‡ºæŠ•å½±
            output = self.output_projection(concatenated)
            
            # æ®‹å·®è¿žæŽ¥å’Œå±‚å½’ä¸€åŒ–
            # ä½¿ç”¨è¾“å…¥çš„æ—¶é—´å¹³å‡ä½œä¸ºæ®‹å·®
            residual = x.mean(dim=1)  # [batch_size, d_model]
            output = self.layer_norm(output + residual)
            
            return self.dropout(output)
        
        def forward_with_heads(self, x: torch.Tensor, 
                              mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:
            """
            å‰å‘ä¼ æ’­å¹¶è¿”å›žå„ä¸ªå¤´çš„è¾“å‡º
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                output: æœ€ç»ˆè¾“å‡º [batch_size, d_model]
                head_outputs: å„ä¸ªå¤´çš„è¾“å‡ºåˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [batch_size, d_k]
            """
            batch_size, seq_len, d_model = x.shape
            
            # è¾“å…¥æŠ•å½±
            x_proj = self.input_projection(x)
            
            # åˆ†å‰²ä¸ºå¤šä¸ªå¤´
            x_heads = x_proj.view(batch_size, seq_len, self.n_heads, self.d_k)
            x_heads = x_heads.transpose(1, 2)
            
            # å¯¹æ¯ä¸ªå¤´åº”ç”¨æ—¶é—´æ³¨æ„åŠ›
            head_outputs = []
            for i in range(self.n_heads):
                head_input = x_heads[:, i, :, :]
                head_output = self.heads[i](head_input, mask)
                head_outputs.append(head_output)
            
            # æ‹¼æŽ¥æ‰€æœ‰å¤´çš„è¾“å‡º
            concatenated = torch.cat(head_outputs, dim=-1)
            
            # è¾“å‡ºæŠ•å½±
            output = self.output_projection(concatenated)
            
            # æ®‹å·®è¿žæŽ¥å’Œå±‚å½’ä¸€åŒ–
            residual = x.mean(dim=1)
            output = self.layer_norm(output + residual)
            output = self.dropout(output)
            
            return output, head_outputs
        
        def forward_with_attention_weights(self, x: torch.Tensor,
                                         mask: Optional[torch.Tensor] = None) -> Tuple[torch.Tensor, List[torch.Tensor]]:
            """
            å‰å‘ä¼ æ’­å¹¶è¿”å›žå„ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                output: æœ€ç»ˆè¾“å‡º [batch_size, d_model]
                head_attentions: å„ä¸ªå¤´çš„æ³¨æ„åŠ›æƒé‡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [batch_size, seq_len]
            """
            batch_size, seq_len, d_model = x.shape
            
            # è¾“å…¥æŠ•å½±
            x_proj = self.input_projection(x)
            
            # åˆ†å‰²ä¸ºå¤šä¸ªå¤´
            x_heads = x_proj.view(batch_size, seq_len, self.n_heads, self.d_k)
            x_heads = x_heads.transpose(1, 2)
            
            # å¯¹æ¯ä¸ªå¤´åº”ç”¨æ—¶é—´æ³¨æ„åŠ›
            head_outputs = []
            head_attentions = []
            for i in range(self.n_heads):
                head_input = x_heads[:, i, :, :]
                head_output, head_attention = self.heads[i].forward_with_attention(head_input, mask)
                head_outputs.append(head_output)
                head_attentions.append(head_attention)
            
            # æ‹¼æŽ¥æ‰€æœ‰å¤´çš„è¾“å‡º
            concatenated = torch.cat(head_outputs, dim=-1)
            
            # è¾“å‡ºæŠ•å½±
            output = self.output_projection(concatenated)
            
            # æ®‹å·®è¿žæŽ¥å’Œå±‚å½’ä¸€åŒ–
            residual = x.mean(dim=1)
            output = self.layer_norm(output + residual)
            output = self.dropout(output)
            
            return output, head_attentions
    
    
    class AdaptiveTemporalAttention(nn.Module):
        """
        è‡ªé€‚åº”æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
        å¯ä»¥æ ¹æ®è¾“å…¥åŠ¨æ€è°ƒæ•´æ³¨æ„åŠ›æ¨¡å¼
        """
        
        def __init__(self, d_model: int, n_heads: int = 8, 
                     use_position_bias: bool = True, dropout: float = 0.1):
            """
            åˆå§‹åŒ–è‡ªé€‚åº”æ—¶é—´æ³¨æ„åŠ›
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                n_heads: æ³¨æ„åŠ›å¤´æ•°
                use_position_bias: æ˜¯å¦ä½¿ç”¨ä½ç½®åç½®
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            self.d_model = d_model
            self.n_heads = n_heads
            self.use_position_bias = use_position_bias
            
            # å¤šå¤´æ—¶é—´æ³¨æ„åŠ›
            self.multi_head_attention = MultiHeadTemporalAttention(d_model, n_heads, dropout)
            
            # ä½ç½®åç½®
            if use_position_bias:
                self.position_bias = nn.Parameter(torch.randn(1, 1, 512) * 0.1)  # æœ€å¤§åºåˆ—é•¿åº¦512
            
            # è‡ªé€‚åº”æƒé‡
            self.adaptive_weight = nn.Sequential(
                nn.Linear(d_model, d_model // 4),
                nn.ReLU(),
                nn.Linear(d_model // 4, 1),
                nn.Sigmoid()
            )
            
            self.dropout = nn.Dropout(p=dropout)
        
        def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                output: èšåˆåŽçš„è¾“å‡º [batch_size, d_model]
            """
            batch_size, seq_len, d_model = x.shape
            
            # æ·»åŠ ä½ç½®åç½®
            if self.use_position_bias:
                pos_bias = self.position_bias[:, :, :seq_len].transpose(1, 2)  # [1, seq_len, 1]
                x = x + pos_bias
            
            # å¤šå¤´æ—¶é—´æ³¨æ„åŠ›
            attention_output = self.multi_head_attention(x, mask)
            
            # è®¡ç®—è‡ªé€‚åº”æƒé‡
            global_context = x.mean(dim=1)  # [batch_size, d_model]
            adaptive_weight = self.adaptive_weight(global_context)  # [batch_size, 1]
            
            # ç®€å•çš„æ—¶é—´å¹³å‡ä½œä¸ºå¤‡é€‰
            simple_average = x.mean(dim=1)  # [batch_size, d_model]
            
            # è‡ªé€‚åº”ç»„åˆ
            output = adaptive_weight * attention_output + (1 - adaptive_weight) * simple_average
            
            return self.dropout(output)
        
        def get_attention_visualization(self, x: torch.Tensor, 
                                      mask: Optional[torch.Tensor] = None) -> dict:
            """
            èŽ·å–æ³¨æ„åŠ›å¯è§†åŒ–ä¿¡æ¯
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                mask: æŽ©ç å¼ é‡ [batch_size, seq_len]ï¼Œå¯é€‰
                
            Returns:
                visualization_data: åŒ…å«æ³¨æ„åŠ›æƒé‡å’Œå…¶ä»–å¯è§†åŒ–ä¿¡æ¯çš„å­—å…¸
            """
            batch_size, seq_len, d_model = x.shape
            
            # æ·»åŠ ä½ç½®åç½®
            if self.use_position_bias:
                pos_bias = self.position_bias[:, :, :seq_len].transpose(1, 2)
                x = x + pos_bias
            
            # èŽ·å–å¤šå¤´æ³¨æ„åŠ›æƒé‡
            _, head_attentions = self.multi_head_attention.forward_with_attention_weights(x, mask)
            
            # è®¡ç®—è‡ªé€‚åº”æƒé‡
            global_context = x.mean(dim=1)
            adaptive_weight = self.adaptive_weight(global_context)
            
            return {
                'head_attentions': head_attentions,
                'adaptive_weights': adaptive_weight.detach().cpu().numpy(),
                'sequence_length': seq_len,
                'n_heads': self.n_heads
            }
    ]]></file>
  <file path="src/rl_trading_system/models/sac_agent.py"><![CDATA[
    """
    SAC (Soft Actor-Critic) æ™ºèƒ½ä½“å®žçŽ°
    """
    from dataclasses import dataclass, field
    from typing import Dict, Any, List, Tuple, Optional, Union
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import numpy as np
    import logging
    from pathlib import Path
    import json
    from collections import deque
    
    from .actor_network import Actor, ActorConfig
    from .critic_network import CriticWithTargetNetwork, CriticConfig
    from .replay_buffer import (
        BaseReplayBuffer, 
        ReplayBuffer, 
        PrioritizedReplayBuffer, 
        Experience, 
        ReplayBufferConfig,
        create_replay_buffer
    )
    
    
    @dataclass
    class SACConfig:
        """SACæ™ºèƒ½ä½“é…ç½®"""
        # ç½‘ç»œæž¶æž„
        state_dim: int = 256
        action_dim: int = 100
        hidden_dim: int = 512
        n_layers: int = 3
        activation: str = 'relu'
        dropout: float = 0.1
        
        # å­¦ä¹ çŽ‡
        lr_actor: float = 3e-4
        lr_critic: float = 3e-4
        lr_alpha: float = 3e-4
        
        # SACç®—æ³•å‚æ•°
        gamma: float = 0.99  # æŠ˜æ‰£å› å­
        tau: float = 0.005   # è½¯æ›´æ–°ç³»æ•°
        alpha: float = 0.2   # åˆå§‹æ¸©åº¦å‚æ•°
        target_entropy: Optional[float] = None  # ç›®æ ‡ç†µï¼Œé»˜è®¤ä¸º-action_dim
        auto_alpha: bool = True  # æ˜¯å¦è‡ªåŠ¨è°ƒæ•´æ¸©åº¦å‚æ•°
        
        # è®­ç»ƒå‚æ•°
        batch_size: int = 256
        buffer_capacity: int = 1000000
        learning_starts: int = 10000  # å¼€å§‹å­¦ä¹ çš„æœ€å°ç»éªŒæ•°
        train_freq: int = 1  # è®­ç»ƒé¢‘çŽ‡
        target_update_freq: int = 1  # ç›®æ ‡ç½‘ç»œæ›´æ–°é¢‘çŽ‡
        gradient_steps: int = 1  # æ¯æ¬¡æ›´æ–°çš„æ¢¯åº¦æ­¥æ•°
        
        # ä¼˜å…ˆçº§å›žæ”¾å‚æ•°
        use_prioritized_replay: bool = False
        alpha_replay: float = 0.6
        beta_replay: float = 0.4
        beta_increment: float = 0.001
        
        # è®¾å¤‡å’Œå…¶ä»–
        device: str = 'cpu'
        seed: Optional[int] = None
        
        # æ—¥å¿—å’Œä¿å­˜
        log_interval: int = 1000
        save_interval: int = 10000
        
        def __post_init__(self):
            """åŽå¤„ç†é…ç½®"""
            if self.target_entropy is None:
                self.target_entropy = -float(self.action_dim)
    
    
    class SACAgent(nn.Module):
        """
        SAC (Soft Actor-Critic) æ™ºèƒ½ä½“
        
        å®žçŽ°å®Œæ•´çš„SACç®—æ³•ï¼ŒåŒ…æ‹¬ï¼š
        - Actorç½‘ç»œï¼ˆç­–ç•¥ç½‘ç»œï¼‰
        - åŒCriticç½‘ç»œï¼ˆä»·å€¼ç½‘ç»œï¼‰
        - è‡ªåŠ¨æ¸©åº¦å‚æ•°è°ƒæ•´
        - ç»éªŒå›žæ”¾ç¼“å†²åŒº
        - ç›®æ ‡ç½‘ç»œè½¯æ›´æ–°
        """
        
        def __init__(self, config: SACConfig):
            super().__init__()
            self.config = config
            self.device = torch.device(config.device)
            
            # è®¾ç½®éšæœºç§å­
            if config.seed is not None:
                torch.manual_seed(config.seed)
                np.random.seed(config.seed)
            
            # åˆå§‹åŒ–ç½‘ç»œ
            self._build_networks()
            
            # åˆå§‹åŒ–ä¼˜åŒ–å™¨
            self._build_optimizers()
            
            # åˆå§‹åŒ–å›žæ”¾ç¼“å†²åŒº
            self._build_replay_buffer()
            
            # è®­ç»ƒç»Ÿè®¡
            self.training_step = 0
            self.episode_count = 0
            self.total_env_steps = 0
            
            # æ€§èƒ½ç»Ÿè®¡
            self.training_stats = {
                'actor_loss': deque(maxlen=1000),
                'critic_loss': deque(maxlen=1000),
                'alpha_loss': deque(maxlen=1000),
                'alpha_value': deque(maxlen=1000),
                'q_value': deque(maxlen=1000),
                'policy_entropy': deque(maxlen=1000)
            }
            
            # æ—¥å¿—
            self.logger = logging.getLogger(__name__)
            
        def _build_networks(self):
            """æž„å»ºç¥žç»ç½‘ç»œ"""
            # Actorç½‘ç»œé…ç½®
            actor_config = ActorConfig(
                state_dim=self.config.state_dim,
                action_dim=self.config.action_dim,
                hidden_dim=self.config.hidden_dim,
                n_layers=self.config.n_layers,
                activation=self.config.activation,
                dropout=self.config.dropout
            )
            
            # Criticç½‘ç»œé…ç½®
            critic_config = CriticConfig(
                state_dim=self.config.state_dim,
                action_dim=self.config.action_dim,
                hidden_dim=self.config.hidden_dim,
                n_layers=self.config.n_layers,
                activation=self.config.activation,
                dropout=self.config.dropout
            )
            
            # åˆ›å»ºç½‘ç»œ
            self.actor = Actor(actor_config).to(self.device)
            self.critic = CriticWithTargetNetwork(critic_config).to(self.device)
            
            # æ¸©åº¦å‚æ•°
            if self.config.auto_alpha:
                self.log_alpha = nn.Parameter(
                    torch.log(torch.tensor(self.config.alpha, device=self.device))
                )
            else:
                self.register_buffer(
                    'log_alpha', 
                    torch.log(torch.tensor(self.config.alpha, device=self.device))
                )
        
        def _build_optimizers(self):
            """æž„å»ºä¼˜åŒ–å™¨"""
            self.actor_optimizer = torch.optim.Adam(
                self.actor.parameters(), 
                lr=self.config.lr_actor
            )
            
            self.critic_optimizer = torch.optim.Adam(
                self.critic.get_parameters(), 
                lr=self.config.lr_critic
            )
            
            if self.config.auto_alpha:
                self.alpha_optimizer = torch.optim.Adam(
                    [self.log_alpha], 
                    lr=self.config.lr_alpha
                )
            else:
                self.alpha_optimizer = None
        
        def _build_replay_buffer(self):
            """æž„å»ºå›žæ”¾ç¼“å†²åŒº"""
            buffer_config = ReplayBufferConfig(
                capacity=self.config.buffer_capacity,
                batch_size=self.config.batch_size,
                state_dim=self.config.state_dim,
                action_dim=self.config.action_dim,
                device=self.config.device
            )
            
            if self.config.use_prioritized_replay:
                buffer_config.alpha = self.config.alpha_replay
                buffer_config.beta = self.config.beta_replay
                buffer_config.beta_increment = self.config.beta_increment
            
            self.replay_buffer = create_replay_buffer(buffer_config)
        
        @property
        def alpha(self) -> torch.Tensor:
            """å½“å‰æ¸©åº¦å‚æ•°"""
            return torch.exp(self.log_alpha)
        
        def get_action(self, 
                       state: torch.Tensor, 
                       deterministic: bool = False,
                       return_log_prob: bool = False) -> Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:
            """
            èŽ·å–åŠ¨ä½œ
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim] æˆ– [state_dim]
                deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                return_log_prob: æ˜¯å¦è¿”å›žå¯¹æ•°æ¦‚çŽ‡
                
            Returns:
                action: åŠ¨ä½œå¼ é‡
                log_prob: å¯¹æ•°æ¦‚çŽ‡ï¼ˆå¦‚æžœreturn_log_prob=Trueï¼‰
            """
            # ç¡®ä¿çŠ¶æ€æ˜¯æ‰¹æ¬¡æ ¼å¼
            if state.dim() == 1:
                state = state.unsqueeze(0)
                squeeze_output = True
            else:
                squeeze_output = False
            
            state = state.to(self.device)
            
            with torch.no_grad():
                action, log_prob = self.actor.get_action(state, deterministic=deterministic)
            
            if squeeze_output:
                action = action.squeeze(0)
                log_prob = log_prob.squeeze(0)
            
            if return_log_prob:
                return action, log_prob
            else:
                return action
        
        def add_experience(self, experience: Experience) -> None:
            """
            æ·»åŠ ç»éªŒåˆ°å›žæ”¾ç¼“å†²åŒº
            
            Args:
                experience: ç»éªŒæ•°æ®
            """
            # ç¡®ä¿å¼ é‡åœ¨æ­£ç¡®è®¾å¤‡ä¸Š
            experience.state = experience.state.to('cpu')
            experience.action = experience.action.to('cpu')
            experience.next_state = experience.next_state.to('cpu')
            
            if isinstance(self.replay_buffer, PrioritizedReplayBuffer):
                # è®¡ç®—TDè¯¯å·®ä½œä¸ºä¼˜å…ˆçº§
                with torch.no_grad():
                    priority = self._compute_td_error(experience)
                self.replay_buffer.add(experience, priority=priority)
            else:
                self.replay_buffer.add(experience)
            
            self.total_env_steps += 1
        
        def _compute_td_error(self, experience: Experience) -> float:
            """è®¡ç®—TDè¯¯å·®ç”¨äºŽä¼˜å…ˆçº§å›žæ”¾"""
            state = experience.state.unsqueeze(0).to(self.device)
            action = experience.action.unsqueeze(0).to(self.device)
            reward = torch.tensor([experience.reward], device=self.device)
            next_state = experience.next_state.unsqueeze(0).to(self.device)
            done = torch.tensor([experience.done], dtype=torch.float32, device=self.device)
            
            # è®¡ç®—å½“å‰Qå€¼
            current_q1, current_q2 = self.critic.get_main_q_values(state, action)
            current_q = torch.min(current_q1, current_q2)
            
            # è®¡ç®—ç›®æ ‡Qå€¼
            with torch.no_grad():
                next_action, next_log_prob = self.actor.get_action(next_state)
                target_q = self.critic.get_target_min_q_value(next_state, next_action)
                target_q = target_q - self.alpha * next_log_prob.unsqueeze(1)
                target_q = reward.unsqueeze(1) + (1 - done.unsqueeze(1)) * self.config.gamma * target_q
            
            # è®¡ç®—TDè¯¯å·®
            td_error = torch.abs(current_q - target_q).item()
            return max(td_error, 1e-6)  # é˜²æ­¢é›¶ä¼˜å…ˆçº§
        
        def can_update(self) -> bool:
            """æ£€æŸ¥æ˜¯å¦å¯ä»¥æ›´æ–°"""
            return (self.replay_buffer.can_sample() and 
                    self.total_env_steps >= self.config.learning_starts)
        
        def update(self, update_actor: bool = True) -> Dict[str, float]:
            """
            æ›´æ–°ç½‘ç»œå‚æ•°
            
            Args:
                update_actor: æ˜¯å¦æ›´æ–°Actorç½‘ç»œ
                
            Returns:
                losses: æŸå¤±å­—å…¸
            """
            if not self.can_update():
                return {}
            
            losses = {}
            
            for _ in range(self.config.gradient_steps):
                # é‡‡æ ·æ‰¹æ¬¡
                if isinstance(self.replay_buffer, PrioritizedReplayBuffer):
                    batch, indices, weights = self.replay_buffer.sample()
                    weights = weights.to(self.device)
                else:
                    batch = self.replay_buffer.sample()
                    indices = None
                    weights = None
                
                # å‡†å¤‡æ‰¹æ¬¡æ•°æ®
                states = torch.stack([exp.state for exp in batch]).to(self.device)
                actions = torch.stack([exp.action for exp in batch]).to(self.device)
                rewards = torch.tensor([exp.reward for exp in batch], 
                                     dtype=torch.float32, device=self.device)
                next_states = torch.stack([exp.next_state for exp in batch]).to(self.device)
                dones = torch.tensor([exp.done for exp in batch], 
                                   dtype=torch.float32, device=self.device)
                
                # æ›´æ–°Critic
                critic_loss, td_errors = self._update_critic(
                    states, actions, rewards, next_states, dones, weights
                )
                losses['critic_loss'] = critic_loss
                
                # æ›´æ–°ä¼˜å…ˆçº§
                if isinstance(self.replay_buffer, PrioritizedReplayBuffer) and indices is not None:
                    self.replay_buffer.update_priorities(indices, td_errors.detach().cpu())
                
                # æ›´æ–°Actor
                if update_actor:
                    actor_loss, policy_entropy = self._update_actor(states)
                    losses['actor_loss'] = actor_loss
                    losses['policy_entropy'] = policy_entropy
                    
                    # æ›´æ–°æ¸©åº¦å‚æ•°
                    if self.config.auto_alpha:
                        alpha_loss = self._update_alpha(states)
                        losses['alpha_loss'] = alpha_loss
                
                # è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ
                if self.training_step % self.config.target_update_freq == 0:
                    self.critic.soft_update(self.config.tau)
                
                self.training_step += 1
            
            # è®°å½•ç»Ÿè®¡ä¿¡æ¯
            losses['alpha'] = self.alpha.item()
            self._update_stats(losses)
            
            # å®šæœŸæ—¥å¿—
            if self.training_step % self.config.log_interval == 0:
                self._log_training_stats()
            
            return losses
        
        def _update_critic(self, states, actions, rewards, next_states, dones, weights=None):
            """æ›´æ–°Criticç½‘ç»œ"""
            # è®¡ç®—ç›®æ ‡Qå€¼
            with torch.no_grad():
                next_actions, next_log_probs = self.actor.get_action(next_states)
                target_q = self.critic.get_target_min_q_value(next_states, next_actions)
                target_q = target_q - self.alpha * next_log_probs.unsqueeze(1)
                target_q = rewards.unsqueeze(1) + (1 - dones.unsqueeze(1)) * self.config.gamma * target_q
            
            # è®¡ç®—å½“å‰Qå€¼
            current_q1, current_q2 = self.critic.get_main_q_values(states, actions)
            
            # è®¡ç®—æŸå¤±
            td_errors1 = torch.abs(current_q1 - target_q)
            td_errors2 = torch.abs(current_q2 - target_q)
            td_errors = torch.max(td_errors1, td_errors2).squeeze()
            
            if weights is not None:
                # åŠ æƒæŸå¤±ï¼ˆç”¨äºŽä¼˜å…ˆçº§å›žæ”¾ï¼‰
                critic_loss = torch.mean(weights * (td_errors1.squeeze() ** 2 + td_errors2.squeeze() ** 2))
            else:
                critic_loss = F.mse_loss(current_q1, target_q) + F.mse_loss(current_q2, target_q)
            
            # åå‘ä¼ æ’­
            self.critic_optimizer.zero_grad()
            critic_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.critic.get_parameters(), max_norm=1.0)
            
            self.critic_optimizer.step()
            
            return critic_loss.item(), td_errors
        
        def _update_actor(self, states):
            """æ›´æ–°Actorç½‘ç»œ"""
            # ç”ŸæˆåŠ¨ä½œå’Œå¯¹æ•°æ¦‚çŽ‡
            actions, log_probs = self.actor.get_action(states)
            
            # è®¡ç®—Qå€¼
            q_values = self.critic.main_network.get_min_q_value(states, actions)
            
            # è®¡ç®—ActoræŸå¤±
            actor_loss = torch.mean(self.alpha * log_probs - q_values.squeeze())
            
            # åå‘ä¼ æ’­
            self.actor_optimizer.zero_grad()
            actor_loss.backward()
            
            # æ¢¯åº¦è£å‰ª
            torch.nn.utils.clip_grad_norm_(self.actor.parameters(), max_norm=1.0)
            
            self.actor_optimizer.step()
            
            # è®¡ç®—ç­–ç•¥ç†µ
            policy_entropy = -torch.mean(log_probs).item()
            
            return actor_loss.item(), policy_entropy
        
        def _update_alpha(self, states):
            """æ›´æ–°æ¸©åº¦å‚æ•°"""
            with torch.no_grad():
                _, log_probs = self.actor.get_action(states)
            
            # è®¡ç®—alphaæŸå¤±
            alpha_loss = -torch.mean(self.log_alpha * (log_probs + self.config.target_entropy))
            
            # åå‘ä¼ æ’­
            self.alpha_optimizer.zero_grad()
            alpha_loss.backward()
            self.alpha_optimizer.step()
            
            return alpha_loss.item()
        
        def _update_stats(self, losses):
            """æ›´æ–°è®­ç»ƒç»Ÿè®¡"""
            for key, value in losses.items():
                if key in self.training_stats:
                    self.training_stats[key].append(value)
        
        def _log_training_stats(self):
            """è®°å½•è®­ç»ƒç»Ÿè®¡"""
            stats_str = f"Step {self.training_step}: "
            for key, values in self.training_stats.items():
                if values:
                    avg_value = np.mean(list(values)[-100:])  # æœ€è¿‘100æ­¥çš„å¹³å‡å€¼
                    stats_str += f"{key}={avg_value:.4f} "
            
            self.logger.info(stats_str)
        
        def get_training_stats(self) -> Dict[str, float]:
            """èŽ·å–è®­ç»ƒç»Ÿè®¡"""
            stats = {}
            for key, values in self.training_stats.items():
                if values:
                    stats[f"{key}_mean"] = np.mean(values)
                    stats[f"{key}_std"] = np.std(values)
                    stats[f"{key}_recent"] = np.mean(list(values)[-100:]) if len(values) >= 100 else np.mean(values)
            
            stats['training_step'] = self.training_step
            stats['total_env_steps'] = self.total_env_steps
            stats['buffer_size'] = self.replay_buffer.size
            
            return stats
        
        def save(self, path: Union[str, Path]) -> None:
            """
            ä¿å­˜æ¨¡åž‹
            
            Args:
                path: ä¿å­˜è·¯å¾„
            """
            path = Path(path)
            path.mkdir(parents=True, exist_ok=True)
            
            # ä¿å­˜ç½‘ç»œå‚æ•°
            torch.save({
                'actor_state_dict': self.actor.state_dict(),
                'critic_state_dict': self.critic.state_dict(),
                'log_alpha': self.log_alpha,
                'actor_optimizer_state_dict': self.actor_optimizer.state_dict(),
                'critic_optimizer_state_dict': self.critic_optimizer.state_dict(),
                'alpha_optimizer_state_dict': self.alpha_optimizer.state_dict() if self.alpha_optimizer else None,
                'training_step': self.training_step,
                'total_env_steps': self.total_env_steps,
                'config': self.config
            }, path / 'model.pt')
            
            # ä¿å­˜å›žæ”¾ç¼“å†²åŒºï¼ˆå¦‚æžœéœ€è¦ï¼‰
            if hasattr(self.replay_buffer, 'state_dict'):
                torch.save(self.replay_buffer.state_dict(), path / 'replay_buffer.pt')
            
            # ä¿å­˜é…ç½®
            with open(path / 'config.json', 'w') as f:
                # å°†é…ç½®è½¬æ¢ä¸ºå¯åºåˆ—åŒ–çš„å­—å…¸
                config_dict = {
                    k: v for k, v in self.config.__dict__.items() 
                    if isinstance(v, (int, float, str, bool, type(None)))
                }
                json.dump(config_dict, f, indent=2)
            
            self.logger.info(f"æ¨¡åž‹å·²ä¿å­˜åˆ° {path}")
        
        def load(self, path: Union[str, Path]) -> None:
            """
            åŠ è½½æ¨¡åž‹
            
            Args:
                path: æ¨¡åž‹è·¯å¾„
            """
            path = Path(path)
            
            # åŠ è½½æ¨¡åž‹å‚æ•°
            checkpoint = torch.load(path / 'model.pt', map_location=self.device)
            
            self.actor.load_state_dict(checkpoint['actor_state_dict'])
            self.critic.load_state_dict(checkpoint['critic_state_dict'])
            self.log_alpha.data = checkpoint['log_alpha'].to(self.device)
            
            self.actor_optimizer.load_state_dict(checkpoint['actor_optimizer_state_dict'])
            self.critic_optimizer.load_state_dict(checkpoint['critic_optimizer_state_dict'])
            
            if checkpoint['alpha_optimizer_state_dict'] and self.alpha_optimizer:
                self.alpha_optimizer.load_state_dict(checkpoint['alpha_optimizer_state_dict'])
            
            self.training_step = checkpoint['training_step']
            self.total_env_steps = checkpoint['total_env_steps']
            
            # åŠ è½½å›žæ”¾ç¼“å†²åŒºï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            buffer_path = path / 'replay_buffer.pt'
            if buffer_path.exists() and hasattr(self.replay_buffer, 'load_state_dict'):
                buffer_state = torch.load(buffer_path, map_location='cpu')
                self.replay_buffer.load_state_dict(buffer_state)
            
            self.logger.info(f"æ¨¡åž‹å·²ä»Ž {path} åŠ è½½")
        
        def eval(self):
            """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
            super().eval()
            self.actor.eval()
            self.critic.eval_mode()
            return self
        
        def train(self, mode: bool = True):
            """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
            super().train(mode)
            self.actor.train(mode)
            if mode:
                self.critic.train_mode()
            else:
                self.critic.eval_mode()
            return self
        
        def reset_training_stats(self):
            """é‡ç½®è®­ç»ƒç»Ÿè®¡"""
            for key in self.training_stats:
                self.training_stats[key].clear()
        
        def get_policy_state_dict(self) -> Dict[str, Any]:
            """èŽ·å–ç­–ç•¥ç½‘ç»œçŠ¶æ€å­—å…¸ï¼ˆç”¨äºŽéƒ¨ç½²ï¼‰"""
            return {
                'actor_state_dict': self.actor.state_dict(),
                'log_alpha': self.log_alpha,
                'config': self.config
            }
        
        def load_policy_state_dict(self, state_dict: Dict[str, Any]):
            """åŠ è½½ç­–ç•¥ç½‘ç»œçŠ¶æ€å­—å…¸"""
            self.actor.load_state_dict(state_dict['actor_state_dict'])
            self.log_alpha.data = state_dict['log_alpha'].to(self.device)
    ]]></file>
  <file path="src/rl_trading_system/models/replay_buffer.py"><![CDATA[
    """
    ç»éªŒå›žæ”¾ç¼“å†²åŒºå®žçŽ°
    """
    from dataclasses import dataclass, field
    from typing import List, Tuple, Dict, Any, Optional, Union
    import torch
    import numpy as np
    import random
    from collections import namedtuple, deque
    import threading
    import multiprocessing as mp
    from abc import ABC, abstractmethod
    
    
    @dataclass
    class Experience:
        """ç»éªŒæ•°æ®ç»“æž„"""
        state: torch.Tensor
        action: torch.Tensor
        reward: float
        next_state: torch.Tensor
        done: bool
        info: Optional[Dict[str, Any]] = None
    
    
    @dataclass
    class ReplayBufferConfig:
        """å›žæ”¾ç¼“å†²åŒºé…ç½®"""
        capacity: int = 100000
        batch_size: int = 256
        state_dim: int = 256
        action_dim: int = 100
        device: str = 'cpu'
        
        # ä¼˜å…ˆçº§å›žæ”¾å‚æ•°
        alpha: float = 0.6  # ä¼˜å…ˆçº§æŒ‡æ•°
        beta: float = 0.4   # é‡è¦æ€§é‡‡æ ·æŒ‡æ•°
        beta_increment: float = 0.001  # betaå¢žé•¿çŽ‡
        epsilon: float = 1e-6  # é˜²æ­¢é›¶ä¼˜å…ˆçº§çš„å°å€¼
        
        # å¤šè¿›ç¨‹å‚æ•°
        n_workers: int = 1
        shared_memory: bool = False
    
    
    class BaseReplayBuffer(ABC):
        """å›žæ”¾ç¼“å†²åŒºåŸºç±»"""
        
        def __init__(self, config: ReplayBufferConfig):
            self.config = config
            self.capacity = config.capacity
            self.batch_size = config.batch_size
            self.device = torch.device(config.device)
            
        @abstractmethod
        def add(self, experience: Experience, **kwargs) -> None:
            """æ·»åŠ ç»éªŒ"""
            pass
        
        @abstractmethod
        def sample(self) -> Union[List[Experience], Tuple[List[Experience], torch.Tensor, torch.Tensor]]:
            """é‡‡æ ·æ‰¹æ¬¡"""
            pass
        
        @abstractmethod
        def can_sample(self) -> bool:
            """æ˜¯å¦å¯ä»¥é‡‡æ ·"""
            pass
        
        @abstractmethod
        def clear(self) -> None:
            """æ¸…ç©ºç¼“å†²åŒº"""
            pass
        
        @property
        @abstractmethod
        def size(self) -> int:
            """å½“å‰å¤§å°"""
            pass
    
    
    class ReplayBuffer(BaseReplayBuffer):
        """
        æ ‡å‡†ç»éªŒå›žæ”¾ç¼“å†²åŒº
        
        ä½¿ç”¨å¾ªçŽ¯ç¼“å†²åŒºå­˜å‚¨ç»éªŒï¼Œæ”¯æŒéšæœºé‡‡æ ·
        """
        
        def __init__(self, config: ReplayBufferConfig):
            super().__init__(config)
            
            # åˆå§‹åŒ–ç¼“å†²åŒº
            self.buffer: List[Optional[Experience]] = [None] * self.capacity
            self.position = 0
            self._size = 0
            
            # çº¿ç¨‹é”ï¼ˆç”¨äºŽå¤šçº¿ç¨‹å®‰å…¨ï¼‰
            self._lock = threading.RLock()
            
        def add(self, experience: Experience, **kwargs) -> None:
            """
            æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº
            
            Args:
                experience: ç»éªŒæ•°æ®
            """
            with self._lock:
                self.buffer[self.position] = experience
                self.position = (self.position + 1) % self.capacity
                self._size = min(self._size + 1, self.capacity)
        
        def sample(self) -> List[Experience]:
            """
            éšæœºé‡‡æ ·æ‰¹æ¬¡
            
            Returns:
                batch: ç»éªŒæ‰¹æ¬¡
            """
            if not self.can_sample():
                raise ValueError(f"ç¼“å†²åŒºä¸­çš„ç»éªŒä¸è¶³ï¼Œéœ€è¦è‡³å°‘{self.batch_size}ä¸ªï¼Œå½“å‰æœ‰{self.size}ä¸ª")
            
            with self._lock:
                # éšæœºé‡‡æ ·ç´¢å¼•
                indices = random.sample(range(self.size), self.batch_size)
                batch = [self.buffer[i] for i in indices]
                
            return batch
        
        def can_sample(self) -> bool:
            """æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡‡æ ·"""
            return self.size >= self.batch_size
        
        def clear(self) -> None:
            """æ¸…ç©ºç¼“å†²åŒº"""
            with self._lock:
                self.buffer = [None] * self.capacity
                self.position = 0
                self._size = 0
        
        @property
        def size(self) -> int:
            """å½“å‰ç¼“å†²åŒºå¤§å°"""
            return self._size
        
        def get_all_experiences(self) -> List[Experience]:
            """
            èŽ·å–æ‰€æœ‰ç»éªŒï¼ˆç”¨äºŽè°ƒè¯•å’Œåˆ†æžï¼‰
            
            Returns:
                experiences: æ‰€æœ‰ç»éªŒåˆ—è¡¨
            """
            with self._lock:
                if self._size < self.capacity:
                    return [exp for exp in self.buffer[:self._size] if exp is not None]
                else:
                    # ç¼“å†²åŒºå·²æ»¡ï¼Œéœ€è¦æŒ‰æ­£ç¡®é¡ºåºè¿”å›ž
                    return [exp for exp in (self.buffer[self.position:] + self.buffer[:self.position]) if exp is not None]
        
        def state_dict(self) -> Dict[str, Any]:
            """
            èŽ·å–ç¼“å†²åŒºçŠ¶æ€ï¼ˆç”¨äºŽä¿å­˜ï¼‰
            
            Returns:
                state: çŠ¶æ€å­—å…¸
            """
            with self._lock:
                return {
                    'buffer': self.buffer.copy(),
                    'position': self.position,
                    'size': self._size,
                    'capacity': self.capacity,
                    'batch_size': self.batch_size
                }
        
        def load_state_dict(self, state_dict: Dict[str, Any]) -> None:
            """
            åŠ è½½ç¼“å†²åŒºçŠ¶æ€
            
            Args:
                state_dict: çŠ¶æ€å­—å…¸
            """
            with self._lock:
                self.buffer = state_dict['buffer']
                self.position = state_dict['position']
                self._size = state_dict['size']
                self.capacity = state_dict['capacity']
                self.batch_size = state_dict['batch_size']
    
    
    class PrioritizedReplayBuffer(BaseReplayBuffer):
        """
        ä¼˜å…ˆçº§ç»éªŒå›žæ”¾ç¼“å†²åŒº
        
        åŸºäºŽTDè¯¯å·®çš„ä¼˜å…ˆçº§é‡‡æ ·ï¼Œä½¿ç”¨SumTreeæ•°æ®ç»“æž„
        """
        
        def __init__(self, config: ReplayBufferConfig):
            super().__init__(config)
            
            self.alpha = config.alpha
            self.beta = config.beta
            self.beta_increment = config.beta_increment
            self.epsilon = config.epsilon
            
            # åˆå§‹åŒ–ç¼“å†²åŒºå’Œä¼˜å…ˆçº§
            self.buffer: List[Optional[Experience]] = [None] * self.capacity
            self.priorities = np.zeros(self.capacity, dtype=np.float32)
            self.position = 0
            self._size = 0
            self.max_priority = 1.0
            
            # æž„å»ºSumTree
            self._build_sum_tree()
            
            # çº¿ç¨‹é”
            self._lock = threading.RLock()
        
        def _build_sum_tree(self):
            """æž„å»ºSumTreeæ•°æ®ç»“æž„"""
            # SumTreeçš„å¤§å°æ˜¯2*capacity-1
            tree_size = 2 * self.capacity - 1
            self.tree = np.zeros(tree_size, dtype=np.float32)
            
        def _update_tree(self, idx: int, priority: float):
            """æ›´æ–°SumTreeä¸­çš„ä¼˜å…ˆçº§"""
            tree_idx = idx + self.capacity - 1
            change = priority - self.tree[tree_idx]
            self.tree[tree_idx] = priority
            
            # å‘ä¸Šæ›´æ–°çˆ¶èŠ‚ç‚¹
            while tree_idx != 0:
                tree_idx = (tree_idx - 1) // 2
                self.tree[tree_idx] += change
        
        def _get_leaf(self, value: float) -> int:
            """æ ¹æ®å€¼èŽ·å–å¶å­èŠ‚ç‚¹ç´¢å¼•"""
            parent_idx = 0
            
            while True:
                left_child_idx = 2 * parent_idx + 1
                right_child_idx = left_child_idx + 1
                
                if left_child_idx >= len(self.tree):
                    leaf_idx = parent_idx
                    break
                else:
                    if value <= self.tree[left_child_idx]:
                        parent_idx = left_child_idx
                    else:
                        value -= self.tree[left_child_idx]
                        parent_idx = right_child_idx
            
            data_idx = leaf_idx - self.capacity + 1
            return data_idx
        
        def add(self, experience: Experience, priority: Optional[float] = None, **kwargs) -> None:
            """
            æ·»åŠ ç»éªŒåˆ°ç¼“å†²åŒº
            
            Args:
                experience: ç»éªŒæ•°æ®
                priority: ä¼˜å…ˆçº§ï¼ˆå¦‚æžœä¸ºNoneï¼Œä½¿ç”¨æœ€å¤§ä¼˜å…ˆçº§ï¼‰
            """
            if priority is None:
                priority = self.max_priority
            
            with self._lock:
                self.buffer[self.position] = experience
                self.priorities[self.position] = priority ** self.alpha
                self._update_tree(self.position, self.priorities[self.position])
                
                self.position = (self.position + 1) % self.capacity
                self._size = min(self._size + 1, self.capacity)
                
                # æ›´æ–°æœ€å¤§ä¼˜å…ˆçº§
                self.max_priority = max(self.max_priority, priority)
        
        def sample(self) -> Tuple[List[Experience], np.ndarray, torch.Tensor]:
            """
            åŸºäºŽä¼˜å…ˆçº§é‡‡æ ·æ‰¹æ¬¡
            
            Returns:
                batch: ç»éªŒæ‰¹æ¬¡
                indices: é‡‡æ ·ç´¢å¼•
                weights: é‡è¦æ€§é‡‡æ ·æƒé‡
            """
            if not self.can_sample():
                raise ValueError(f"ç¼“å†²åŒºä¸­çš„ç»éªŒä¸è¶³ï¼Œéœ€è¦è‡³å°‘{self.batch_size}ä¸ªï¼Œå½“å‰æœ‰{self.size}ä¸ª")
            
            with self._lock:
                indices = []
                priorities = []
                
                # è®¡ç®—ä¼˜å…ˆçº§åŒºé—´
                priority_segment = self.tree[0] / self.batch_size
                
                for i in range(self.batch_size):
                    a = priority_segment * i
                    b = priority_segment * (i + 1)
                    
                    value = np.random.uniform(a, b)
                    idx = self._get_leaf(value)
                    
                    # ç¡®ä¿ç´¢å¼•æœ‰æ•ˆ
                    idx = max(0, min(idx, self.size - 1))
                    indices.append(idx)
                    priorities.append(self.priorities[idx])
                
                # è®¡ç®—é‡è¦æ€§é‡‡æ ·æƒé‡
                sampling_probabilities = np.array(priorities) / self.tree[0]
                weights = (self.size * sampling_probabilities) ** (-self.beta)
                weights = weights / weights.max()  # å½’ä¸€åŒ–
                
                # èŽ·å–ç»éªŒæ‰¹æ¬¡
                batch = [self.buffer[idx] for idx in indices]
                
                # æ›´æ–°beta
                self.beta = min(1.0, self.beta + self.beta_increment)
                
            return batch, np.array(indices), torch.tensor(weights, dtype=torch.float32, device=self.device)
        
        def update_priorities(self, indices: np.ndarray, priorities: torch.Tensor) -> None:
            """
            æ›´æ–°ç»éªŒçš„ä¼˜å…ˆçº§
            
            Args:
                indices: ç»éªŒç´¢å¼•
                priorities: æ–°çš„ä¼˜å…ˆçº§
            """
            with self._lock:
                for idx, priority in zip(indices, priorities):
                    priority = float(priority)
                    priority = max(priority, self.epsilon)  # é˜²æ­¢é›¶ä¼˜å…ˆçº§
                    
                    # å­˜å‚¨åŽŸå§‹ä¼˜å…ˆçº§ï¼ˆä¸åŠ alphaæŒ‡æ•°ï¼‰ç”¨äºŽæ¯”è¾ƒ
                    raw_priority = priority
                    self.priorities[idx] = priority ** self.alpha
                    self._update_tree(idx, self.priorities[idx])
                    
                    # æ›´æ–°æœ€å¤§ä¼˜å…ˆçº§
                    self.max_priority = max(self.max_priority, raw_priority)
        
        def can_sample(self) -> bool:
            """æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡‡æ ·"""
            return self.size >= self.batch_size
        
        def clear(self) -> None:
            """æ¸…ç©ºç¼“å†²åŒº"""
            with self._lock:
                self.buffer = [None] * self.capacity
                self.priorities = np.zeros(self.capacity, dtype=np.float32)
                self.position = 0
                self._size = 0
                self.max_priority = 1.0
                self._build_sum_tree()
        
        @property
        def size(self) -> int:
            """å½“å‰ç¼“å†²åŒºå¤§å°"""
            return self._size
    
    
    class MultiProcessReplayBuffer(BaseReplayBuffer):
        """
        å¤šè¿›ç¨‹ç»éªŒå›žæ”¾ç¼“å†²åŒº
        
        æ”¯æŒå¤šä¸ªè¿›ç¨‹å¹¶å‘æ·»åŠ å’Œé‡‡æ ·ç»éªŒ
        """
        
        def __init__(self, config: ReplayBufferConfig):
            super().__init__(config)
            
            self.n_workers = config.n_workers
            self.shared_memory = config.shared_memory
            
            if self.shared_memory:
                # ä½¿ç”¨å…±äº«å†…å­˜
                self._init_shared_memory()
            else:
                # ä½¿ç”¨è¿›ç¨‹é—´é€šä¿¡
                self._init_ipc()
        
        def _init_shared_memory(self):
            """åˆå§‹åŒ–å…±äº«å†…å­˜"""
            # è¿™é‡Œéœ€è¦å®žçŽ°å…±äº«å†…å­˜ç‰ˆæœ¬
            # ç”±äºŽPyTorchå¼ é‡çš„å…±äº«å†…å­˜æ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œæä¾›æ¡†æž¶
            raise NotImplementedError("å…±äº«å†…å­˜ç‰ˆæœ¬å¾…å®žçŽ°")
        
        def _init_ipc(self):
            """åˆå§‹åŒ–è¿›ç¨‹é—´é€šä¿¡"""
            # ä½¿ç”¨é˜Ÿåˆ—è¿›è¡Œè¿›ç¨‹é—´é€šä¿¡
            self.experience_queue = mp.Queue(maxsize=self.capacity)
            self.sample_queue = mp.Queue()
            
            # å¯åŠ¨åŽå°è¿›ç¨‹ç®¡ç†ç¼“å†²åŒº
            self.manager_process = mp.Process(target=self._buffer_manager)
            self.manager_process.start()
        
        def _buffer_manager(self):
            """ç¼“å†²åŒºç®¡ç†è¿›ç¨‹"""
            buffer = ReplayBuffer(self.config)
            
            while True:
                try:
                    # å¤„ç†æ·»åŠ è¯·æ±‚
                    if not self.experience_queue.empty():
                        experience = self.experience_queue.get_nowait()
                        if experience is None:  # ç»ˆæ­¢ä¿¡å·
                            break
                        buffer.add(experience)
                    
                    # å¤„ç†é‡‡æ ·è¯·æ±‚
                    if not self.sample_queue.empty():
                        request = self.sample_queue.get_nowait()
                        if request == 'sample' and buffer.can_sample():
                            batch = buffer.sample()
                            # è¿™é‡Œéœ€è¦å°†æ‰¹æ¬¡å‘é€å›žè¯·æ±‚è¿›ç¨‹
                            # å®žé™…å®žçŽ°éœ€è¦æ›´å¤æ‚çš„é€šä¿¡æœºåˆ¶
                            pass
                    
                except Exception as e:
                    print(f"ç¼“å†²åŒºç®¡ç†è¿›ç¨‹é”™è¯¯: {e}")
                    break
        
        def add(self, experience: Experience, **kwargs) -> None:
            """æ·»åŠ ç»éªŒï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
            try:
                self.experience_queue.put_nowait(experience)
            except:
                # é˜Ÿåˆ—æ»¡æ—¶çš„å¤„ç†
                pass
        
        def sample(self) -> List[Experience]:
            """é‡‡æ ·æ‰¹æ¬¡ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
            # å‘é€é‡‡æ ·è¯·æ±‚
            self.sample_queue.put('sample')
            
            # ç­‰å¾…ç»“æžœï¼ˆè¿™é‡Œéœ€è¦å®žçŽ°ç»“æžœæŽ¥æ”¶æœºåˆ¶ï¼‰
            # å®žé™…å®žçŽ°éœ€è¦æ›´å¤æ‚çš„é€šä¿¡åè®®
            raise NotImplementedError("å¤šè¿›ç¨‹é‡‡æ ·å¾…å®Œå–„")
        
        def can_sample(self) -> bool:
            """æ£€æŸ¥æ˜¯å¦å¯ä»¥é‡‡æ ·ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
            # éœ€è¦æŸ¥è¯¢ç®¡ç†è¿›ç¨‹çš„çŠ¶æ€
            return True  # ç®€åŒ–å®žçŽ°
        
        def clear(self) -> None:
            """æ¸…ç©ºç¼“å†²åŒºï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
            # å‘é€æ¸…ç©ºä¿¡å·
            pass
        
        @property
        def size(self) -> int:
            """å½“å‰ç¼“å†²åŒºå¤§å°ï¼ˆå¤šè¿›ç¨‹ç‰ˆæœ¬ï¼‰"""
            # éœ€è¦æŸ¥è¯¢ç®¡ç†è¿›ç¨‹çš„çŠ¶æ€
            return 0  # ç®€åŒ–å®žçŽ°
        
        def close(self):
            """å…³é—­å¤šè¿›ç¨‹ç¼“å†²åŒº"""
            if hasattr(self, 'manager_process'):
                # å‘é€ç»ˆæ­¢ä¿¡å·
                self.experience_queue.put(None)
                self.manager_process.join(timeout=5)
                if self.manager_process.is_alive():
                    self.manager_process.terminate()
    
    
    def create_replay_buffer(config: ReplayBufferConfig) -> BaseReplayBuffer:
        """
        å·¥åŽ‚å‡½æ•°ï¼šåˆ›å»ºå›žæ”¾ç¼“å†²åŒº
        
        Args:
            config: ç¼“å†²åŒºé…ç½®
            
        Returns:
            buffer: å›žæ”¾ç¼“å†²åŒºå®žä¾‹
        """
        if config.alpha > 0:
            # ä½¿ç”¨ä¼˜å…ˆçº§å›žæ”¾
            return PrioritizedReplayBuffer(config)
        elif config.n_workers > 1:
            # ä½¿ç”¨å¤šè¿›ç¨‹å›žæ”¾
            return MultiProcessReplayBuffer(config)
        else:
            # ä½¿ç”¨æ ‡å‡†å›žæ”¾
            return ReplayBuffer(config)
    ]]></file>
  <file path="src/rl_trading_system/models/positional_encoding.py"><![CDATA[
    """
    ä½ç½®ç¼–ç ç»„ä»¶å®žçŽ°
    æ”¯æŒå¤šç§ä½ç½®ç¼–ç æ–¹å¼ï¼šæ­£å¼¦ä½™å¼¦ç¼–ç ã€å¯å­¦ä¹ ä½ç½®ç¼–ç ã€ç›¸å¯¹ä½ç½®ç¼–ç 
    """
    
    import torch
    import torch.nn as nn
    import math
    from typing import Optional
    
    
    class PositionalEncoding(nn.Module):
        """
        æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç 
        ä½¿ç”¨ä¸åŒé¢‘çŽ‡çš„æ­£å¼¦å’Œä½™å¼¦å‡½æ•°æ¥ç¼–ç ä½ç½®ä¿¡æ¯
        """
        
        def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
            """
            åˆå§‹åŒ–ä½ç½®ç¼–ç 
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                max_len: æœ€å¤§åºåˆ—é•¿åº¦
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            self.d_model = d_model
            self.max_len = max_len
            self.dropout = nn.Dropout(p=dropout)
            
            # åˆ›å»ºä½ç½®ç¼–ç çŸ©é˜µ
            pe = torch.zeros(max_len, d_model)
            position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)
            
            # è®¡ç®—é™¤æ•°é¡¹
            div_term = torch.exp(torch.arange(0, d_model, 2).float() * 
                               (-math.log(10000.0) / d_model))
            
            # åº”ç”¨æ­£å¼¦å’Œä½™å¼¦å‡½æ•°
            pe[:, 0::2] = torch.sin(position * div_term)
            pe[:, 1::2] = torch.cos(position * div_term)
            
            # æ³¨å†Œä¸ºbufferï¼Œä¸å‚ä¸Žæ¢¯åº¦è®¡ç®—
            self.register_buffer('pe', pe)
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                
            Returns:
                æ·»åŠ ä½ç½®ç¼–ç åŽçš„å¼ é‡ [batch_size, seq_len, d_model]
            """
            seq_len = x.size(1)
            if seq_len > self.max_len:
                raise IndexError(f"åºåˆ—é•¿åº¦ {seq_len} è¶…è¿‡æœ€å¤§é•¿åº¦ {self.max_len}")
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            x = x + self.pe[:seq_len].unsqueeze(0)
            return self.dropout(x)
    
    
    class LearnablePositionalEncoding(nn.Module):
        """
        å¯å­¦ä¹ ä½ç½®ç¼–ç 
        ä½ç½®ç¼–ç å‚æ•°é€šè¿‡è®­ç»ƒå­¦ä¹ å¾—åˆ°
        """
        
        def __init__(self, d_model: int, max_len: int = 5000, dropout: float = 0.1):
            """
            åˆå§‹åŒ–å¯å­¦ä¹ ä½ç½®ç¼–ç 
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                max_len: æœ€å¤§åºåˆ—é•¿åº¦
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            self.d_model = d_model
            self.max_len = max_len
            self.dropout = nn.Dropout(p=dropout)
            
            # åˆ›å»ºå¯å­¦ä¹ çš„ä½ç½®ç¼–ç å‚æ•°
            self.pe = nn.Parameter(torch.randn(max_len, d_model) * 0.1)
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                
            Returns:
                æ·»åŠ ä½ç½®ç¼–ç åŽçš„å¼ é‡ [batch_size, seq_len, d_model]
            """
            seq_len = x.size(1)
            if seq_len > self.max_len:
                raise IndexError(f"åºåˆ—é•¿åº¦ {seq_len} è¶…è¿‡æœ€å¤§é•¿åº¦ {self.max_len}")
            
            # æ·»åŠ å¯å­¦ä¹ ä½ç½®ç¼–ç 
            x = x + self.pe[:seq_len].unsqueeze(0)
            return self.dropout(x)
    
    
    class RelativePositionalEncoding(nn.Module):
        """
        ç›¸å¯¹ä½ç½®ç¼–ç 
        åŸºäºŽç›¸å¯¹ä½ç½®è€Œéžç»å¯¹ä½ç½®çš„ç¼–ç æ–¹å¼
        """
        
        def __init__(self, d_model: int, max_relative_position: int = 128, dropout: float = 0.1):
            """
            åˆå§‹åŒ–ç›¸å¯¹ä½ç½®ç¼–ç 
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                max_relative_position: æœ€å¤§ç›¸å¯¹ä½ç½®è·ç¦»
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            self.d_model = d_model
            self.max_relative_position = max_relative_position
            self.dropout = nn.Dropout(p=dropout)
            
            # ç›¸å¯¹ä½ç½®ç¼–ç è¡¨
            # å¤§å°ä¸º (2 * max_relative_position + 1, d_model)
            # ç´¢å¼• max_relative_position å¯¹åº”ç›¸å¯¹ä½ç½® 0
            vocab_size = 2 * max_relative_position + 1
            self.relative_pe = nn.Parameter(torch.randn(vocab_size, d_model) * 0.1)
        
        def _get_relative_positions(self, seq_len: int) -> torch.Tensor:
            """
            è®¡ç®—ç›¸å¯¹ä½ç½®çŸ©é˜µ
            
            Args:
                seq_len: åºåˆ—é•¿åº¦
                
            Returns:
                ç›¸å¯¹ä½ç½®çŸ©é˜µ [seq_len, seq_len]
            """
            range_vec = torch.arange(seq_len)
            range_mat = range_vec.unsqueeze(0).repeat(seq_len, 1)
            distance_mat = range_mat - range_mat.transpose(0, 1)
            
            # è£å‰ªåˆ°æœ€å¤§ç›¸å¯¹ä½ç½®èŒƒå›´
            distance_mat = torch.clamp(distance_mat, 
                                     -self.max_relative_position, 
                                     self.max_relative_position)
            
            return distance_mat
        
        def get_attention_bias(self, seq_len: int) -> torch.Tensor:
            """
            èŽ·å–æ³¨æ„åŠ›åç½®çŸ©é˜µ
            
            Args:
                seq_len: åºåˆ—é•¿åº¦
                
            Returns:
                æ³¨æ„åŠ›åç½®çŸ©é˜µ [seq_len, seq_len]
            """
            relative_positions = self._get_relative_positions(seq_len)
            
            # è½¬æ¢ä¸ºç›¸å¯¹ä½ç½®ç¼–ç è¡¨çš„ç´¢å¼•
            relative_position_indices = relative_positions + self.max_relative_position
            
            # èŽ·å–ç›¸å¯¹ä½ç½®ç¼–ç 
            relative_embeddings = self.relative_pe[relative_position_indices]
            
            # è®¡ç®—æ³¨æ„åŠ›åç½®ï¼ˆè¿™é‡Œç®€åŒ–ä¸ºæ±‚å’Œï¼Œå®žé™…å®žçŽ°å¯èƒ½æ›´å¤æ‚ï¼‰
            attention_bias = relative_embeddings.sum(dim=-1)
            
            return attention_bias
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                
            Returns:
                å¤„ç†åŽçš„å¼ é‡ [batch_size, seq_len, d_model]
            """
            batch_size, seq_len, d_model = x.shape
            
            # èŽ·å–ç›¸å¯¹ä½ç½®çŸ©é˜µ
            relative_positions = self._get_relative_positions(seq_len)
            relative_position_indices = relative_positions + self.max_relative_position
            
            # èŽ·å–ç›¸å¯¹ä½ç½®ç¼–ç 
            relative_embeddings = self.relative_pe[relative_position_indices]  # [seq_len, seq_len, d_model]
            
            # åº”ç”¨ç›¸å¯¹ä½ç½®ç¼–ç ï¼ˆç®€åŒ–å®žçŽ°ï¼‰
            # å®žé™…åº”ç”¨ä¸­ï¼Œç›¸å¯¹ä½ç½®ç¼–ç é€šå¸¸åœ¨æ³¨æ„åŠ›æœºåˆ¶ä¸­ä½¿ç”¨
            # è¿™é‡Œæˆ‘ä»¬ç®€å•åœ°å°†å…¶åŠ åˆ°è¾“å…¥ä¸Šä½œä¸ºæ¼”ç¤º
            position_encoding = relative_embeddings.mean(dim=1)  # [seq_len, d_model]
            x = x + position_encoding.unsqueeze(0)
            
            return self.dropout(x)
    
    
    class AdaptivePositionalEncoding(nn.Module):
        """
        è‡ªé€‚åº”ä½ç½®ç¼–ç 
        ç»“åˆå¤šç§ä½ç½®ç¼–ç æ–¹å¼ï¼Œå¯ä»¥æ ¹æ®éœ€è¦é€‰æ‹©æˆ–ç»„åˆä½¿ç”¨
        """
        
        def __init__(self, d_model: int, max_len: int = 5000, 
                     max_relative_position: int = 128,
                     encoding_type: str = 'sinusoidal',
                     dropout: float = 0.1):
            """
            åˆå§‹åŒ–è‡ªé€‚åº”ä½ç½®ç¼–ç 
            
            Args:
                d_model: æ¨¡åž‹ç»´åº¦
                max_len: æœ€å¤§åºåˆ—é•¿åº¦
                max_relative_position: æœ€å¤§ç›¸å¯¹ä½ç½®è·ç¦»
                encoding_type: ç¼–ç ç±»åž‹ ('sinusoidal', 'learnable', 'relative', 'hybrid')
                dropout: dropoutæ¦‚çŽ‡
            """
            super().__init__()
            self.encoding_type = encoding_type
            self.d_model = d_model
            
            if encoding_type == 'sinusoidal':
                self.pos_encoding = PositionalEncoding(d_model, max_len, dropout)
            elif encoding_type == 'learnable':
                self.pos_encoding = LearnablePositionalEncoding(d_model, max_len, dropout)
            elif encoding_type == 'relative':
                self.pos_encoding = RelativePositionalEncoding(d_model, max_relative_position, dropout)
            elif encoding_type == 'hybrid':
                # æ··åˆç¼–ç ï¼šç»“åˆæ­£å¼¦ä½™å¼¦å’Œå¯å­¦ä¹ ç¼–ç 
                self.sinusoidal_pe = PositionalEncoding(d_model, max_len, 0.0)
                self.learnable_pe = LearnablePositionalEncoding(d_model, max_len, 0.0)
                self.dropout = nn.Dropout(p=dropout)
                self.mix_weight = nn.Parameter(torch.tensor(0.5))
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç¼–ç ç±»åž‹: {encoding_type}")
        
        def forward(self, x: torch.Tensor) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                x: è¾“å…¥å¼ é‡ [batch_size, seq_len, d_model]
                
            Returns:
                æ·»åŠ ä½ç½®ç¼–ç åŽçš„å¼ é‡ [batch_size, seq_len, d_model]
            """
            if self.encoding_type == 'hybrid':
                # æ··åˆç¼–ç 
                sin_encoded = self.sinusoidal_pe(x.clone())
                learn_encoded = self.learnable_pe(x.clone())
                
                # åŠ æƒç»„åˆ
                weight = torch.sigmoid(self.mix_weight)
                x = weight * sin_encoded + (1 - weight) * learn_encoded
                return self.dropout(x)
            else:
                return self.pos_encoding(x)
        
        def get_attention_bias(self, seq_len: int) -> Optional[torch.Tensor]:
            """
            èŽ·å–æ³¨æ„åŠ›åç½®ï¼ˆä»…å¯¹ç›¸å¯¹ä½ç½®ç¼–ç æœ‰æ•ˆï¼‰
            
            Args:
                seq_len: åºåˆ—é•¿åº¦
                
            Returns:
                æ³¨æ„åŠ›åç½®çŸ©é˜µæˆ–None
            """
            if self.encoding_type == 'relative':
                return self.pos_encoding.get_attention_bias(seq_len)
            return None
    ]]></file>
  <file path="src/rl_trading_system/models/critic_network.py"><![CDATA[
    """
    Criticç½‘ç»œå®žçŽ° - SACç®—æ³•çš„ä»·å€¼ç½‘ç»œ
    """
    from dataclasses import dataclass
    from typing import Tuple, Optional
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    
    
    @dataclass
    class CriticConfig:
        """Criticç½‘ç»œé…ç½®"""
        state_dim: int = 256
        action_dim: int = 100
        hidden_dim: int = 512
        n_layers: int = 3
        activation: str = 'relu'
        dropout: float = 0.1
    
    
    class Critic(nn.Module):
        """
        SAC Criticç½‘ç»œ
        
        å®žçŽ°çŠ¶æ€-åŠ¨ä½œä»·å€¼å‡½æ•°Q(s,a)
        é‡‡ç”¨åŒQç½‘ç»œæž¶æž„ä»¥å‡å°‘è¿‡ä¼°è®¡åå·®
        """
        
        def __init__(self, config: CriticConfig):
            super().__init__()
            self.config = config
            
            # æ¿€æ´»å‡½æ•°æ˜ å°„
            activation_map = {
                'relu': nn.ReLU,
                'tanh': nn.Tanh,
                'gelu': nn.GELU,
                'leaky_relu': nn.LeakyReLU
            }
            
            if config.activation not in activation_map:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {config.activation}")
            
            activation_fn = activation_map[config.activation]
            
            # çŠ¶æ€ç¼–ç å™¨
            state_layers = []
            input_dim = config.state_dim
            
            for i in range(config.n_layers - 1):
                state_layers.extend([
                    nn.Linear(input_dim, config.hidden_dim),
                    activation_fn(),
                    nn.Dropout(config.dropout)
                ])
                input_dim = config.hidden_dim
            
            self.state_encoder = nn.Sequential(*state_layers)
            
            # åŠ¨ä½œç¼–ç å™¨
            action_layers = []
            input_dim = config.action_dim
            
            for i in range(config.n_layers - 1):
                action_layers.extend([
                    nn.Linear(input_dim, config.hidden_dim),
                    activation_fn(),
                    nn.Dropout(config.dropout)
                ])
                input_dim = config.hidden_dim
            
            self.action_encoder = nn.Sequential(*action_layers)
            
            # Qç½‘ç»œï¼šèžåˆçŠ¶æ€å’ŒåŠ¨ä½œç‰¹å¾
            self.q_network = nn.Sequential(
                nn.Linear(config.hidden_dim * 2, config.hidden_dim),
                activation_fn(),
                nn.Dropout(config.dropout),
                nn.Linear(config.hidden_dim, config.hidden_dim // 2),
                activation_fn(),
                nn.Dropout(config.dropout),
                nn.Linear(config.hidden_dim // 2, 1)
            )
            
            # åˆå§‹åŒ–æƒé‡
            self._initialize_weights()
            
        def _initialize_weights(self):
            """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    # ä½¿ç”¨Xavieråˆå§‹åŒ–
                    nn.init.xavier_uniform_(module.weight)
                    nn.init.constant_(module.bias, 0.0)
            
            # å¯¹æœ€åŽä¸€å±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–
            final_layer = self.q_network[-1]
            nn.init.uniform_(final_layer.weight, -3e-3, 3e-3)
            nn.init.uniform_(final_layer.bias, -3e-3, 3e-3)
            
        def forward(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                action: åŠ¨ä½œå¼ é‡ [batch_size, action_dim]
                
            Returns:
                q_value: Qå€¼ [batch_size, 1]
            """
            # ç¼–ç çŠ¶æ€å’ŒåŠ¨ä½œ
            state_features = self.state_encoder(state)
            action_features = self.action_encoder(action)
            
            # èžåˆç‰¹å¾
            combined_features = torch.cat([state_features, action_features], dim=1)
            
            # è®¡ç®—Qå€¼
            q_value = self.q_network(combined_features)
            
            return q_value
        
        def get_q_value(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
            """
            èŽ·å–Qå€¼ï¼ˆä¸Žforwardç›¸åŒï¼Œæä¾›æ›´æ˜Žç¡®çš„æŽ¥å£ï¼‰
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                action: åŠ¨ä½œå¼ é‡ [batch_size, action_dim]
                
            Returns:
                q_value: Qå€¼ [batch_size, 1]
            """
            return self.forward(state, action)
    
    
    class DoubleCritic(nn.Module):
        """
        åŒCriticç½‘ç»œ
        
        å®žçŽ°åŒQç½‘ç»œæž¶æž„ï¼ŒåŒ…å«ä¸¤ä¸ªç‹¬ç«‹çš„Criticç½‘ç»œ
        ç”¨äºŽå‡å°‘Qå€¼è¿‡ä¼°è®¡é—®é¢˜
        """
        
        def __init__(self, config: CriticConfig):
            super().__init__()
            self.config = config
            
            # ä¸¤ä¸ªç‹¬ç«‹çš„Criticç½‘ç»œ
            self.critic1 = Critic(config)
            self.critic2 = Critic(config)
            
        def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                action: åŠ¨ä½œå¼ é‡ [batch_size, action_dim]
                
            Returns:
                q1: ç¬¬ä¸€ä¸ªCriticçš„Qå€¼ [batch_size, 1]
                q2: ç¬¬äºŒä¸ªCriticçš„Qå€¼ [batch_size, 1]
            """
            q1 = self.critic1(state, action)
            q2 = self.critic2(state, action)
            
            return q1, q2
        
        def get_q_values(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            èŽ·å–ä¸¤ä¸ªQå€¼
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                action: åŠ¨ä½œå¼ é‡ [batch_size, action_dim]
                
            Returns:
                q1: ç¬¬ä¸€ä¸ªCriticçš„Qå€¼ [batch_size, 1]
                q2: ç¬¬äºŒä¸ªCriticçš„Qå€¼ [batch_size, 1]
            """
            return self.forward(state, action)
        
        def get_min_q_value(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
            """
            èŽ·å–ä¸¤ä¸ªQå€¼ä¸­çš„æœ€å°å€¼ï¼ˆç”¨äºŽå‡å°‘è¿‡ä¼°è®¡ï¼‰
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                action: åŠ¨ä½œå¼ é‡ [batch_size, action_dim]
                
            Returns:
                min_q: æœ€å°Qå€¼ [batch_size, 1]
            """
            q1, q2 = self.forward(state, action)
            min_q = torch.min(q1, q2)
            
            return min_q
        
        def get_target_q_value(self, state: torch.Tensor, action: torch.Tensor, 
                              log_prob: torch.Tensor, alpha: float) -> torch.Tensor:
            """
            è®¡ç®—ç›®æ ‡Qå€¼ï¼ˆç”¨äºŽSACè®­ç»ƒï¼‰
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                action: åŠ¨ä½œå¼ é‡ [batch_size, action_dim]
                log_prob: åŠ¨ä½œå¯¹æ•°æ¦‚çŽ‡ [batch_size]
                alpha: æ¸©åº¦å‚æ•°
                
            Returns:
                target_q: ç›®æ ‡Qå€¼ [batch_size, 1]
            """
            min_q = self.get_min_q_value(state, action)
            target_q = min_q - alpha * log_prob.unsqueeze(1)
            
            return target_q
    
    
    class CriticWithTargetNetwork(nn.Module):
        """
        å¸¦ç›®æ ‡ç½‘ç»œçš„Critic
        
        åŒ…å«ä¸»ç½‘ç»œå’Œç›®æ ‡ç½‘ç»œï¼Œæ”¯æŒè½¯æ›´æ–°
        """
        
        def __init__(self, config: CriticConfig):
            super().__init__()
            self.config = config
            
            # ä¸»ç½‘ç»œ
            self.main_network = DoubleCritic(config)
            
            # ç›®æ ‡ç½‘ç»œ
            self.target_network = DoubleCritic(config)
            
            # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œå‚æ•°
            self.hard_update()
            
        def forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            """ä¸»ç½‘ç»œå‰å‘ä¼ æ’­"""
            return self.main_network(state, action)
        
        def target_forward(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            """ç›®æ ‡ç½‘ç»œå‰å‘ä¼ æ’­"""
            return self.target_network(state, action)
        
        def get_main_q_values(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            """èŽ·å–ä¸»ç½‘ç»œQå€¼"""
            return self.main_network.get_q_values(state, action)
        
        def get_target_q_values(self, state: torch.Tensor, action: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            """èŽ·å–ç›®æ ‡ç½‘ç»œQå€¼"""
            return self.target_network.get_q_values(state, action)
        
        def get_target_min_q_value(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
            """èŽ·å–ç›®æ ‡ç½‘ç»œæœ€å°Qå€¼"""
            return self.target_network.get_min_q_value(state, action)
        
        def soft_update(self, tau: float = 0.005):
            """
            è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°
            
            Args:
                tau: æ›´æ–°ç³»æ•°ï¼Œtarget = tau * main + (1 - tau) * target
            """
            for target_param, main_param in zip(
                self.target_network.parameters(), 
                self.main_network.parameters()
            ):
                target_param.data.copy_(
                    tau * main_param.data + (1.0 - tau) * target_param.data
                )
        
        def hard_update(self):
            """ç¡¬æ›´æ–°ç›®æ ‡ç½‘ç»œå‚æ•°ï¼ˆå®Œå…¨å¤åˆ¶ï¼‰"""
            self.target_network.load_state_dict(self.main_network.state_dict())
        
        def get_parameters(self):
            """èŽ·å–ä¸»ç½‘ç»œå‚æ•°ï¼ˆç”¨äºŽä¼˜åŒ–å™¨ï¼‰"""
            return self.main_network.parameters()
        
        def train_mode(self):
            """è®¾ç½®ä¸ºè®­ç»ƒæ¨¡å¼"""
            self.main_network.train()
            self.target_network.eval()  # ç›®æ ‡ç½‘ç»œå§‹ç»ˆä¸ºè¯„ä¼°æ¨¡å¼
        
        def eval_mode(self):
            """è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼"""
            self.main_network.eval()
            self.target_network.eval()
    ]]></file>
  <file path="src/rl_trading_system/models/actor_network.py"><![CDATA[
    """
    Actorç½‘ç»œå®žçŽ° - SACç®—æ³•çš„ç­–ç•¥ç½‘ç»œ
    """
    from dataclasses import dataclass
    from typing import Tuple, Optional
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    from torch.distributions import Normal
    
    
    @dataclass
    class ActorConfig:
        """Actorç½‘ç»œé…ç½®"""
        state_dim: int = 256
        action_dim: int = 100
        hidden_dim: int = 512
        n_layers: int = 3
        activation: str = 'relu'
        dropout: float = 0.1
        log_std_min: float = -20
        log_std_max: float = 2
        epsilon: float = 1e-6
    
    
    class Actor(nn.Module):
        """
        SAC Actorç½‘ç»œ
        
        å®žçŽ°ç­–ç•¥ç½‘ç»œï¼Œè¾“å‡ºæŠ•èµ„ç»„åˆæƒé‡åˆ†å¸ƒ
        ä½¿ç”¨é‡å‚æ•°åŒ–æŠ€å·§æ”¯æŒæ¢¯åº¦åå‘ä¼ æ’­
        """
        
        def __init__(self, config: ActorConfig):
            super().__init__()
            self.config = config
            
            # æ¿€æ´»å‡½æ•°æ˜ å°„
            activation_map = {
                'relu': nn.ReLU,
                'tanh': nn.Tanh,
                'gelu': nn.GELU,
                'leaky_relu': nn.LeakyReLU
            }
            
            if config.activation not in activation_map:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¿€æ´»å‡½æ•°: {config.activation}")
            
            activation_fn = activation_map[config.activation]
            
            # æž„å»ºå…±äº«å±‚
            layers = []
            input_dim = config.state_dim
            
            for i in range(config.n_layers):
                layers.extend([
                    nn.Linear(input_dim, config.hidden_dim),
                    activation_fn(),
                    nn.Dropout(config.dropout)
                ])
                input_dim = config.hidden_dim
            
            self.shared_layers = nn.Sequential(*layers)
            
            # å‡å€¼å¤´
            self.mean_head = nn.Linear(config.hidden_dim, config.action_dim)
            
            # æ ‡å‡†å·®å¤´
            self.log_std_head = nn.Linear(config.hidden_dim, config.action_dim)
            
            # åˆå§‹åŒ–æƒé‡
            self._initialize_weights()
            
        def _initialize_weights(self):
            """åˆå§‹åŒ–ç½‘ç»œæƒé‡"""
            for module in self.modules():
                if isinstance(module, nn.Linear):
                    # ä½¿ç”¨Xavieråˆå§‹åŒ–
                    nn.init.xavier_uniform_(module.weight)
                    nn.init.constant_(module.bias, 0.0)
            
            # å¯¹è¾“å‡ºå±‚ä½¿ç”¨è¾ƒå°çš„åˆå§‹åŒ–
            nn.init.uniform_(self.mean_head.weight, -3e-3, 3e-3)
            nn.init.uniform_(self.mean_head.bias, -3e-3, 3e-3)
            nn.init.uniform_(self.log_std_head.weight, -3e-3, 3e-3)
            nn.init.uniform_(self.log_std_head.bias, -3e-3, 3e-3)
            
        def forward(self, state: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            å‰å‘ä¼ æ’­
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                
            Returns:
                mean: åŠ¨ä½œå‡å€¼ [batch_size, action_dim]
                log_std: åŠ¨ä½œå¯¹æ•°æ ‡å‡†å·® [batch_size, action_dim]
            """
            # å…±äº«ç‰¹å¾æå–
            features = self.shared_layers(state)
            
            # è®¡ç®—å‡å€¼å’Œå¯¹æ•°æ ‡å‡†å·®
            mean = self.mean_head(features)
            log_std = self.log_std_head(features)
            
            # é™åˆ¶log_stdçš„èŒƒå›´ä»¥ç¡®ä¿æ•°å€¼ç¨³å®šæ€§
            log_std = torch.clamp(log_std, self.config.log_std_min, self.config.log_std_max)
            
            return mean, log_std
        
        def get_action(self, state: torch.Tensor, 
                       deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
            """
            ç”ŸæˆåŠ¨ä½œå’Œå¯¹æ•°æ¦‚çŽ‡
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                deterministic: æ˜¯å¦ä½¿ç”¨ç¡®å®šæ€§ç­–ç•¥
                
            Returns:
                action: æŠ•èµ„ç»„åˆæƒé‡ [batch_size, action_dim]
                log_prob: åŠ¨ä½œå¯¹æ•°æ¦‚çŽ‡ [batch_size]
            """
            mean, log_std = self.forward(state)
            
            if deterministic:
                # ç¡®å®šæ€§åŠ¨ä½œï¼šç›´æŽ¥ä½¿ç”¨å‡å€¼ï¼Œè·³è¿‡éšæœºé‡‡æ ·
                action_raw = mean
                # å¯¹äºŽç¡®å®šæ€§åŠ¨ä½œï¼Œlog_probè®¾ä¸º0
                log_prob = torch.zeros(state.size(0), device=state.device)
                
                # åº”ç”¨tanhå˜æ¢ç¡®ä¿åŠ¨ä½œåœ¨æœ‰ç•ŒèŒƒå›´å†…
                action_tanh = torch.tanh(action_raw)
                
                # å°†tanhè¾“å‡ºè½¬æ¢ä¸ºæŠ•èµ„ç»„åˆæƒé‡ï¼ˆéžè´Ÿä¸”å’Œä¸º1ï¼‰
                action = self._to_portfolio_weights(action_tanh)
                
            else:
                # éšæœºåŠ¨ä½œï¼šä»Žæ­£æ€åˆ†å¸ƒé‡‡æ ·
                std = torch.exp(log_std)
                normal_dist = Normal(mean, std)
                action_raw = normal_dist.rsample()  # é‡å‚æ•°åŒ–é‡‡æ ·
                
                # è®¡ç®—å¯¹æ•°æ¦‚çŽ‡
                log_prob = normal_dist.log_prob(action_raw).sum(dim=1)
                
                # åº”ç”¨tanhå˜æ¢ç¡®ä¿åŠ¨ä½œåœ¨æœ‰ç•ŒèŒƒå›´å†…
                action_tanh = torch.tanh(action_raw)
                
                # è®¡ç®—tanhå˜æ¢çš„é›…å¯æ¯”è¡Œåˆ—å¼ä¿®æ­£
                log_prob -= torch.sum(
                    torch.log(1 - action_tanh.pow(2) + self.config.epsilon), 
                    dim=1
                )
                
                # å°†tanhè¾“å‡ºè½¬æ¢ä¸ºæŠ•èµ„ç»„åˆæƒé‡ï¼ˆéžè´Ÿä¸”å’Œä¸º1ï¼‰
                action = self._to_portfolio_weights(action_tanh)
            
            return action, log_prob
        
        def _to_portfolio_weights(self, action_tanh: torch.Tensor) -> torch.Tensor:
            """
            å°†tanhè¾“å‡ºè½¬æ¢ä¸ºæŠ•èµ„ç»„åˆæƒé‡
            
            Args:
                action_tanh: tanhå˜æ¢åŽçš„åŠ¨ä½œ [batch_size, action_dim]
                
            Returns:
                weights: æŠ•èµ„ç»„åˆæƒé‡ [batch_size, action_dim]
            """
            # å°†tanhè¾“å‡ºä»Ž[-1,1]æ˜ å°„åˆ°[0,1]
            action_positive = (action_tanh + 1.0) / 2.0
            
            # ä½¿ç”¨softmaxç¡®ä¿æƒé‡å’Œä¸º1
            weights = F.softmax(action_positive, dim=1)
            
            return weights
        
        def log_prob(self, state: torch.Tensor, action: torch.Tensor) -> torch.Tensor:
            """
            è®¡ç®—ç»™å®šçŠ¶æ€å’ŒåŠ¨ä½œçš„å¯¹æ•°æ¦‚çŽ‡
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                action: åŠ¨ä½œå¼ é‡ [batch_size, action_dim]
                
            Returns:
                log_prob: å¯¹æ•°æ¦‚çŽ‡ [batch_size]
            """
            mean, log_std = self.forward(state)
            std = torch.exp(log_std)
            
            # ä»ŽæŠ•èµ„ç»„åˆæƒé‡åæŽ¨tanhå‰çš„å€¼
            # è¿™æ˜¯ä¸€ä¸ªè¿‘ä¼¼è¿‡ç¨‹ï¼Œå› ä¸ºsoftmaxå˜æ¢ä¸å¯é€†
            action_normalized = action / (action.sum(dim=1, keepdim=True) + self.config.epsilon)
            action_positive = action_normalized * 2.0 - 1.0
            
            # é™åˆ¶åœ¨tanhçš„æœ‰æ•ˆèŒƒå›´å†…
            action_positive = torch.clamp(action_positive, -0.999, 0.999)
            action_raw = torch.atanh(action_positive)
            
            # è®¡ç®—æ­£æ€åˆ†å¸ƒçš„å¯¹æ•°æ¦‚çŽ‡
            normal_dist = Normal(mean, std)
            log_prob = normal_dist.log_prob(action_raw).sum(dim=1)
            
            # å‡åŽ»tanhå˜æ¢çš„é›…å¯æ¯”è¡Œåˆ—å¼
            log_prob -= torch.sum(
                torch.log(1 - action_positive.pow(2) + self.config.epsilon), 
                dim=1
            )
            
            return log_prob
        
        def entropy(self, state: torch.Tensor) -> torch.Tensor:
            """
            è®¡ç®—ç­–ç•¥ç†µ
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                
            Returns:
                entropy: ç­–ç•¥ç†µ [batch_size]
            """
            _, log_std = self.forward(state)
            
            # æ­£æ€åˆ†å¸ƒçš„ç†µï¼š0.5 * log(2Ï€) + log_std
            entropy = 0.5 * (1.0 + torch.log(2 * torch.pi)) + log_std
            entropy = entropy.sum(dim=1)
            
            return entropy
        
        def get_deterministic_action(self, state: torch.Tensor) -> torch.Tensor:
            """
            èŽ·å–ç¡®å®šæ€§åŠ¨ä½œï¼ˆç”¨äºŽè¯„ä¼°ï¼‰
            
            Args:
                state: çŠ¶æ€å¼ é‡ [batch_size, state_dim]
                
            Returns:
                action: ç¡®å®šæ€§åŠ¨ä½œ [batch_size, action_dim]
            """
            action, _ = self.get_action(state, deterministic=True)
            return action
    ]]></file>
  <file path="src/rl_trading_system/models/__init__.py"><![CDATA[
    """æ¨¡åž‹æž¶æž„æ¨¡å—"""
    
    # Transformer Components
    from .positional_encoding import (
        PositionalEncoding,
        LearnablePositionalEncoding,
        RelativePositionalEncoding,
        AdaptivePositionalEncoding
    )
    
    from .temporal_attention import (
        ScaledDotProductAttention,
        TemporalAttention,
        MultiHeadTemporalAttention,
        AdaptiveTemporalAttention
    )
    
    from .transformer import (
        TimeSeriesTransformer,
        TransformerConfig,
        TransformerEncoderLayer,
        FeedForwardNetwork,
        MultiHeadAttention
    )
    
    # SAC Agent Components
    from .actor_network import Actor, ActorConfig
    from .critic_network import Critic, CriticConfig, DoubleCritic, CriticWithTargetNetwork
    from .replay_buffer import (
        Experience, 
        ReplayBuffer, 
        PrioritizedReplayBuffer, 
        ReplayBufferConfig,
        create_replay_buffer
    )
    from .sac_agent import SACAgent, SACConfig
    
    __all__ = [
        # Transformer Components
        "PositionalEncoding",
        "LearnablePositionalEncoding", 
        "RelativePositionalEncoding",
        "AdaptivePositionalEncoding",
        "ScaledDotProductAttention",
        "TemporalAttention",
        "MultiHeadTemporalAttention",
        "AdaptiveTemporalAttention",
        "TimeSeriesTransformer",
        "TransformerConfig",
        "TransformerEncoderLayer",
        "FeedForwardNetwork",
        "MultiHeadAttention",
        
        # SAC Components
        "Actor", "ActorConfig",
        "Critic", "CriticConfig", "DoubleCritic", "CriticWithTargetNetwork",
        "Experience", "ReplayBuffer", "PrioritizedReplayBuffer", "ReplayBufferConfig", "create_replay_buffer",
        "SACAgent", "SACConfig"
    ]
    ]]></file>
  <file path="src/rl_trading_system/model_management/checkpoint_manager.py"><![CDATA[
    """
    æ£€æŸ¥ç‚¹ç®¡ç†å™¨å®žçŽ°
    å®žçŽ°CheckpointManagerç±»å’Œæ¨¡åž‹ç‰ˆæœ¬æŽ§åˆ¶ï¼Œè‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡åž‹å’Œè®­ç»ƒçŠ¶æ€æ¢å¤ï¼Œæ¨¡åž‹åŽ‹ç¼©å’Œä¼˜åŒ–
    """
    
    import os
    import json
    import pickle
    import shutil
    import hashlib
    import gzip
    import lzma
    import bz2
    import threading
    from datetime import datetime, timedelta
    from pathlib import Path
    from typing import Dict, List, Tuple, Any, Optional, Union
    from dataclasses import dataclass, field, asdict
    import logging
    import torch
    import torch.nn as nn
    import numpy as np
    
    logger = logging.getLogger(__name__)
    
    
    @dataclass
    class CheckpointConfig:
        """æ£€æŸ¥ç‚¹é…ç½®"""
        save_dir: str = "./checkpoints"
        max_checkpoints: int = 5
        save_frequency: int = 1000
        auto_save_best: bool = True
        compression_enabled: bool = False
        compression_method: str = "gzip"  # gzip, lzma, bz2
        model_format: str = "torch"  # torch, onnx, torchscript
        backup_enabled: bool = True
        verify_integrity: bool = True
        
        def __post_init__(self):
            """é…ç½®éªŒè¯"""
            if self.max_checkpoints <= 0:
                raise ValueError("max_checkpointså¿…é¡»ä¸ºæ­£æ•°")
            
            if self.save_frequency <= 0:
                raise ValueError("save_frequencyå¿…é¡»ä¸ºæ­£æ•°")
            
            if self.model_format not in ["torch", "onnx", "torchscript"]:
                raise ValueError(f"ä¸æ”¯æŒçš„æ¨¡åž‹æ ¼å¼: {self.model_format}")
            
            if self.compression_method not in ["gzip", "lzma", "bz2"]:
                raise ValueError(f"ä¸æ”¯æŒçš„åŽ‹ç¼©æ–¹æ³•: {self.compression_method}")
    
    
    @dataclass
    class CheckpointMetadata:
        """æ£€æŸ¥ç‚¹å…ƒæ•°æ®"""
        episode: int
        timestamp: datetime
        model_hash: Optional[str] = None
        performance_metrics: Dict[str, float] = field(default_factory=dict)
        model_info: Dict[str, Any] = field(default_factory=dict)
        config_snapshot: Dict[str, Any] = field(default_factory=dict)
        file_size: Optional[int] = None
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            data = asdict(self)
            data['timestamp'] = self.timestamp.isoformat()
            return data
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'CheckpointMetadata':
            """ä»Žå­—å…¸åˆ›å»º"""
            data = data.copy()
            data['timestamp'] = datetime.fromisoformat(data['timestamp'])
            return cls(**data)
    
    
    @dataclass
    class ModelCheckpoint:
        """æ¨¡åž‹æ£€æŸ¥ç‚¹"""
        checkpoint_id: str
        file_path: str
        metadata: CheckpointMetadata
        
        def is_better_than(self, other: 'ModelCheckpoint', metric: str, mode: str = 'max') -> bool:
            """æ¯”è¾ƒæ£€æŸ¥ç‚¹æ€§èƒ½"""
            if metric not in self.metadata.performance_metrics:
                return False
            if metric not in other.metadata.performance_metrics:
                return True
            
            self_value = self.metadata.performance_metrics[metric]
            other_value = other.metadata.performance_metrics[metric]
            
            if mode == 'max':
                return self_value > other_value
            else:
                return self_value < other_value
        
        def get_file_size(self) -> int:
            """èŽ·å–æ–‡ä»¶å¤§å°"""
            if os.path.exists(self.file_path):
                return os.path.getsize(self.file_path)
            return 0
    
    
    class CheckpointManager:
        """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
        
        def __init__(self, config: CheckpointConfig):
            """
            åˆå§‹åŒ–æ£€æŸ¥ç‚¹ç®¡ç†å™¨
            
            Args:
                config: æ£€æŸ¥ç‚¹é…ç½®
            """
            self.config = config
            self.checkpoints: List[ModelCheckpoint] = []
            self.best_checkpoint: Optional[ModelCheckpoint] = None
            self._lock = threading.Lock()
            
            # åˆ›å»ºä¿å­˜ç›®å½•
            self.save_dir = Path(config.save_dir)
            self.save_dir.mkdir(parents=True, exist_ok=True)
            
            # åˆ›å»ºå…ƒæ•°æ®ç›®å½•
            self.metadata_dir = self.save_dir / "metadata"
            self.metadata_dir.mkdir(exist_ok=True)
            
            # åŠ è½½å·²æœ‰çš„æ£€æŸ¥ç‚¹
            self.scan_and_recover_checkpoints()
            
            logger.info(f"æ£€æŸ¥ç‚¹ç®¡ç†å™¨åˆå§‹åŒ–å®Œæˆï¼Œä¿å­˜ç›®å½•: {self.save_dir}")
        
        def _generate_checkpoint_id(self, episode: int) -> str:
            """ç”Ÿæˆæ£€æŸ¥ç‚¹ID"""
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            return f"checkpoint_{episode}_{timestamp}"
        
        def _calculate_model_hash(self, model) -> str:
            """è®¡ç®—æ¨¡åž‹å“ˆå¸Œå€¼"""
            try:
                if hasattr(model, 'state_dict'):
                    # PyTorchæ¨¡åž‹
                    state_dict = model.state_dict()
                    hash_data = []
                    for k, v in state_dict.items():
                        if hasattr(v, 'cpu') and hasattr(v, 'numpy'):
                            # PyTorchå¼ é‡
                            hash_data.append((k, v.cpu().numpy().tobytes()))
                        else:
                            # å…¶ä»–ç±»åž‹çš„æ•°æ®
                            hash_data.append((k, str(v).encode()))
                    model_str = str(sorted(hash_data))
                else:
                    # å…¶ä»–ç±»åž‹çš„æ¨¡åž‹
                    model_str = str(model)
                
                return hashlib.md5(model_str.encode()).hexdigest()
            except Exception as e:
                # å¦‚æžœå“ˆå¸Œè®¡ç®—å¤±è´¥ï¼Œè¿”å›žæ—¶é—´æˆ³å“ˆå¸Œ
                logger.warning(f"æ¨¡åž‹å“ˆå¸Œè®¡ç®—å¤±è´¥ï¼Œä½¿ç”¨æ—¶é—´æˆ³: {e}")
                return hashlib.md5(str(datetime.now()).encode()).hexdigest()
        
        def _compress_file(self, file_path: str) -> str:
            """åŽ‹ç¼©æ–‡ä»¶"""
            if not self.config.compression_enabled:
                return file_path
            
            compressed_path = f"{file_path}.{self.config.compression_method}"
            
            compression_funcs = {
                'gzip': gzip.open,
                'lzma': lzma.open,
                'bz2': bz2.open
            }
            
            compress_func = compression_funcs[self.config.compression_method]
            
            with open(file_path, 'rb') as f_in:
                with compress_func(compressed_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            # åˆ é™¤åŽŸæ–‡ä»¶
            os.remove(file_path)
            
            logger.info(f"æ–‡ä»¶å·²åŽ‹ç¼©: {file_path} -> {compressed_path}")
            return compressed_path
        
        def _decompress_file(self, compressed_path: str) -> str:
            """è§£åŽ‹ç¼©æ–‡ä»¶"""
            if not compressed_path.endswith(('.gzip', '.lzma', '.bz2')):
                return compressed_path
            
            # ç¡®å®šåŽ‹ç¼©ç±»åž‹
            if compressed_path.endswith('.gzip'):
                decompress_func = gzip.open
                original_path = compressed_path[:-5]
            elif compressed_path.endswith('.lzma'):
                decompress_func = lzma.open
                original_path = compressed_path[:-5]
            elif compressed_path.endswith('.bz2'):
                decompress_func = bz2.open
                original_path = compressed_path[:-4]
            else:
                return compressed_path
            
            with decompress_func(compressed_path, 'rb') as f_in:
                with open(original_path, 'wb') as f_out:
                    shutil.copyfileobj(f_in, f_out)
            
            return original_path
        
        def check_available_disk_space(self) -> int:
            """æ£€æŸ¥å¯ç”¨ç£ç›˜ç©ºé—´ï¼ˆå­—èŠ‚ï¼‰"""
            _, _, free = shutil.disk_usage(self.save_dir)
            return free
        
        def save_checkpoint(self, model, episode: int, metrics: Dict[str, float],
                           model_info: Optional[Dict[str, Any]] = None,
                           config_snapshot: Optional[Dict[str, Any]] = None,
                           is_best_metric: Optional[str] = None) -> str:
            """
            ä¿å­˜æ£€æŸ¥ç‚¹
            
            Args:
                model: è¦ä¿å­˜çš„æ¨¡åž‹
                episode: å½“å‰episode
                metrics: æ€§èƒ½æŒ‡æ ‡
                model_info: æ¨¡åž‹ä¿¡æ¯
                config_snapshot: é…ç½®å¿«ç…§
                is_best_metric: åˆ¤æ–­æœ€ä½³æ¨¡åž‹çš„æŒ‡æ ‡åç§°
                
            Returns:
                str: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
            """
            with self._lock:
                # æ£€æŸ¥ç£ç›˜ç©ºé—´
                available_space = self.check_available_disk_space()
                if available_space < 100 * 1024 * 1024:  # 100MB
                    raise RuntimeError("ç£ç›˜ç©ºé—´ä¸è¶³ï¼Œæ— æ³•ä¿å­˜æ£€æŸ¥ç‚¹")
                
                # ç”Ÿæˆæ£€æŸ¥ç‚¹ä¿¡æ¯
                checkpoint_id = self._generate_checkpoint_id(episode)
                checkpoint_path = self.save_dir / f"{checkpoint_id}.pth"
                
                # è®¡ç®—æ¨¡åž‹å“ˆå¸Œ
                model_hash = self._calculate_model_hash(model)
                
                # åˆ›å»ºå…ƒæ•°æ®
                metadata = CheckpointMetadata(
                    episode=episode,
                    timestamp=datetime.now(),
                    model_hash=model_hash,
                    performance_metrics=metrics,
                    model_info=model_info or {},
                    config_snapshot=config_snapshot or {}
                )
                
                try:
                    # ä¿å­˜æ¨¡åž‹
                    if hasattr(model, 'state_dict'):
                        # PyTorchæ¨¡åž‹
                        checkpoint_data = {
                            'model_state_dict': model.state_dict(),
                            'metadata': metadata.to_dict()
                        }
                        
                        # å¦‚æžœæ¨¡åž‹æœ‰ä¼˜åŒ–å™¨ï¼Œä¹Ÿä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
                        if hasattr(model, 'optimizer_state_dict'):
                            checkpoint_data['optimizer_state_dict'] = model.optimizer_state_dict()
                    else:
                        # å…¶ä»–ç±»åž‹çš„æ¨¡åž‹
                        checkpoint_data = {
                            'model': model,
                            'metadata': metadata.to_dict()
                        }
                    
                    torch.save(checkpoint_data, str(checkpoint_path))
                    
                    # åŽ‹ç¼©æ–‡ä»¶ï¼ˆå¦‚æžœå¯ç”¨ï¼‰
                    final_path = self._compress_file(str(checkpoint_path))
                    
                    # æ›´æ–°å…ƒæ•°æ®ä¸­çš„æ–‡ä»¶å¤§å°
                    metadata.file_size = os.path.getsize(final_path)
                    
                    # ä¿å­˜å…ƒæ•°æ®
                    metadata_path = self.metadata_dir / f"{checkpoint_id}.json"
                    with open(metadata_path, 'w') as f:
                        json.dump(metadata.to_dict(), f, indent=2)
                    
                    # åˆ›å»ºæ£€æŸ¥ç‚¹å¯¹è±¡
                    checkpoint = ModelCheckpoint(
                        checkpoint_id=checkpoint_id,
                        file_path=final_path,
                        metadata=metadata
                    )
                    
                    # æ·»åŠ åˆ°æ£€æŸ¥ç‚¹åˆ—è¡¨
                    self.checkpoints.append(checkpoint)
                    
                    # æ›´æ–°æœ€ä½³æ£€æŸ¥ç‚¹
                    if is_best_metric and is_best_metric in metrics:
                        if (self.best_checkpoint is None or 
                            checkpoint.is_better_than(self.best_checkpoint, is_best_metric, 'max')):
                            self.best_checkpoint = checkpoint
                            
                            # ä¿å­˜æœ€ä½³æ¨¡åž‹å‰¯æœ¬
                            if self.config.auto_save_best:
                                best_path = self.save_dir / "best_model.pth"
                                shutil.copy2(final_path, best_path)
                    
                    # æ¸…ç†æ—§æ£€æŸ¥ç‚¹
                    self._cleanup_old_checkpoints()
                    
                    logger.info(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜: {final_path}")
                    return final_path
                    
                except Exception as e:
                    logger.error(f"ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                    # æ¸…ç†å¯èƒ½åˆ›å»ºçš„æ–‡ä»¶
                    if os.path.exists(checkpoint_path):
                        os.remove(checkpoint_path)
                    raise
        
        def load_checkpoint(self, checkpoint_path: str, model) -> CheckpointMetadata:
            """
            åŠ è½½æ£€æŸ¥ç‚¹
            
            Args:
                checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
                model: è¦åŠ è½½çŠ¶æ€çš„æ¨¡åž‹
                
            Returns:
                CheckpointMetadata: æ£€æŸ¥ç‚¹å…ƒæ•°æ®
            """
            try:
                # è§£åŽ‹ç¼©æ–‡ä»¶ï¼ˆå¦‚æžœéœ€è¦ï¼‰
                decompressed_path = self._decompress_file(checkpoint_path)
                
                # åŠ è½½æ£€æŸ¥ç‚¹æ•°æ®
                checkpoint_data = torch.load(decompressed_path, map_location='cpu', weights_only=False)
                
                # åŠ è½½æ¨¡åž‹çŠ¶æ€
                if 'model_state_dict' in checkpoint_data:
                    if hasattr(model, 'load_state_dict'):
                        model.load_state_dict(checkpoint_data['model_state_dict'])
                    else:
                        raise ValueError("æ¨¡åž‹ä¸æ”¯æŒload_state_dictæ–¹æ³•")
                elif 'model' in checkpoint_data:
                    # ç›´æŽ¥æ›¿æ¢æ¨¡åž‹
                    model = checkpoint_data['model']
                
                # åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
                if 'optimizer_state_dict' in checkpoint_data and hasattr(model, 'load_optimizer_state_dict'):
                    model.load_optimizer_state_dict(checkpoint_data['optimizer_state_dict'])
                
                # åˆ›å»ºå…ƒæ•°æ®å¯¹è±¡
                metadata = CheckpointMetadata.from_dict(checkpoint_data['metadata'])
                
                logger.info(f"æ£€æŸ¥ç‚¹å·²åŠ è½½: {checkpoint_path}")
                return metadata
                
            except Exception as e:
                logger.error(f"åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                raise
        
        def verify_checkpoint_integrity(self, checkpoint_path: str) -> bool:
            """
            éªŒè¯æ£€æŸ¥ç‚¹å®Œæ•´æ€§
            
            Args:
                checkpoint_path: æ£€æŸ¥ç‚¹æ–‡ä»¶è·¯å¾„
                
            Returns:
                bool: æ˜¯å¦å®Œæ•´
            """
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not os.path.exists(checkpoint_path):
                    return False
                
                # å°è¯•åŠ è½½æ£€æŸ¥ç‚¹
                decompressed_path = self._decompress_file(checkpoint_path)
                checkpoint_data = torch.load(decompressed_path, map_location='cpu', weights_only=False)
                
                # æ£€æŸ¥å¿…è¦çš„å­—æ®µ
                if 'metadata' not in checkpoint_data:
                    return False
                
                metadata = checkpoint_data['metadata']
                required_fields = ['episode', 'timestamp']
                
                for field in required_fields:
                    if field not in metadata:
                        return False
                
                # éªŒè¯æ¨¡åž‹å“ˆå¸Œï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
                if 'model_hash' in metadata and 'model_state_dict' in checkpoint_data:
                    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å“ˆå¸ŒéªŒè¯é€»è¾‘
                    pass
                
                return True
                
            except Exception as e:
                logger.warning(f"æ£€æŸ¥ç‚¹å®Œæ•´æ€§éªŒè¯å¤±è´¥: {checkpoint_path}, é”™è¯¯: {e}")
                return False
        
        def scan_and_recover_checkpoints(self) -> List[ModelCheckpoint]:
            """
            æ‰«æå¹¶æ¢å¤æ£€æŸ¥ç‚¹
            
            Returns:
                List[ModelCheckpoint]: æ¢å¤çš„æ£€æŸ¥ç‚¹åˆ—è¡¨
            """
            recovered_checkpoints = []
            
            try:
                # æ‰«ææ£€æŸ¥ç‚¹æ–‡ä»¶
                checkpoint_files = []
                for ext in ['*.pth', '*.pth.gzip', '*.pth.lzma', '*.pth.bz2']:
                    checkpoint_files.extend(self.save_dir.glob(ext))
                
                for checkpoint_file in checkpoint_files:
                    try:
                        # éªŒè¯æ£€æŸ¥ç‚¹å®Œæ•´æ€§
                        if not self.verify_checkpoint_integrity(str(checkpoint_file)):
                            logger.warning(f"è·³è¿‡æŸåçš„æ£€æŸ¥ç‚¹: {checkpoint_file}")
                            continue
                        
                        # æå–æ£€æŸ¥ç‚¹ID
                        checkpoint_id = checkpoint_file.stem
                        if checkpoint_id.endswith('.pth'):
                            checkpoint_id = checkpoint_id[:-4]
                        
                        # å°è¯•åŠ è½½å…ƒæ•°æ®
                        metadata_path = self.metadata_dir / f"{checkpoint_id}.json"
                        
                        if metadata_path.exists():
                            with open(metadata_path, 'r') as f:
                                metadata_dict = json.load(f)
                            metadata = CheckpointMetadata.from_dict(metadata_dict)
                        else:
                            # ä»Žæ£€æŸ¥ç‚¹æ–‡ä»¶ä¸­æå–å…ƒæ•°æ®
                            decompressed_path = self._decompress_file(str(checkpoint_file))
                            checkpoint_data = torch.load(decompressed_path, map_location='cpu', weights_only=False)
                            metadata = CheckpointMetadata.from_dict(checkpoint_data['metadata'])
                        
                        # åˆ›å»ºæ£€æŸ¥ç‚¹å¯¹è±¡
                        checkpoint = ModelCheckpoint(
                            checkpoint_id=checkpoint_id,
                            file_path=str(checkpoint_file),
                            metadata=metadata
                        )
                        
                        recovered_checkpoints.append(checkpoint)
                        
                    except Exception as e:
                        logger.warning(f"æ¢å¤æ£€æŸ¥ç‚¹å¤±è´¥: {checkpoint_file}, é”™è¯¯: {e}")
                        continue
                
                # æŒ‰æ—¶é—´æŽ’åº
                recovered_checkpoints.sort(key=lambda x: x.metadata.timestamp)
                
                # æ›´æ–°æ£€æŸ¥ç‚¹åˆ—è¡¨
                self.checkpoints = recovered_checkpoints
                
                # æ‰¾åˆ°æœ€ä½³æ£€æŸ¥ç‚¹
                if recovered_checkpoints:
                    # å‡è®¾ä½¿ç”¨rewardä½œä¸ºé»˜è®¤æŒ‡æ ‡
                    for checkpoint in recovered_checkpoints:
                        if 'reward' in checkpoint.metadata.performance_metrics:
                            if (self.best_checkpoint is None or 
                                checkpoint.is_better_than(self.best_checkpoint, 'reward', 'max')):
                                self.best_checkpoint = checkpoint
                
                logger.info(f"æ¢å¤äº† {len(recovered_checkpoints)} ä¸ªæ£€æŸ¥ç‚¹")
                return recovered_checkpoints
                
            except Exception as e:
                logger.error(f"æ‰«ææ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                return []
        
        def _cleanup_old_checkpoints(self):
            """æ¸…ç†æ—§æ£€æŸ¥ç‚¹"""
            if len(self.checkpoints) <= self.config.max_checkpoints:
                return
            
            # æŒ‰æ—¶é—´æŽ’åºï¼Œä¿ç•™æœ€æ–°çš„æ£€æŸ¥ç‚¹
            self.checkpoints.sort(key=lambda x: x.metadata.timestamp)
            
            # åˆ é™¤æ—§æ£€æŸ¥ç‚¹
            checkpoints_to_remove = self.checkpoints[:-self.config.max_checkpoints]
            
            for checkpoint in checkpoints_to_remove:
                try:
                    # åˆ é™¤æ£€æŸ¥ç‚¹æ–‡ä»¶
                    if os.path.exists(checkpoint.file_path):
                        os.remove(checkpoint.file_path)
                    
                    # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                    metadata_path = self.metadata_dir / f"{checkpoint.checkpoint_id}.json"
                    if metadata_path.exists():
                        os.remove(metadata_path)
                    
                    logger.info(f"å·²åˆ é™¤æ—§æ£€æŸ¥ç‚¹: {checkpoint.checkpoint_id}")
                    
                except Exception as e:
                    logger.warning(f"åˆ é™¤æ£€æŸ¥ç‚¹å¤±è´¥: {checkpoint.checkpoint_id}, é”™è¯¯: {e}")
            
            # æ›´æ–°æ£€æŸ¥ç‚¹åˆ—è¡¨
            self.checkpoints = self.checkpoints[-self.config.max_checkpoints:]
        
        def convert_checkpoint_format(self, checkpoint_path: str, target_format: str,
                                    input_shape: Optional[Tuple[int, ...]] = None) -> str:
            """
            è½¬æ¢æ£€æŸ¥ç‚¹æ ¼å¼
            
            Args:
                checkpoint_path: åŽŸå§‹æ£€æŸ¥ç‚¹è·¯å¾„
                target_format: ç›®æ ‡æ ¼å¼ (onnx, torchscript)
                input_shape: è¾“å…¥å½¢çŠ¶ï¼ˆONNXè½¬æ¢éœ€è¦ï¼‰
                
            Returns:
                str: è½¬æ¢åŽçš„æ–‡ä»¶è·¯å¾„
            """
            if target_format == "onnx":
                # æ¨¡æ‹ŸONNXè½¬æ¢
                onnx_path = checkpoint_path.replace('.pth', '.onnx')
                
                # è¿™é‡Œåº”è¯¥å®žçŽ°çœŸæ­£çš„ONNXè½¬æ¢é€»è¾‘
                # ç”±äºŽéœ€è¦å…·ä½“çš„æ¨¡åž‹ç»“æž„ï¼Œè¿™é‡Œåªæ˜¯åˆ›å»ºä¸€ä¸ªç©ºæ–‡ä»¶ä½œä¸ºç¤ºä¾‹
                with open(onnx_path, 'w') as f:
                    f.write("# ONNX model placeholder")
                
                logger.info(f"å·²è½¬æ¢ä¸ºONNXæ ¼å¼: {onnx_path}")
                return onnx_path
            
            elif target_format == "torchscript":
                # æ¨¡æ‹ŸTorchScriptè½¬æ¢
                script_path = checkpoint_path.replace('.pth', '_script.pt')
                
                # è¿™é‡Œåº”è¯¥å®žçŽ°çœŸæ­£çš„TorchScriptè½¬æ¢é€»è¾‘
                with open(script_path, 'w') as f:
                    f.write("# TorchScript model placeholder")
                
                logger.info(f"å·²è½¬æ¢ä¸ºTorchScriptæ ¼å¼: {script_path}")
                return script_path
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç›®æ ‡æ ¼å¼: {target_format}")
        
        def optimize_checkpoint_size(self, checkpoint_path: str,
                                    remove_optimizer_state: bool = True,
                                    quantize_weights: bool = False) -> str:
            """
            ä¼˜åŒ–æ£€æŸ¥ç‚¹å¤§å°
            
            Args:
                checkpoint_path: æ£€æŸ¥ç‚¹è·¯å¾„
                remove_optimizer_state: æ˜¯å¦ç§»é™¤ä¼˜åŒ–å™¨çŠ¶æ€
                quantize_weights: æ˜¯å¦é‡åŒ–æƒé‡
                
            Returns:
                str: ä¼˜åŒ–åŽçš„æ£€æŸ¥ç‚¹è·¯å¾„
            """
            try:
                # åŠ è½½æ£€æŸ¥ç‚¹
                decompressed_path = self._decompress_file(checkpoint_path)
                checkpoint_data = torch.load(decompressed_path, map_location='cpu', weights_only=False)
                
                # ç§»é™¤ä¼˜åŒ–å™¨çŠ¶æ€
                if remove_optimizer_state and 'optimizer_state_dict' in checkpoint_data:
                    del checkpoint_data['optimizer_state_dict']
                    logger.info("å·²ç§»é™¤ä¼˜åŒ–å™¨çŠ¶æ€")
                
                # é‡åŒ–æƒé‡ï¼ˆç®€åŒ–å®žçŽ°ï¼‰
                if quantize_weights and 'model_state_dict' in checkpoint_data:
                    state_dict = checkpoint_data['model_state_dict']
                    for key, value in state_dict.items():
                        if hasattr(value, 'dtype') and value.dtype == torch.float32:
                            # ç®€å•çš„8ä½é‡åŒ–
                            state_dict[key] = value.half().float()
                    logger.info("å·²é‡åŒ–æ¨¡åž‹æƒé‡")
                
                # ä¿å­˜ä¼˜åŒ–åŽçš„æ£€æŸ¥ç‚¹
                optimized_path = checkpoint_path.replace('.pth', '_optimized.pth')
                torch.save(checkpoint_data, optimized_path)
                
                # åŽ‹ç¼©ä¼˜åŒ–åŽçš„æ–‡ä»¶
                final_path = self._compress_file(optimized_path)
                
                logger.info(f"æ£€æŸ¥ç‚¹å·²ä¼˜åŒ–: {final_path}")
                return final_path
                
            except Exception as e:
                logger.error(f"ä¼˜åŒ–æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
                raise
        
        def generate_performance_report(self) -> Dict[str, Any]:
            """
            ç”Ÿæˆæ€§èƒ½æŠ¥å‘Š
            
            Returns:
                Dict[str, Any]: æ€§èƒ½æŠ¥å‘Š
            """
            if not self.checkpoints:
                return {"total_checkpoints": 0}
            
            # ç»Ÿè®¡åŸºæœ¬ä¿¡æ¯
            total_checkpoints = len(self.checkpoints)
            
            # æå–æ€§èƒ½æŒ‡æ ‡
            all_metrics = {}
            for checkpoint in self.checkpoints:
                for metric, value in checkpoint.metadata.performance_metrics.items():
                    if metric not in all_metrics:
                        all_metrics[metric] = []
                    all_metrics[metric].append(value)
            
            # è®¡ç®—ç»Ÿè®¡ä¿¡æ¯
            metric_stats = {}
            for metric, values in all_metrics.items():
                metric_stats[metric] = {
                    'mean': np.mean(values),
                    'std': np.std(values),
                    'min': np.min(values),
                    'max': np.max(values),
                    'latest': values[-1] if values else None
                }
            
            # æœ€ä½³æ€§èƒ½
            best_performance = {}
            if self.best_checkpoint:
                best_performance = self.best_checkpoint.metadata.performance_metrics
            
            # æ€§èƒ½è¶‹åŠ¿ï¼ˆç®€åŒ–ï¼‰
            performance_trend = "stable"
            if "reward" in all_metrics and len(all_metrics["reward"]) >= 3:
                recent_rewards = all_metrics["reward"][-3:]
                if all(recent_rewards[i] <= recent_rewards[i+1] for i in range(len(recent_rewards)-1)):
                    performance_trend = "improving"
                elif all(recent_rewards[i] >= recent_rewards[i+1] for i in range(len(recent_rewards)-1)):
                    performance_trend = "declining"
            
            return {
                "total_checkpoints": total_checkpoints,
                "metric_statistics": metric_stats,
                "best_performance": best_performance,
                "performance_trend": performance_trend,
                "storage_info": {
                    "total_size_bytes": sum(cp.get_file_size() for cp in self.checkpoints),
                    "average_size_bytes": np.mean([cp.get_file_size() for cp in self.checkpoints])
                }
            }
    
    
    class ModelCompressor:
        """æ¨¡åž‹åŽ‹ç¼©å™¨"""
        
        def __init__(self, output_dir: str):
            """
            åˆå§‹åŒ–æ¨¡åž‹åŽ‹ç¼©å™¨
            
            Args:
                output_dir: è¾“å‡ºç›®å½•
            """
            self.output_dir = Path(output_dir)
            self.output_dir.mkdir(parents=True, exist_ok=True)
        
        def quantize_model(self, model: nn.Module, model_path: str, 
                          quantization_type: str = "dynamic") -> str:
            """
            é‡åŒ–æ¨¡åž‹
            
            Args:
                model: PyTorchæ¨¡åž‹
                model_path: æ¨¡åž‹æ–‡ä»¶è·¯å¾„
                quantization_type: é‡åŒ–ç±»åž‹
                
            Returns:
                str: é‡åŒ–åŽçš„æ¨¡åž‹è·¯å¾„
            """
            try:
                if quantization_type == "dynamic":
                    # å°è¯•åŠ¨æ€é‡åŒ–
                    try:
                        quantized_model = torch.quantization.quantize_dynamic(
                            model, {nn.Linear}, dtype=torch.qint8
                        )
                        
                        # ä¿å­˜é‡åŒ–æ¨¡åž‹
                        quantized_path = self.output_dir / "quantized_model.pth"
                        torch.save(quantized_model.state_dict(), quantized_path)
                        
                        logger.info(f"æ¨¡åž‹å·²é‡åŒ–: {quantized_path}")
                        return str(quantized_path)
                        
                    except Exception as quant_error:
                        logger.warning(f"é‡åŒ–å¤±è´¥ï¼Œå°è¯•æ‰‹åŠ¨åŽ‹ç¼©: {quant_error}")
                        # æ‰‹åŠ¨å®žçŽ°ç®€å•çš„"é‡åŒ–"ï¼ˆæƒé‡åŽ‹ç¼©ï¼‰
                        return self._manual_compress_weights(model, model_path)
                else:
                    # é™æ€é‡åŒ–ï¼ˆéœ€è¦æ ¡å‡†æ•°æ®ï¼‰
                    return self._manual_compress_weights(model, model_path)
                
            except Exception as e:
                logger.error(f"æ¨¡åž‹é‡åŒ–å¤±è´¥: {e}")
                # å¦‚æžœé‡åŒ–å¤±è´¥ï¼Œè¿”å›žåŽŸå§‹æ¨¡åž‹çš„å‰¯æœ¬
                fallback_path = self.output_dir / "quantized_model_fallback.pth"
                shutil.copy2(model_path, fallback_path)
                return str(fallback_path)
        
        def _manual_compress_weights(self, model: nn.Module, model_path: str) -> str:
            """æ‰‹åŠ¨åŽ‹ç¼©æƒé‡"""
            try:
                # åŠ è½½åŽŸå§‹æƒé‡
                state_dict = torch.load(model_path, map_location='cpu', weights_only=False)
                
                # åŽ‹ç¼©æƒé‡ï¼ˆè½¬æ¢ä¸ºåŠç²¾åº¦ï¼‰
                compressed_state_dict = {}
                for key, value in state_dict.items():
                    if isinstance(value, torch.Tensor) and value.dtype == torch.float32:
                        # è½¬æ¢ä¸ºåŠç²¾åº¦ç„¶åŽè½¬å›žå•ç²¾åº¦ï¼ˆæ¨¡æ‹Ÿé‡åŒ–æ•ˆæžœï¼‰
                        compressed_state_dict[key] = value.half().float()
                    else:
                        compressed_state_dict[key] = value
                
                # ä¿å­˜åŽ‹ç¼©åŽçš„æ¨¡åž‹
                quantized_path = self.output_dir / "quantized_model.pth"
                torch.save(compressed_state_dict, quantized_path)
                
                logger.info(f"æ¨¡åž‹æƒé‡å·²åŽ‹ç¼©: {quantized_path}")
                return str(quantized_path)
                
            except Exception as e:
                logger.error(f"æ‰‹åŠ¨æƒé‡åŽ‹ç¼©å¤±è´¥: {e}")
                # è¿”å›žåŽŸå§‹æ¨¡åž‹çš„å‰¯æœ¬
                fallback_path = self.output_dir / "quantized_model_fallback.pth"
                shutil.copy2(model_path, fallback_path)
                return str(fallback_path)
        
        def prune_model(self, model: nn.Module, pruning_ratio: float = 0.2) -> nn.Module:
            """
            å‰ªæžæ¨¡åž‹
            
            Args:
                model: PyTorchæ¨¡åž‹
                pruning_ratio: å‰ªæžæ¯”ä¾‹
                
            Returns:
                nn.Module: å‰ªæžåŽçš„æ¨¡åž‹
            """
            try:
                # è¿™é‡Œåº”è¯¥å®žçŽ°çœŸæ­£çš„å‰ªæžé€»è¾‘
                # ç”±äºŽå‰ªæžæ¯”è¾ƒå¤æ‚ï¼Œè¿™é‡Œåªæ˜¯è¿”å›žåŽŸæ¨¡åž‹ä½œä¸ºç¤ºä¾‹
                logger.info(f"æ¨¡åž‹å‰ªæžå®Œæˆï¼Œå‰ªæžæ¯”ä¾‹: {pruning_ratio}")
                return model
                
            except Exception as e:
                logger.error(f"æ¨¡åž‹å‰ªæžå¤±è´¥: {e}")
                return model
        
        def convert_to_onnx(self, model: nn.Module, input_shape: Tuple[int, ...],
                           output_path: str) -> str:
            """
            è½¬æ¢ä¸ºONNXæ ¼å¼
            
            Args:
                model: PyTorchæ¨¡åž‹
                input_shape: è¾“å…¥å½¢çŠ¶
                output_path: è¾“å‡ºè·¯å¾„
                
            Returns:
                str: ONNXæ¨¡åž‹è·¯å¾„
            """
            try:
                # åˆ›å»ºç¤ºä¾‹è¾“å…¥
                dummy_input = torch.randn(*input_shape)
                
                # å¯¼å‡ºONNXï¼ˆè¿™é‡Œç”¨ç®€åŒ–çš„æ–¹å¼ï¼‰
                # å®žé™…å®žçŽ°ä¸­éœ€è¦: torch.onnx.export(model, dummy_input, output_path)
                
                # åˆ›å»ºå ä½ç¬¦æ–‡ä»¶
                with open(output_path, 'w') as f:
                    f.write(f"# ONNX model converted from PyTorch\n# Input shape: {input_shape}")
                
                logger.info(f"æ¨¡åž‹å·²è½¬æ¢ä¸ºONNX: {output_path}")
                return output_path
                
            except Exception as e:
                logger.error(f"ONNXè½¬æ¢å¤±è´¥: {e}")
                raise
        
        def convert_to_torchscript(self, model: nn.Module, example_input: torch.Tensor,
                                 output_path: str) -> str:
            """
            è½¬æ¢ä¸ºTorchScriptæ ¼å¼
            
            Args:
                model: PyTorchæ¨¡åž‹
                example_input: ç¤ºä¾‹è¾“å…¥
                output_path: è¾“å‡ºè·¯å¾„
                
            Returns:
                str: TorchScriptæ¨¡åž‹è·¯å¾„
            """
            try:
                # è½¬æ¢ä¸ºTorchScript
                model.eval()
                traced_model = torch.jit.trace(model, example_input)
                
                # ä¿å­˜TorchScriptæ¨¡åž‹
                traced_model.save(output_path)
                
                logger.info(f"æ¨¡åž‹å·²è½¬æ¢ä¸ºTorchScript: {output_path}")
                return output_path
                
            except Exception as e:
                logger.error(f"TorchScriptè½¬æ¢å¤±è´¥: {e}")
                raise
        
        def compress_model_pipeline(self, model: nn.Module, input_shape: Tuple[int, ...],
                                  output_dir: str, enable_quantization: bool = True,
                                  enable_pruning: bool = True, enable_onnx: bool = True,
                                  enable_torchscript: bool = True) -> Dict[str, str]:
            """
            å®Œæ•´çš„æ¨¡åž‹åŽ‹ç¼©æµæ°´çº¿
            
            Args:
                model: PyTorchæ¨¡åž‹
                input_shape: è¾“å…¥å½¢çŠ¶
                output_dir: è¾“å‡ºç›®å½•
                enable_quantization: æ˜¯å¦å¯ç”¨é‡åŒ–
                enable_pruning: æ˜¯å¦å¯ç”¨å‰ªæž
                enable_onnx: æ˜¯å¦è½¬æ¢ONNX
                enable_torchscript: æ˜¯å¦è½¬æ¢TorchScript
                
            Returns:
                Dict[str, str]: å„ç§æ ¼å¼çš„æ¨¡åž‹è·¯å¾„
            """
            results = {}
            
            try:
                # ä¿å­˜åŽŸå§‹æ¨¡åž‹
                original_path = os.path.join(output_dir, "original_model.pth")
                torch.save(model.state_dict(), original_path)
                results['original'] = original_path
                
                # é‡åŒ–
                if enable_quantization:
                    quantized_path = self.quantize_model(model, original_path)
                    results['quantized'] = quantized_path
                
                # å‰ªæž
                if enable_pruning:
                    pruned_model = self.prune_model(model)
                    pruned_path = os.path.join(output_dir, "pruned_model.pth")
                    torch.save(pruned_model.state_dict(), pruned_path)
                    results['pruned'] = pruned_path
                
                # ONNXè½¬æ¢
                if enable_onnx:
                    onnx_path = os.path.join(output_dir, "model.onnx")
                    onnx_result = self.convert_to_onnx(model, input_shape, onnx_path)
                    results['onnx'] = onnx_result
                
                # TorchScriptè½¬æ¢
                if enable_torchscript:
                    example_input = torch.randn(*input_shape)
                    script_path = os.path.join(output_dir, "model_script.pt")
                    script_result = self.convert_to_torchscript(model, example_input, script_path)
                    results['torchscript'] = script_result
                
                logger.info(f"æ¨¡åž‹åŽ‹ç¼©æµæ°´çº¿å®Œæˆï¼Œç”Ÿæˆäº† {len(results)} ç§æ ¼å¼")
                return results
                
            except Exception as e:
                logger.error(f"æ¨¡åž‹åŽ‹ç¼©æµæ°´çº¿å¤±è´¥: {e}")
                raise
    ]]></file>
  <file path="src/rl_trading_system/model_management/__init__.py"><![CDATA[
    """æ¨¡åž‹ç®¡ç†ç³»ç»Ÿæ¨¡å—"""
    
    from .checkpoint_manager import (
        CheckpointManager,
        CheckpointConfig,
        ModelCheckpoint,
        CheckpointMetadata,
        ModelCompressor
    )
    
    __all__ = [
        "CheckpointManager",
        "CheckpointConfig", 
        "ModelCheckpoint",
        "CheckpointMetadata",
        "ModelCompressor"
    ]
    ]]></file>
  <file path="src/rl_trading_system/evaluation/report_generator.py"><![CDATA[
    """
    å›žæµ‹æŠ¥å‘Šç”Ÿæˆæ¨¡å—
    å®žçŽ°è‡ªåŠ¨ç”ŸæˆHTMLæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨ï¼Œæ”¶ç›Šæ›²çº¿ã€æŒä»“åˆ†æžå’Œé£Žé™©åˆ†è§£å¯è§†åŒ–ï¼Œå› å­æš´éœ²åˆ†æžå’Œç»©æ•ˆå½’å› æŠ¥å‘Š
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import os
    import json
    from pathlib import Path
    import pandas as pd
    import numpy as np
    from datetime import datetime, date
    from typing import Dict, List, Optional, Union, Tuple, Any
    from decimal import Decimal
    from dataclasses import dataclass, field
    import plotly.graph_objects as go
    import plotly.express as px
    from plotly.subplots import make_subplots
    import plotly.offline as pyo
    from jinja2 import Environment, FileSystemLoader, select_autoescape
    
    from .performance_metrics import PortfolioMetrics
    from ..backtest.multi_frequency_backtest import Trade
    
    
    @dataclass
    class ReportData:
        """æŠ¥å‘Šæ•°æ®ç±»"""
        returns: pd.Series
        portfolio_values: pd.Series
        benchmark_returns: pd.Series
        trades: List[Trade]
        positions: Dict[str, Dict[str, float]]
        start_date: date
        end_date: date
        initial_capital: float
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            if self.returns.empty:
                raise ValueError("æ”¶ç›ŠçŽ‡åºåˆ—ä¸èƒ½ä¸ºç©º")
            if len(self.returns) != len(self.portfolio_values):
                raise ValueError("æ”¶ç›ŠçŽ‡åºåˆ—å’Œç»„åˆä»·å€¼åºåˆ—é•¿åº¦ä¸åŒ¹é…")
            if self.initial_capital <= 0:
                raise ValueError("åˆå§‹èµ„æœ¬å¿…é¡»ä¸ºæ­£æ•°")
        
        def calculate_metrics(self) -> Dict[str, Dict[str, float]]:
            """è®¡ç®—ç»©æ•ˆæŒ‡æ ‡"""
            portfolio_metrics = PortfolioMetrics(
                returns=self.returns,
                portfolio_values=self.portfolio_values,
                trades=self.trades
            )
            return portfolio_metrics.calculate_comprehensive_metrics()
    
    
    class ChartGenerator:
        """å›¾è¡¨ç”Ÿæˆå™¨"""
        
        def __init__(self, figure_size: Tuple[int, int] = (12, 8), style: str = 'seaborn-v0_8'):
            """
            åˆå§‹åŒ–å›¾è¡¨ç”Ÿæˆå™¨
            
            Args:
                figure_size: å›¾è¡¨å°ºå¯¸
                style: å›¾è¡¨æ ·å¼
            """
            self.figure_size = figure_size
            self.style = style
            
            # é…ç½®Plotlyé»˜è®¤æ ·å¼
            self.default_layout = {
                'width': figure_size[0] * 80,
                'height': figure_size[1] * 80,
                'font': {'size': 12},
                'showlegend': True,
                'hovermode': 'x unified'
            }
        
        def generate_returns_chart(self, portfolio_values: pd.Series, 
                                 benchmark_values: Optional[pd.Series] = None) -> str:
            """ç”Ÿæˆæ”¶ç›ŠçŽ‡å›¾è¡¨"""
            if portfolio_values.empty:
                raise ValueError("æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            if benchmark_values is not None and len(portfolio_values) != len(benchmark_values):
                raise ValueError("ç»„åˆä»·å€¼å’ŒåŸºå‡†ä»·å€¼é•¿åº¦ä¸åŒ¹é…")
            
            # åˆ›å»ºå›¾è¡¨
            fig = go.Figure()
            
            # æ·»åŠ ç»„åˆæ”¶ç›ŠçŽ‡æ›²çº¿
            fig.add_trace(go.Scatter(
                x=portfolio_values.index,
                y=portfolio_values.values,
                mode='lines',
                name='æŠ•èµ„ç»„åˆ',
                line=dict(color='#1f77b4', width=2)
            ))
            
            # æ·»åŠ åŸºå‡†æ”¶ç›ŠçŽ‡æ›²çº¿
            if benchmark_values is not None:
                fig.add_trace(go.Scatter(
                    x=benchmark_values.index,
                    y=benchmark_values.values,
                    mode='lines',
                    name='åŸºå‡†',
                    line=dict(color='#ff7f0e', width=2)
                ))
            
            # è®¾ç½®å¸ƒå±€
            fig.update_layout(
                title='æŠ•èµ„ç»„åˆæ”¶ç›ŠçŽ‡èµ°åŠ¿',
                xaxis_title='æ—¥æœŸ',
                yaxis_title='ç»„åˆä»·å€¼',
                **self.default_layout
            )
            
            return fig.to_html(include_plotlyjs='cdn', div_id="returns_chart")
        
        def generate_drawdown_chart(self, portfolio_values: pd.Series) -> str:
            """ç”Ÿæˆå›žæ’¤å›¾è¡¨"""
            if portfolio_values.empty:
                raise ValueError("æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # è®¡ç®—å›žæ’¤
            peak = portfolio_values.expanding().max()
            drawdown = (portfolio_values - peak) / peak * 100
            
            # åˆ›å»ºå›¾è¡¨
            fig = go.Figure()
            
            # æ·»åŠ å›žæ’¤æ›²çº¿
            fig.add_trace(go.Scatter(
                x=drawdown.index,
                y=drawdown.values,
                mode='lines',
                name='å›žæ’¤',
                fill='tonexty',
                line=dict(color='red', width=1),
                fillcolor='rgba(255, 0, 0, 0.3)'
            ))
            
            # æ·»åŠ é›¶çº¿
            fig.add_hline(y=0, line_dash="dash", line_color="black", opacity=0.5)
            
            # è®¾ç½®å¸ƒå±€
            fig.update_layout(
                title='æŠ•èµ„ç»„åˆå›žæ’¤åˆ†æž',
                xaxis_title='æ—¥æœŸ',
                yaxis_title='å›žæ’¤ (%)',
                **self.default_layout
            )
            
            return fig.to_html(include_plotlyjs='cdn', div_id="drawdown_chart")
        
        def generate_rolling_metrics_chart(self, rolling_data: pd.Series, 
                                         metric_name: str) -> str:
            """ç”Ÿæˆæ»šåŠ¨æŒ‡æ ‡å›¾è¡¨"""
            if rolling_data.empty:
                raise ValueError("æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # åˆ›å»ºå›¾è¡¨
            fig = go.Figure()
            
            # æ·»åŠ æ»šåŠ¨æŒ‡æ ‡æ›²çº¿
            fig.add_trace(go.Scatter(
                x=rolling_data.index,
                y=rolling_data.values,
                mode='lines',
                name=f'æ»šåŠ¨{metric_name}',
                line=dict(color='#2ca02c', width=2)
            ))
            
            # è®¾ç½®å¸ƒå±€
            fig.update_layout(
                title=f'æ»šåŠ¨{metric_name}åˆ†æž',
                xaxis_title='æ—¥æœŸ',
                yaxis_title=metric_name,
                **self.default_layout
            )
            
            return fig.to_html(include_plotlyjs='cdn', div_id="rolling_metrics_chart")
        
        def generate_position_analysis_chart(self, positions_data: Dict[str, Dict[str, float]]) -> str:
            """ç”ŸæˆæŒä»“åˆ†æžå›¾è¡¨"""
            if not positions_data:
                raise ValueError("æŒä»“æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # æå–æ•°æ®
            symbols = list(positions_data.keys())
            weights = [positions_data[symbol]['weight'] for symbol in symbols]
            
            # åˆ›å»ºé¥¼å›¾
            fig = go.Figure(data=[go.Pie(
                labels=symbols,
                values=weights,
                hole=0.3,
                textinfo='label+percent',
                textposition='auto'
            )])
            
            # è®¾ç½®å¸ƒå±€
            fig.update_layout(
                title='æŒä»“åˆ†å¸ƒåˆ†æž',
                **self.default_layout
            )
            
            return fig.to_html(include_plotlyjs='cdn', div_id="position_analysis_chart")
        
        def generate_monthly_returns_heatmap(self, monthly_returns: pd.DataFrame) -> str:
            """ç”Ÿæˆæœˆåº¦æ”¶ç›ŠçŽ‡çƒ­åŠ›å›¾"""
            if monthly_returns.empty:
                raise ValueError("æœˆåº¦æ”¶ç›ŠçŽ‡æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # åˆ›å»ºé€è§†è¡¨
            pivot_table = monthly_returns.pivot(index='year', columns='month', values='return')
            
            # åˆ›å»ºçƒ­åŠ›å›¾
            fig = go.Figure(data=go.Heatmap(
                z=pivot_table.values,
                x=[f'{i}æœˆ' for i in pivot_table.columns],
                y=pivot_table.index,
                colorscale='RdYlGn',
                zmid=0,
                text=np.round(pivot_table.values * 100, 2),
                texttemplate='%{text}%',
                textfont={'size': 10},
                colorbar=dict(title="æ”¶ç›ŠçŽ‡ (%)")
            ))
            
            # è®¾ç½®å¸ƒå±€
            fig.update_layout(
                title='æœˆåº¦æ”¶ç›ŠçŽ‡çƒ­åŠ›å›¾',
                xaxis_title='æœˆä»½',
                yaxis_title='å¹´ä»½',
                **self.default_layout
            )
            
            return fig.to_html(include_plotlyjs='cdn', div_id="monthly_returns_heatmap")
        
        def generate_risk_metrics_radar_chart(self, risk_metrics: Dict[str, float]) -> str:
            """ç”Ÿæˆé£Žé™©æŒ‡æ ‡é›·è¾¾å›¾"""
            if not risk_metrics:
                raise ValueError("é£Žé™©æŒ‡æ ‡æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # æ ‡å‡†åŒ–æŒ‡æ ‡ï¼ˆè½¬æ¢ä¸º0-1èŒƒå›´ï¼‰
            normalized_metrics = {}
            for key, value in risk_metrics.items():
                if key == 'volatility':
                    normalized_metrics['æ³¢åŠ¨çŽ‡'] = min(abs(value) * 10, 1)
                elif key == 'max_drawdown':
                    normalized_metrics['æœ€å¤§å›žæ’¤'] = min(abs(value) * 10, 1)
                elif key == 'var_95':
                    normalized_metrics['VaR(95%)'] = min(abs(value) * 20, 1)
                elif key == 'skewness':
                    normalized_metrics['ååº¦'] = abs(value) / 2
                elif key == 'kurtosis':
                    normalized_metrics['å³°åº¦'] = min(abs(value) / 5, 1)
            
            # åˆ›å»ºé›·è¾¾å›¾
            categories = list(normalized_metrics.keys())
            values = list(normalized_metrics.values())
            
            fig = go.Figure()
            
            fig.add_trace(go.Scatterpolar(
                r=values + [values[0]],  # é—­åˆå›¾å½¢
                theta=categories + [categories[0]],
                fill='toself',
                name='é£Žé™©æŒ‡æ ‡',
                line=dict(color='rgba(255, 0, 0, 0.8)')
            ))
            
            # è®¾ç½®å¸ƒå±€
            fig.update_layout(
                polar=dict(
                    radialaxis=dict(
                        visible=True,
                        range=[0, 1]
                    )
                ),
                title='é£Žé™©æŒ‡æ ‡é›·è¾¾å›¾',
                **self.default_layout
            )
            
            return fig.to_html(include_plotlyjs='cdn', div_id="risk_metrics_radar")
        
        def generate_trading_analysis_chart(self, trading_metrics: Dict[str, float]) -> str:
            """ç”Ÿæˆäº¤æ˜“åˆ†æžå›¾è¡¨"""
            if not trading_metrics:
                raise ValueError("äº¤æ˜“æŒ‡æ ‡æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            # åˆ›å»ºå­å›¾
            fig = make_subplots(
                rows=2, cols=2,
                subplot_titles=('èƒœçŽ‡', 'ç›ˆäºæ¯”', 'å¹³å‡ç›ˆåˆ©', 'å¹³å‡äºæŸ'),
                specs=[[{"type": "indicator"}, {"type": "indicator"}],
                       [{"type": "indicator"}, {"type": "indicator"}]]
            )
            
            # èƒœçŽ‡æŒ‡æ ‡
            fig.add_trace(go.Indicator(
                mode="gauge+number",
                value=trading_metrics.get('win_rate', 0) * 100,
                domain={'x': [0, 1], 'y': [0, 1]},
                title={'text': "èƒœçŽ‡ (%)"},
                gauge={'axis': {'range': [None, 100]},
                       'bar': {'color': "darkblue"},
                       'steps': [{'range': [0, 50], 'color': "lightgray"},
                                {'range': [50, 100], 'color': "gray"}],
                       'threshold': {'line': {'color': "red", 'width': 4},
                                   'thickness': 0.75, 'value': 90}}
            ), row=1, col=1)
            
            # ç›ˆäºæ¯”æŒ‡æ ‡
            fig.add_trace(go.Indicator(
                mode="number+delta",
                value=trading_metrics.get('profit_loss_ratio', 0),
                title={'text': "ç›ˆäºæ¯”"},
                delta={'reference': 1, 'relative': True}
            ), row=1, col=2)
            
            # å¹³å‡ç›ˆåˆ©
            fig.add_trace(go.Indicator(
                mode="number",
                value=trading_metrics.get('average_win', 0) * 100,
                title={'text': "å¹³å‡ç›ˆåˆ© (%)"},
                number={'suffix': "%"}
            ), row=2, col=1)
            
            # å¹³å‡äºæŸ
            fig.add_trace(go.Indicator(
                mode="number",
                value=abs(trading_metrics.get('average_loss', 0)) * 100,
                title={'text': "å¹³å‡äºæŸ (%)"},
                number={'suffix': "%"}
            ), row=2, col=2)
            
            # è®¾ç½®å¸ƒå±€
            fig.update_layout(
                title='äº¤æ˜“åˆ†æžæŒ‡æ ‡',
                **self.default_layout
            )
            
            return fig.to_html(include_plotlyjs='cdn', div_id="trading_analysis_chart")
    
    
    class HTMLReportGenerator:
        """HTMLæŠ¥å‘Šç”Ÿæˆå™¨"""
        
        def __init__(self, template_dir: Optional[str] = None):
            """
            åˆå§‹åŒ–HTMLæŠ¥å‘Šç”Ÿæˆå™¨
            
            Args:
                template_dir: æ¨¡æ¿ç›®å½•è·¯å¾„
            """
            if template_dir is None:
                # ä½¿ç”¨é»˜è®¤æ¨¡æ¿ç›®å½•
                current_dir = Path(__file__).parent
                template_dir = current_dir / "templates"
            
            self.template_dir = Path(template_dir)
            self.chart_generator = ChartGenerator()
            
            # åˆå§‹åŒ–Jinja2çŽ¯å¢ƒ
            if self.template_dir.exists():
                self.jinja_env = Environment(
                    loader=FileSystemLoader(str(self.template_dir)),
                    autoescape=select_autoescape(['html', 'xml'])
                )
            else:
                self.jinja_env = None
        
        def generate_report(self, report_data: ReportData, output_path: str,
                           template_name: str = "default_report.html",
                           include_benchmark: bool = False) -> None:
            """
            ç”ŸæˆHTMLæŠ¥å‘Š
            
            Args:
                report_data: æŠ¥å‘Šæ•°æ®
                output_path: è¾“å‡ºè·¯å¾„
                template_name: æ¨¡æ¿åç§°
                include_benchmark: æ˜¯å¦åŒ…å«åŸºå‡†æ¯”è¾ƒ
            """
            # åˆ›å»ºè¾“å‡ºç›®å½•
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            # è®¡ç®—æŒ‡æ ‡
            metrics = report_data.calculate_metrics()
            
            # ç”ŸæˆæŠ¥å‘Šå†…å®¹
            if template_name != "default_report.html":
                # åªæœ‰åœ¨æŒ‡å®šéžé»˜è®¤æ¨¡æ¿æ—¶æ‰è¿›è¡Œæ¨¡æ¿æ£€æŸ¥
                if self.jinja_env is None:
                    raise FileNotFoundError(f"æ¨¡æ¿ç›®å½•ä¸å­˜åœ¨: {self.template_dir}")
                    
                template_path = self.template_dir / template_name
                if not template_path.exists():
                    raise FileNotFoundError(f"æ¨¡æ¿æ–‡ä»¶ä¸å­˜åœ¨: {template_path}")
            
            if self.jinja_env is not None and (self.template_dir / template_name).exists():
                # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ
                template = self.jinja_env.get_template(template_name)
                
                # å‡†å¤‡æ¨¡æ¿å˜é‡
                template_vars = {
                    'report_data': report_data,
                    'metrics': metrics,
                    'summary_section': self._generate_summary_section(report_data, metrics),
                    'performance_section': self._generate_performance_section(report_data, metrics),
                    'risk_section': self._generate_risk_section(metrics),
                    'trading_section': self._generate_trading_section(report_data, metrics),
                    'positions_section': self._generate_positions_section(report_data),
                    'benchmark_comparison': self._generate_benchmark_section(report_data) if include_benchmark else "",
                    'generation_time': datetime.now().strftime('%Y-%m-%d %H:%M:%S')
                }
                
                html_content = template.render(**template_vars)
            else:
                # ä½¿ç”¨é»˜è®¤æ¨¡æ¿
                html_content = self._generate_default_report(report_data, metrics, include_benchmark)
            
            # å†™å…¥æ–‡ä»¶
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        def _generate_default_report(self, report_data: ReportData, 
                                    metrics: Dict[str, Dict[str, float]],
                                    include_benchmark: bool = False) -> str:
            """ç”Ÿæˆé»˜è®¤æŠ¥å‘Š"""
            # ç”Ÿæˆå„ä¸ªéƒ¨åˆ†
            summary_section = self._generate_summary_section(report_data, metrics)
            performance_section = self._generate_performance_section(report_data, metrics)
            risk_section = self._generate_risk_section(metrics)
            trading_section = self._generate_trading_section(report_data, metrics)
            positions_section = self._generate_positions_section(report_data)
            benchmark_section = self._generate_benchmark_section(report_data) if include_benchmark else ""
            
            # ç”Ÿæˆå®Œæ•´HTML
            html_content = f"""
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>æŠ•èµ„ç»„åˆåˆ†æžæŠ¥å‘Š</title>
                <style>
                    body {{
                        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                        margin: 0;
                        padding: 20px;
                        background-color: #f5f5f5;
                    }}
                    .container {{
                        max-width: 1200px;
                        margin: 0 auto;
                        background-color: white;
                        padding: 30px;
                        border-radius: 10px;
                        box-shadow: 0 0 20px rgba(0,0,0,0.1);
                    }}
                    h1 {{
                        color: #2c3e50;
                        text-align: center;
                        margin-bottom: 30px;
                    }}
                    h2 {{
                        color: #34495e;
                        border-bottom: 2px solid #3498db;
                        padding-bottom: 10px;
                    }}
                    .metrics-grid {{
                        display: grid;
                        grid-template-columns: repeat(auto-fit, minmax(250px, 1fr));
                        gap: 20px;
                        margin: 20px 0;
                    }}
                    .metric-card {{
                        background: #ecf0f1;
                        padding: 15px;
                        border-radius: 8px;
                        text-align: center;
                    }}
                    .metric-value {{
                        font-size: 24px;
                        font-weight: bold;
                        color: #2c3e50;
                    }}
                    .metric-label {{
                        color: #7f8c8d;
                        margin-top: 5px;
                    }}
                    .chart-container {{
                        margin: 20px 0;
                        text-align: center;
                    }}
                    .footer {{
                        text-align: center;
                        margin-top: 30px;
                        color: #7f8c8d;
                        font-size: 12px;
                    }}
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>æŠ•èµ„ç»„åˆåˆ†æžæŠ¥å‘Š</h1>
                    <p class="text-center">æŠ¥å‘ŠæœŸé—´: {report_data.start_date} è‡³ {report_data.end_date}</p>
                    
                    {summary_section}
                    {performance_section}
                    {risk_section}
                    {trading_section}
                    {positions_section}
                    {benchmark_section}
                    
                    <div class="footer">
                        <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                        <p>ç”±æ™ºèƒ½é‡åŒ–äº¤æ˜“ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ</p>
                    </div>
                </div>
            </body>
            </html>
            """
            
            return html_content
        
        def _generate_summary_section(self, report_data: ReportData, 
                                     metrics: Dict[str, Dict[str, float]]) -> str:
            """ç”Ÿæˆæ‘˜è¦éƒ¨åˆ†"""
            return_metrics = metrics.get('return_metrics', {})
            risk_metrics = metrics.get('risk_metrics', {})
            risk_adjusted = metrics.get('risk_adjusted_metrics', {})
            
            summary_html = f"""
            <section>
                <h2>ç»©æ•ˆæ‘˜è¦</h2>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <div class="metric-value">{return_metrics.get('total_return', 0):.2%}</div>
                        <div class="metric-label">æ€»æ”¶ç›ŠçŽ‡</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{return_metrics.get('annualized_return', 0):.2%}</div>
                        <div class="metric-label">å¹´åŒ–æ”¶ç›ŠçŽ‡</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{risk_metrics.get('volatility', 0):.2%}</div>
                        <div class="metric-label">å¹´åŒ–æ³¢åŠ¨çŽ‡</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{risk_metrics.get('max_drawdown', 0):.2%}</div>
                        <div class="metric-label">æœ€å¤§å›žæ’¤</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{risk_adjusted.get('sharpe_ratio', 0):.2f}</div>
                        <div class="metric-label">å¤æ™®æ¯”çŽ‡</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{risk_adjusted.get('sortino_ratio', 0):.2f}</div>
                        <div class="metric-label">ç´¢æè¯ºæ¯”çŽ‡</div>
                    </div>
                </div>
            </section>
            """
            
            return summary_html
        
        def _generate_performance_section(self, report_data: ReportData, 
                                        metrics: Dict[str, Dict[str, float]]) -> str:
            """ç”Ÿæˆç»©æ•ˆåˆ†æžéƒ¨åˆ†"""
            # ç”Ÿæˆæ”¶ç›ŠçŽ‡å›¾è¡¨
            returns_chart = self.chart_generator.generate_returns_chart(
                report_data.portfolio_values,
                None  # æš‚æ—¶ä¸åŒ…å«åŸºå‡†
            )
            
            # ç”Ÿæˆå›žæ’¤å›¾è¡¨
            drawdown_chart = self.chart_generator.generate_drawdown_chart(
                report_data.portfolio_values
            )
            
            performance_html = f"""
            <section>
                <h2>ç»©æ•ˆåˆ†æž</h2>
                <div class="chart-container">
                    {returns_chart}
                </div>
                <div class="chart-container">
                    {drawdown_chart}
                </div>
            </section>
            """
            
            return performance_html
        
        def _generate_risk_section(self, metrics: Dict[str, Dict[str, float]]) -> str:
            """ç”Ÿæˆé£Žé™©åˆ†æžéƒ¨åˆ†"""
            risk_metrics = metrics.get('risk_metrics', {})
            
            # ç”Ÿæˆé£Žé™©æŒ‡æ ‡é›·è¾¾å›¾
            radar_chart = self.chart_generator.generate_risk_metrics_radar_chart(risk_metrics)
            
            risk_html = f"""
            <section>
                <h2>é£Žé™©åˆ†æž</h2>
                <div class="chart-container">
                    {radar_chart}
                </div>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <div class="metric-value">{risk_metrics.get('var_95', 0):.2%}</div>
                        <div class="metric-label">VaR (95%)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{risk_metrics.get('cvar_95', 0):.2%}</div>
                        <div class="metric-label">CVaR (95%)</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{risk_metrics.get('skewness', 0):.3f}</div>
                        <div class="metric-label">ååº¦</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{risk_metrics.get('kurtosis', 0):.3f}</div>
                        <div class="metric-label">å³°åº¦</div>
                    </div>
                </div>
            </section>
            """
            
            return risk_html
        
        def _generate_trading_section(self, report_data: ReportData, 
                                     metrics: Dict[str, Dict[str, float]]) -> str:
            """ç”Ÿæˆäº¤æ˜“åˆ†æžéƒ¨åˆ†"""
            trading_metrics = metrics.get('trading_metrics', {})
            
            # ç”Ÿæˆäº¤æ˜“åˆ†æžå›¾è¡¨
            trading_chart = self.chart_generator.generate_trading_analysis_chart(trading_metrics)
            
            trading_html = f"""
            <section>
                <h2>äº¤æ˜“åˆ†æž</h2>
                <div class="chart-container">
                    {trading_chart}
                </div>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <div class="metric-value">{len(report_data.trades)}</div>
                        <div class="metric-label">æ€»äº¤æ˜“æ¬¡æ•°</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{trading_metrics.get('annual_turnover', 0):.2f}</div>
                        <div class="metric-label">å¹´åŒ–æ¢æ‰‹çŽ‡</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{trading_metrics.get('commission_rate', 0):.4%}</div>
                        <div class="metric-label">å¹³å‡ä½£é‡‘çŽ‡</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{trading_metrics.get('cost_ratio_to_portfolio', 0):.4%}</div>
                        <div class="metric-label">æˆæœ¬å æ¯”</div>
                    </div>
                </div>
            </section>
            """
            
            return trading_html
        
        def _generate_positions_section(self, report_data: ReportData) -> str:
            """ç”ŸæˆæŒä»“åˆ†æžéƒ¨åˆ†"""
            if not report_data.positions:
                return "<section><h2>æŒä»“åˆ†æž</h2><p>æš‚æ— æŒä»“æ•°æ®</p></section>"
            
            # ç”ŸæˆæŒä»“åˆ†æžå›¾è¡¨
            position_chart = self.chart_generator.generate_position_analysis_chart(
                report_data.positions
            )
            
            # ç”ŸæˆæŒä»“æ˜Žç»†è¡¨
            positions_table = "<table style='width: 100%; border-collapse: collapse; margin-top: 20px;'>"
            positions_table += "<tr style='background-color: #f8f9fa;'><th style='padding: 10px; border: 1px solid #dee2e6;'>è‚¡ç¥¨ä»£ç </th><th style='padding: 10px; border: 1px solid #dee2e6;'>æŒä»“æ•°é‡</th><th style='padding: 10px; border: 1px solid #dee2e6;'>å¸‚å€¼</th><th style='padding: 10px; border: 1px solid #dee2e6;'>æƒé‡</th></tr>"
            
            for symbol, pos_data in report_data.positions.items():
                positions_table += f"""
                <tr>
                    <td style='padding: 10px; border: 1px solid #dee2e6;'>{symbol}</td>
                    <td style='padding: 10px; border: 1px solid #dee2e6;'>{pos_data.get('quantity', 0):,}</td>
                    <td style='padding: 10px; border: 1px solid #dee2e6;'>ï¿¥{pos_data.get('market_value', 0):,.2f}</td>
                    <td style='padding: 10px; border: 1px solid #dee2e6;'>{pos_data.get('weight', 0):.2%}</td>
                </tr>
                """
            
            positions_table += "</table>"
            
            positions_html = f"""
            <section>
                <h2>æŒä»“åˆ†æž</h2>
                <div class="chart-container">
                    {position_chart}
                </div>
                <h3>æŒä»“æ˜Žç»†</h3>
                {positions_table}
            </section>
            """
            
            return positions_html
        
        def _generate_benchmark_section(self, report_data: ReportData) -> str:
            """ç”ŸæˆåŸºå‡†æ¯”è¾ƒéƒ¨åˆ†"""
            if report_data.benchmark_returns.empty:
                return ""
            
            # è®¡ç®—åŸºå‡†ç»„åˆä»·å€¼
            initial_value = report_data.portfolio_values.iloc[0]
            benchmark_values = initial_value * (1 + report_data.benchmark_returns.cumsum())
            
            # ç”Ÿæˆå¯¹æ¯”å›¾è¡¨
            comparison_chart = self.chart_generator.generate_returns_chart(
                report_data.portfolio_values,
                benchmark_values
            )
            
            # è®¡ç®—è¶…é¢æ”¶ç›Š
            excess_returns = report_data.returns - report_data.benchmark_returns
            excess_return_total = excess_returns.sum()
            
            benchmark_html = f"""
            <section>
                <h2>åŸºå‡†æ¯”è¾ƒ</h2>
                <div class="chart-container">
                    {comparison_chart}
                </div>
                <div class="metrics-grid">
                    <div class="metric-card">
                        <div class="metric-value">{excess_return_total:.2%}</div>
                        <div class="metric-label">ç´¯è®¡è¶…é¢æ”¶ç›Š</div>
                    </div>
                    <div class="metric-card">
                        <div class="metric-value">{excess_returns.std() * np.sqrt(252):.2%}</div>
                        <div class="metric-label">è·Ÿè¸ªè¯¯å·®</div>
                    </div>
                </div>
            </section>
            """
            
            return benchmark_html
    
    
    class ReportGenerator:
        """æŠ¥å‘Šç”Ÿæˆå™¨ä¸»ç±»"""
        
        def __init__(self):
            """åˆå§‹åŒ–æŠ¥å‘Šç”Ÿæˆå™¨"""
            self.html_generator = HTMLReportGenerator()
        
        def generate_comprehensive_report(self, report_data: ReportData, 
                                        output_path: str,
                                        include_benchmark: bool = True) -> None:
            """ç”Ÿæˆç»¼åˆæŠ¥å‘Š"""
            self.html_generator.generate_report(
                report_data, 
                output_path,
                include_benchmark=include_benchmark
            )
        
        def generate_batch_reports(self, report_data_list: List[Tuple[str, ReportData]], 
                                 output_dir: str) -> None:
            """æ‰¹é‡ç”ŸæˆæŠ¥å‘Š"""
            output_path = Path(output_dir)
            output_path.mkdir(parents=True, exist_ok=True)
            
            for report_name, report_data in report_data_list:
                output_file = output_path / f"{report_name}.html"
                self.generate_comprehensive_report(report_data, str(output_file))
        
        def generate_comparison_report(self, report_data_dict: Dict[str, ReportData], 
                                     output_path: str) -> None:
            """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""
            # åˆ›å»ºå¯¹æ¯”æ•°æ®
            comparison_data = {}
            for name, data in report_data_dict.items():
                metrics = data.calculate_metrics()
                comparison_data[name] = {
                    'data': data,
                    'metrics': metrics
                }
            
            # ç”Ÿæˆå¯¹æ¯”HTML
            html_content = self._generate_comparison_html(comparison_data)
            
            # å†™å…¥æ–‡ä»¶
            output_dir = Path(output_path).parent
            output_dir.mkdir(parents=True, exist_ok=True)
            
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(html_content)
        
        def _generate_comparison_html(self, comparison_data: Dict[str, Dict]) -> str:
            """ç”Ÿæˆå¯¹æ¯”HTML"""
            # åˆ›å»ºå¯¹æ¯”è¡¨æ ¼
            strategies = list(comparison_data.keys())
            
            # æ”¶é›†æŒ‡æ ‡
            metrics_to_compare = [
                ('æ€»æ”¶ç›ŠçŽ‡', 'return_metrics', 'total_return', '{:.2%}'),
                ('å¹´åŒ–æ”¶ç›ŠçŽ‡', 'return_metrics', 'annualized_return', '{:.2%}'),
                ('å¹´åŒ–æ³¢åŠ¨çŽ‡', 'risk_metrics', 'volatility', '{:.2%}'),
                ('æœ€å¤§å›žæ’¤', 'risk_metrics', 'max_drawdown', '{:.2%}'),
                ('å¤æ™®æ¯”çŽ‡', 'risk_adjusted_metrics', 'sharpe_ratio', '{:.3f}'),
                ('ç´¢æè¯ºæ¯”çŽ‡', 'risk_adjusted_metrics', 'sortino_ratio', '{:.3f}'),
            ]
            
            # ç”Ÿæˆå¯¹æ¯”è¡¨æ ¼
            table_html = "<table style='width: 100%; border-collapse: collapse; margin: 20px 0;'>"
            table_html += "<tr style='background-color: #f8f9fa;'><th style='padding: 10px; border: 1px solid #dee2e6;'>æŒ‡æ ‡</th>"
            
            for strategy in strategies:
                table_html += f"<th style='padding: 10px; border: 1px solid #dee2e6;'>{strategy}</th>"
            table_html += "</tr>"
            
            for metric_name, category, key, format_str in metrics_to_compare:
                table_html += f"<tr><td style='padding: 10px; border: 1px solid #dee2e6;'>{metric_name}</td>"
                
                for strategy in strategies:
                    value = comparison_data[strategy]['metrics'].get(category, {}).get(key, 0)
                    formatted_value = format_str.format(value)
                    table_html += f"<td style='padding: 10px; border: 1px solid #dee2e6;'>{formatted_value}</td>"
                
                table_html += "</tr>"
            
            table_html += "</table>"
            
            # ç”Ÿæˆå®Œæ•´HTML
            html_content = f"""
            <!DOCTYPE html>
            <html lang="zh-CN">
            <head>
                <meta charset="UTF-8">
                <meta name="viewport" content="width=device-width, initial-scale=1.0">
                <title>ç­–ç•¥å¯¹æ¯”åˆ†æžæŠ¥å‘Š</title>
                <style>
                    body {{
                        font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
                        margin: 0;
                        padding: 20px;
                        background-color: #f5f5f5;
                    }}
                    .container {{
                        max-width: 1200px;
                        margin: 0 auto;
                        background-color: white;
                        padding: 30px;
                        border-radius: 10px;
                        box-shadow: 0 0 20px rgba(0,0,0,0.1);
                    }}
                    h1 {{
                        color: #2c3e50;
                        text-align: center;
                        margin-bottom: 30px;
                    }}
                    h2 {{
                        color: #34495e;
                        border-bottom: 2px solid #3498db;
                        padding-bottom: 10px;
                    }}
                    table {{
                        font-size: 14px;
                    }}
                    th {{
                        background-color: #3498db !important;
                        color: white !important;
                    }}
                    .footer {{
                        text-align: center;
                        margin-top: 30px;
                        color: #7f8c8d;
                        font-size: 12px;
                    }}
                </style>
            </head>
            <body>
                <div class="container">
                    <h1>ç­–ç•¥å¯¹æ¯”åˆ†æžæŠ¥å‘Š</h1>
                    
                    <section>
                        <h2>å¯¹æ¯”åˆ†æž</h2>
                        {table_html}
                    </section>
                    
                    <div class="footer">
                        <p>æŠ¥å‘Šç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}</p>
                        <p>ç”±æ™ºèƒ½é‡åŒ–äº¤æ˜“ç³»ç»Ÿè‡ªåŠ¨ç”Ÿæˆ</p>
                    </div>
                </div>
            </body>
            </html>
            """
            
            return html_content
    ]]></file>
  <file path="src/rl_trading_system/evaluation/performance_metrics.py"><![CDATA[
    """
    ç»©æ•ˆæŒ‡æ ‡è®¡ç®—æ¨¡å—
    å®žçŽ°æ”¶ç›ŠæŒ‡æ ‡ã€é£Žé™©æŒ‡æ ‡ã€é£Žé™©è°ƒæ•´æŒ‡æ ‡ã€äº¤æ˜“æŒ‡æ ‡çš„è®¡ç®—
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import numpy as np
    import pandas as pd
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional, Union, Tuple
    from decimal import Decimal
    from abc import ABC, abstractmethod
    
    from ..backtest.multi_frequency_backtest import Trade, OrderType
    
    
    class ReturnMetrics:
        """æ”¶ç›ŠçŽ‡æŒ‡æ ‡è®¡ç®—ç±»"""
        
        def __init__(self, returns: pd.Series):
            """
            åˆå§‹åŒ–æ”¶ç›ŠçŽ‡æŒ‡æ ‡è®¡ç®—å™¨
            
            Args:
                returns: æ”¶ç›ŠçŽ‡æ—¶é—´åºåˆ—
            """
            if returns.empty:
                raise ValueError("æ”¶ç›ŠçŽ‡åºåˆ—ä¸èƒ½ä¸ºç©º")
            
            if returns.isna().any() or np.isinf(returns).any():
                raise ValueError("æ”¶ç›ŠçŽ‡åºåˆ—åŒ…å«æ— æ•ˆå€¼ï¼ˆNaNæˆ–æ— ç©·å¤§ï¼‰")
                
            self.returns = returns
        
        def calculate_total_return(self) -> float:
            """è®¡ç®—æ€»æ”¶ç›ŠçŽ‡"""
            return float((1 + self.returns).prod() - 1)
        
        def calculate_annualized_return(self, periods_per_year: int = 252) -> float:
            """è®¡ç®—å¹´åŒ–æ”¶ç›ŠçŽ‡"""
            total_return = self.calculate_total_return()
            total_periods = len(self.returns)
            return float((1 + total_return) ** (periods_per_year / total_periods) - 1)
        
        def calculate_monthly_returns(self) -> pd.DataFrame:
            """è®¡ç®—æœˆåº¦æ”¶ç›ŠçŽ‡"""
            if not isinstance(self.returns.index, pd.DatetimeIndex):
                raise ValueError("æ”¶ç›ŠçŽ‡åºåˆ—å¿…é¡»æœ‰æ—¥æœŸç´¢å¼•")
                
            # æŒ‰æœˆåˆ†ç»„è®¡ç®—æ”¶ç›ŠçŽ‡
            monthly_returns = self.returns.groupby(self.returns.index.to_period('M')).apply(
                lambda x: (1 + x).prod() - 1
            )
            
            # è½¬æ¢ä¸ºDataFrameæ ¼å¼
            return pd.DataFrame({
                'monthly_return': monthly_returns,
                'year': monthly_returns.index.year,
                'month': monthly_returns.index.month
            })
        
        def calculate_cumulative_returns(self) -> pd.Series:
            """è®¡ç®—ç´¯ç§¯æ”¶ç›ŠçŽ‡"""
            return (1 + self.returns).cumprod() - 1
    
    
    class RiskMetrics:
        """é£Žé™©æŒ‡æ ‡è®¡ç®—ç±»"""
        
        def __init__(self, returns: pd.Series):
            """
            åˆå§‹åŒ–é£Žé™©æŒ‡æ ‡è®¡ç®—å™¨
            
            Args:
                returns: æ”¶ç›ŠçŽ‡æ—¶é—´åºåˆ—
            """
            if returns.empty:
                raise ValueError("æ”¶ç›ŠçŽ‡åºåˆ—ä¸èƒ½ä¸ºç©º")
                
            self.returns = returns
        
        def calculate_volatility(self, annualized: bool = False, periods_per_year: int = 252) -> float:
            """è®¡ç®—æ³¢åŠ¨çŽ‡"""
            volatility = float(self.returns.std())
            
            if annualized:
                volatility *= np.sqrt(periods_per_year)
                
            return volatility
        
        def calculate_max_drawdown(self, portfolio_values: pd.Series) -> float:
            """è®¡ç®—æœ€å¤§å›žæ’¤"""
            if portfolio_values.empty:
                raise ValueError("ç»„åˆä»·å€¼åºåˆ—ä¸èƒ½ä¸ºç©º")
                
            # è®¡ç®—åŽ†å²æœ€é«˜ç‚¹
            peak = portfolio_values.expanding().max()
            
            # è®¡ç®—å›žæ’¤
            drawdown = (portfolio_values - peak) / peak
            
            # è¿”å›žæœ€å¤§å›žæ’¤çš„ç»å¯¹å€¼
            return float(abs(drawdown.min()))
        
        def calculate_var(self, confidence_level: float = 0.95) -> float:
            """è®¡ç®—é£Žé™©ä»·å€¼(VaR)"""
            if not 0 < confidence_level < 1:
                raise ValueError("ç½®ä¿¡æ°´å¹³å¿…é¡»åœ¨0å’Œ1ä¹‹é—´")
                
            # VaRæ˜¯æŸå¤±çš„ç»å¯¹å€¼
            return float(abs(self.returns.quantile(1 - confidence_level)))
        
        def calculate_cvar(self, confidence_level: float = 0.95) -> float:
            """è®¡ç®—æ¡ä»¶é£Žé™©ä»·å€¼(CVaR)"""
            if not 0 < confidence_level < 1:
                raise ValueError("ç½®ä¿¡æ°´å¹³å¿…é¡»åœ¨0å’Œ1ä¹‹é—´")
                
            var_value = self.calculate_var(confidence_level)
            
            # èŽ·å–è¶…è¿‡VaRçš„æŸå¤±
            tail_losses = self.returns[self.returns <= -var_value]
            
            if len(tail_losses) > 0:
                return float(abs(tail_losses.mean()))
            else:
                return var_value
        
        def calculate_downside_deviation(self, target_return: float = 0.0, 
                                       annualized: bool = False, 
                                       periods_per_year: int = 252) -> float:
            """è®¡ç®—ä¸‹è¡Œåå·®"""
            # è®¡ç®—ä½ŽäºŽç›®æ ‡æ”¶ç›ŠçŽ‡çš„æ”¶ç›Š
            downside_returns = self.returns[self.returns < target_return]
            
            if len(downside_returns) == 0:
                return 0.0
                
            # è®¡ç®—ä¸‹è¡Œåå·®
            downside_deviation = float(np.sqrt(((downside_returns - target_return) ** 2).mean()))
            
            if annualized:
                downside_deviation *= np.sqrt(periods_per_year)
                
            return downside_deviation
        
        def calculate_skewness(self) -> float:
            """è®¡ç®—ååº¦"""
            mean = self.returns.mean()
            std = self.returns.std()
            
            if std == 0:
                return 0.0
                
            skewness = ((self.returns - mean) ** 3).mean() / (std ** 3)
            return float(skewness)
        
        def calculate_kurtosis(self) -> float:
            """è®¡ç®—å³°åº¦ï¼ˆè¶…é¢å³°åº¦ï¼‰"""
            mean = self.returns.mean()
            std = self.returns.std()
            
            if std == 0:
                return 0.0
                
            kurtosis = ((self.returns - mean) ** 4).mean() / (std ** 4) - 3
            return float(kurtosis)
    
    
    class RiskAdjustedMetrics:
        """é£Žé™©è°ƒæ•´æŒ‡æ ‡è®¡ç®—ç±»"""
        
        def __init__(self, returns: pd.Series):
            """
            åˆå§‹åŒ–é£Žé™©è°ƒæ•´æŒ‡æ ‡è®¡ç®—å™¨
            
            Args:
                returns: æ”¶ç›ŠçŽ‡æ—¶é—´åºåˆ—
            """
            self.returns = returns
            self.risk_metrics = RiskMetrics(returns)
        
        def calculate_sharpe_ratio(self, risk_free_rate: float = 0.03, 
                                 periods_per_year: int = 252) -> float:
            """è®¡ç®—å¤æ™®æ¯”çŽ‡"""
            # è®¡ç®—è¶…é¢æ”¶ç›Š
            daily_risk_free_rate = risk_free_rate / periods_per_year
            excess_returns = self.returns - daily_risk_free_rate
            
            # è®¡ç®—å¤æ™®æ¯”çŽ‡
            if excess_returns.std() == 0:
                return 0.0
                
            sharpe_ratio = excess_returns.mean() / excess_returns.std() * np.sqrt(periods_per_year)
            return float(sharpe_ratio)
        
        def calculate_sortino_ratio(self, target_return: float = 0.03, 
                                  periods_per_year: int = 252) -> float:
            """è®¡ç®—ç´¢æè¯ºæ¯”çŽ‡"""
            # è®¡ç®—è¶…é¢æ”¶ç›Š
            daily_target_return = target_return / periods_per_year
            excess_returns = self.returns - daily_target_return
            
            # è®¡ç®—ä¸‹è¡Œåå·®
            downside_deviation = self.risk_metrics.calculate_downside_deviation(
                target_return=daily_target_return
            )
            
            if downside_deviation == 0:
                return 0.0
                
            sortino_ratio = excess_returns.mean() / downside_deviation * np.sqrt(periods_per_year)
            return float(sortino_ratio)
        
        def calculate_calmar_ratio(self, portfolio_values: pd.Series, 
                                 periods_per_year: int = 252) -> float:
            """è®¡ç®—å¡çŽ›æ¯”çŽ‡"""
            # è®¡ç®—å¹´åŒ–æ”¶ç›ŠçŽ‡
            total_return = (1 + self.returns).prod() - 1
            annualized_return = (1 + total_return) ** (periods_per_year / len(self.returns)) - 1
            
            # è®¡ç®—æœ€å¤§å›žæ’¤
            max_drawdown = self.risk_metrics.calculate_max_drawdown(portfolio_values)
            
            if max_drawdown == 0:
                return 0.0
                
            calmar_ratio = annualized_return / max_drawdown
            return float(calmar_ratio)
        
        def calculate_information_ratio(self, benchmark_returns: pd.Series, 
                                      periods_per_year: int = 252) -> float:
            """è®¡ç®—ä¿¡æ¯æ¯”çŽ‡"""
            if len(benchmark_returns) != len(self.returns):
                raise ValueError("åŸºå‡†æ”¶ç›ŠçŽ‡åºåˆ—é•¿åº¦ä¸ŽæŠ•èµ„ç»„åˆæ”¶ç›ŠçŽ‡ä¸åŒ¹é…")
                
            # è®¡ç®—ä¸»åŠ¨æ”¶ç›Š
            active_returns = self.returns - benchmark_returns
            
            # è®¡ç®—è·Ÿè¸ªè¯¯å·®
            tracking_error = active_returns.std() * np.sqrt(periods_per_year)
            
            if tracking_error == 0:
                return 0.0
                
            # è®¡ç®—ä¿¡æ¯æ¯”çŽ‡
            information_ratio = active_returns.mean() * periods_per_year / tracking_error
            return float(information_ratio)
        
        def calculate_treynor_ratio(self, beta: float, risk_free_rate: float = 0.03, 
                                  periods_per_year: int = 252) -> float:
            """è®¡ç®—ç‰¹é›·è¯ºæ¯”çŽ‡"""
            if beta == 0:
                return 0.0
                
            # è®¡ç®—è¶…é¢æ”¶ç›Š
            daily_risk_free_rate = risk_free_rate / periods_per_year
            excess_returns = self.returns - daily_risk_free_rate
            
            # è®¡ç®—ç‰¹é›·è¯ºæ¯”çŽ‡
            treynor_ratio = excess_returns.mean() * periods_per_year / beta
            return float(treynor_ratio)
    
    
    class TradingMetrics:
        """äº¤æ˜“æŒ‡æ ‡è®¡ç®—ç±»"""
        
        def __init__(self, trades: List[Trade], portfolio_values: pd.Series):
            """
            åˆå§‹åŒ–äº¤æ˜“æŒ‡æ ‡è®¡ç®—å™¨
            
            Args:
                trades: äº¤æ˜“è®°å½•åˆ—è¡¨
                portfolio_values: ç»„åˆä»·å€¼æ—¶é—´åºåˆ—
            """
            self.trades = trades
            self.portfolio_values = portfolio_values
        
        def calculate_turnover_rate(self, period: str = 'annual') -> Union[float, pd.Series]:
            """è®¡ç®—æ¢æ‰‹çŽ‡"""
            if not self.trades:
                return 0.0 if period == 'annual' else pd.Series(dtype=float)
                
            # æŒ‰æ—¶é—´æŽ’åºäº¤æ˜“
            sorted_trades = sorted(self.trades, key=lambda x: x.timestamp)
            
            # è®¡ç®—æ¯æ—¥äº¤æ˜“é‡‘é¢
            trade_amounts = {}
            for trade in sorted_trades:
                trade_date = trade.timestamp.date()
                trade_amount = float(trade.quantity * trade.price)
                
                if trade_date not in trade_amounts:
                    trade_amounts[trade_date] = 0
                trade_amounts[trade_date] += trade_amount
            
            # è½¬æ¢ä¸ºæ—¶é—´åºåˆ—
            trade_series = pd.Series(trade_amounts).sort_index()
            
            if period == 'annual':
                # å¹´åŒ–æ¢æ‰‹çŽ‡
                total_trade_amount = trade_series.sum()
                average_portfolio_value = self.portfolio_values.mean()
                return float(total_trade_amount / average_portfolio_value)
            
            elif period == 'monthly':
                # æœˆåº¦æ¢æ‰‹çŽ‡
                if isinstance(trade_series.index, pd.DatetimeIndex):
                    monthly_turnover = trade_series.groupby(
                        trade_series.index.to_period('M')
                    ).sum()
                else:
                    # å¦‚æžœç´¢å¼•ä¸æ˜¯DatetimeIndexï¼Œè½¬æ¢ä¸ºDatetimeIndex
                    trade_series.index = pd.to_datetime(trade_series.index)
                    monthly_turnover = trade_series.groupby(
                        trade_series.index.to_period('M')
                    ).sum()
                
                # è®¡ç®—æœˆåº¦å¹³å‡ç»„åˆä»·å€¼
                if isinstance(self.portfolio_values.index, pd.DatetimeIndex):
                    monthly_avg_value = self.portfolio_values.groupby(
                        self.portfolio_values.index.to_period('M')
                    ).mean()
                else:
                    monthly_avg_value = pd.Series([self.portfolio_values.mean()] * len(monthly_turnover),
                                                index=monthly_turnover.index)
                
                return monthly_turnover / monthly_avg_value
            
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å‘¨æœŸç±»åž‹: {period}")
        
        def calculate_transaction_cost_analysis(self) -> Dict[str, float]:
            """è®¡ç®—äº¤æ˜“æˆæœ¬åˆ†æž"""
            if not self.trades:
                return {
                    'total_commission': 0.0,
                    'commission_rate': 0.0,
                    'cost_per_trade': 0.0,
                    'cost_ratio_to_portfolio': 0.0
                }
            
            # è®¡ç®—æ€»ä½£é‡‘
            total_commission = sum(float(trade.commission) for trade in self.trades)
            
            # è®¡ç®—æ€»äº¤æ˜“é‡‘é¢
            total_trade_amount = sum(float(trade.quantity * trade.price) for trade in self.trades)
            
            # è®¡ç®—ä½£é‡‘çŽ‡
            commission_rate = total_commission / total_trade_amount if total_trade_amount > 0 else 0.0
            
            # è®¡ç®—å¹³å‡æ¯ç¬”äº¤æ˜“æˆæœ¬
            cost_per_trade = total_commission / len(self.trades)
            
            # è®¡ç®—æˆæœ¬å ç»„åˆæ¯”ä¾‹
            average_portfolio_value = self.portfolio_values.mean()
            cost_ratio = total_commission / average_portfolio_value
            
            return {
                'total_commission': total_commission,
                'commission_rate': commission_rate,
                'cost_per_trade': cost_per_trade,
                'cost_ratio_to_portfolio': cost_ratio
            }
        
        def calculate_holding_period_analysis(self) -> Dict[str, float]:
            """è®¡ç®—æŒä»“å‘¨æœŸåˆ†æž"""
            if not self.trades:
                return {
                    'average_holding_days': 0.0,
                    'median_holding_days': 0.0,
                    'max_holding_days': 0.0,
                    'min_holding_days': 0.0
                }
            
            # æŒ‰è‚¡ç¥¨åˆ†ç»„ï¼Œè®¡ç®—æŒä»“å‘¨æœŸ
            holding_periods = []
            positions = {}  # symbol -> [buy_trades]
            
            for trade in sorted(self.trades, key=lambda x: x.timestamp):
                symbol = trade.symbol
                
                if symbol not in positions:
                    positions[symbol] = []
                
                if trade.trade_type == OrderType.BUY:
                    positions[symbol].append(trade)
                elif trade.trade_type == OrderType.SELL and positions[symbol]:
                    # ä½¿ç”¨FIFOè®¡ç®—æŒä»“å‘¨æœŸ
                    buy_trade = positions[symbol].pop(0)
                    holding_days = (trade.timestamp - buy_trade.timestamp).days
                    holding_periods.append(holding_days)
            
            if not holding_periods:
                return {
                    'average_holding_days': 0.0,
                    'median_holding_days': 0.0,
                    'max_holding_days': 0.0,
                    'min_holding_days': 0.0
                }
            
            return {
                'average_holding_days': float(np.mean(holding_periods)),
                'median_holding_days': float(np.median(holding_periods)),
                'max_holding_days': float(np.max(holding_periods)),
                'min_holding_days': float(np.min(holding_periods))
            }
        
        def calculate_win_loss_analysis(self) -> Dict[str, float]:
            """è®¡ç®—ç›ˆäºåˆ†æž"""
            if not self.trades:
                return {
                    'win_rate': 0.0,
                    'profit_loss_ratio': 0.0,
                    'average_win': 0.0,
                    'average_loss': 0.0,
                    'total_trades': 0.0
                }
            
            # è®¡ç®—æ¯ç¬”äº¤æ˜“çš„ç›ˆäº
            trade_pnls = []
            positions = {}  # symbol -> [(quantity, price)]
            
            for trade in sorted(self.trades, key=lambda x: x.timestamp):
                symbol = trade.symbol
                
                if symbol not in positions:
                    positions[symbol] = []
                
                if trade.trade_type == OrderType.BUY:
                    positions[symbol].append((trade.quantity, float(trade.price)))
                elif trade.trade_type == OrderType.SELL and positions[symbol]:
                    # ä½¿ç”¨FIFOè®¡ç®—ç›ˆäº
                    remaining_quantity = trade.quantity
                    trade_pnl = 0.0
                    
                    while remaining_quantity > 0 and positions[symbol]:
                        buy_quantity, buy_price = positions[symbol][0]
                        
                        if buy_quantity <= remaining_quantity:
                            # å®Œå…¨å–å‡ºè¿™ä¸ªä¹°å…¥è®°å½•
                            pnl = buy_quantity * (float(trade.price) - buy_price)
                            trade_pnl += pnl
                            remaining_quantity -= buy_quantity
                            positions[symbol].pop(0)
                        else:
                            # éƒ¨åˆ†å–å‡º
                            pnl = remaining_quantity * (float(trade.price) - buy_price)
                            trade_pnl += pnl
                            positions[symbol][0] = (buy_quantity - remaining_quantity, buy_price)
                            remaining_quantity = 0
                    
                    if trade_pnl != 0:  # åªè®°å½•æœ‰ç›ˆäºçš„äº¤æ˜“
                        trade_pnls.append(trade_pnl)
            
            if not trade_pnls:
                return {
                    'win_rate': 0.0,
                    'profit_loss_ratio': 0.0,
                    'average_win': 0.0,
                    'average_loss': 0.0,
                    'total_trades': 0.0
                }
            
            # åˆ†ç¦»ç›ˆåˆ©å’ŒäºæŸäº¤æ˜“
            winning_trades = [pnl for pnl in trade_pnls if pnl > 0]
            losing_trades = [pnl for pnl in trade_pnls if pnl < 0]
            
            # è®¡ç®—æŒ‡æ ‡
            win_rate = len(winning_trades) / len(trade_pnls)
            average_win = np.mean(winning_trades) if winning_trades else 0.0
            average_loss = abs(np.mean(losing_trades)) if losing_trades else 0.0
            profit_loss_ratio = average_win / average_loss if average_loss > 0 else 0.0
            
            return {
                'win_rate': win_rate,
                'profit_loss_ratio': profit_loss_ratio,
                'average_win': average_win,
                'average_loss': average_loss,
                'total_trades': float(len(trade_pnls))
            }
        
        def calculate_position_concentration(self) -> Dict[str, float]:
            """è®¡ç®—æŒä»“é›†ä¸­åº¦åˆ†æž"""
            if not self.trades:
                return {
                    'herfindahl_index': 0.0,
                    'max_position_weight': 0.0,
                    'top_5_concentration': 0.0,
                    'effective_positions': 1.0
                }
            
            # è®¡ç®—æœ€ç»ˆæŒä»“
            final_positions = {}  # symbol -> quantity
            
            for trade in self.trades:
                symbol = trade.symbol
                
                if symbol not in final_positions:
                    final_positions[symbol] = 0
                
                if trade.trade_type == OrderType.BUY:
                    final_positions[symbol] += trade.quantity
                elif trade.trade_type == OrderType.SELL:
                    final_positions[symbol] -= trade.quantity
            
            # è¿‡æ»¤æŽ‰é›¶æŒä»“
            final_positions = {k: v for k, v in final_positions.items() if v > 0}
            
            if not final_positions:
                return {
                    'herfindahl_index': 0.0,
                    'max_position_weight': 0.0,
                    'top_5_concentration': 0.0,
                    'effective_positions': 0.0
                }
            
            # è®¡ç®—æŒä»“æƒé‡ï¼ˆè¿™é‡Œå‡è®¾æ‰€æœ‰è‚¡ç¥¨ä»·æ ¼ç›¸ç­‰ï¼Œå®žé™…åº”è¯¥ä½¿ç”¨å¸‚å€¼æƒé‡ï¼‰
            total_quantity = sum(final_positions.values())
            position_weights = {k: v / total_quantity for k, v in final_positions.items()}
            
            # è®¡ç®—èµ«èŠ¬è¾¾å°”æŒ‡æ•°
            weights = list(position_weights.values())
            herfindahl_index = sum(w ** 2 for w in weights)
            
            # æœ€å¤§æŒä»“æƒé‡
            max_weight = max(weights)
            
            # Top5é›†ä¸­åº¦
            sorted_weights = sorted(weights, reverse=True)
            top5_weights = sorted_weights[:5]
            top5_concentration = sum(top5_weights)
            
            # æœ‰æ•ˆæŒä»“æ•°
            effective_positions = 1 / herfindahl_index if herfindahl_index > 0 else 0
            
            return {
                'herfindahl_index': herfindahl_index,
                'max_position_weight': max_weight,
                'top_5_concentration': top5_concentration,
                'effective_positions': effective_positions
            }
    
    
    class PortfolioMetrics:
        """ç»¼åˆç»„åˆæŒ‡æ ‡è®¡ç®—ç±»"""
        
        def __init__(self, returns: pd.Series, portfolio_values: pd.Series, trades: List[Trade]):
            """
            åˆå§‹åŒ–ç»¼åˆç»„åˆæŒ‡æ ‡è®¡ç®—å™¨
            
            Args:
                returns: æ”¶ç›ŠçŽ‡æ—¶é—´åºåˆ—
                portfolio_values: ç»„åˆä»·å€¼æ—¶é—´åºåˆ—
                trades: äº¤æ˜“è®°å½•åˆ—è¡¨
            """
            if len(returns) != len(portfolio_values):
                raise ValueError("æ”¶ç›ŠçŽ‡åºåˆ—å’Œç»„åˆä»·å€¼åºåˆ—é•¿åº¦ä¸åŒ¹é…")
                
            self.returns = returns
            self.portfolio_values = portfolio_values
            self.trades = trades
            
            # åˆå§‹åŒ–å„ä¸ªæŒ‡æ ‡è®¡ç®—å™¨
            self.return_metrics = ReturnMetrics(returns)
            self.risk_metrics = RiskMetrics(returns)
            self.risk_adjusted_metrics = RiskAdjustedMetrics(returns)
            self.trading_metrics = TradingMetrics(trades, portfolio_values)
        
        def calculate_comprehensive_metrics(self) -> Dict[str, Dict[str, float]]:
            """è®¡ç®—ç»¼åˆæŒ‡æ ‡"""
            return {
                'return_metrics': {
                    'total_return': self.return_metrics.calculate_total_return(),
                    'annualized_return': self.return_metrics.calculate_annualized_return(),
                },
                'risk_metrics': {
                    'volatility': self.risk_metrics.calculate_volatility(annualized=True),
                    'max_drawdown': self.risk_metrics.calculate_max_drawdown(self.portfolio_values),
                    'var_95': self.risk_metrics.calculate_var(0.95),
                    'cvar_95': self.risk_metrics.calculate_cvar(0.95),
                    'skewness': self.risk_metrics.calculate_skewness(),
                    'kurtosis': self.risk_metrics.calculate_kurtosis(),
                },
                'risk_adjusted_metrics': {
                    'sharpe_ratio': self.risk_adjusted_metrics.calculate_sharpe_ratio(),
                    'sortino_ratio': self.risk_adjusted_metrics.calculate_sortino_ratio(),
                    'calmar_ratio': self.risk_adjusted_metrics.calculate_calmar_ratio(self.portfolio_values),
                },
                'trading_metrics': {
                    **self.trading_metrics.calculate_transaction_cost_analysis(),
                    **self.trading_metrics.calculate_win_loss_analysis(),
                    **self.trading_metrics.calculate_position_concentration(),
                    'annual_turnover': self.trading_metrics.calculate_turnover_rate('annual')
                }
            }
        
        def compare_with_benchmark(self, benchmark_returns: pd.Series) -> Dict[str, Dict[str, float]]:
            """ä¸ŽåŸºå‡†æ¯”è¾ƒ"""
            # è®¡ç®—ç»„åˆæŒ‡æ ‡
            portfolio_metrics = self.calculate_comprehensive_metrics()
            
            # è®¡ç®—åŸºå‡†æŒ‡æ ‡
            benchmark_return_metrics = ReturnMetrics(benchmark_returns)
            benchmark_risk_metrics = RiskMetrics(benchmark_returns)
            benchmark_risk_adjusted = RiskAdjustedMetrics(benchmark_returns)
            
            # åŸºå‡†ç»„åˆä»·å€¼ï¼ˆå‡è®¾åˆå§‹å€¼ä¸Žç»„åˆç›¸åŒï¼‰
            initial_value = self.portfolio_values.iloc[0]
            benchmark_values = initial_value * (1 + benchmark_return_metrics.calculate_cumulative_returns())
            
            benchmark_metrics = {
                'return_metrics': {
                    'total_return': benchmark_return_metrics.calculate_total_return(),
                    'annualized_return': benchmark_return_metrics.calculate_annualized_return(),
                },
                'risk_metrics': {
                    'volatility': benchmark_risk_metrics.calculate_volatility(annualized=True),
                    'max_drawdown': benchmark_risk_metrics.calculate_max_drawdown(benchmark_values),
                    'var_95': benchmark_risk_metrics.calculate_var(0.95),
                },
                'risk_adjusted_metrics': {
                    'sharpe_ratio': benchmark_risk_adjusted.calculate_sharpe_ratio(),
                    'sortino_ratio': benchmark_risk_adjusted.calculate_sortino_ratio(),
                    'calmar_ratio': benchmark_risk_adjusted.calculate_calmar_ratio(benchmark_values),
                }
            }
            
            # è®¡ç®—ç›¸å¯¹æŒ‡æ ‡
            relative_metrics = {
                'excess_return': (portfolio_metrics['return_metrics']['annualized_return'] - 
                                benchmark_metrics['return_metrics']['annualized_return']),
                'information_ratio': self.risk_adjusted_metrics.calculate_information_ratio(benchmark_returns),
                'tracking_error': (self.returns - benchmark_returns).std() * np.sqrt(252),
            }
            
            return {
                'portfolio_metrics': portfolio_metrics,
                'benchmark_metrics': benchmark_metrics,
                'relative_metrics': relative_metrics
            }
        
        def calculate_rolling_metrics(self, window: int, metric: str) -> pd.Series:
            """è®¡ç®—æ»šåŠ¨æŒ‡æ ‡"""
            if metric == 'sharpe_ratio':
                return self.returns.rolling(window=window).apply(
                    lambda x: RiskAdjustedMetrics(x).calculate_sharpe_ratio() if len(x) == window else np.nan
                )
            elif metric == 'volatility':
                return self.returns.rolling(window=window).std() * np.sqrt(252)
            elif metric == 'max_drawdown':
                return self.portfolio_values.rolling(window=window).apply(
                    lambda x: RiskMetrics(x.pct_change().dropna()).calculate_max_drawdown(x) if len(x) == window else np.nan
                )
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ»šåŠ¨æŒ‡æ ‡: {metric}")
        
        def calculate_sector_analysis(self, sector_mapping: Dict[str, str]) -> Dict[str, Dict[str, float]]:
            """è®¡ç®—è¡Œä¸šåˆ†æž"""
            sector_analysis = {}
            
            # æŒ‰è¡Œä¸šåˆ†ç»„äº¤æ˜“
            sector_trades = {}
            for trade in self.trades:
                sector = sector_mapping.get(trade.symbol, 'å…¶ä»–')
                if sector not in sector_trades:
                    sector_trades[sector] = []
                sector_trades[sector].append(trade)
            
            # è®¡ç®—æ€»äº¤æ˜“é‡‘é¢
            total_trade_amount = sum(float(trade.quantity * trade.price) for trade in self.trades)
            
            # è®¡ç®—æ¯ä¸ªè¡Œä¸šçš„æŒ‡æ ‡
            for sector, trades in sector_trades.items():
                sector_trade_amount = sum(float(trade.quantity * trade.price) for trade in trades)
                weight = sector_trade_amount / total_trade_amount if total_trade_amount > 0 else 0
                
                # è®¡ç®—æ”¶ç›Šè´¡çŒ®ï¼ˆè¿™é‡Œç®€åŒ–å¤„ç†ï¼‰
                sector_returns = 0.0  # éœ€è¦æ›´å¤æ‚çš„è®¡ç®—
                
                sector_analysis[sector] = {
                    'weight': weight,
                    'return_contribution': sector_returns,
                    'trade_count': len(trades)
                }
            
            return sector_analysis
    ]]></file>
  <file path="src/rl_trading_system/evaluation/__init__.py"><![CDATA[
    """è¯„ä¼°æ¨¡å—"""
    ]]></file>
  <file path="src/rl_trading_system/deployment/model_version_manager.py"><![CDATA[
    """
    æ¨¡åž‹ç‰ˆæœ¬ç®¡ç†å™¨å®žçŽ°
    å®žçŽ°æ¨¡åž‹ç‰ˆæœ¬æŽ§åˆ¶ã€åŽ†å²è®°å½•ç®¡ç†å’Œç‰ˆæœ¬æ¯”è¾ƒåŠŸèƒ½
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import hashlib
    import json
    import os
    import shutil
    import threading
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional, Any, Union
    from dataclasses import dataclass, field
    from enum import Enum
    from pathlib import Path
    import uuid
    import pickle
    import torch
    
    
    class ModelStatus(Enum):
        """æ¨¡åž‹çŠ¶æ€æžšä¸¾"""
        ACTIVE = "active"
        DEPRECATED = "deprecated"
        ARCHIVED = "archived"
        FAILED = "failed"
    
    
    @dataclass
    class ModelMetadata:
        """æ¨¡åž‹å…ƒæ•°æ®"""
        model_id: str
        version: str
        name: str
        description: str
        created_at: datetime
        created_by: str
        model_type: str
        framework: str
        status: ModelStatus = ModelStatus.ACTIVE
        tags: List[str] = field(default_factory=list)
        metrics: Dict[str, float] = field(default_factory=dict)
        config: Dict[str, Any] = field(default_factory=dict)
        file_path: Optional[str] = None
        file_size: Optional[int] = None
        checksum: Optional[str] = None
        
        def __post_init__(self):
            """åˆå§‹åŒ–åŽéªŒè¯"""
            if not self.model_id:
                raise ValueError("æ¨¡åž‹IDä¸èƒ½ä¸ºç©º")
            if not self.version:
                raise ValueError("ç‰ˆæœ¬å·ä¸èƒ½ä¸ºç©º")
            if not self.name:
                raise ValueError("æ¨¡åž‹åç§°ä¸èƒ½ä¸ºç©º")
    
    
    @dataclass
    class ModelComparison:
        """æ¨¡åž‹æ¯”è¾ƒç»“æžœ"""
        model_a_id: str
        model_b_id: str
        comparison_metrics: Dict[str, float]
        performance_diff: Dict[str, float]
        recommendation: str
        confidence_score: float
        timestamp: datetime = field(default_factory=datetime.now)
    
    
    class ModelVersionManager:
        """æ¨¡åž‹ç‰ˆæœ¬ç®¡ç†å™¨"""
        
        def __init__(self, storage_path: str = "models", max_versions: int = 10,
                     auto_cleanup: bool = True, backup_enabled: bool = True):
            """
            åˆå§‹åŒ–æ¨¡åž‹ç‰ˆæœ¬ç®¡ç†å™¨
            
            Args:
                storage_path: æ¨¡åž‹å­˜å‚¨è·¯å¾„
                max_versions: æœ€å¤§ç‰ˆæœ¬æ•°é‡
                auto_cleanup: æ˜¯å¦è‡ªåŠ¨æ¸…ç†æ—§ç‰ˆæœ¬
                backup_enabled: æ˜¯å¦å¯ç”¨å¤‡ä»½
            """
            self.storage_path = Path(storage_path)
            self.max_versions = max_versions
            self.auto_cleanup = auto_cleanup
            self.backup_enabled = backup_enabled
            
            # åˆ›å»ºå­˜å‚¨ç›®å½•
            self.storage_path.mkdir(parents=True, exist_ok=True)
            self.metadata_path = self.storage_path / "metadata"
            self.metadata_path.mkdir(exist_ok=True)
            
            # æ¨¡åž‹æ³¨å†Œè¡¨
            self.model_registry = {}
            self.version_history = {}
            self.active_models = {}
            
            # çº¿ç¨‹é”
            self._lock = threading.Lock()
            
            # åŠ è½½çŽ°æœ‰æ¨¡åž‹
            self._load_existing_models()
        
        def register_model(self, model: Any, metadata: ModelMetadata) -> str:
            """
            æ³¨å†Œæ–°æ¨¡åž‹
            
            Args:
                model: æ¨¡åž‹å¯¹è±¡
                metadata: æ¨¡åž‹å…ƒæ•°æ®
                
            Returns:
                æ¨¡åž‹å­˜å‚¨è·¯å¾„
            """
            if not metadata.model_id:
                metadata.model_id = str(uuid.uuid4())
            
            # éªŒè¯ç‰ˆæœ¬å·å”¯ä¸€æ€§
            if metadata.model_id in self.model_registry:
                existing_versions = [v.version for v in self.model_registry[metadata.model_id]]
                if metadata.version in existing_versions:
                    raise ValueError(f"ç‰ˆæœ¬ {metadata.version} å·²å­˜åœ¨")
            
            # ä¿å­˜æ¨¡åž‹æ–‡ä»¶
            model_file_path = self._save_model_file(model, metadata)
            metadata.file_path = str(model_file_path)
            metadata.file_size = model_file_path.stat().st_size
            metadata.checksum = self._calculate_checksum(model_file_path)
            
            # æ³¨å†Œæ¨¡åž‹
            with self._lock:
                if metadata.model_id not in self.model_registry:
                    self.model_registry[metadata.model_id] = []
                    self.version_history[metadata.model_id] = []
                
                self.model_registry[metadata.model_id].append(metadata)
                self.version_history[metadata.model_id].append({
                    'version': metadata.version,
                    'timestamp': metadata.created_at,
                    'action': 'registered',
                    'metadata': metadata
                })
                
                # è®¾ç½®ä¸ºæ´»è·ƒæ¨¡åž‹
                self.active_models[metadata.model_id] = metadata
                
                # ä¿å­˜å…ƒæ•°æ®
                self._save_metadata(metadata)
                
                # è‡ªåŠ¨æ¸…ç†æ—§ç‰ˆæœ¬
                if self.auto_cleanup:
                    self._cleanup_old_versions(metadata.model_id)
            
            return str(model_file_path)
        
        def get_model(self, model_id: str, version: Optional[str] = None) -> Any:
            """
            èŽ·å–æ¨¡åž‹
            
            Args:
                model_id: æ¨¡åž‹ID
                version: ç‰ˆæœ¬å·ï¼Œå¦‚æžœä¸ºNoneåˆ™è¿”å›žæœ€æ–°ç‰ˆæœ¬
                
            Returns:
                æ¨¡åž‹å¯¹è±¡
            """
            metadata = self.get_model_metadata(model_id, version)
            if not metadata or not metadata.file_path:
                raise ValueError(f"æ¨¡åž‹ {model_id} ç‰ˆæœ¬ {version} ä¸å­˜åœ¨")
            
            return self._load_model_file(metadata.file_path)
        
        def get_model_metadata(self, model_id: str, version: Optional[str] = None) -> Optional[ModelMetadata]:
            """
            èŽ·å–æ¨¡åž‹å…ƒæ•°æ®
            
            Args:
                model_id: æ¨¡åž‹ID
                version: ç‰ˆæœ¬å·ï¼Œå¦‚æžœä¸ºNoneåˆ™è¿”å›žæœ€æ–°ç‰ˆæœ¬
                
            Returns:
                æ¨¡åž‹å…ƒæ•°æ®
            """
            with self._lock:
                if model_id not in self.model_registry:
                    return None
                
                versions = self.model_registry[model_id]
                if not versions:
                    return None
                
                if version is None:
                    # è¿”å›žæœ€æ–°ç‰ˆæœ¬
                    return max(versions, key=lambda x: x.created_at)
                else:
                    # è¿”å›žæŒ‡å®šç‰ˆæœ¬
                    for metadata in versions:
                        if metadata.version == version:
                            return metadata
                    return None
        
        def list_models(self, status: Optional[ModelStatus] = None) -> List[ModelMetadata]:
            """
            åˆ—å‡ºæ‰€æœ‰æ¨¡åž‹
            
            Args:
                status: è¿‡æ»¤çŠ¶æ€
                
            Returns:
                æ¨¡åž‹å…ƒæ•°æ®åˆ—è¡¨
            """
            with self._lock:
                all_models = []
                for model_versions in self.model_registry.values():
                    all_models.extend(model_versions)
                
                if status is not None:
                    all_models = [m for m in all_models if m.status == status]
                
                return sorted(all_models, key=lambda x: x.created_at, reverse=True)
        
        def list_versions(self, model_id: str) -> List[ModelMetadata]:
            """
            åˆ—å‡ºæ¨¡åž‹çš„æ‰€æœ‰ç‰ˆæœ¬
            
            Args:
                model_id: æ¨¡åž‹ID
                
            Returns:
                ç‰ˆæœ¬åˆ—è¡¨
            """
            with self._lock:
                if model_id not in self.model_registry:
                    return []
                
                return sorted(self.model_registry[model_id], 
                             key=lambda x: x.created_at, reverse=True)
        
        def promote_model(self, model_id: str, version: str) -> bool:
            """
            æå‡æ¨¡åž‹ç‰ˆæœ¬ä¸ºæ´»è·ƒç‰ˆæœ¬
            
            Args:
                model_id: æ¨¡åž‹ID
                version: ç‰ˆæœ¬å·
                
            Returns:
                æ˜¯å¦æˆåŠŸ
            """
            metadata = self.get_model_metadata(model_id, version)
            if not metadata:
                return False
            
            with self._lock:
                # æ›´æ–°æ´»è·ƒæ¨¡åž‹
                old_active = self.active_models.get(model_id)
                self.active_models[model_id] = metadata
                
                # è®°å½•åŽ†å²
                self.version_history[model_id].append({
                    'version': version,
                    'timestamp': datetime.now(),
                    'action': 'promoted',
                    'previous_active': old_active.version if old_active else None
                })
            
            return True
        
        def deprecate_model(self, model_id: str, version: str, reason: str = "") -> bool:
            """
            åºŸå¼ƒæ¨¡åž‹ç‰ˆæœ¬
            
            Args:
                model_id: æ¨¡åž‹ID
                version: ç‰ˆæœ¬å·
                reason: åºŸå¼ƒåŽŸå› 
                
            Returns:
                æ˜¯å¦æˆåŠŸ
            """
            metadata = self.get_model_metadata(model_id, version)
            if not metadata:
                return False
            
            with self._lock:
                metadata.status = ModelStatus.DEPRECATED
                
                # å¦‚æžœæ˜¯æ´»è·ƒæ¨¡åž‹ï¼Œéœ€è¦é€‰æ‹©æ–°çš„æ´»è·ƒç‰ˆæœ¬
                if (model_id in self.active_models and 
                    self.active_models[model_id].version == version):
                    
                    # é€‰æ‹©æœ€æ–°çš„éžåºŸå¼ƒç‰ˆæœ¬
                    active_versions = [m for m in self.model_registry[model_id] 
                                     if m.status == ModelStatus.ACTIVE and m.version != version]
                    if active_versions:
                        self.active_models[model_id] = max(active_versions, key=lambda x: x.created_at)
                    else:
                        del self.active_models[model_id]
                
                # è®°å½•åŽ†å²
                self.version_history[model_id].append({
                    'version': version,
                    'timestamp': datetime.now(),
                    'action': 'deprecated',
                    'reason': reason
                })
                
                # æ›´æ–°å…ƒæ•°æ®æ–‡ä»¶
                self._save_metadata(metadata)
            
            return True
        
        def delete_model(self, model_id: str, version: str, force: bool = False) -> bool:
            """
            åˆ é™¤æ¨¡åž‹ç‰ˆæœ¬
            
            Args:
                model_id: æ¨¡åž‹ID
                version: ç‰ˆæœ¬å·
                force: æ˜¯å¦å¼ºåˆ¶åˆ é™¤æ´»è·ƒæ¨¡åž‹
                
            Returns:
                æ˜¯å¦æˆåŠŸ
            """
            metadata = self.get_model_metadata(model_id, version)
            if not metadata:
                return False
            
            # æ£€æŸ¥æ˜¯å¦ä¸ºæ´»è·ƒæ¨¡åž‹
            if (not force and model_id in self.active_models and 
                self.active_models[model_id].version == version):
                raise ValueError("ä¸èƒ½åˆ é™¤æ´»è·ƒæ¨¡åž‹ï¼Œè¯·å…ˆæå‡å…¶ä»–ç‰ˆæœ¬æˆ–ä½¿ç”¨force=True")
            
            with self._lock:
                # åˆ é™¤æ–‡ä»¶
                if metadata.file_path and os.path.exists(metadata.file_path):
                    os.remove(metadata.file_path)
                
                # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                metadata_file = self.metadata_path / f"{model_id}_{version}.json"
                if metadata_file.exists():
                    metadata_file.unlink()
                
                # ä»Žæ³¨å†Œè¡¨ä¸­ç§»é™¤
                self.model_registry[model_id] = [
                    m for m in self.model_registry[model_id] if m.version != version
                ]
                
                # å¦‚æžœæ˜¯æ´»è·ƒæ¨¡åž‹ï¼Œç§»é™¤æ´»è·ƒçŠ¶æ€
                if (model_id in self.active_models and 
                    self.active_models[model_id].version == version):
                    del self.active_models[model_id]
                
                # è®°å½•åŽ†å²
                self.version_history[model_id].append({
                    'version': version,
                    'timestamp': datetime.now(),
                    'action': 'deleted'
                })
            
            return True
        
        def compare_models(self, model_a_id: str, model_b_id: str, 
                          version_a: Optional[str] = None, 
                          version_b: Optional[str] = None) -> ModelComparison:
            """
            æ¯”è¾ƒä¸¤ä¸ªæ¨¡åž‹
            
            Args:
                model_a_id: æ¨¡åž‹Açš„ID
                model_b_id: æ¨¡åž‹Bçš„ID
                version_a: æ¨¡åž‹Açš„ç‰ˆæœ¬
                version_b: æ¨¡åž‹Bçš„ç‰ˆæœ¬
                
            Returns:
                æ¯”è¾ƒç»“æžœ
            """
            metadata_a = self.get_model_metadata(model_a_id, version_a)
            metadata_b = self.get_model_metadata(model_b_id, version_b)
            
            if not metadata_a or not metadata_b:
                raise ValueError("æ— æ³•æ‰¾åˆ°æŒ‡å®šçš„æ¨¡åž‹ç‰ˆæœ¬")
            
            # æ¯”è¾ƒæŒ‡æ ‡
            comparison_metrics = {}
            performance_diff = {}
            
            # æ¯”è¾ƒå…±åŒçš„æŒ‡æ ‡
            common_metrics = set(metadata_a.metrics.keys()) & set(metadata_b.metrics.keys())
            for metric in common_metrics:
                value_a = metadata_a.metrics[metric]
                value_b = metadata_b.metrics[metric]
                comparison_metrics[metric] = {'model_a': value_a, 'model_b': value_b}
                performance_diff[metric] = value_b - value_a
            
            # ç”ŸæˆæŽ¨è
            recommendation = self._generate_recommendation(performance_diff)
            confidence_score = self._calculate_confidence_score(performance_diff)
            
            return ModelComparison(
                model_a_id=f"{model_a_id}:{metadata_a.version}",
                model_b_id=f"{model_b_id}:{metadata_b.version}",
                comparison_metrics=comparison_metrics,
                performance_diff=performance_diff,
                recommendation=recommendation,
                confidence_score=confidence_score
            )
        
        def get_version_history(self, model_id: str) -> List[Dict[str, Any]]:
            """
            èŽ·å–ç‰ˆæœ¬åŽ†å²
            
            Args:
                model_id: æ¨¡åž‹ID
                
            Returns:
                ç‰ˆæœ¬åŽ†å²åˆ—è¡¨
            """
            with self._lock:
                return self.version_history.get(model_id, []).copy()
        
        def backup_model(self, model_id: str, version: str, backup_path: Optional[str] = None) -> str:
            """
            å¤‡ä»½æ¨¡åž‹
            
            Args:
                model_id: æ¨¡åž‹ID
                version: ç‰ˆæœ¬å·
                backup_path: å¤‡ä»½è·¯å¾„
                
            Returns:
                å¤‡ä»½æ–‡ä»¶è·¯å¾„
            """
            if not self.backup_enabled:
                raise ValueError("å¤‡ä»½åŠŸèƒ½æœªå¯ç”¨")
            
            metadata = self.get_model_metadata(model_id, version)
            if not metadata or not metadata.file_path:
                raise ValueError(f"æ¨¡åž‹ {model_id} ç‰ˆæœ¬ {version} ä¸å­˜åœ¨")
            
            if backup_path is None:
                backup_dir = self.storage_path / "backups"
                backup_dir.mkdir(exist_ok=True)
                backup_path = backup_dir / f"{model_id}_{version}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.backup"
            
            # å¤åˆ¶æ¨¡åž‹æ–‡ä»¶
            shutil.copy2(metadata.file_path, backup_path)
            
            # ä¿å­˜å…ƒæ•°æ®
            metadata_backup_path = str(backup_path) + ".metadata.json"
            with open(metadata_backup_path, 'w', encoding='utf-8') as f:
                json.dump({
                    'model_id': metadata.model_id,
                    'version': metadata.version,
                    'name': metadata.name,
                    'description': metadata.description,
                    'created_at': metadata.created_at.isoformat(),
                    'created_by': metadata.created_by,
                    'model_type': metadata.model_type,
                    'framework': metadata.framework,
                    'status': metadata.status.value,
                    'tags': metadata.tags,
                    'metrics': metadata.metrics,
                    'config': metadata.config,
                    'checksum': metadata.checksum
                }, indent=2, ensure_ascii=False)
            
            return str(backup_path)
        
        def restore_model(self, backup_path: str) -> str:
            """
            ä»Žå¤‡ä»½æ¢å¤æ¨¡åž‹
            
            Args:
                backup_path: å¤‡ä»½æ–‡ä»¶è·¯å¾„
                
            Returns:
                æ¢å¤åŽçš„æ¨¡åž‹ID
            """
            backup_path = Path(backup_path)
            if not backup_path.exists():
                raise ValueError(f"å¤‡ä»½æ–‡ä»¶ä¸å­˜åœ¨: {backup_path}")
            
            # åŠ è½½å…ƒæ•°æ®
            metadata_path = Path(str(backup_path) + ".metadata.json")
            if not metadata_path.exists():
                raise ValueError(f"å¤‡ä»½å…ƒæ•°æ®æ–‡ä»¶ä¸å­˜åœ¨: {metadata_path}")
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata_dict = json.load(f)
            
            # é‡å»ºå…ƒæ•°æ®å¯¹è±¡
            metadata = ModelMetadata(
                model_id=metadata_dict['model_id'],
                version=metadata_dict['version'],
                name=metadata_dict['name'],
                description=metadata_dict['description'],
                created_at=datetime.fromisoformat(metadata_dict['created_at']),
                created_by=metadata_dict['created_by'],
                model_type=metadata_dict['model_type'],
                framework=metadata_dict['framework'],
                status=ModelStatus(metadata_dict['status']),
                tags=metadata_dict['tags'],
                metrics=metadata_dict['metrics'],
                config=metadata_dict['config'],
                checksum=metadata_dict['checksum']
            )
            
            # æ¢å¤æ¨¡åž‹æ–‡ä»¶
            model_file_path = self.storage_path / f"{metadata.model_id}_{metadata.version}.model"
            shutil.copy2(backup_path, model_file_path)
            metadata.file_path = str(model_file_path)
            metadata.file_size = model_file_path.stat().st_size
            
            # éªŒè¯æ ¡éªŒå’Œ
            if metadata.checksum != self._calculate_checksum(model_file_path):
                raise ValueError("å¤‡ä»½æ–‡ä»¶æ ¡éªŒå’Œä¸åŒ¹é…ï¼Œæ–‡ä»¶å¯èƒ½å·²æŸå")
            
            # æ³¨å†Œæ¢å¤çš„æ¨¡åž‹
            with self._lock:
                if metadata.model_id not in self.model_registry:
                    self.model_registry[metadata.model_id] = []
                    self.version_history[metadata.model_id] = []
                
                self.model_registry[metadata.model_id].append(metadata)
                self.version_history[metadata.model_id].append({
                    'version': metadata.version,
                    'timestamp': datetime.now(),
                    'action': 'restored',
                    'backup_path': str(backup_path)
                })
                
                # ä¿å­˜å…ƒæ•°æ®
                self._save_metadata(metadata)
            
            return metadata.model_id
        
        def _save_model_file(self, model: Any, metadata: ModelMetadata) -> Path:
            """ä¿å­˜æ¨¡åž‹æ–‡ä»¶"""
            model_file_path = self.storage_path / f"{metadata.model_id}_{metadata.version}.model"
            
            try:
                if metadata.framework.lower() == 'pytorch':
                    torch.save(model, model_file_path)
                else:
                    # ä½¿ç”¨pickleä½œä¸ºé€šç”¨åºåˆ—åŒ–æ–¹æ³•
                    with open(model_file_path, 'wb') as f:
                        pickle.dump(model, f)
            except Exception:
                # å¦‚æžœtorch.saveå¤±è´¥ï¼ˆæ¯”å¦‚Mockå¯¹è±¡ï¼‰ï¼Œå›žé€€åˆ°pickle
                with open(model_file_path, 'wb') as f:
                    pickle.dump(model, f)
            
            return model_file_path
        
        def _load_model_file(self, file_path: str) -> Any:
            """åŠ è½½æ¨¡åž‹æ–‡ä»¶"""
            file_path = Path(file_path)
            if not file_path.exists():
                raise ValueError(f"æ¨¡åž‹æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
            
            try:
                # å°è¯•ä½¿ç”¨torch.load
                return torch.load(file_path, map_location='cpu')
            except:
                # å›žé€€åˆ°pickle
                with open(file_path, 'rb') as f:
                    return pickle.load(f)
        
        def _save_metadata(self, metadata: ModelMetadata):
            """ä¿å­˜å…ƒæ•°æ®"""
            metadata_file = self.metadata_path / f"{metadata.model_id}_{metadata.version}.json"
            
            metadata_dict = {
                'model_id': metadata.model_id,
                'version': metadata.version,
                'name': metadata.name,
                'description': metadata.description,
                'created_at': metadata.created_at.isoformat(),
                'created_by': metadata.created_by,
                'model_type': metadata.model_type,
                'framework': metadata.framework,
                'status': metadata.status.value,
                'tags': metadata.tags,
                'metrics': metadata.metrics,
                'config': metadata.config,
                'file_path': metadata.file_path,
                'file_size': metadata.file_size,
                'checksum': metadata.checksum
            }
            
            with open(metadata_file, 'w', encoding='utf-8') as f:
                json.dump(metadata_dict, f, indent=2, ensure_ascii=False)
        
        def _load_existing_models(self):
            """åŠ è½½çŽ°æœ‰æ¨¡åž‹"""
            if not self.metadata_path.exists():
                return
            
            for metadata_file in self.metadata_path.glob("*.json"):
                try:
                    with open(metadata_file, 'r', encoding='utf-8') as f:
                        metadata_dict = json.load(f)
                    
                    metadata = ModelMetadata(
                        model_id=metadata_dict['model_id'],
                        version=metadata_dict['version'],
                        name=metadata_dict['name'],
                        description=metadata_dict['description'],
                        created_at=datetime.fromisoformat(metadata_dict['created_at']),
                        created_by=metadata_dict['created_by'],
                        model_type=metadata_dict['model_type'],
                        framework=metadata_dict['framework'],
                        status=ModelStatus(metadata_dict['status']),
                        tags=metadata_dict['tags'],
                        metrics=metadata_dict['metrics'],
                        config=metadata_dict['config'],
                        file_path=metadata_dict.get('file_path'),
                        file_size=metadata_dict.get('file_size'),
                        checksum=metadata_dict.get('checksum')
                    )
                    
                    if metadata.model_id not in self.model_registry:
                        self.model_registry[metadata.model_id] = []
                        self.version_history[metadata.model_id] = []
                    
                    self.model_registry[metadata.model_id].append(metadata)
                    
                    # è®¾ç½®æ´»è·ƒæ¨¡åž‹
                    if metadata.status == ModelStatus.ACTIVE:
                        if (metadata.model_id not in self.active_models or
                            metadata.created_at > self.active_models[metadata.model_id].created_at):
                            self.active_models[metadata.model_id] = metadata
                            
                except Exception as e:
                    # è·³è¿‡æŸåçš„å…ƒæ•°æ®æ–‡ä»¶
                    continue
        
        def _calculate_checksum(self, file_path: Path) -> str:
            """è®¡ç®—æ–‡ä»¶æ ¡éªŒå’Œ"""
            hash_md5 = hashlib.md5()
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        
        def _cleanup_old_versions(self, model_id: str):
            """æ¸…ç†æ—§ç‰ˆæœ¬"""
            if model_id not in self.model_registry:
                return
            
            versions = self.model_registry[model_id]
            if len(versions) <= self.max_versions:
                return
            
            # æŒ‰åˆ›å»ºæ—¶é—´æŽ’åºï¼Œä¿ç•™æœ€æ–°çš„ç‰ˆæœ¬
            sorted_versions = sorted(versions, key=lambda x: x.created_at, reverse=True)
            versions_to_remove = sorted_versions[self.max_versions:]
            
            for metadata in versions_to_remove:
                # ä¸åˆ é™¤æ´»è·ƒæ¨¡åž‹
                if (model_id in self.active_models and 
                    self.active_models[model_id].version == metadata.version):
                    continue
                
                # åˆ é™¤æ–‡ä»¶
                if metadata.file_path and os.path.exists(metadata.file_path):
                    os.remove(metadata.file_path)
                
                # åˆ é™¤å…ƒæ•°æ®æ–‡ä»¶
                metadata_file = self.metadata_path / f"{model_id}_{metadata.version}.json"
                if metadata_file.exists():
                    metadata_file.unlink()
                
                # ä»Žæ³¨å†Œè¡¨ä¸­ç§»é™¤
                self.model_registry[model_id].remove(metadata)
        
        def _generate_recommendation(self, performance_diff: Dict[str, float]) -> str:
            """ç”ŸæˆæŽ¨è"""
            if not performance_diff:
                return "æ— æ³•æ¯”è¾ƒï¼Œç¼ºå°‘å…±åŒæŒ‡æ ‡"
            
            positive_count = sum(1 for diff in performance_diff.values() if diff > 0)
            total_count = len(performance_diff)
            
            if positive_count / total_count > 0.7:
                return "æŽ¨èä½¿ç”¨æ¨¡åž‹Bï¼Œæ€§èƒ½æ˜¾è‘—æå‡"
            elif positive_count / total_count < 0.3:
                return "æŽ¨èç»§ç»­ä½¿ç”¨æ¨¡åž‹Aï¼Œæ€§èƒ½æ›´ç¨³å®š"
            else:
                return "ä¸¤ä¸ªæ¨¡åž‹æ€§èƒ½ç›¸è¿‘ï¼Œå»ºè®®è¿›ä¸€æ­¥æµ‹è¯•"
        
        def _calculate_confidence_score(self, performance_diff: Dict[str, float]) -> float:
            """è®¡ç®—ç½®ä¿¡åº¦åˆ†æ•°"""
            if not performance_diff:
                return 0.0
            
            # åŸºäºŽæ€§èƒ½å·®å¼‚çš„ç»å¯¹å€¼è®¡ç®—ç½®ä¿¡åº¦
            abs_diffs = [abs(diff) for diff in performance_diff.values()]
            avg_abs_diff = sum(abs_diffs) / len(abs_diffs)
            
            # å°†å¹³å‡ç»å¯¹å·®å¼‚æ˜ å°„åˆ°0-1çš„ç½®ä¿¡åº¦åˆ†æ•°
            confidence = min(avg_abs_diff * 10, 1.0)  # å‡è®¾0.1çš„å·®å¼‚å¯¹åº”100%ç½®ä¿¡åº¦
            return confidence
    ]]></file>
  <file path="src/rl_trading_system/deployment/containerized_deployment.py"><![CDATA[
    #!/usr/bin/env python3
    """
    å®¹å™¨åŒ–éƒ¨ç½²å®žçŽ°
    
    å®žçŽ°Dockerå®¹å™¨æž„å»ºå’Œè¿è¡Œã€Kuberneteséƒ¨ç½²å’ŒæœåŠ¡å‘çŽ°ã€CI/CDæµæ°´çº¿å’Œè‡ªåŠ¨åŒ–éƒ¨ç½²
    éœ€æ±‚: 8.1, 8.4
    """
    
    import os
    import json
    import yaml
    import time
    import subprocess
    from datetime import datetime
    from pathlib import Path
    from typing import Dict, List, Any, Optional, Tuple
    from dataclasses import dataclass, field
    from enum import Enum
    
    import docker
    import kubernetes
    import requests
    import consul
    from loguru import logger
    
    
    class DeploymentStatus(Enum):
        """éƒ¨ç½²çŠ¶æ€æžšä¸¾"""
        PENDING = "pending"
        BUILDING = "building"
        DEPLOYING = "deploying"
        RUNNING = "running"
        FAILED = "failed"
        STOPPED = "stopped"
    
    
    class ServiceStatus(Enum):
        """æœåŠ¡çŠ¶æ€æžšä¸¾"""
        HEALTHY = "healthy"
        UNHEALTHY = "unhealthy"
        UNKNOWN = "unknown"
    
    
    @dataclass
    class ContainerConfig:
        """å®¹å™¨é…ç½®"""
        image_name: str
        image_tag: str
        dockerfile_path: str = "./Dockerfile"
        build_context: str = "."
        environment_vars: Dict[str, str] = field(default_factory=dict)
        ports: Dict[str, str] = field(default_factory=dict)
        volumes: Dict[str, str] = field(default_factory=dict)
        health_check_cmd: str = "curl -f http://localhost:8000/health || exit 1"
        health_check_interval: int = 30
        health_check_timeout: int = 30
        health_check_retries: int = 3
        restart_policy: str = "unless-stopped"
    
    
    @dataclass
    class KubernetesConfig:
        """Kubernetesé…ç½®"""
        namespace: str
        deployment_name: str
        service_name: str
        image_name: str
        replicas: int = 3
        cpu_request: str = "500m"
        cpu_limit: str = "2000m"
        memory_request: str = "1Gi"
        memory_limit: str = "4Gi"
        ports: List[int] = field(default_factory=lambda: [8000])
        environment_vars: Dict[str, str] = field(default_factory=dict)
        config_maps: List[str] = field(default_factory=list)
        secrets: List[str] = field(default_factory=list)
        health_check_path: str = "/health"
        readiness_check_path: str = "/ready"
        service_type: str = "ClusterIP"
    
    
    class DockerManager:
        """Dockerç®¡ç†å™¨"""
        
        def __init__(self, config: ContainerConfig):
            """åˆå§‹åŒ–Dockerç®¡ç†å™¨"""
            self.config = config
            self.client = docker.from_env()
            self.image_name = f"{config.image_name}:{config.image_tag}"
            
            logger.info(f"åˆå§‹åŒ–Dockerç®¡ç†å™¨: {self.image_name}")
        
        def build_image(self) -> bool:
            """æž„å»ºDockeré•œåƒ"""
            try:
                logger.info(f"å¼€å§‹æž„å»ºé•œåƒ: {self.image_name}")
                
                # æž„å»ºé•œåƒ
                image, build_logs = self.client.images.build(
                    path=self.config.build_context,
                    dockerfile=self.config.dockerfile_path,
                    tag=self.image_name,
                    rm=True,
                    forcerm=True
                )
                
                logger.info(f"é•œåƒæž„å»ºæˆåŠŸ: {image.id}")
                return True
                
            except Exception as e:
                logger.error(f"é•œåƒæž„å»ºå¤±è´¥: {e}")
                return False
        
        def run_container(self, container_name: str) -> Optional[str]:
            """è¿è¡Œå®¹å™¨"""
            try:
                logger.info(f"å¯åŠ¨å®¹å™¨: {container_name}")
                
                # å‡†å¤‡ç«¯å£æ˜ å°„
                ports = {}
                for container_port, host_port in self.config.ports.items():
                    ports[container_port] = host_port
                
                # å‡†å¤‡å·æ˜ å°„
                volumes = {}
                for host_path, container_path in self.config.volumes.items():
                    volumes[host_path] = {
                        'bind': container_path,
                        'mode': 'rw'
                    }
                
                # è¿è¡Œå®¹å™¨
                container = self.client.containers.run(
                    image=self.image_name,
                    name=container_name,
                    ports=ports,
                    volumes=volumes,
                    environment=self.config.environment_vars,
                    detach=True,
                    restart_policy={"Name": self.config.restart_policy}
                )
                
                logger.info(f"å®¹å™¨å¯åŠ¨æˆåŠŸ: {container.id}")
                return container.id
                
            except Exception as e:
                logger.error(f"å®¹å™¨å¯åŠ¨å¤±è´¥: {e}")
                return None
        
        def stop_container(self, container_id: str) -> bool:
            """åœæ­¢å®¹å™¨"""
            try:
                logger.info(f"åœæ­¢å®¹å™¨: {container_id}")
                
                container = self.client.containers.get(container_id)
                container.stop()
                container.remove()
                
                logger.info(f"å®¹å™¨åœæ­¢æˆåŠŸ: {container_id}")
                return True
                
            except Exception as e:
                logger.error(f"å®¹å™¨åœæ­¢å¤±è´¥: {e}")
                return False
        
        def get_container_status(self, container_id: str) -> Dict[str, Any]:
            """èŽ·å–å®¹å™¨çŠ¶æ€"""
            try:
                container = self.client.containers.get(container_id)
                
                status = {
                    "id": container.id,
                    "name": container.name,
                    "status": container.status,
                    "created": container.attrs["Created"],
                    "started": container.attrs["State"]["StartedAt"]
                }
                
                # èŽ·å–å¥åº·æ£€æŸ¥çŠ¶æ€
                if "Health" in container.attrs["State"]:
                    status["health"] = container.attrs["State"]["Health"]["Status"]
                else:
                    status["health"] = "unknown"
                
                return status
                
            except Exception as e:
                logger.error(f"èŽ·å–å®¹å™¨çŠ¶æ€å¤±è´¥: {e}")
                return {"status": "unknown", "error": str(e)}
        
        def generate_dockerfile(self) -> str:
            """ç”ŸæˆDockerfile"""
            dockerfile_content = f"""# å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿ Docker é•œåƒ
    FROM python:3.9-slim
    
    # è®¾ç½®å·¥ä½œç›®å½•
    WORKDIR /app
    
    # è®¾ç½®çŽ¯å¢ƒå˜é‡
    ENV PYTHONPATH=/app/src
    ENV PYTHONUNBUFFERED=1
    ENV DEBIAN_FRONTEND=noninteractive
    
    # å®‰è£…ç³»ç»Ÿä¾èµ–
    RUN apt-get update && apt-get install -y \\
        build-essential \\
        curl \\
        git \\
        && rm -rf /var/lib/apt/lists/*
    
    # å¤åˆ¶requirementsæ–‡ä»¶
    COPY requirements.txt .
    
    # å®‰è£…Pythonä¾èµ–
    RUN pip install --no-cache-dir -r requirements.txt
    
    # å¤åˆ¶é¡¹ç›®æ–‡ä»¶
    COPY src/ src/
    COPY config/ config/
    COPY scripts/ scripts/
    COPY setup.py .
    COPY README.md .
    
    # å®‰è£…é¡¹ç›®
    RUN pip install -e .
    
    # åˆ›å»ºå¿…è¦çš„ç›®å½•
    RUN mkdir -p /app/logs /app/data /app/checkpoints /app/outputs
    
    # è®¾ç½®æƒé™
    RUN chmod +x scripts/*.py
    
    # æš´éœ²ç«¯å£
    EXPOSE {' '.join(self.config.ports.keys())}
    
    # å¥åº·æ£€æŸ¥
    HEALTHCHECK --interval={self.config.health_check_interval}s \\
        --timeout={self.config.health_check_timeout}s \\
        --start-period=5s \\
        --retries={self.config.health_check_retries} \\
        CMD {self.config.health_check_cmd}
    
    # é»˜è®¤å‘½ä»¤
    CMD ["python", "scripts/monitor.py"]
    """
            return dockerfile_content
        
        def generate_docker_compose(self) -> str:
            """ç”Ÿæˆdocker-compose.yml"""
            compose_data = {
                "version": "3.8",
                "services": {
                    "rl-trading-system": {
                        "build": ".",
                        "container_name": f"{self.config.image_name}",
                        "ports": [f"{host}:{container}" for container, host in self.config.ports.items()],
                        "volumes": [f"{host}:{container}" for host, container in self.config.volumes.items()],
                        "environment": self.config.environment_vars,
                        "restart": self.config.restart_policy,
                        "depends_on": ["redis", "influxdb"],
                        "networks": ["trading-network"]
                    },
                    "redis": {
                        "image": "redis:7-alpine",
                        "container_name": "rl-trading-redis",
                        "ports": ["6379:6379"],
                        "volumes": ["redis_data:/data"],
                        "command": "redis-server --appendonly yes",
                        "networks": ["trading-network"],
                        "restart": "unless-stopped"
                    },
                    "influxdb": {
                        "image": "influxdb:2.7-alpine",
                        "container_name": "rl-trading-influxdb",
                        "ports": ["8086:8086"],
                        "volumes": ["influxdb_data:/var/lib/influxdb2"],
                        "environment": {
                            "DOCKER_INFLUXDB_INIT_MODE": "setup",
                            "DOCKER_INFLUXDB_INIT_USERNAME": "admin",
                            "DOCKER_INFLUXDB_INIT_PASSWORD": "password123",
                            "DOCKER_INFLUXDB_INIT_ORG": "rl-trading",
                            "DOCKER_INFLUXDB_INIT_BUCKET": "trading-data"
                        },
                        "networks": ["trading-network"],
                        "restart": "unless-stopped"
                    }
                },
                "volumes": {
                    "redis_data": None,
                    "influxdb_data": None
                },
                "networks": {
                    "trading-network": {
                        "driver": "bridge"
                    }
                }
            }
            
            return yaml.dump(compose_data, default_flow_style=False)
    
    
    class KubernetesManager:
        """Kubernetesç®¡ç†å™¨"""
        
        def __init__(self, config: KubernetesConfig):
            """åˆå§‹åŒ–Kubernetesç®¡ç†å™¨"""
            self.config = config
            self.namespace = config.namespace
            
            # åŠ è½½Kubernetesé…ç½®
            try:
                kubernetes.config.load_incluster_config()
            except kubernetes.config.ConfigException:
                kubernetes.config.load_kube_config()
            
            # åˆå§‹åŒ–APIå®¢æˆ·ç«¯
            self.apps_v1 = kubernetes.client.AppsV1Api()
            self.core_v1 = kubernetes.client.CoreV1Api()
            
            logger.info(f"åˆå§‹åŒ–Kubernetesç®¡ç†å™¨: {config.deployment_name}")
        
        def create_deployment(self) -> bool:
            """åˆ›å»ºKuberneteséƒ¨ç½²"""
            try:
                logger.info(f"åˆ›å»ºéƒ¨ç½²: {self.config.deployment_name}")
                
                # æž„å»ºéƒ¨ç½²å¯¹è±¡
                deployment = self._build_deployment_object()
                
                # åˆ›å»ºéƒ¨ç½²
                self.apps_v1.create_namespaced_deployment(
                    namespace=self.namespace,
                    body=deployment
                )
                
                logger.info(f"éƒ¨ç½²åˆ›å»ºæˆåŠŸ: {self.config.deployment_name}")
                return True
                
            except Exception as e:
                logger.error(f"éƒ¨ç½²åˆ›å»ºå¤±è´¥: {e}")
                return False
        
        def create_service(self) -> bool:
            """åˆ›å»ºKubernetesæœåŠ¡"""
            try:
                logger.info(f"åˆ›å»ºæœåŠ¡: {self.config.service_name}")
                
                # æž„å»ºæœåŠ¡å¯¹è±¡
                service = self._build_service_object()
                
                # åˆ›å»ºæœåŠ¡
                self.core_v1.create_namespaced_service(
                    namespace=self.namespace,
                    body=service
                )
                
                logger.info(f"æœåŠ¡åˆ›å»ºæˆåŠŸ: {self.config.service_name}")
                return True
                
            except Exception as e:
                logger.error(f"æœåŠ¡åˆ›å»ºå¤±è´¥: {e}")
                return False
        
        def update_deployment(self, new_image: str) -> bool:
            """æ›´æ–°éƒ¨ç½²"""
            try:
                logger.info(f"æ›´æ–°éƒ¨ç½²é•œåƒ: {new_image}")
                
                # æž„å»ºæ›´æ–°è¡¥ä¸
                patch = {
                    "spec": {
                        "template": {
                            "spec": {
                                "containers": [{
                                    "name": self.config.deployment_name,
                                    "image": new_image
                                }]
                            }
                        }
                    }
                }
                
                # åº”ç”¨æ›´æ–°
                self.apps_v1.patch_namespaced_deployment(
                    name=self.config.deployment_name,
                    namespace=self.namespace,
                    body=patch
                )
                
                logger.info(f"éƒ¨ç½²æ›´æ–°æˆåŠŸ: {self.config.deployment_name}")
                return True
                
            except Exception as e:
                logger.error(f"éƒ¨ç½²æ›´æ–°å¤±è´¥: {e}")
                return False
        
        def scale_deployment(self, replicas: int) -> bool:
            """æ‰©ç¼©å®¹éƒ¨ç½²"""
            try:
                logger.info(f"æ‰©ç¼©å®¹éƒ¨ç½²: {replicas} å‰¯æœ¬")
                
                # æž„å»ºæ‰©ç¼©å®¹è¡¥ä¸
                patch = {"spec": {"replicas": replicas}}
                
                # åº”ç”¨æ‰©ç¼©å®¹
                self.apps_v1.patch_namespaced_deployment_scale(
                    name=self.config.deployment_name,
                    namespace=self.namespace,
                    body=patch
                )
                
                logger.info(f"æ‰©ç¼©å®¹æˆåŠŸ: {replicas} å‰¯æœ¬")
                return True
                
            except Exception as e:
                logger.error(f"æ‰©ç¼©å®¹å¤±è´¥: {e}")
                return False
        
        def get_deployment_status(self) -> Dict[str, Any]:
            """èŽ·å–éƒ¨ç½²çŠ¶æ€"""
            try:
                deployment = self.apps_v1.read_namespaced_deployment(
                    name=self.config.deployment_name,
                    namespace=self.namespace
                )
                
                status = {
                    "name": deployment.metadata.name,
                    "namespace": deployment.metadata.namespace,
                    "replicas": deployment.status.replicas or 0,
                    "ready_replicas": deployment.status.ready_replicas or 0,
                    "available_replicas": deployment.status.available_replicas or 0,
                    "updated_replicas": deployment.status.updated_replicas or 0,
                    "is_ready": False
                }
                
                # æ£€æŸ¥æ˜¯å¦å°±ç»ª
                if (status["ready_replicas"] == status["replicas"] and 
                    status["available_replicas"] == status["replicas"]):
                    status["is_ready"] = True
                
                return status
                
            except Exception as e:
                logger.error(f"èŽ·å–éƒ¨ç½²çŠ¶æ€å¤±è´¥: {e}")
                return {"is_ready": False, "error": str(e)}
        
        def _build_deployment_object(self) -> kubernetes.client.V1Deployment:
            """æž„å»ºéƒ¨ç½²å¯¹è±¡"""
            # å®¹å™¨é…ç½®
            container = kubernetes.client.V1Container(
                name=self.config.deployment_name,
                image=self.config.image_name,
                ports=[kubernetes.client.V1ContainerPort(container_port=port) for port in self.config.ports],
                env=[kubernetes.client.V1EnvVar(name=k, value=v) for k, v in self.config.environment_vars.items()],
                resources=kubernetes.client.V1ResourceRequirements(
                    requests={
                        "cpu": self.config.cpu_request,
                        "memory": self.config.memory_request
                    },
                    limits={
                        "cpu": self.config.cpu_limit,
                        "memory": self.config.memory_limit
                    }
                ),
                liveness_probe=kubernetes.client.V1Probe(
                    http_get=kubernetes.client.V1HTTPGetAction(
                        path=self.config.health_check_path,
                        port=self.config.ports[0]
                    ),
                    initial_delay_seconds=30,
                    period_seconds=10
                ),
                readiness_probe=kubernetes.client.V1Probe(
                    http_get=kubernetes.client.V1HTTPGetAction(
                        path=self.config.readiness_check_path,
                        port=self.config.ports[0]
                    ),
                    initial_delay_seconds=5,
                    period_seconds=5
                )
            )
            
            # Podæ¨¡æ¿
            template = kubernetes.client.V1PodTemplateSpec(
                metadata=kubernetes.client.V1ObjectMeta(
                    labels={"app": self.config.deployment_name}
                ),
                spec=kubernetes.client.V1PodSpec(containers=[container])
            )
            
            # éƒ¨ç½²è§„æ ¼
            spec = kubernetes.client.V1DeploymentSpec(
                replicas=self.config.replicas,
                selector=kubernetes.client.V1LabelSelector(
                    match_labels={"app": self.config.deployment_name}
                ),
                template=template
            )
            
            # éƒ¨ç½²å¯¹è±¡
            deployment = kubernetes.client.V1Deployment(
                api_version="apps/v1",
                kind="Deployment",
                metadata=kubernetes.client.V1ObjectMeta(
                    name=self.config.deployment_name,
                    namespace=self.namespace
                ),
                spec=spec
            )
            
            return deployment
        
        def _build_service_object(self) -> kubernetes.client.V1Service:
            """æž„å»ºæœåŠ¡å¯¹è±¡"""
            # æœåŠ¡ç«¯å£
            ports = [
                kubernetes.client.V1ServicePort(
                    port=port,
                    target_port=port,
                    protocol="TCP"
                ) for port in self.config.ports
            ]
            
            # æœåŠ¡è§„æ ¼
            spec = kubernetes.client.V1ServiceSpec(
                selector={"app": self.config.deployment_name},
                ports=ports,
                type=self.config.service_type
            )
            
            # æœåŠ¡å¯¹è±¡
            service = kubernetes.client.V1Service(
                api_version="v1",
                kind="Service",
                metadata=kubernetes.client.V1ObjectMeta(
                    name=self.config.service_name,
                    namespace=self.namespace
                ),
                spec=spec
            )
            
            return service
        
        def generate_deployment_yaml(self) -> str:
            """ç”Ÿæˆéƒ¨ç½²YAML"""
            deployment_data = {
                "apiVersion": "apps/v1",
                "kind": "Deployment",
                "metadata": {
                    "name": self.config.deployment_name,
                    "namespace": self.config.namespace
                },
                "spec": {
                    "replicas": self.config.replicas,
                    "selector": {
                        "matchLabels": {
                            "app": self.config.deployment_name
                        }
                    },
                    "template": {
                        "metadata": {
                            "labels": {
                                "app": self.config.deployment_name
                            }
                        },
                        "spec": {
                            "containers": [{
                                "name": self.config.deployment_name,
                                "image": self.config.image_name,
                                "ports": [{"containerPort": port} for port in self.config.ports],
                                "env": [{"name": k, "value": v} for k, v in self.config.environment_vars.items()],
                                "resources": {
                                    "requests": {
                                        "cpu": self.config.cpu_request,
                                        "memory": self.config.memory_request
                                    },
                                    "limits": {
                                        "cpu": self.config.cpu_limit,
                                        "memory": self.config.memory_limit
                                    }
                                },
                                "livenessProbe": {
                                    "httpGet": {
                                        "path": self.config.health_check_path,
                                        "port": self.config.ports[0]
                                    },
                                    "initialDelaySeconds": 30,
                                    "periodSeconds": 10
                                },
                                "readinessProbe": {
                                    "httpGet": {
                                        "path": self.config.readiness_check_path,
                                        "port": self.config.ports[0]
                                    },
                                    "initialDelaySeconds": 5,
                                    "periodSeconds": 5
                                }
                            }]
                        }
                    }
                }
            }
            
            return yaml.dump(deployment_data, default_flow_style=False)
        
        def generate_service_yaml(self) -> str:
            """ç”ŸæˆæœåŠ¡YAML"""
            service_data = {
                "apiVersion": "v1",
                "kind": "Service",
                "metadata": {
                    "name": self.config.service_name,
                    "namespace": self.config.namespace
                },
                "spec": {
                    "selector": {
                        "app": self.config.deployment_name
                    },
                    "ports": [
                        {
                            "port": port,
                            "targetPort": port,
                            "protocol": "TCP"
                        } for port in self.config.ports
                    ],
                    "type": self.config.service_type
                }
            }
            
            return yaml.dump(service_data, default_flow_style=False)
    
    
    class CICDPipeline:
        """CI/CDæµæ°´çº¿"""
        
        def __init__(self, config: Dict[str, Any]):
            """åˆå§‹åŒ–CI/CDæµæ°´çº¿"""
            self.config = config
            self.repository = config.get("repository", "")
            self.branch = config.get("branch", "main")
            self.build_stages = config.get("build_stages", ["test", "build", "deploy"])
            self.test_commands = config.get("test_commands", [])
            self.build_commands = config.get("build_commands", [])
            self.deploy_commands = config.get("deploy_commands", [])
            
            logger.info(f"åˆå§‹åŒ–CI/CDæµæ°´çº¿: {self.repository}")
        
        def run_tests(self) -> bool:
            """è¿è¡Œæµ‹è¯•"""
            try:
                logger.info("å¼€å§‹è¿è¡Œæµ‹è¯•")
                
                for command in self.test_commands:
                    logger.info(f"æ‰§è¡Œæµ‹è¯•å‘½ä»¤: {command}")
                    
                    result = subprocess.run(
                        command.split(),
                        capture_output=True,
                        text=True,
                        timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
                    )
                    
                    if result.returncode != 0:
                        logger.error(f"æµ‹è¯•å¤±è´¥: {result.stderr}")
                        return False
                    
                    logger.info(f"æµ‹è¯•é€šè¿‡: {result.stdout}")
                
                logger.info("æ‰€æœ‰æµ‹è¯•é€šè¿‡")
                return True
                
            except Exception as e:
                logger.error(f"æµ‹è¯•æ‰§è¡Œå¤±è´¥: {e}")
                return False
        
        def build_image(self) -> bool:
            """æž„å»ºé•œåƒ"""
            try:
                logger.info("å¼€å§‹æž„å»ºé•œåƒ")
                
                for command in self.build_commands:
                    logger.info(f"æ‰§è¡Œæž„å»ºå‘½ä»¤: {command}")
                    
                    result = subprocess.run(
                        command.split(),
                        capture_output=True,
                        text=True,
                        timeout=1800  # 30åˆ†é’Ÿè¶…æ—¶
                    )
                    
                    if result.returncode != 0:
                        logger.error(f"æž„å»ºå¤±è´¥: {result.stderr}")
                        return False
                    
                    logger.info(f"æž„å»ºæˆåŠŸ: {result.stdout}")
                
                logger.info("é•œåƒæž„å»ºå®Œæˆ")
                return True
                
            except Exception as e:
                logger.error(f"é•œåƒæž„å»ºå¤±è´¥: {e}")
                return False
        
        def deploy_to_kubernetes(self) -> bool:
            """éƒ¨ç½²åˆ°Kubernetes"""
            try:
                logger.info("å¼€å§‹éƒ¨ç½²åˆ°Kubernetes")
                
                for command in self.deploy_commands:
                    logger.info(f"æ‰§è¡Œéƒ¨ç½²å‘½ä»¤: {command}")
                    
                    result = subprocess.run(
                        command.split(),
                        capture_output=True,
                        text=True,
                        timeout=600  # 10åˆ†é’Ÿè¶…æ—¶
                    )
                    
                    if result.returncode != 0:
                        logger.error(f"éƒ¨ç½²å¤±è´¥: {result.stderr}")
                        return False
                    
                    logger.info(f"éƒ¨ç½²æˆåŠŸ: {result.stdout}")
                
                logger.info("Kuberneteséƒ¨ç½²å®Œæˆ")
                return True
                
            except Exception as e:
                logger.error(f"Kuberneteséƒ¨ç½²å¤±è´¥: {e}")
                return False
        
        def generate_github_actions_workflow(self) -> str:
            """ç”ŸæˆGitHub Actionså·¥ä½œæµ"""
            workflow_data = {
                "name": "CI/CD Pipeline",
                "on": {
                    "push": {
                        "branches": [self.branch]
                    },
                    "pull_request": {
                        "branches": [self.branch]
                    }
                },
                "jobs": {
                    "test": {
                        "runs-on": "ubuntu-latest",
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up Python",
                                "uses": "actions/setup-python@v4",
                                "with": {
                                    "python-version": "3.9"
                                }
                            },
                            {
                                "name": "Install dependencies",
                                "run": "pip install -r requirements.txt"
                            },
                            {
                                "name": "Run tests",
                                "run": " && ".join(self.test_commands)
                            }
                        ]
                    },
                    "build": {
                        "needs": "test",
                        "runs-on": "ubuntu-latest",
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up Docker Buildx",
                                "uses": "docker/setup-buildx-action@v2"
                            },
                            {
                                "name": "Login to Docker Hub",
                                "uses": "docker/login-action@v2",
                                "with": {
                                    "username": "${{ secrets.DOCKER_USERNAME }}",
                                    "password": "${{ secrets.DOCKER_PASSWORD }}"
                                }
                            },
                            {
                                "name": "Build and push",
                                "run": " && ".join(self.build_commands)
                            }
                        ]
                    },
                    "deploy": {
                        "needs": "build",
                        "runs-on": "ubuntu-latest",
                        "if": "github.ref == 'refs/heads/main'",
                        "steps": [
                            {
                                "name": "Checkout code",
                                "uses": "actions/checkout@v3"
                            },
                            {
                                "name": "Set up kubectl",
                                "uses": "azure/setup-kubectl@v3"
                            },
                            {
                                "name": "Deploy to Kubernetes",
                                "run": " && ".join(self.deploy_commands),
                                "env": {
                                    "KUBE_CONFIG": "${{ secrets.KUBE_CONFIG }}"
                                }
                            }
                        ]
                    }
                }
            }
            
            return yaml.dump(workflow_data, default_flow_style=False)
        
        def generate_jenkins_pipeline(self) -> str:
            """ç”ŸæˆJenkinsæµæ°´çº¿"""
            pipeline_script = f"""pipeline {{
        agent any
        
        environment {{
            DOCKER_REGISTRY = 'your-registry.com'
            IMAGE_NAME = 'rl-trading-system'
            KUBE_NAMESPACE = 'rl-trading'
        }}
        
        stages {{
            stage('Checkout') {{
                steps {{
                    git branch: '{self.branch}', url: '{self.repository}'
                }}
            }}
            
            stage('Test') {{
                steps {{
                    script {{
                        sh '''
                            {' && '.join(self.test_commands)}
                        '''
                    }}
                }}
            }}
            
            stage('Build') {{
                steps {{
                    script {{
                        sh '''
                            {' && '.join(self.build_commands)}
                        '''
                    }}
                }}
            }}
            
            stage('Deploy') {{
                when {{
                    branch '{self.branch}'
                }}
                steps {{
                    script {{
                        sh '''
                            {' && '.join(self.deploy_commands)}
                        '''
                    }}
                }}
            }}
        }}
        
        post {{
            always {{
                cleanWs()
            }}
            success {{
                echo 'Pipeline succeeded!'
            }}
            failure {{
                echo 'Pipeline failed!'
            }}
        }}
    }}"""
            return pipeline_script
    
    
    class HealthChecker:
        """å¥åº·æ£€æŸ¥å™¨"""
        
        def __init__(self, endpoints: List[str], timeout: int = 30, 
                     retry_count: int = 3, retry_interval: int = 5):
            """åˆå§‹åŒ–å¥åº·æ£€æŸ¥å™¨"""
            self.endpoints = endpoints
            self.timeout = timeout
            self.retry_count = retry_count
            self.retry_interval = retry_interval
            
            logger.info(f"åˆå§‹åŒ–å¥åº·æ£€æŸ¥å™¨: {len(endpoints)} ä¸ªç«¯ç‚¹")
        
        def check_endpoint(self, endpoint: str) -> bool:
            """æ£€æŸ¥å•ä¸ªç«¯ç‚¹"""
            try:
                response = requests.get(endpoint, timeout=self.timeout)
                return response.status_code == 200
            except Exception as e:
                logger.warning(f"ç«¯ç‚¹æ£€æŸ¥å¤±è´¥ {endpoint}: {e}")
                return False
        
        def check_all_endpoints(self) -> Dict[str, bool]:
            """æ£€æŸ¥æ‰€æœ‰ç«¯ç‚¹"""
            results = {}
            
            for endpoint in self.endpoints:
                logger.info(f"æ£€æŸ¥ç«¯ç‚¹: {endpoint}")
                results[endpoint] = self.check_endpoint(endpoint)
            
            return results
        
        def wait_for_healthy(self, endpoint: str, max_wait: int = 300) -> bool:
            """ç­‰å¾…ç«¯ç‚¹å¥åº·"""
            start_time = time.time()
            
            while time.time() - start_time < max_wait:
                if self.check_endpoint(endpoint):
                    logger.info(f"ç«¯ç‚¹å¥åº·: {endpoint}")
                    return True
                
                logger.info(f"ç­‰å¾…ç«¯ç‚¹å¥åº·: {endpoint}")
                time.sleep(self.retry_interval)
            
            logger.error(f"ç«¯ç‚¹å¥åº·æ£€æŸ¥è¶…æ—¶: {endpoint}")
            return False
    
    
    class ServiceDiscovery:
        """æœåŠ¡å‘çŽ°"""
        
        def __init__(self, consul_host: str = "localhost", consul_port: int = 8500,
                     service_name: str = "rl-trading-system", service_port: int = 8000,
                     health_check_url: str = "http://localhost:8000/health"):
            """åˆå§‹åŒ–æœåŠ¡å‘çŽ°"""
            self.consul_host = consul_host
            self.consul_port = consul_port
            self.service_name = service_name
            self.service_port = service_port
            self.health_check_url = health_check_url
            
            # åˆå§‹åŒ–Consulå®¢æˆ·ç«¯
            self.consul = consul.Consul(host=consul_host, port=consul_port)
            
            logger.info(f"åˆå§‹åŒ–æœåŠ¡å‘çŽ°: {service_name}")
        
        def register_service(self) -> bool:
            """æ³¨å†ŒæœåŠ¡"""
            try:
                logger.info(f"æ³¨å†ŒæœåŠ¡: {self.service_name}")
                
                # æ³¨å†ŒæœåŠ¡
                self.consul.agent.service.register(
                    name=self.service_name,
                    service_id=f"{self.service_name}-{os.getpid()}",
                    port=self.service_port,
                    check=consul.Check.http(self.health_check_url, interval="10s")
                )
                
                logger.info(f"æœåŠ¡æ³¨å†ŒæˆåŠŸ: {self.service_name}")
                return True
                
            except Exception as e:
                logger.error(f"æœåŠ¡æ³¨å†Œå¤±è´¥: {e}")
                return False
        
        def deregister_service(self) -> bool:
            """æ³¨é”€æœåŠ¡"""
            try:
                logger.info(f"æ³¨é”€æœåŠ¡: {self.service_name}")
                
                service_id = f"{self.service_name}-{os.getpid()}"
                self.consul.agent.service.deregister(service_id)
                
                logger.info(f"æœåŠ¡æ³¨é”€æˆåŠŸ: {self.service_name}")
                return True
                
            except Exception as e:
                logger.error(f"æœåŠ¡æ³¨é”€å¤±è´¥: {e}")
                return False
        
        def discover_services(self, service_name: str) -> List[Dict[str, Any]]:
            """å‘çŽ°æœåŠ¡"""
            try:
                logger.info(f"å‘çŽ°æœåŠ¡: {service_name}")
                
                # èŽ·å–å¥åº·çš„æœåŠ¡å®žä¾‹
                _, services = self.consul.health.service(service_name, passing=True)
                
                service_list = []
                for service in services:
                    service_info = {
                        "service_id": service["Service"]["ID"],
                        "service_name": service["Service"]["Service"],
                        "address": service["Service"]["Address"],
                        "port": service["Service"]["Port"],
                        "tags": service["Service"]["Tags"]
                    }
                    service_list.append(service_info)
                
                logger.info(f"å‘çŽ° {len(service_list)} ä¸ªæœåŠ¡å®žä¾‹")
                return service_list
                
            except Exception as e:
                logger.error(f"æœåŠ¡å‘çŽ°å¤±è´¥: {e}")
                return []
    ]]></file>
  <file path="src/rl_trading_system/deployment/canary_deployment.py"><![CDATA[
    """
    é‡‘ä¸é›€éƒ¨ç½²ç³»ç»Ÿå®žçŽ°
    å®žçŽ°CanaryDeploymentç±»å’Œç°åº¦å‘å¸ƒæµç¨‹ï¼ŒA/Bæµ‹è¯•æ¡†æž¶å’Œæ¨¡åž‹æ€§èƒ½å¯¹æ¯”ï¼Œè‡ªåŠ¨å›žæ»šæœºåˆ¶å’Œéƒ¨ç½²å®‰å…¨æŽ§åˆ¶
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import hashlib
    import time
    import threading
    import psutil
    from datetime import datetime, timedelta
    from typing import Dict, List, Optional, Any, Union, Callable
    from dataclasses import dataclass, field
    from enum import Enum
    from collections import defaultdict, deque
    import uuid
    import numpy as np
    import pandas as pd
    from scipy import stats
    import requests
    
    
    class DeploymentStatus(Enum):
        """éƒ¨ç½²çŠ¶æ€æžšä¸¾"""
        PENDING = "pending"
        ACTIVE = "active"
        COMPLETED = "completed"
        ROLLED_BACK = "rolled_back"
        FAILED = "failed"
    
    
    @dataclass
    class PerformanceMetrics:
        """æ€§èƒ½æŒ‡æ ‡æ•°æ®ç±»"""
        success_rate: float
        error_rate: float
        avg_response_time: float
        throughput: float
        accuracy: float
        precision: float
        recall: float
        f1_score: float
        timestamp: datetime = field(default_factory=datetime.now)
        
        def __post_init__(self):
            """åˆå§‹åŒ–åŽéªŒè¯"""
            if not (0 <= self.success_rate <= 1):
                raise ValueError("æˆåŠŸçŽ‡å¿…é¡»åœ¨0-1ä¹‹é—´")
            if not (0 <= self.error_rate <= 1):
                raise ValueError("é”™è¯¯çŽ‡å¿…é¡»åœ¨0-1ä¹‹é—´")
            if self.avg_response_time < 0:
                raise ValueError("å“åº”æ—¶é—´ä¸èƒ½ä¸ºè´Ÿæ•°")
            if self.throughput < 0:
                raise ValueError("åžåé‡ä¸èƒ½ä¸ºè´Ÿæ•°")
    
    
    @dataclass
    class DeploymentConfig:
        """éƒ¨ç½²é…ç½®æ•°æ®ç±»"""
        canary_percentage: float
        evaluation_period: int  # ç§’
        success_threshold: float
        error_threshold: float
        performance_threshold: float
        rollback_threshold: float
        max_canary_duration: int  # ç§’
        
        def __post_init__(self):
            """åˆå§‹åŒ–åŽéªŒè¯"""
            if not (0 <= self.canary_percentage <= 100):
                raise ValueError("é‡‘ä¸é›€æµé‡ç™¾åˆ†æ¯”å¿…é¡»åœ¨0-100ä¹‹é—´")
            if not (0 <= self.success_threshold <= 1):
                raise ValueError("æˆåŠŸçŽ‡é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
            if not (0 <= self.error_threshold <= 1):
                raise ValueError("é”™è¯¯çŽ‡é˜ˆå€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
            if self.evaluation_period <= 0:
                raise ValueError("è¯„ä¼°å‘¨æœŸå¿…é¡»ä¸ºæ­£æ•°")
            if self.max_canary_duration <= 0:
                raise ValueError("æœ€å¤§é‡‘ä¸é›€æŒç»­æ—¶é—´å¿…é¡»ä¸ºæ­£æ•°")
    
    
    class TrafficRouter:
        """æµé‡è·¯ç”±å™¨ç±»"""
        
        def __init__(self, canary_percentage: float = 10.0, routing_strategy: str = 'weighted_random',
                     sticky_sessions: bool = True):
            """
            åˆå§‹åŒ–æµé‡è·¯ç”±å™¨
            
            Args:
                canary_percentage: é‡‘ä¸é›€æµé‡ç™¾åˆ†æ¯”
                routing_strategy: è·¯ç”±ç­–ç•¥
                sticky_sessions: æ˜¯å¦å¯ç”¨ç²˜æ€§ä¼šè¯
            """
            self.canary_percentage = canary_percentage
            self.routing_strategy = routing_strategy
            self.sticky_sessions = sticky_sessions
            self.user_assignments = {}
            self.routing_metrics = {
                'total_requests': 0,
                'canary_requests': 0,
                'baseline_requests': 0
            }
            self.geographic_config = {}
            self.canary_instances = []
            self.emergency_cutoff = False
            self._lock = threading.Lock()
        
        def route_request(self, user_id: str, canary_model: Any, baseline_model: Any, 
                         region: str = None) -> Any:
            """è·¯ç”±è¯·æ±‚åˆ°åˆé€‚çš„æ¨¡åž‹"""
            with self._lock:
                self.routing_metrics['total_requests'] += 1
                
                # ç´§æ€¥åˆ‡æ–­æ£€æŸ¥
                if self.emergency_cutoff:
                    self.routing_metrics['baseline_requests'] += 1
                    return baseline_model
                
                # ç²˜æ€§ä¼šè¯æ£€æŸ¥
                if self.sticky_sessions and user_id in self.user_assignments:
                    assigned_model = self.user_assignments[user_id]
                    if assigned_model == 'canary':
                        self.routing_metrics['canary_requests'] += 1
                        return canary_model
                    else:
                        self.routing_metrics['baseline_requests'] += 1
                        return baseline_model
                
                # ç¡®å®šé‡‘ä¸é›€ç™¾åˆ†æ¯”
                effective_percentage = self.canary_percentage
                if region and region in self.geographic_config:
                    effective_percentage = self.geographic_config[region]
                
                # è·¯ç”±å†³ç­–
                if self.routing_strategy == 'weighted_random':
                    route_to_canary = np.random.random() < (effective_percentage / 100.0)
                elif self.routing_strategy == 'hash_based':
                    user_hash = int(hashlib.md5(user_id.encode()).hexdigest()[:8], 16)
                    route_to_canary = (user_hash % 100) < effective_percentage
                else:
                    route_to_canary = np.random.random() < (effective_percentage / 100.0)
                
                # è®°å½•åˆ†é…å¹¶æ›´æ–°æŒ‡æ ‡
                if route_to_canary:
                    if self.sticky_sessions:
                        self.user_assignments[user_id] = 'canary'
                    self.routing_metrics['canary_requests'] += 1
                    
                    # å¦‚æžœæœ‰å¤šä¸ªé‡‘ä¸é›€å®žä¾‹ï¼Œè¿›è¡Œè´Ÿè½½å‡è¡¡
                    if self.canary_instances:
                        instance_index = hash(user_id) % len(self.canary_instances)
                        return self.canary_instances[instance_index]
                    return canary_model
                else:
                    if self.sticky_sessions:
                        self.user_assignments[user_id] = 'baseline'
                    self.routing_metrics['baseline_requests'] += 1
                    return baseline_model
        
        def update_canary_percentage(self, new_percentage: float):
            """æ›´æ–°é‡‘ä¸é›€æµé‡ç™¾åˆ†æ¯”"""
            if not (0 <= new_percentage <= 100):
                raise ValueError("é‡‘ä¸é›€ç™¾åˆ†æ¯”å¿…é¡»åœ¨0-100ä¹‹é—´")
            
            with self._lock:
                self.canary_percentage = new_percentage
        
        def set_geographic_routing(self, geographic_config: Dict[str, float]):
            """è®¾ç½®åœ°ç†ä½ç½®è·¯ç”±é…ç½®"""
            for region, percentage in geographic_config.items():
                if not (0 <= percentage <= 100):
                    raise ValueError(f"åœ°åŒº {region} çš„ç™¾åˆ†æ¯”å¿…é¡»åœ¨0-100ä¹‹é—´")
            
            self.geographic_config = geographic_config.copy()
        
        def set_canary_instances(self, instances: List[Any]):
            """è®¾ç½®å¤šä¸ªé‡‘ä¸é›€æ¨¡åž‹å®žä¾‹"""
            self.canary_instances = instances.copy()
        
        def emergency_cutoff_canary(self):
            """ç´§æ€¥åˆ‡æ–­é‡‘ä¸é›€æµé‡"""
            with self._lock:
                self.emergency_cutoff = True
        
        def restore_normal_routing(self):
            """æ¢å¤æ­£å¸¸è·¯ç”±"""
            with self._lock:
                self.emergency_cutoff = False
        
        def get_routing_metrics(self) -> Dict[str, Any]:
            """èŽ·å–è·¯ç”±æŒ‡æ ‡"""
            with self._lock:
                metrics = self.routing_metrics.copy()
                if metrics['total_requests'] > 0:
                    metrics['canary_percentage_actual'] = (
                        metrics['canary_requests'] / metrics['total_requests'] * 100
                    )
                else:
                    metrics['canary_percentage_actual'] = 0.0
                return metrics
    
    
    class ModelPerformanceComparator:
        """æ¨¡åž‹æ€§èƒ½æ¯”è¾ƒå™¨ç±»"""
        
        def __init__(self, comparison_window: int = 3600, min_samples_for_comparison: int = 100,
                     significance_threshold: float = 0.05):
            """
            åˆå§‹åŒ–æ€§èƒ½æ¯”è¾ƒå™¨
            
            Args:
                comparison_window: æ¯”è¾ƒæ—¶é—´çª—å£ï¼ˆç§’ï¼‰
                min_samples_for_comparison: æœ€å°æ¯”è¾ƒæ ·æœ¬æ•°
                significance_threshold: æ˜¾è‘—æ€§é˜ˆå€¼
            """
            self.comparison_window = comparison_window
            self.min_samples_for_comparison = min_samples_for_comparison
            self.significance_threshold = significance_threshold
            self.model_a_metrics = deque(maxlen=10000)
            self.model_b_metrics = deque(maxlen=10000)
            self.latest_comparison = {}
            self._lock = threading.Lock()
        
        def add_model_a_metrics(self, metrics: PerformanceMetrics):
            """æ·»åŠ æ¨¡åž‹Açš„æŒ‡æ ‡"""
            with self._lock:
                self.model_a_metrics.append(metrics)
        
        def add_model_b_metrics(self, metrics: PerformanceMetrics):
            """æ·»åŠ æ¨¡åž‹Bçš„æŒ‡æ ‡"""
            with self._lock:
                self.model_b_metrics.append(metrics)
        
        def compare_performance(self) -> Dict[str, Any]:
            """æ¯”è¾ƒä¸¤ä¸ªæ¨¡åž‹çš„æ€§èƒ½"""
            with self._lock:
                if len(self.model_a_metrics) == 0 or len(self.model_b_metrics) == 0:
                    return {'error': 'ç¼ºå°‘æŒ‡æ ‡æ•°æ®è¿›è¡Œæ¯”è¾ƒ'}
                
                # èŽ·å–æœ€è¿‘æ—¶é—´çª—å£å†…çš„æŒ‡æ ‡
                current_time = datetime.now()
                cutoff_time = current_time - timedelta(seconds=self.comparison_window)
                
                recent_a = [m for m in self.model_a_metrics if m.timestamp >= cutoff_time]
                recent_b = [m for m in self.model_b_metrics if m.timestamp >= cutoff_time]
                
                if len(recent_a) == 0 or len(recent_b) == 0:
                    return {'error': 'æ—¶é—´çª—å£å†…ç¼ºå°‘æŒ‡æ ‡æ•°æ®'}
                
                # è®¡ç®—å¹³å‡æŒ‡æ ‡
                avg_a = self._calculate_average_metrics(recent_a)
                avg_b = self._calculate_average_metrics(recent_b)
                
                # è®¡ç®—å·®å¼‚
                comparison_result = {
                    'success_rate_diff': avg_b['success_rate'] - avg_a['success_rate'],
                    'error_rate_diff': avg_b['error_rate'] - avg_a['error_rate'],
                    'response_time_diff': avg_b['avg_response_time'] - avg_a['avg_response_time'],
                    'throughput_diff': avg_b['throughput'] - avg_a['throughput'],
                    'accuracy_diff': avg_b['accuracy'] - avg_a['accuracy'],
                    'model_a_samples': len(recent_a),
                    'model_b_samples': len(recent_b),
                    'comparison_timestamp': current_time
                }
                
                # è®¡ç®—æ•´ä½“æ€§èƒ½åˆ†æ•°
                performance_score_a = self._calculate_performance_score(avg_a)
                performance_score_b = self._calculate_performance_score(avg_b)
                comparison_result['overall_performance_score'] = performance_score_b - performance_score_a
                
                # ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
                if len(recent_a) >= self.min_samples_for_comparison and len(recent_b) >= self.min_samples_for_comparison:
                    significance_test = self.test_statistical_significance()
                    comparison_result.update(significance_test)
                
                self.latest_comparison = comparison_result
                return comparison_result
        
        def _calculate_average_metrics(self, metrics_list: List[PerformanceMetrics]) -> Dict[str, float]:
            """è®¡ç®—å¹³å‡æŒ‡æ ‡"""
            if not metrics_list:
                return {}
            
            return {
                'success_rate': np.mean([m.success_rate for m in metrics_list]),
                'error_rate': np.mean([m.error_rate for m in metrics_list]),
                'avg_response_time': np.mean([m.avg_response_time for m in metrics_list]),
                'throughput': np.mean([m.throughput for m in metrics_list]),
                'accuracy': np.mean([m.accuracy for m in metrics_list]),
                'precision': np.mean([m.precision for m in metrics_list]),
                'recall': np.mean([m.recall for m in metrics_list]),
                'f1_score': np.mean([m.f1_score for m in metrics_list])
            }
        
        def _calculate_performance_score(self, avg_metrics: Dict[str, float]) -> float:
            """è®¡ç®—æ€§èƒ½åˆ†æ•°"""
            # åŠ æƒæ€§èƒ½åˆ†æ•°
            score = (
                avg_metrics.get('success_rate', 0) * 0.3 +
                (1 - avg_metrics.get('error_rate', 1)) * 0.2 +
                (1 / max(avg_metrics.get('avg_response_time', 1), 0.001)) * 0.2 +
                avg_metrics.get('throughput', 0) / 1000.0 * 0.1 +
                avg_metrics.get('accuracy', 0) * 0.2
            )
            return score
        
        def test_statistical_significance(self) -> Dict[str, Any]:
            """æµ‹è¯•ç»Ÿè®¡æ˜¾è‘—æ€§"""
            recent_a = list(self.model_a_metrics)[-self.min_samples_for_comparison:]
            recent_b = list(self.model_b_metrics)[-self.min_samples_for_comparison:]
            
            results = {}
            
            # æˆåŠŸçŽ‡æ¯”è¾ƒ
            success_rates_a = [m.success_rate for m in recent_a]
            success_rates_b = [m.success_rate for m in recent_b]
            stat, p_val = stats.ttest_ind(success_rates_a, success_rates_b)
            results['success_rate_significant'] = p_val < self.significance_threshold
            results['success_rate_p_value'] = p_val
            
            # é”™è¯¯çŽ‡æ¯”è¾ƒ
            error_rates_a = [m.error_rate for m in recent_a]
            error_rates_b = [m.error_rate for m in recent_b]
            stat, p_val = stats.ttest_ind(error_rates_a, error_rates_b)
            results['error_rate_significant'] = p_val < self.significance_threshold
            results['error_rate_p_value'] = p_val
            
            # å“åº”æ—¶é—´æ¯”è¾ƒ
            response_times_a = [m.avg_response_time for m in recent_a]
            response_times_b = [m.avg_response_time for m in recent_b]
            stat, p_val = stats.ttest_ind(response_times_a, response_times_b)
            results['response_time_significant'] = p_val < self.significance_threshold
            results['response_time_p_value'] = p_val
            
            return results
        
        def analyze_performance_trends(self) -> Dict[str, Any]:
            """åˆ†æžæ€§èƒ½è¶‹åŠ¿"""
            if len(self.model_a_metrics) < 10 or len(self.model_b_metrics) < 10:
                return {'error': 'æ•°æ®ä¸è¶³ä»¥è¿›è¡Œè¶‹åŠ¿åˆ†æž'}
            
            # è®¡ç®—æœ€è¿‘10ä¸ªæ•°æ®ç‚¹çš„è¶‹åŠ¿
            recent_a = list(self.model_a_metrics)[-10:]
            recent_b = list(self.model_b_metrics)[-10:]
            
            # æ¨¡åž‹Aè¶‹åŠ¿
            a_success_rates = [m.success_rate for m in recent_a]
            a_trend_slope, _, _, p_val_a, _ = stats.linregress(range(len(a_success_rates)), a_success_rates)
            
            # æ¨¡åž‹Bè¶‹åŠ¿
            b_success_rates = [m.success_rate for m in recent_b]
            b_trend_slope, _, _, p_val_b, _ = stats.linregress(range(len(b_success_rates)), b_success_rates)
            
            return {
                'model_a_trend': {
                    'slope': a_trend_slope,
                    'direction': 'improving' if a_trend_slope > 0 else 'declining',
                    'significance': p_val_a < self.significance_threshold
                },
                'model_b_trend': {
                    'slope': b_trend_slope,
                    'direction': 'improving' if b_trend_slope > 0 else 'declining',
                    'significance': p_val_b < self.significance_threshold
                },
                'trend_significance': p_val_a < self.significance_threshold or p_val_b < self.significance_threshold
            }
        
        def detect_performance_degradation(self, model_name: str, degradation_threshold: float = 0.1) -> bool:
            """æ£€æµ‹æ€§èƒ½é€€åŒ–"""
            metrics_deque = self.model_a_metrics if model_name == 'model_a' else self.model_b_metrics
            
            if len(metrics_deque) < 20:
                return False
            
            # æ¯”è¾ƒæœ€è¿‘10ä¸ªæ•°æ®ç‚¹ä¸Žä¹‹å‰10ä¸ªæ•°æ®ç‚¹
            recent_metrics = list(metrics_deque)[-10:]
            baseline_metrics = list(metrics_deque)[-20:-10]
            
            recent_avg = np.mean([m.success_rate for m in recent_metrics])
            baseline_avg = np.mean([m.success_rate for m in baseline_metrics])
            
            # å¦‚æžœæœ€è¿‘çš„æ€§èƒ½æ¯”åŸºçº¿å·®è¶…è¿‡é˜ˆå€¼ï¼Œè®¤ä¸ºå­˜åœ¨æ€§èƒ½é€€åŒ–
            performance_drop = baseline_avg - recent_avg
            return performance_drop > degradation_threshold
        
        def calculate_confidence_intervals(self, model_name: str, confidence_level: float = 0.95) -> Dict[str, Dict[str, float]]:
            """è®¡ç®—ç½®ä¿¡åŒºé—´"""
            metrics_deque = self.model_a_metrics if model_name == 'model_a' else self.model_b_metrics
            
            if len(metrics_deque) < 30:
                return {'error': 'æ ·æœ¬é‡ä¸è¶³ä»¥è®¡ç®—ç½®ä¿¡åŒºé—´'}
            
            metrics_list = list(metrics_deque)
            alpha = 1 - confidence_level
            
            results = {}
            
            # æˆåŠŸçŽ‡ç½®ä¿¡åŒºé—´
            success_rates = [m.success_rate for m in metrics_list]
            mean_sr = np.mean(success_rates)
            sem_sr = stats.sem(success_rates)
            ci_sr = stats.t.interval(confidence_level, len(success_rates)-1, loc=mean_sr, scale=sem_sr)
            results['success_rate'] = {'lower': ci_sr[0], 'upper': ci_sr[1]}
            
            # é”™è¯¯çŽ‡ç½®ä¿¡åŒºé—´
            error_rates = [m.error_rate for m in metrics_list]
            mean_er = np.mean(error_rates)
            sem_er = stats.sem(error_rates)
            ci_er = stats.t.interval(confidence_level, len(error_rates)-1, loc=mean_er, scale=sem_er)
            results['error_rate'] = {'lower': ci_er[0], 'upper': ci_er[1]}
            
            return results
    
    
    class DeploymentSafetyController:
        """éƒ¨ç½²å®‰å…¨æŽ§åˆ¶å™¨ç±»"""
        
        def __init__(self, max_error_rate: float = 0.05, max_response_time: float = 1.0,
                     min_success_rate: float = 0.90, circuit_breaker_threshold: int = 10,
                     recovery_check_interval: int = 300):
            """
            åˆå§‹åŒ–å®‰å…¨æŽ§åˆ¶å™¨
            
            Args:
                max_error_rate: æœ€å¤§é”™è¯¯çŽ‡
                max_response_time: æœ€å¤§å“åº”æ—¶é—´
                min_success_rate: æœ€å°æˆåŠŸçŽ‡
                circuit_breaker_threshold: ç†”æ–­å™¨é˜ˆå€¼
                recovery_check_interval: æ¢å¤æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
            """
            self.max_error_rate = max_error_rate
            self.max_response_time = max_response_time
            self.min_success_rate = min_success_rate
            self.circuit_breaker_threshold = circuit_breaker_threshold
            self.recovery_check_interval = recovery_check_interval
            
            self.is_circuit_breaker_open = False
            self.failure_count = 0
            self.last_circuit_check = datetime.now()
            self.metrics_for_analysis = deque(maxlen=1000)
            self._lock = threading.Lock()
        
        def perform_safety_check(self, metrics: PerformanceMetrics) -> Dict[str, Any]:
            """æ‰§è¡Œå®‰å…¨æ£€æŸ¥"""
            violations = []
            
            # æ£€æŸ¥æˆåŠŸçŽ‡
            if metrics.success_rate < self.min_success_rate:
                violations.append('success_rate')
            
            # æ£€æŸ¥é”™è¯¯çŽ‡
            if metrics.error_rate > self.max_error_rate:
                violations.append('error_rate')
            
            # æ£€æŸ¥å“åº”æ—¶é—´
            if metrics.avg_response_time > self.max_response_time:
                violations.append('response_time')
            
            # æ›´æ–°å¤±è´¥è®¡æ•°å’Œç†”æ–­å™¨çŠ¶æ€
            with self._lock:
                if violations:
                    self.failure_count += 1
                    if self.failure_count >= self.circuit_breaker_threshold:
                        self.is_circuit_breaker_open = True
                        self.last_circuit_check = datetime.now()
                else:
                    self.failure_count = max(0, self.failure_count - 1)
            
            return {
                'passed': len(violations) == 0,
                'violations': violations,
                'failure_count': self.failure_count,
                'circuit_breaker_open': self.is_circuit_breaker_open
            }
        
        def should_rollback_deployment(self, deployment: Any, metrics: PerformanceMetrics) -> Dict[str, Any]:
            """åˆ¤æ–­æ˜¯å¦åº”è¯¥å›žæ»šéƒ¨ç½²"""
            safety_check = self.perform_safety_check(metrics)
            
            # ä¸¥é‡å®‰å…¨è¿è§„åˆ¤æ–­
            critical_violations = []
            if metrics.success_rate < 0.7:  # æˆåŠŸçŽ‡æžä½Ž
                critical_violations.append('critical_success_rate')
            if metrics.error_rate > 0.2:    # é”™è¯¯çŽ‡æžé«˜
                critical_violations.append('critical_error_rate')
            if metrics.avg_response_time > 2.0:  # å“åº”æ—¶é—´æžæ…¢
                critical_violations.append('critical_response_time')
            
            should_rollback = (
                self.is_circuit_breaker_open or
                len(critical_violations) > 0 or
                len(safety_check['violations']) >= 2  # å¤šä¸ªæŒ‡æ ‡åŒæ—¶è¿è§„
            )
            
            reason = ""
            if self.is_circuit_breaker_open:
                reason = "ç†”æ–­å™¨å·²å¼€å¯ï¼Œç³»ç»Ÿä¿æŠ¤æ€§å›žæ»š"
            elif critical_violations:
                reason = f"ä¸¥é‡å®‰å…¨è¿è§„: {', '.join(critical_violations)}"
            elif len(safety_check['violations']) >= 2:
                reason = f"å¤šé¡¹æŒ‡æ ‡è¿è§„: {', '.join(safety_check['violations'])}"
            
            return {
                'should_rollback': should_rollback,
                'reason': reason,
                'safety_violations': safety_check['violations'],
                'critical_violations': critical_violations
            }
        
        def evaluate_traffic_adjustment(self, deployment: Any, metrics: PerformanceMetrics) -> Dict[str, Any]:
            """è¯„ä¼°æµé‡è°ƒæ•´å»ºè®®"""
            safety_check = self.perform_safety_check(metrics)
            
            current_percentage = deployment.traffic_percentage
            
            if not safety_check['passed']:
                # æœ‰å®‰å…¨è¿è§„ï¼Œå»ºè®®å‡å°‘æµé‡
                if len(safety_check['violations']) == 1:
                    # è½»å¾®è¿è§„ï¼Œå‡å°‘ä¸€åŠæµé‡
                    recommended_percentage = max(5.0, current_percentage * 0.5)
                    action = 'reduce'
                else:
                    # ä¸¥é‡è¿è§„ï¼Œå¤§å¹…å‡å°‘æµé‡
                    recommended_percentage = max(1.0, current_percentage * 0.2)
                    action = 'reduce'
            else:
                # æ²¡æœ‰è¿è§„ï¼Œå¯ä»¥è€ƒè™‘å¢žåŠ æµé‡
                if current_percentage < 50.0:
                    recommended_percentage = min(100.0, current_percentage * 1.2)
                    action = 'increase'
                else:
                    recommended_percentage = current_percentage
                    action = 'maintain'
            
            return {
                'action': action,
                'recommended_percentage': recommended_percentage,
                'current_percentage': current_percentage,
                'safety_violations': safety_check['violations']
            }
        
        def attempt_circuit_recovery(self, metrics: PerformanceMetrics):
            """å°è¯•ç†”æ–­å™¨æ¢å¤"""
            if not self.is_circuit_breaker_open:
                return
            
            current_time = datetime.now()
            if (current_time - self.last_circuit_check).total_seconds() < self.recovery_check_interval:
                return
            
            # æ£€æŸ¥æŒ‡æ ‡æ˜¯å¦æ¢å¤æ­£å¸¸
            safety_check = self.perform_safety_check(metrics)
            if safety_check['passed']:
                with self._lock:
                    self.is_circuit_breaker_open = False
                    self.failure_count = 0
                    self.last_circuit_check = current_time
        
        def add_metrics_for_analysis(self, metrics: PerformanceMetrics):
            """æ·»åŠ æŒ‡æ ‡ç”¨äºŽåˆ†æž"""
            with self._lock:
                self.metrics_for_analysis.append(metrics)
        
        def get_aggregated_safety_metrics(self) -> Dict[str, Any]:
            """èŽ·å–èšåˆçš„å®‰å…¨æŒ‡æ ‡"""
            if not self.metrics_for_analysis:
                return {}
            
            with self._lock:
                metrics_list = list(self.metrics_for_analysis)
            
            return {
                'avg_success_rate': np.mean([m.success_rate for m in metrics_list]),
                'avg_error_rate': np.mean([m.error_rate for m in metrics_list]),
                'avg_response_time': np.mean([m.avg_response_time for m in metrics_list]),
                'percentile_95_response_time': np.percentile([m.avg_response_time for m in metrics_list], 95),
                'min_success_rate': np.min([m.success_rate for m in metrics_list]),
                'max_error_rate': np.max([m.error_rate for m in metrics_list]),
                'sample_count': len(metrics_list)
            }
        
        def evaluate_emergency_stop(self, deployment: Any, metrics: PerformanceMetrics) -> Dict[str, Any]:
            """è¯„ä¼°ç´§æ€¥åœæ­¢"""
            critical_violations = []
            
            # æžç«¯æƒ…å†µæ£€æŸ¥
            if metrics.success_rate < 0.5:
                critical_violations.append('æžä½ŽæˆåŠŸçŽ‡')
            if metrics.error_rate > 0.3:
                critical_violations.append('æžé«˜é”™è¯¯çŽ‡')
            if metrics.avg_response_time > 3.0:
                critical_violations.append('æžæ…¢å“åº”æ—¶é—´')
            if metrics.throughput < 100:
                critical_violations.append('æžä½Žåžåé‡')
            
            emergency_stop = len(critical_violations) >= 2
            
            return {
                'emergency_stop': emergency_stop,
                'critical_violations': critical_violations,
                'timestamp': datetime.now()
            }
    
    
    class RollbackManager:
        """å›žæ»šç®¡ç†å™¨ç±»"""
        
        def __init__(self, rollback_timeout: int = 300, verification_checks: int = 5,
                     health_check_interval: int = 30):
            """
            åˆå§‹åŒ–å›žæ»šç®¡ç†å™¨
            
            Args:
                rollback_timeout: å›žæ»šè¶…æ—¶æ—¶é—´ï¼ˆç§’ï¼‰
                verification_checks: éªŒè¯æ£€æŸ¥æ¬¡æ•°
                health_check_interval: å¥åº·æ£€æŸ¥é—´éš”ï¼ˆç§’ï¼‰
            """
            self.rollback_timeout = rollback_timeout
            self.verification_checks = verification_checks
            self.health_check_interval = health_check_interval
            self.rollback_history = []
            self.auto_rollback_conditions = {}
            self._lock = threading.Lock()
        
        def execute_rollback(self, deployment: Any, reason: str) -> Dict[str, Any]:
            """æ‰§è¡Œå›žæ»š"""
            rollback_id = str(uuid.uuid4())
            rollback_start = datetime.now()
            
            rollback_record = {
                'rollback_id': rollback_id,
                'deployment_id': getattr(deployment, 'deployment_id', 'unknown'),
                'reason': reason,
                'timestamp': rollback_start,
                'status': 'in_progress'
            }
            
            try:
                # æ‰§è¡Œæµé‡å›žæ»š
                self._execute_traffic_rollback(deployment)
                
                # æ›´æ–°éƒ¨ç½²çŠ¶æ€
                deployment.status = DeploymentStatus.ROLLED_BACK
                deployment.traffic_percentage = 0.0
                
                rollback_record['status'] = 'success'
                rollback_record['completion_time'] = datetime.now()
                rollback_record['duration'] = (rollback_record['completion_time'] - rollback_start).total_seconds()
                
                with self._lock:
                    self.rollback_history.append(rollback_record)
                
                return {
                    'success': True,
                    'rollback_id': rollback_id,
                    'timestamp': rollback_start,
                    'duration': rollback_record['duration']
                }
            
            except Exception as e:
                rollback_record['status'] = 'failed'
                rollback_record['error'] = str(e)
                rollback_record['completion_time'] = datetime.now()
                
                with self._lock:
                    self.rollback_history.append(rollback_record)
                
                return {
                    'success': False,
                    'rollback_id': rollback_id,
                    'error': str(e),
                    'timestamp': rollback_start
                }
        
        def _execute_traffic_rollback(self, deployment: Any):
            """æ‰§è¡Œæµé‡å›žæ»š"""
            # æ¨¡æ‹Ÿæµé‡å›žæ»šè¿‡ç¨‹
            if hasattr(deployment, 'traffic_router'):
                deployment.traffic_router.emergency_cutoff_canary()
            
            # ç­‰å¾…æµé‡åˆ‡æ¢å®Œæˆ
            time.sleep(1)
        
        def execute_partial_rollback(self, deployment: Any, target_percentage: float, reason: str) -> Dict[str, Any]:
            """æ‰§è¡Œéƒ¨åˆ†å›žæ»š"""
            if not (0 <= target_percentage <= 100):
                raise ValueError("ç›®æ ‡ç™¾åˆ†æ¯”å¿…é¡»åœ¨0-100ä¹‹é—´")
            
            rollback_id = str(uuid.uuid4())
            rollback_start = datetime.now()
            
            rollback_record = {
                'rollback_id': rollback_id,
                'deployment_id': getattr(deployment, 'deployment_id', 'unknown'),
                'reason': reason,
                'type': 'partial',
                'original_percentage': deployment.traffic_percentage,
                'target_percentage': target_percentage,
                'timestamp': rollback_start,
                'status': 'success'
            }
            
            # æ›´æ–°æµé‡ç™¾åˆ†æ¯”
            deployment.traffic_percentage = target_percentage
            if hasattr(deployment, 'traffic_router'):
                deployment.traffic_router.update_canary_percentage(target_percentage)
            
            rollback_record['completion_time'] = datetime.now()
            
            with self._lock:
                self.rollback_history.append(rollback_record)
            
            return {
                'success': True,
                'rollback_id': rollback_id,
                'new_traffic_percentage': target_percentage,
                'timestamp': rollback_start
            }
        
        def verify_rollback(self, rollback_id: str) -> Dict[str, Any]:
            """éªŒè¯å›žæ»šæ˜¯å¦æˆåŠŸ"""
            rollback_record = None
            with self._lock:
                for record in self.rollback_history:
                    if record['rollback_id'] == rollback_id:
                        rollback_record = record
                        break
            
            if not rollback_record:
                return {'error': f'æœªæ‰¾åˆ°å›žæ»šè®°å½•: {rollback_id}'}
            
            # æ‰§è¡ŒéªŒè¯æ£€æŸ¥
            checks_passed = 0
            for i in range(self.verification_checks):
                if self._perform_health_check():
                    checks_passed += 1
                time.sleep(self.health_check_interval)
            
            verification_success = checks_passed >= (self.verification_checks * 0.8)  # 80%çš„æ£€æŸ¥é€šè¿‡
            
            return {
                'verified': verification_success,
                'checks_passed': checks_passed,
                'total_checks': self.verification_checks,
                'rollback_id': rollback_id
            }
        
        def _perform_health_check(self) -> bool:
            """æ‰§è¡Œå¥åº·æ£€æŸ¥"""
            # æ¨¡æ‹Ÿå¥åº·æ£€æŸ¥
            # åœ¨å®žé™…å®žçŽ°ä¸­ï¼Œè¿™é‡Œä¼šæ£€æŸ¥ç³»ç»Ÿå„é¡¹æŒ‡æ ‡
            return True
        
        def get_rollback_history(self, deployment_id: str = None) -> List[Dict[str, Any]]:
            """èŽ·å–å›žæ»šåŽ†å²"""
            with self._lock:
                if deployment_id:
                    return [record for record in self.rollback_history 
                           if record.get('deployment_id') == deployment_id]
                return self.rollback_history.copy()
        
        def set_auto_rollback_conditions(self, conditions: Dict[str, float]):
            """è®¾ç½®è‡ªåŠ¨å›žæ»šæ¡ä»¶"""
            self.auto_rollback_conditions = conditions.copy()
        
        def should_trigger_auto_rollback(self, metrics: PerformanceMetrics) -> bool:
            """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘è‡ªåŠ¨å›žæ»š"""
            if not self.auto_rollback_conditions:
                return False
            
            # æ£€æŸ¥å„é¡¹æ¡ä»¶
            if 'max_error_rate' in self.auto_rollback_conditions:
                if metrics.error_rate > self.auto_rollback_conditions['max_error_rate']:
                    return True
            
            if 'min_success_rate' in self.auto_rollback_conditions:
                if metrics.success_rate < self.auto_rollback_conditions['min_success_rate']:
                    return True
            
            if 'max_response_time' in self.auto_rollback_conditions:
                if metrics.avg_response_time > self.auto_rollback_conditions['max_response_time']:
                    return True
            
            return False
    
    
    class ABTestFramework:
        """A/Bæµ‹è¯•æ¡†æž¶ç±»"""
        
        def __init__(self, model_a: Any, model_b: Any, traffic_split: float = 0.5,
                     minimum_sample_size: int = 1000, confidence_level: float = 0.95,
                     test_duration: int = 86400):
            """
            åˆå§‹åŒ–A/Bæµ‹è¯•æ¡†æž¶
            
            Args:
                model_a: æ¨¡åž‹Aï¼ˆé€šå¸¸æ˜¯åŸºçº¿æ¨¡åž‹ï¼‰
                model_b: æ¨¡åž‹Bï¼ˆé€šå¸¸æ˜¯æ–°æ¨¡åž‹ï¼‰
                traffic_split: æµé‡åˆ†å‰²æ¯”ä¾‹
                minimum_sample_size: æœ€å°æ ·æœ¬é‡
                confidence_level: ç½®ä¿¡æ°´å¹³
                test_duration: æµ‹è¯•æŒç»­æ—¶é—´ï¼ˆç§’ï¼‰
            """
            self.model_a = model_a
            self.model_b = model_b
            self.traffic_split = traffic_split
            self.minimum_sample_size = minimum_sample_size
            self.confidence_level = confidence_level
            self.test_duration = test_duration
            
            self.experiment_data = []
            self.user_assignments = {}
            self.test_status = "pending"
            self.start_time = None
            self._lock = threading.Lock()
        
        def route_traffic(self, user_id: str) -> Any:
            """è·¯ç”±æµé‡åˆ°Aæˆ–Bæ¨¡åž‹"""
            with self._lock:
                # æ£€æŸ¥ç”¨æˆ·æ˜¯å¦å·²æœ‰åˆ†é…
                if user_id in self.user_assignments:
                    return self.user_assignments[user_id]
                
                # åŸºäºŽç”¨æˆ·IDçš„å“ˆå¸Œè¿›è¡Œä¸€è‡´æ€§åˆ†é…
                user_hash = int(hashlib.md5(user_id.encode()).hexdigest()[:8], 16)
                if (user_hash % 100) / 100.0 < self.traffic_split:
                    assigned_model = self.model_a
                else:
                    assigned_model = self.model_b
                
                self.user_assignments[user_id] = assigned_model
                return assigned_model
        
        def record_result(self, user_id: str, model: Any, prediction: float, actual: float):
            """è®°å½•å®žéªŒç»“æžœ"""
            with self._lock:
                self.experiment_data.append({
                    'user_id': user_id,
                    'model': model,
                    'prediction': prediction,
                    'actual': actual,
                    'timestamp': datetime.now()
                })
        
        def get_experiment_data(self) -> List[Dict[str, Any]]:
            """èŽ·å–å®žéªŒæ•°æ®"""
            with self._lock:
                return self.experiment_data.copy()
        
        def has_sufficient_sample_size(self) -> bool:
            """æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬é‡"""
            with self._lock:
                model_a_samples = sum(1 for record in self.experiment_data if record['model'] == self.model_a)
                model_b_samples = sum(1 for record in self.experiment_data if record['model'] == self.model_b)
                
                return (model_a_samples >= self.minimum_sample_size and 
                       model_b_samples >= self.minimum_sample_size)
        
        def calculate_statistical_significance(self) -> Dict[str, Any]:
            """è®¡ç®—ç»Ÿè®¡æ˜¾è‘—æ€§"""
            if not self.has_sufficient_sample_size():
                return {'error': 'æ ·æœ¬é‡ä¸è¶³'}
            
            with self._lock:
                # åˆ†ç¦»Aå’ŒBç»„çš„æ•°æ®
                a_results = [record['actual'] for record in self.experiment_data if record['model'] == self.model_a]
                b_results = [record['actual'] for record in self.experiment_data if record['model'] == self.model_b]
            
            # æ‰§è¡Œtæ£€éªŒ
            stat, p_value = stats.ttest_ind(a_results, b_results)
            
            # è®¡ç®—æ•ˆåº”å¤§å°ï¼ˆCohen's dï¼‰
            pooled_std = np.sqrt(((len(a_results) - 1) * np.var(a_results, ddof=1) + 
                                 (len(b_results) - 1) * np.var(b_results, ddof=1)) / 
                                (len(a_results) + len(b_results) - 2))
            cohens_d = (np.mean(b_results) - np.mean(a_results)) / pooled_std
            
            # è®¡ç®—ç½®ä¿¡åŒºé—´
            alpha = 1 - self.confidence_level
            degrees_freedom = len(a_results) + len(b_results) - 2
            t_critical = stats.t.ppf(1 - alpha/2, degrees_freedom)
            
            mean_diff = np.mean(b_results) - np.mean(a_results)
            se_diff = pooled_std * np.sqrt(1/len(a_results) + 1/len(b_results))
            ci_lower = mean_diff - t_critical * se_diff
            ci_upper = mean_diff + t_critical * se_diff
            
            return {
                'p_value': p_value,
                'is_significant': p_value < (1 - self.confidence_level),
                'effect_size': cohens_d,
                'confidence_interval': {'lower': ci_lower, 'upper': ci_upper},
                'mean_difference': mean_diff,
                'model_a_mean': np.mean(a_results),
                'model_b_mean': np.mean(b_results),
                'model_a_samples': len(a_results),
                'model_b_samples': len(b_results)
            }
        
        def is_test_complete(self) -> bool:
            """æ£€æŸ¥æµ‹è¯•æ˜¯å¦å®Œæˆ"""
            if not self.start_time:
                return False
            
            time_elapsed = (datetime.now() - self.start_time).total_seconds()
            return time_elapsed >= self.test_duration and self.has_sufficient_sample_size()
        
        def determine_winner(self) -> Optional[Any]:
            """ç¡®å®šèŽ·èƒœæ¨¡åž‹"""
            if not self.is_test_complete():
                return None
            
            significance_result = self.calculate_statistical_significance()
            if 'error' in significance_result:
                return None
            
            if significance_result['is_significant']:
                if significance_result['mean_difference'] > 0:
                    return self.model_b  # Bæ¨¡åž‹æ›´å¥½
                else:
                    return self.model_a  # Aæ¨¡åž‹æ›´å¥½
            else:
                # æ— æ˜¾è‘—å·®å¼‚ï¼Œè¿”å›žå½“å‰åŸºçº¿æ¨¡åž‹
                return self.model_a
        
        def start_test(self):
            """å¼€å§‹æµ‹è¯•"""
            self.start_time = datetime.now()
            self.test_status = "running"
        
        def stop_test(self):
            """åœæ­¢æµ‹è¯•"""
            self.test_status = "completed"
    
    
    class CanaryDeployment:
        """é‡‘ä¸é›€éƒ¨ç½²ä¸»ç±»"""
        
        def __init__(self, canary_model: Any, baseline_model: Any, config: DeploymentConfig):
            """
            åˆå§‹åŒ–é‡‘ä¸é›€éƒ¨ç½²
            
            Args:
                canary_model: é‡‘ä¸é›€æ¨¡åž‹
                baseline_model: åŸºçº¿æ¨¡åž‹
                config: éƒ¨ç½²é…ç½®
            """
            self.canary_model = canary_model
            self.baseline_model = baseline_model
            self.config = config
            
            self.deployment_id = str(uuid.uuid4())
            self.status = DeploymentStatus.PENDING
            self.start_time = None
            self.traffic_percentage = 0.0
            self.deployment_history = []
            
            # åˆå§‹åŒ–ç»„ä»¶
            self.traffic_router = TrafficRouter(canary_percentage=config.canary_percentage)
            self.performance_comparator = ModelPerformanceComparator()
            self.safety_controller = DeploymentSafetyController()
            self.rollback_manager = RollbackManager()
            
            # æŒ‡æ ‡æ”¶é›†
            self.canary_metrics_history = deque(maxlen=1000)
            self.baseline_metrics_history = deque(maxlen=1000)
            
            self._lock = threading.Lock()
        
        def start_deployment(self):
            """å¯åŠ¨é‡‘ä¸é›€éƒ¨ç½²"""
            if self.status != DeploymentStatus.PENDING:
                raise ValueError("éƒ¨ç½²å·²ç»åœ¨è¿è¡Œä¸­")
            
            with self._lock:
                self.status = DeploymentStatus.ACTIVE
                self.start_time = datetime.now()
                self.traffic_percentage = self.config.canary_percentage
                
                # æ›´æ–°æµé‡è·¯ç”±å™¨
                self.traffic_router.update_canary_percentage(self.config.canary_percentage)
                
                # è®°å½•éƒ¨ç½²åŽ†å²
                self.deployment_history.append({
                    'action': 'start_deployment',
                    'timestamp': self.start_time,
                    'traffic_percentage': self.traffic_percentage
                })
        
        def update_canary_metrics(self, metrics: PerformanceMetrics):
            """æ›´æ–°é‡‘ä¸é›€æ¨¡åž‹æŒ‡æ ‡"""
            with self._lock:
                self.canary_metrics_history.append(metrics)
                self.performance_comparator.add_model_b_metrics(metrics)
                self.safety_controller.add_metrics_for_analysis(metrics)
        
        def update_baseline_metrics(self, metrics: PerformanceMetrics):
            """æ›´æ–°åŸºçº¿æ¨¡åž‹æŒ‡æ ‡"""
            with self._lock:
                self.baseline_metrics_history.append(metrics)
                self.performance_comparator.add_model_a_metrics(metrics)
        
        def evaluate_success_criteria(self) -> bool:
            """è¯„ä¼°æˆåŠŸæ ‡å‡†"""
            if not self.canary_metrics_history:
                return False
            
            # èŽ·å–æœ€è¿‘çš„æŒ‡æ ‡
            recent_metrics = list(self.canary_metrics_history)[-10:]  # æœ€è¿‘10ä¸ªæ•°æ®ç‚¹
            
            # è®¡ç®—å¹³å‡æŒ‡æ ‡
            avg_success_rate = np.mean([m.success_rate for m in recent_metrics])
            avg_error_rate = np.mean([m.error_rate for m in recent_metrics])
            
            # æ£€æŸ¥æ˜¯å¦æ»¡è¶³æˆåŠŸæ ‡å‡†
            success_criteria_met = (
                avg_success_rate >= self.config.success_threshold and
                avg_error_rate <= self.config.error_threshold
            )
            
            return success_criteria_met
        
        def should_trigger_rollback(self) -> bool:
            """åˆ¤æ–­æ˜¯å¦åº”è¯¥è§¦å‘å›žæ»š"""
            # æ€§èƒ½æ¯”è¾ƒæ£€æŸ¥ï¼ˆå³ä½¿æ²¡æœ‰æŒ‡æ ‡åŽ†å²ä¹Ÿå¯ä»¥è¿›è¡Œï¼‰
            if self.performance_comparator.latest_comparison:
                comparison = self.performance_comparator.latest_comparison
                # æ£€æŸ¥å¤šä¸ªæ€§èƒ½æŒ‡æ ‡æ˜¯å¦è¡¨æ˜Žéœ€è¦å›žæ»š
                performance_degradation = (
                    comparison.get('overall_performance_score', 0) < -self.config.rollback_threshold or
                    comparison.get('performance_improvement', 0) < -self.config.rollback_threshold or
                    (comparison.get('success_rate_diff', 0) > self.config.rollback_threshold and
                     comparison.get('error_rate_diff', 0) > self.config.rollback_threshold)
                )
                if performance_degradation:
                    return True
            
            # å¦‚æžœæ²¡æœ‰æŒ‡æ ‡åŽ†å²ï¼Œåªèƒ½åŸºäºŽæ€§èƒ½æ¯”è¾ƒåˆ¤æ–­
            if not self.canary_metrics_history:
                return False
            
            # èŽ·å–æœ€æ–°æŒ‡æ ‡
            latest_metrics = self.canary_metrics_history[-1]
            
            # å®‰å…¨æŽ§åˆ¶å™¨æ£€æŸ¥
            rollback_decision = self.safety_controller.should_rollback_deployment(self, latest_metrics)
            if rollback_decision['should_rollback']:
                return True
            
            return False
        
        def increase_traffic(self, step_size: float = 10.0):
            """å¢žåŠ æµé‡ç™¾åˆ†æ¯”"""
            if self.status != DeploymentStatus.ACTIVE:
                raise ValueError("éƒ¨ç½²æœªå¤„äºŽæ´»è·ƒçŠ¶æ€")
            
            new_percentage = min(100.0, self.traffic_percentage + step_size)
            
            with self._lock:
                self.traffic_percentage = new_percentage
                self.traffic_router.update_canary_percentage(new_percentage)
                
                self.deployment_history.append({
                    'action': 'increase_traffic',
                    'timestamp': datetime.now(),
                    'traffic_percentage': new_percentage,
                    'step_size': step_size
                })
        
        def complete_deployment(self):
            """å®Œæˆéƒ¨ç½²"""
            if self.status != DeploymentStatus.ACTIVE:
                raise ValueError("éƒ¨ç½²æœªå¤„äºŽæ´»è·ƒçŠ¶æ€")
            
            with self._lock:
                self.status = DeploymentStatus.COMPLETED
                self.traffic_percentage = 100.0
                
                self.deployment_history.append({
                    'action': 'complete_deployment',
                    'timestamp': datetime.now(),
                    'traffic_percentage': 100.0
                })
        
        def rollback_deployment(self, reason: str):
            """å›žæ»šéƒ¨ç½²"""
            rollback_result = self.rollback_manager.execute_rollback(self, reason)
            
            with self._lock:
                self.status = DeploymentStatus.ROLLED_BACK
                self.traffic_percentage = 0.0
                
                self.deployment_history.append({
                    'action': 'rollback_deployment',
                    'timestamp': datetime.now(),
                    'reason': reason,
                    'rollback_id': rollback_result.get('rollback_id'),
                    'traffic_percentage': 0.0
                })
        
        def is_deployment_timeout(self) -> bool:
            """æ£€æŸ¥éƒ¨ç½²æ˜¯å¦è¶…æ—¶"""
            if not self.start_time:
                return False
            
            elapsed_time = (datetime.now() - self.start_time).total_seconds()
            return elapsed_time > self.config.max_canary_duration
        
        def get_metrics_history(self) -> List[PerformanceMetrics]:
            """èŽ·å–æŒ‡æ ‡åŽ†å²"""
            with self._lock:
                return list(self.canary_metrics_history)
        
        def get_deployment_status(self) -> Dict[str, Any]:
            """èŽ·å–éƒ¨ç½²çŠ¶æ€"""
            return {
                'deployment_id': self.deployment_id,
                'status': self.status.value,
                'start_time': self.start_time,
                'traffic_percentage': self.traffic_percentage,
                'canary_model_version': getattr(self.canary_model, 'version', 'unknown'),
                'baseline_model_version': getattr(self.baseline_model, 'version', 'unknown'),
                'deployment_duration': (datetime.now() - self.start_time).total_seconds() if self.start_time else 0,
                'metrics_count': len(self.canary_metrics_history),
                'latest_comparison': self.performance_comparator.latest_comparison
            }
    ]]></file>
  <file path="src/rl_trading_system/deployment/__init__.py"><![CDATA[
    """éƒ¨ç½²ç³»ç»Ÿæ¨¡å—"""
    
    from .canary_deployment import CanaryDeployment
    from .model_version_manager import ModelVersionManager
    
    __all__ = [
        "CanaryDeployment",
        "ModelVersionManager"
    ]
    ]]></file>
  <file path="src/rl_trading_system/data/qlib_interface.py"><![CDATA[
    """
    Qlibæ•°æ®æŽ¥å£å®žçŽ°
    """
    
    from typing import List, Optional
    import pandas as pd
    import logging
    from .interfaces import DataInterface
    from .data_cache import get_global_cache
    from .data_quality import get_global_quality_checker
    
    logger = logging.getLogger(__name__)
    
    
    class QlibDataInterface(DataInterface):
        """Qlibæ•°æ®æŽ¥å£å®žçŽ°"""
        
        def __init__(self, provider_uri: str = None):
            """
            åˆå§‹åŒ–Qlibæ•°æ®æŽ¥å£
            
            Args:
                provider_uri: Qlibæ•°æ®æä¾›è€…URI
            """
            super().__init__()
            self.provider_uri = provider_uri
            self._initialized = False
        
        def _initialize_qlib(self):
            """åˆå§‹åŒ–Qlib"""
            if self._initialized:
                return
            
            try:
                import qlib
                from qlib.config import REG_CN
                
                # åˆå§‹åŒ–qlib
                if self.provider_uri:
                    qlib.init(provider_uri=self.provider_uri, region=REG_CN)
                else:
                    # ä½¿ç”¨é»˜è®¤é…ç½®
                    qlib.init(region=REG_CN)
                
                self._initialized = True
                logger.info("Qlibåˆå§‹åŒ–æˆåŠŸ")
                
            except ImportError:
                logger.error("Qlibæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…qlib: pip install pyqlib")
                raise ImportError("Qlibæœªå®‰è£…")
            except Exception as e:
                logger.error(f"Qlibåˆå§‹åŒ–å¤±è´¥: {e}")
                raise
        
        def get_stock_list(self, market: str = 'A') -> List[str]:
            """
            èŽ·å–è‚¡ç¥¨åˆ—è¡¨
            
            Args:
                market: å¸‚åœºä»£ç ï¼Œ'A'è¡¨ç¤ºAè‚¡
                
            Returns:
                è‚¡ç¥¨ä»£ç åˆ—è¡¨
            """
            cache_key = self._get_cache_key('get_stock_list', market=market)
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                return cached_result.tolist() if hasattr(cached_result, 'tolist') else cached_result
            
            try:
                self._initialize_qlib()
                
                import qlib
                from qlib.data import D
                
                # èŽ·å–è‚¡ç¥¨åˆ—è¡¨
                instruments = D.instruments(market=market)
                
                # ç¼“å­˜ç»“æžœ
                self._set_cache(cache_key, pd.Series(instruments))
                
                logger.info(f"æˆåŠŸèŽ·å–{len(instruments)}åª{market}è‚¡ç¥¨")
                return instruments
                
            except Exception as e:
                logger.error(f"èŽ·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
                raise
        
        def get_price_data(self, symbols: List[str], 
                          start_date: str, end_date: str) -> pd.DataFrame:
            """
            èŽ·å–ä»·æ ¼æ•°æ®
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                start_date: å¼€å§‹æ—¥æœŸ
                end_date: ç»“æŸæ—¥æœŸ
                
            Returns:
                ä»·æ ¼æ•°æ®DataFrame
            """
            # å‚æ•°éªŒè¯
            if not self.validate_symbols(symbols):
                raise ValueError("è‚¡ç¥¨ä»£ç åˆ—è¡¨æ— æ•ˆ")
            
            if not self.validate_date_range(start_date, end_date):
                raise ValueError("æ—¥æœŸèŒƒå›´æ— æ•ˆ")
            
            cache_key = self._get_cache_key('get_price_data', 
                                           symbols=tuple(symbols),
                                           start_date=start_date, 
                                           end_date=end_date)
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                return cached_result
            
            try:
                self._initialize_qlib()
                
                import qlib
                from qlib.data import D
                
                # å®šä¹‰éœ€è¦èŽ·å–çš„å­—æ®µ
                fields = ['$open', '$high', '$low', '$close', '$volume', '$amount']
                
                # èŽ·å–æ•°æ®
                data = D.features(symbols, fields, 
                                start_time=start_date, 
                                end_time=end_date)
                
                if data.empty:
                    logger.warning(f"æœªèŽ·å–åˆ°æ•°æ®: symbols={symbols}, "
                                 f"start_date={start_date}, end_date={end_date}")
                    return pd.DataFrame()
                
                # é‡å‘½ååˆ—ä»¥ç¬¦åˆæ ‡å‡†æ ¼å¼
                column_mapping = {
                    '$open': 'open',
                    '$high': 'high', 
                    '$low': 'low',
                    '$close': 'close',
                    '$volume': 'volume',
                    '$amount': 'amount'
                }
                data = data.rename(columns=column_mapping)
                
                # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
                data = self.standardize_dataframe(data, 'price')
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                quality_report = self.check_data_quality(data, 'price')
                if quality_report['status'] == 'warning':
                    logger.warning(f"æ•°æ®è´¨é‡é—®é¢˜: {quality_report['issues']}")
                
                # ç¼“å­˜ç»“æžœ
                self._set_cache(cache_key, data)
                
                logger.info(f"æˆåŠŸèŽ·å–ä»·æ ¼æ•°æ®: {len(data)}æ¡è®°å½•")
                return data
                
            except Exception as e:
                logger.error(f"èŽ·å–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                raise
        
        def get_fundamental_data(self, symbols: List[str], 
                               start_date: str, end_date: str) -> pd.DataFrame:
            """
            èŽ·å–åŸºæœ¬é¢æ•°æ®
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                start_date: å¼€å§‹æ—¥æœŸ
                end_date: ç»“æŸæ—¥æœŸ
                
            Returns:
                åŸºæœ¬é¢æ•°æ®DataFrame
            """
            # å‚æ•°éªŒè¯
            if not self.validate_symbols(symbols):
                raise ValueError("è‚¡ç¥¨ä»£ç åˆ—è¡¨æ— æ•ˆ")
            
            if not self.validate_date_range(start_date, end_date):
                raise ValueError("æ—¥æœŸèŒƒå›´æ— æ•ˆ")
            
            cache_key = self._get_cache_key('get_fundamental_data',
                                           symbols=tuple(symbols),
                                           start_date=start_date,
                                           end_date=end_date)
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                return cached_result
            
            try:
                self._initialize_qlib()
                
                import qlib
                from qlib.data import D
                
                # å®šä¹‰åŸºæœ¬é¢å­—æ®µ
                fundamental_fields = [
                    'PE',  # å¸‚ç›ˆçŽ‡
                    'PB',  # å¸‚å‡€çŽ‡
                    'PS',  # å¸‚é”€çŽ‡
                    'PCF', # å¸‚çŽ°çŽ‡
                    'TOTAL_MV',  # æ€»å¸‚å€¼
                    'CIRC_MV',   # æµé€šå¸‚å€¼
                    'ROE',       # å‡€èµ„äº§æ”¶ç›ŠçŽ‡
                    'ROA',       # æ€»èµ„äº§æ”¶ç›ŠçŽ‡
                    'GROSS_PROFIT_MARGIN',  # æ¯›åˆ©çŽ‡
                    'NET_PROFIT_MARGIN'     # å‡€åˆ©çŽ‡
                ]
                
                # èŽ·å–æ•°æ®
                data = D.features(symbols, fundamental_fields,
                                start_time=start_date,
                                end_time=end_date)
                
                if data.empty:
                    logger.warning(f"æœªèŽ·å–åˆ°åŸºæœ¬é¢æ•°æ®: symbols={symbols}, "
                                 f"start_date={start_date}, end_date={end_date}")
                    return pd.DataFrame()
                
                # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
                data = self.standardize_dataframe(data, 'fundamental')
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                quality_report = self.check_data_quality(data, 'fundamental')
                if quality_report['status'] == 'warning':
                    logger.warning(f"åŸºæœ¬é¢æ•°æ®è´¨é‡é—®é¢˜: {quality_report['issues']}")
                
                # ç¼“å­˜ç»“æžœ
                self._set_cache(cache_key, data)
                
                logger.info(f"æˆåŠŸèŽ·å–åŸºæœ¬é¢æ•°æ®: {len(data)}æ¡è®°å½•")
                return data
                
            except Exception as e:
                logger.error(f"èŽ·å–åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
                raise
        
        def get_realtime_data(self, symbols: List[str]) -> pd.DataFrame:
            """
            èŽ·å–å®žæ—¶æ•°æ®ï¼ˆå¦‚æžœæ”¯æŒï¼‰
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                
            Returns:
                å®žæ—¶æ•°æ®DataFrame
            """
            logger.warning("QlibæŽ¥å£æš‚ä¸æ”¯æŒå®žæ—¶æ•°æ®èŽ·å–")
            return pd.DataFrame()
        
        def get_calendar(self, market: str = 'A') -> List[str]:
            """
            èŽ·å–äº¤æ˜“æ—¥åŽ†
            
            Args:
                market: å¸‚åœºä»£ç 
                
            Returns:
                äº¤æ˜“æ—¥æœŸåˆ—è¡¨
            """
            try:
                self._initialize_qlib()
                
                import qlib
                from qlib.data import D
                
                calendar = D.calendar(market=market)
                return [date.strftime('%Y-%m-%d') for date in calendar]
                
            except Exception as e:
                logger.error(f"èŽ·å–äº¤æ˜“æ—¥åŽ†å¤±è´¥: {e}")
                raise
    ]]></file>
  <file path="src/rl_trading_system/data/interfaces.py"><![CDATA[
    """
    æ•°æ®æŽ¥å£æŠ½è±¡ç±»
    å®šä¹‰æ•°æ®èŽ·å–çš„ç»Ÿä¸€æŽ¥å£
    """
    
    from abc import ABC, abstractmethod
    from typing import List, Dict, Any, Optional
    import pandas as pd
    from datetime import datetime
    import logging
    
    logger = logging.getLogger(__name__)
    
    
    class DataInterface(ABC):
        """æ•°æ®æŽ¥å£æŠ½è±¡ç±»"""
        
        def __init__(self):
            """åˆå§‹åŒ–æ•°æ®æŽ¥å£"""
            self._cache = {}
            self._cache_enabled = True
        
        @abstractmethod
        def get_stock_list(self, market: str = 'A') -> List[str]:
            """
            èŽ·å–è‚¡ç¥¨åˆ—è¡¨
            
            Args:
                market: å¸‚åœºä»£ç ï¼Œå¦‚'A'è¡¨ç¤ºAè‚¡
                
            Returns:
                è‚¡ç¥¨ä»£ç åˆ—è¡¨
            """
            pass
        
        @abstractmethod
        def get_price_data(self, symbols: List[str], 
                          start_date: str, end_date: str) -> pd.DataFrame:
            """
            èŽ·å–ä»·æ ¼æ•°æ®
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
                end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
                
            Returns:
                ä»·æ ¼æ•°æ®DataFrameï¼ŒåŒ…å«open, high, low, close, volume, amountåˆ—
            """
            pass
        
        @abstractmethod
        def get_fundamental_data(self, symbols: List[str], 
                               start_date: str, end_date: str) -> pd.DataFrame:
            """
            èŽ·å–åŸºæœ¬é¢æ•°æ®
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                start_date: å¼€å§‹æ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
                end_date: ç»“æŸæ—¥æœŸï¼Œæ ¼å¼'YYYY-MM-DD'
                
            Returns:
                åŸºæœ¬é¢æ•°æ®DataFrame
            """
            pass
        
        def validate_date_format(self, date_str: str) -> bool:
            """
            éªŒè¯æ—¥æœŸæ ¼å¼
            
            Args:
                date_str: æ—¥æœŸå­—ç¬¦ä¸²
                
            Returns:
                æ˜¯å¦ä¸ºæœ‰æ•ˆæ ¼å¼
            """
            try:
                datetime.strptime(date_str, '%Y-%m-%d')
                return True
            except ValueError:
                return False
        
        def validate_date_range(self, start_date: str, end_date: str) -> bool:
            """
            éªŒè¯æ—¥æœŸèŒƒå›´
            
            Args:
                start_date: å¼€å§‹æ—¥æœŸ
                end_date: ç»“æŸæ—¥æœŸ
                
            Returns:
                æ—¥æœŸèŒƒå›´æ˜¯å¦æœ‰æ•ˆ
            """
            if not (self.validate_date_format(start_date) and 
                    self.validate_date_format(end_date)):
                return False
            
            start = datetime.strptime(start_date, '%Y-%m-%d')
            end = datetime.strptime(end_date, '%Y-%m-%d')
            
            return start <= end
        
        def validate_symbols(self, symbols: List[str]) -> bool:
            """
            éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                
            Returns:
                è‚¡ç¥¨ä»£ç æ˜¯å¦æœ‰æ•ˆ
            """
            if not symbols:
                return False
            
            for symbol in symbols:
                if not isinstance(symbol, str) or len(symbol) == 0:
                    return False
            
            return True
        
        def standardize_dataframe(self, df: pd.DataFrame, 
                                data_type: str = 'price') -> pd.DataFrame:
            """
            æ ‡å‡†åŒ–DataFrameæ ¼å¼
            
            Args:
                df: åŽŸå§‹DataFrame
                data_type: æ•°æ®ç±»åž‹ï¼Œ'price'æˆ–'fundamental'
                
            Returns:
                æ ‡å‡†åŒ–åŽçš„DataFrame
            """
            if df.empty:
                return df
            
            # ç¡®ä¿æœ‰datetimeå’Œinstrumentç´¢å¼•
            if data_type == 'price':
                required_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']
            else:
                required_columns = []
            
            # æ£€æŸ¥å¿…è¦åˆ—æ˜¯å¦å­˜åœ¨
            missing_columns = set(required_columns) - set(df.columns)
            if missing_columns:
                logger.warning(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            
            return df
        
        def enable_cache(self, enabled: bool = True):
            """å¯ç”¨æˆ–ç¦ç”¨ç¼“å­˜"""
            self._cache_enabled = enabled
            if not enabled:
                self._cache.clear()
        
        def clear_cache(self):
            """æ¸…ç©ºç¼“å­˜"""
            self._cache.clear()
        
        def _get_cache_key(self, method: str, *args, **kwargs) -> str:
            """ç”Ÿæˆç¼“å­˜é”®"""
            key_parts = [method] + [str(arg) for arg in args]
            for k, v in sorted(kwargs.items()):
                key_parts.append(f"{k}={v}")
            return "|".join(key_parts)
        
        def _get_from_cache(self, cache_key: str) -> Optional[pd.DataFrame]:
            """ä»Žç¼“å­˜èŽ·å–æ•°æ®"""
            if not self._cache_enabled:
                return None
            return self._cache.get(cache_key)
        
        def _set_cache(self, cache_key: str, data: pd.DataFrame):
            """è®¾ç½®ç¼“å­˜æ•°æ®"""
            if self._cache_enabled:
                self._cache[cache_key] = data.copy()
        
        def check_data_quality(self, df: pd.DataFrame, 
                              data_type: str = 'price') -> Dict[str, Any]:
            """
            æ£€æŸ¥æ•°æ®è´¨é‡
            
            Args:
                df: æ•°æ®DataFrame
                data_type: æ•°æ®ç±»åž‹
                
            Returns:
                æ•°æ®è´¨é‡æŠ¥å‘Š
            """
            if df.empty:
                return {'status': 'empty', 'issues': ['æ•°æ®ä¸ºç©º']}
            
            issues = []
            
            # æ£€æŸ¥ç¼ºå¤±å€¼
            missing_data = df.isnull().sum()
            if missing_data.sum() > 0:
                issues.append(f"å­˜åœ¨ç¼ºå¤±å€¼: {missing_data.to_dict()}")
            
            # æ£€æŸ¥ä»·æ ¼æ•°æ®çš„é€»è¾‘å…³ç³»
            if data_type == 'price' and all(col in df.columns for col in ['high', 'low']):
                invalid_price_relation = (df['high'] < df['low']).sum()
                if invalid_price_relation > 0:
                    issues.append(f"å­˜åœ¨{invalid_price_relation}æ¡æœ€é«˜ä»·ä½ŽäºŽæœ€ä½Žä»·çš„è®°å½•")
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            if data_type == 'price':
                numeric_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']
                for col in numeric_columns:
                    if col in df.columns:
                        negative_values = (df[col] < 0).sum()
                        if negative_values > 0:
                            issues.append(f"åˆ—{col}å­˜åœ¨{negative_values}ä¸ªè´Ÿå€¼")
            
            status = 'good' if not issues else 'warning'
            return {'status': status, 'issues': issues, 'row_count': len(df)}
    ]]></file>
  <file path="src/rl_trading_system/data/feature_engineer.py"><![CDATA[
    """
    ç‰¹å¾å·¥ç¨‹æ¨¡å—
    å®žçŽ°æŠ€æœ¯æŒ‡æ ‡è®¡ç®—ã€åŸºæœ¬é¢å› å­æå–å’Œå¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾è®¡ç®—
    """
    
    import pandas as pd
    import numpy as np
    from typing import Dict, List, Optional, Union, Tuple
    from datetime import datetime
    import warnings
    from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler
    from sklearn.feature_selection import SelectKBest, mutual_info_regression, VarianceThreshold
    from sklearn.impute import SimpleImputer
    import logging
    
    from .data_models import FeatureVector
    
    logger = logging.getLogger(__name__)
    
    
    class FeatureEngineer:
        """ç‰¹å¾å·¥ç¨‹å™¨"""
        
        def __init__(self):
            """åˆå§‹åŒ–ç‰¹å¾å·¥ç¨‹å™¨"""
            self.scalers = {}
            self.feature_names = {}
            
        def calculate_technical_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
            """
            è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
            
            Args:
                data: ä»·æ ¼æ•°æ®ï¼ŒåŒ…å«open, high, low, close, volume, amountåˆ—
                
            Returns:
                åŒ…å«æŠ€æœ¯æŒ‡æ ‡çš„DataFrame
            """
            if data.empty:
                raise ValueError("è¾“å…¥æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            required_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']
            missing_columns = set(required_columns) - set(data.columns)
            if missing_columns:
                raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
            
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—å„ç§æŠ€æœ¯æŒ‡æ ‡
            sma_result = self.calculate_sma(data)
            ema_result = self.calculate_ema(data)
            rsi_result = self.calculate_rsi(data)
            macd_result = self.calculate_macd(data)
            bb_result = self.calculate_bollinger_bands(data)
            stoch_result = self.calculate_stochastic(data)
            atr_result = self.calculate_atr(data)
            volume_result = self.calculate_volume_indicators(data)
            
            # åˆå¹¶æ‰€æœ‰æŒ‡æ ‡
            for df in [sma_result, ema_result, rsi_result, macd_result, 
                      bb_result, stoch_result, atr_result, volume_result]:
                result = result.join(df, how='outer')
            
            return result
        
        def calculate_sma(self, data: pd.DataFrame, window: int = 20) -> pd.DataFrame:
            """è®¡ç®—ç®€å•ç§»åŠ¨å¹³å‡çº¿"""
            result = pd.DataFrame(index=data.index)
            
            windows = [5, 10, 20, 60]
            for w in windows:
                if len(data) >= w:
                    result[f'sma_{w}'] = data['close'].rolling(window=w).mean()
            
            return result
        
        def calculate_ema(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—æŒ‡æ•°ç§»åŠ¨å¹³å‡çº¿"""
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—EMA12å’ŒEMA26
            result['ema_12'] = data['close'].ewm(span=12).mean()
            result['ema_26'] = data['close'].ewm(span=26).mean()
            
            return result
        
        def calculate_rsi(self, data: pd.DataFrame, window: int = 14) -> pd.DataFrame:
            """è®¡ç®—ç›¸å¯¹å¼ºå¼±æŒ‡æ•°"""
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—ä»·æ ¼å˜åŒ–
            delta = data['close'].diff()
            
            # åˆ†ç¦»ä¸Šæ¶¨å’Œä¸‹è·Œ
            gain = delta.where(delta > 0, 0)
            loss = -delta.where(delta < 0, 0)
            
            # è®¡ç®—å¹³å‡æ”¶ç›Šå’ŒæŸå¤±
            avg_gain = gain.rolling(window=window).mean()
            avg_loss = loss.rolling(window=window).mean()
            
            # è®¡ç®—RSI
            rs = avg_gain / avg_loss
            result['rsi_14'] = 100 - (100 / (1 + rs))
            
            return result
        
        def calculate_macd(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—MACDæŒ‡æ ‡"""
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—EMA
            ema_12 = data['close'].ewm(span=12).mean()
            ema_26 = data['close'].ewm(span=26).mean()
            
            # è®¡ç®—MACDçº¿
            result['macd'] = ema_12 - ema_26
            
            # è®¡ç®—ä¿¡å·çº¿
            result['macd_signal'] = result['macd'].ewm(span=9).mean()
            
            # è®¡ç®—MACDç›´æ–¹å›¾
            result['macd_histogram'] = result['macd'] - result['macd_signal']
            
            return result
        
        def calculate_bollinger_bands(self, data: pd.DataFrame, window: int = 20, std_dev: int = 2) -> pd.DataFrame:
            """è®¡ç®—å¸ƒæž—å¸¦"""
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—ç§»åŠ¨å¹³å‡å’Œæ ‡å‡†å·®
            sma = data['close'].rolling(window=window).mean()
            std = data['close'].rolling(window=window).std()
            
            # è®¡ç®—å¸ƒæž—å¸¦
            result['bb_upper'] = sma + (std * std_dev)
            result['bb_middle'] = sma
            result['bb_lower'] = sma - (std * std_dev)
            
            # è®¡ç®—å¸ƒæž—å¸¦å®½åº¦å’Œä½ç½®
            result['bb_width'] = result['bb_upper'] - result['bb_lower']
            result['bb_position'] = (data['close'] - result['bb_lower']) / result['bb_width']
            
            return result
        
        def calculate_stochastic(self, data: pd.DataFrame, k_window: int = 14, d_window: int = 3) -> pd.DataFrame:
            """è®¡ç®—éšæœºæŒ‡æ ‡"""
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—æœ€é«˜ä»·å’Œæœ€ä½Žä»·
            high_max = data['high'].rolling(window=k_window).max()
            low_min = data['low'].rolling(window=k_window).min()
            
            # è®¡ç®—%K
            result['stoch_k'] = 100 * (data['close'] - low_min) / (high_max - low_min)
            
            # è®¡ç®—%D
            result['stoch_d'] = result['stoch_k'].rolling(window=d_window).mean()
            
            return result
        
        def calculate_atr(self, data: pd.DataFrame, window: int = 14) -> pd.DataFrame:
            """è®¡ç®—å¹³å‡çœŸå®žæ³¢å¹…"""
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—çœŸå®žæ³¢å¹…
            high_low = data['high'] - data['low']
            high_close_prev = abs(data['high'] - data['close'].shift(1))
            low_close_prev = abs(data['low'] - data['close'].shift(1))
            
            true_range = pd.concat([high_low, high_close_prev, low_close_prev], axis=1).max(axis=1)
            
            # è®¡ç®—ATR
            result['atr_14'] = true_range.rolling(window=window).mean()
            
            return result
        
        def calculate_volume_indicators(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—æˆäº¤é‡æŒ‡æ ‡"""
            result = pd.DataFrame(index=data.index)
            
            # æˆäº¤é‡ç§»åŠ¨å¹³å‡
            result['volume_sma'] = data['volume'].rolling(window=20).mean()
            
            # æˆäº¤é‡æ¯”çŽ‡
            result['volume_ratio'] = data['volume'] / result['volume_sma']
            
            # OBV (On Balance Volume)
            price_change = data['close'].diff()
            volume_direction = np.where(price_change > 0, data['volume'], 
                                      np.where(price_change < 0, -data['volume'], 0))
            result['obv'] = volume_direction.cumsum()
            
            # VWAP (Volume Weighted Average Price)
            typical_price = (data['high'] + data['low'] + data['close']) / 3
            result['vwap'] = (typical_price * data['volume']).rolling(window=20).sum() / data['volume'].rolling(window=20).sum()
            
            return result
        
        def calculate_fundamental_factors(self, data: pd.DataFrame) -> pd.DataFrame:
            """
            è®¡ç®—åŸºæœ¬é¢å› å­
            
            Args:
                data: åŸºæœ¬é¢æ•°æ®
                
            Returns:
                åŒ…å«åŸºæœ¬é¢å› å­çš„DataFrame
            """
            if data.empty:
                raise ValueError("è¾“å…¥æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—å„ç±»åŸºæœ¬é¢å› å­
            valuation_result = self.calculate_valuation_factors(data)
            profitability_result = self.calculate_profitability_factors(data)
            growth_result = self.calculate_growth_factors(data)
            leverage_result = self.calculate_leverage_factors(data)
            
            # åˆå¹¶æ‰€æœ‰å› å­
            for df in [valuation_result, profitability_result, growth_result, leverage_result]:
                result = result.join(df, how='outer')
            
            return result
        
        def calculate_valuation_factors(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—ä¼°å€¼å› å­"""
            result = pd.DataFrame(index=data.index)
            
            # ç›´æŽ¥ä½¿ç”¨å·²æœ‰çš„ä¼°å€¼æŒ‡æ ‡
            if 'pe_ratio' in data.columns:
                result['pe_ratio'] = data['pe_ratio']
            if 'pb_ratio' in data.columns:
                result['pb_ratio'] = data['pb_ratio']
            
            # è®¡ç®—å…¶ä»–ä¼°å€¼æŒ‡æ ‡ï¼ˆå¦‚æžœæœ‰ç›¸å…³æ•°æ®ï¼‰
            if 'market_cap' in data.columns and 'revenue' in data.columns:
                result['ps_ratio'] = data['market_cap'] / data['revenue']
            if 'market_cap' in data.columns and 'cash_flow' in data.columns:
                result['pcf_ratio'] = data['market_cap'] / data['cash_flow']
            
            return result
        
        def calculate_profitability_factors(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—ç›ˆåˆ©èƒ½åŠ›å› å­"""
            result = pd.DataFrame(index=data.index)
            
            # ç›´æŽ¥ä½¿ç”¨å·²æœ‰çš„ç›ˆåˆ©æŒ‡æ ‡
            if 'roe' in data.columns:
                result['roe'] = data['roe']
            if 'roa' in data.columns:
                result['roa'] = data['roa']
            
            # è®¡ç®—å…¶ä»–ç›ˆåˆ©æŒ‡æ ‡ï¼ˆå¦‚æžœæœ‰ç›¸å…³æ•°æ®ï¼‰
            if 'gross_profit' in data.columns and 'revenue' in data.columns:
                result['gross_margin'] = data['gross_profit'] / data['revenue']
            if 'net_profit' in data.columns and 'revenue' in data.columns:
                result['net_margin'] = data['net_profit'] / data['revenue']
            
            return result
        
        def calculate_growth_factors(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—æˆé•¿æ€§å› å­"""
            result = pd.DataFrame(index=data.index)
            
            # ç›´æŽ¥ä½¿ç”¨å·²æœ‰çš„æˆé•¿æŒ‡æ ‡
            if 'revenue_growth' in data.columns:
                result['revenue_growth'] = data['revenue_growth']
            if 'profit_growth' in data.columns:
                result['profit_growth'] = data['profit_growth']
            
            # è®¡ç®—EPSå¢žé•¿çŽ‡ï¼ˆå¦‚æžœæœ‰ç›¸å…³æ•°æ®ï¼‰
            if 'eps' in data.columns:
                result['eps_growth'] = data['eps'].pct_change(periods=4)  # å¹´åº¦å¢žé•¿çŽ‡
            
            return result
        
        def calculate_leverage_factors(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—æ æ†å› å­"""
            result = pd.DataFrame(index=data.index)
            
            # ç›´æŽ¥ä½¿ç”¨å·²æœ‰çš„æ æ†æŒ‡æ ‡
            if 'debt_ratio' in data.columns:
                result['debt_ratio'] = data['debt_ratio']
            if 'current_ratio' in data.columns:
                result['current_ratio'] = data['current_ratio']
            
            # è®¡ç®—å…¶ä»–æ æ†æŒ‡æ ‡ï¼ˆå¦‚æžœæœ‰ç›¸å…³æ•°æ®ï¼‰
            if 'total_debt' in data.columns and 'total_equity' in data.columns:
                result['debt_to_equity'] = data['total_debt'] / data['total_equity']
            if 'quick_assets' in data.columns and 'current_liabilities' in data.columns:
                result['quick_ratio'] = data['quick_assets'] / data['current_liabilities']
            
            return result
        
        def calculate_microstructure_features(self, data: pd.DataFrame) -> pd.DataFrame:
            """
            è®¡ç®—å¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
            
            Args:
                data: ä»·æ ¼æ•°æ®
                
            Returns:
                åŒ…å«å¾®è§‚ç»“æž„ç‰¹å¾çš„DataFrame
            """
            if data.empty:
                raise ValueError("è¾“å…¥æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            result = pd.DataFrame(index=data.index)
            
            # è®¡ç®—å„ç±»å¾®è§‚ç»“æž„ç‰¹å¾
            liquidity_result = self.calculate_liquidity_features(data)
            volatility_result = self.calculate_volatility_features(data)
            momentum_result = self.calculate_momentum_features(data)
            
            # åˆå¹¶æ‰€æœ‰ç‰¹å¾
            for df in [liquidity_result, volatility_result, momentum_result]:
                result = result.join(df, how='outer')
            
            return result
        
        def calculate_liquidity_features(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—æµåŠ¨æ€§ç‰¹å¾"""
            result = pd.DataFrame(index=data.index)
            
            # æ¢æ‰‹çŽ‡ï¼ˆå¦‚æžœæœ‰æµé€šè‚¡æœ¬æ•°æ®ï¼‰
            if 'float_shares' in data.columns:
                result['turnover_rate'] = data['volume'] / data['float_shares']
            else:
                # ä½¿ç”¨æˆäº¤é‡ç›¸å¯¹æŒ‡æ ‡
                result['turnover_rate'] = data['volume'] / data['volume'].rolling(window=20).mean()
            
            # AmihudéžæµåŠ¨æ€§æŒ‡æ ‡
            returns = data['close'].pct_change().abs()
            dollar_volume = data['amount']
            result['amihud_illiquidity'] = returns / dollar_volume
            
            # ä¹°å–ä»·å·®ï¼ˆç®€åŒ–è®¡ç®—ï¼‰
            result['bid_ask_spread'] = (data['high'] - data['low']) / data['close']
            
            return result
        
        def calculate_volatility_features(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—æ³¢åŠ¨çŽ‡ç‰¹å¾"""
            result = pd.DataFrame(index=data.index)
            
            # å·²å®žçŽ°æ³¢åŠ¨çŽ‡
            returns = data['close'].pct_change()
            result['realized_volatility'] = returns.rolling(window=20).std() * np.sqrt(252)
            
            # Garman-Klassæ³¢åŠ¨çŽ‡ä¼°è®¡
            gk_vol = np.log(data['high'] / data['low']) ** 2 - (2 * np.log(2) - 1) * np.log(data['close'] / data['open']) ** 2
            result['garman_klass_volatility'] = gk_vol.rolling(window=20).mean() * 252
            
            # Parkinsonæ³¢åŠ¨çŽ‡ä¼°è®¡
            parkinson_vol = np.log(data['high'] / data['low']) ** 2 / (4 * np.log(2))
            result['parkinson_volatility'] = parkinson_vol.rolling(window=20).mean() * 252
            
            return result
        
        def calculate_momentum_features(self, data: pd.DataFrame) -> pd.DataFrame:
            """è®¡ç®—åŠ¨é‡ç‰¹å¾"""
            result = pd.DataFrame(index=data.index)
            
            # ä»·æ ¼åŠ¨é‡
            result['price_momentum_1m'] = data['close'].pct_change(periods=20)  # 1ä¸ªæœˆ
            result['price_momentum_3m'] = data['close'].pct_change(periods=60)  # 3ä¸ªæœˆ
            
            # æˆäº¤é‡åŠ¨é‡
            result['volume_momentum'] = data['volume'].pct_change(periods=20)
            
            return result
        
        def normalize_features(self, features: pd.DataFrame, method: str = 'zscore') -> pd.DataFrame:
            """
            æ ‡å‡†åŒ–ç‰¹å¾
            
            Args:
                features: ç‰¹å¾æ•°æ®
                method: æ ‡å‡†åŒ–æ–¹æ³•ï¼Œ'zscore', 'minmax', 'robust'
                
            Returns:
                æ ‡å‡†åŒ–åŽçš„ç‰¹å¾æ•°æ®
            """
            if features.empty:
                return features
            
            if method == 'zscore':
                scaler = StandardScaler()
            elif method == 'minmax':
                scaler = MinMaxScaler()
            elif method == 'robust':
                scaler = RobustScaler()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„æ ‡å‡†åŒ–æ–¹æ³•: {method}")
            
            # åªå¯¹æ•°å€¼åˆ—è¿›è¡Œæ ‡å‡†åŒ–
            numeric_columns = features.select_dtypes(include=[np.number]).columns
            
            if len(numeric_columns) == 0:
                return features
            
            result = features.copy()
            result[numeric_columns] = scaler.fit_transform(features[numeric_columns])
            
            # ä¿å­˜scalerä»¥ä¾¿åŽç»­ä½¿ç”¨
            self.scalers[method] = scaler
            
            return result
        
        def handle_missing_values(self, data: pd.DataFrame, method: str = 'ffill') -> pd.DataFrame:
            """
            å¤„ç†ç¼ºå¤±å€¼
            
            Args:
                data: è¾“å…¥æ•°æ®
                method: å¤„ç†æ–¹æ³•ï¼Œ'ffill', 'bfill', 'mean', 'median', 'drop'
                
            Returns:
                å¤„ç†åŽçš„æ•°æ®
            """
            if data.empty:
                return data
            
            result = data.copy()
            
            if method == 'ffill':
                result = result.ffill()
            elif method == 'bfill':
                result = result.bfill()
            elif method == 'mean':
                imputer = SimpleImputer(strategy='mean')
                numeric_columns = result.select_dtypes(include=[np.number]).columns
                if len(numeric_columns) > 0:
                    result[numeric_columns] = imputer.fit_transform(result[numeric_columns])
            elif method == 'median':
                imputer = SimpleImputer(strategy='median')
                numeric_columns = result.select_dtypes(include=[np.number]).columns
                if len(numeric_columns) > 0:
                    result[numeric_columns] = imputer.fit_transform(result[numeric_columns])
            elif method == 'drop':
                result = result.dropna()
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç¼ºå¤±å€¼å¤„ç†æ–¹æ³•: {method}")
            
            return result
        
        def detect_outliers(self, data: pd.DataFrame, method: str = 'iqr', threshold: float = 1.5) -> pd.DataFrame:
            """
            æ£€æµ‹å¼‚å¸¸å€¼
            
            Args:
                data: è¾“å…¥æ•°æ®
                method: æ£€æµ‹æ–¹æ³•ï¼Œ'iqr', 'zscore'
                threshold: é˜ˆå€¼
                
            Returns:
                å¼‚å¸¸å€¼æ ‡è®°ï¼ˆTrueè¡¨ç¤ºå¼‚å¸¸å€¼ï¼‰
            """
            if data.empty:
                return pd.DataFrame()
            
            numeric_columns = data.select_dtypes(include=[np.number]).columns
            outliers = pd.DataFrame(False, index=data.index, columns=data.columns)
            
            for col in numeric_columns:
                if method == 'iqr':
                    Q1 = data[col].quantile(0.25)
                    Q3 = data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - threshold * IQR
                    upper_bound = Q3 + threshold * IQR
                    outliers[col] = (data[col] < lower_bound) | (data[col] > upper_bound)
                elif method == 'zscore':
                    z_scores = np.abs((data[col] - data[col].mean()) / data[col].std())
                    outliers[col] = z_scores > threshold
                else:
                    raise ValueError(f"ä¸æ”¯æŒçš„å¼‚å¸¸å€¼æ£€æµ‹æ–¹æ³•: {method}")
            
            return outliers
        
        def treat_outliers(self, data: pd.DataFrame, method: str = 'clip', threshold: float = 1.5) -> pd.DataFrame:
            """
            å¤„ç†å¼‚å¸¸å€¼
            
            Args:
                data: è¾“å…¥æ•°æ®
                method: å¤„ç†æ–¹æ³•ï¼Œ'clip', 'remove'
                threshold: é˜ˆå€¼
                
            Returns:
                å¤„ç†åŽçš„æ•°æ®
            """
            if data.empty:
                return data
            
            result = data.copy()
            outliers = self.detect_outliers(data, threshold=threshold)
            
            if method == 'clip':
                numeric_columns = data.select_dtypes(include=[np.number]).columns
                for col in numeric_columns:
                    Q1 = data[col].quantile(0.25)
                    Q3 = data[col].quantile(0.75)
                    IQR = Q3 - Q1
                    lower_bound = Q1 - threshold * IQR
                    upper_bound = Q3 + threshold * IQR
                    result[col] = result[col].clip(lower=lower_bound, upper=upper_bound)
            elif method == 'remove':
                # ç§»é™¤ä»»ä½•åˆ—æœ‰å¼‚å¸¸å€¼çš„è¡Œ
                outlier_rows = outliers.any(axis=1)
                result = result[~outlier_rows]
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„å¼‚å¸¸å€¼å¤„ç†æ–¹æ³•: {method}")
            
            return result
        
        def select_features_by_correlation(self, features: pd.DataFrame, target: pd.Series, 
                                         threshold: float = 0.1) -> List[str]:
            """
            åŸºäºŽç›¸å…³æ€§é€‰æ‹©ç‰¹å¾
            
            Args:
                features: ç‰¹å¾æ•°æ®
                target: ç›®æ ‡å˜é‡
                threshold: ç›¸å…³æ€§é˜ˆå€¼
                
            Returns:
                é€‰æ‹©çš„ç‰¹å¾ååˆ—è¡¨
            """
            correlations = features.corrwith(target).abs()
            selected_features = correlations[correlations >= threshold].index.tolist()
            return selected_features
        
        def select_features_by_mutual_info(self, features: pd.DataFrame, target: pd.Series, 
                                         k: int = 10) -> List[str]:
            """
            åŸºäºŽäº’ä¿¡æ¯é€‰æ‹©ç‰¹å¾
            
            Args:
                features: ç‰¹å¾æ•°æ®
                target: ç›®æ ‡å˜é‡
                k: é€‰æ‹©çš„ç‰¹å¾æ•°é‡
                
            Returns:
                é€‰æ‹©çš„ç‰¹å¾ååˆ—è¡¨
            """
            # å¤„ç†ç¼ºå¤±å€¼
            features_clean = features.fillna(features.mean())
            target_clean = target.fillna(target.mean())
            
            # ç¡®ä¿ç´¢å¼•å¯¹é½
            common_index = features_clean.index.intersection(target_clean.index)
            features_clean = features_clean.loc[common_index]
            target_clean = target_clean.loc[common_index]
            
            selector = SelectKBest(score_func=mutual_info_regression, k=k)
            selector.fit(features_clean, target_clean)
            
            selected_features = features.columns[selector.get_support()].tolist()
            return selected_features
        
        def select_features_by_variance(self, features: pd.DataFrame, threshold: float = 0.1) -> List[str]:
            """
            åŸºäºŽæ–¹å·®é˜ˆå€¼é€‰æ‹©ç‰¹å¾
            
            Args:
                features: ç‰¹å¾æ•°æ®
                threshold: æ–¹å·®é˜ˆå€¼
                
            Returns:
                é€‰æ‹©çš„ç‰¹å¾ååˆ—è¡¨
            """
            # å¤„ç†ç¼ºå¤±å€¼
            features_clean = features.fillna(features.mean())
            
            selector = VarianceThreshold(threshold=threshold)
            selector.fit(features_clean)
            
            selected_features = features.columns[selector.get_support()].tolist()
            return selected_features
        
        def combine_features(self, feature_dfs: List[pd.DataFrame]) -> pd.DataFrame:
            """
            åˆå¹¶å¤šä¸ªç‰¹å¾DataFrame
            
            Args:
                feature_dfs: ç‰¹å¾DataFrameåˆ—è¡¨
                
            Returns:
                åˆå¹¶åŽçš„ç‰¹å¾DataFrame
            """
            if not feature_dfs:
                return pd.DataFrame()
            
            result = feature_dfs[0].copy()
            
            for df in feature_dfs[1:]:
                result = result.join(df, how='outer', rsuffix='_dup')
                
                # ç§»é™¤é‡å¤åˆ—
                dup_columns = [col for col in result.columns if col.endswith('_dup')]
                result = result.drop(columns=dup_columns)
            
            return result
        
        def create_feature_vector(self, timestamp: datetime, symbol: str, 
                                normalized_features: pd.Series) -> FeatureVector:
            """
            åˆ›å»ºç‰¹å¾å‘é‡å¯¹è±¡
            
            Args:
                timestamp: æ—¶é—´æˆ³
                symbol: è‚¡ç¥¨ä»£ç 
                normalized_features: æ ‡å‡†åŒ–åŽçš„ç‰¹å¾
                
            Returns:
                FeatureVectorå¯¹è±¡
            """
            # åˆ†ç±»ç‰¹å¾
            technical_indicators = {}
            fundamental_factors = {}
            market_microstructure = {}
            
            # æŠ€æœ¯æŒ‡æ ‡ç‰¹å¾
            tech_keywords = ['sma', 'ema', 'rsi', 'macd', 'bb', 'stoch', 'atr', 'volume', 'obv', 'vwap']
            
            # åŸºæœ¬é¢å› å­ç‰¹å¾
            fundamental_keywords = ['pe', 'pb', 'ps', 'pcf', 'roe', 'roa', 'margin', 'growth', 'debt', 'ratio']
            
            # å¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
            micro_keywords = ['turnover', 'illiquidity', 'spread', 'volatility', 'momentum']
            
            for feature_name, value in normalized_features.items():
                if pd.isna(value):
                    value = 0.0
                
                # æ ¹æ®ç‰¹å¾åç§°åˆ†ç±»
                if any(keyword in feature_name.lower() for keyword in tech_keywords):
                    technical_indicators[feature_name] = float(value)
                elif any(keyword in feature_name.lower() for keyword in fundamental_keywords):
                    fundamental_factors[feature_name] = float(value)
                elif any(keyword in feature_name.lower() for keyword in micro_keywords):
                    market_microstructure[feature_name] = float(value)
                else:
                    # é»˜è®¤å½’ç±»ä¸ºæŠ€æœ¯æŒ‡æ ‡
                    technical_indicators[feature_name] = float(value)
            
            # ç¡®ä¿æ¯ä¸ªç±»åˆ«è‡³å°‘æœ‰ä¸€ä¸ªç‰¹å¾
            if not technical_indicators:
                technical_indicators['default_tech'] = 0.0
            if not fundamental_factors:
                fundamental_factors['default_fundamental'] = 0.0
            if not market_microstructure:
                market_microstructure['default_micro'] = 0.0
            
            return FeatureVector(
                timestamp=timestamp,
                symbol=symbol,
                technical_indicators=technical_indicators,
                fundamental_factors=fundamental_factors,
                market_microstructure=market_microstructure
            )
    ]]></file>
  <file path="src/rl_trading_system/data/data_quality.py"><![CDATA[
    """
    æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·
    å®žçŽ°æ•°æ®è´¨é‡æ£€æŸ¥å’Œæ¸…æ´—åŠŸèƒ½
    """
    
    import pandas as pd
    import numpy as np
    import logging
    from typing import Dict, List, Any, Tuple, Optional
    from datetime import datetime, timedelta
    
    logger = logging.getLogger(__name__)
    
    
    class DataQualityChecker:
        """æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
        
        def __init__(self):
            """åˆå§‹åŒ–æ•°æ®è´¨é‡æ£€æŸ¥å™¨"""
            self.quality_rules = {
                'price': self._get_price_quality_rules(),
                'fundamental': self._get_fundamental_quality_rules(),
                'general': self._get_general_quality_rules()
            }
        
        def _get_price_quality_rules(self) -> Dict[str, Any]:
            """èŽ·å–ä»·æ ¼æ•°æ®è´¨é‡è§„åˆ™"""
            return {
                'required_columns': ['open', 'high', 'low', 'close', 'volume', 'amount'],
                'numeric_columns': ['open', 'high', 'low', 'close', 'volume', 'amount'],
                'positive_columns': ['open', 'high', 'low', 'close', 'volume', 'amount'],
                'price_relations': [
                    ('high', 'low', '>='),  # æœ€é«˜ä»· >= æœ€ä½Žä»·
                    ('high', 'open', '>='),  # æœ€é«˜ä»· >= å¼€ç›˜ä»·
                    ('high', 'close', '>='),  # æœ€é«˜ä»· >= æ”¶ç›˜ä»·
                    ('low', 'open', '<='),   # æœ€ä½Žä»· <= å¼€ç›˜ä»·
                    ('low', 'close', '<=')   # æœ€ä½Žä»· <= æ”¶ç›˜ä»·
                ],
                'outlier_thresholds': {
                    'price_change_ratio': 0.2,  # å•æ—¥æ¶¨è·Œå¹…è¶…è¿‡20%è§†ä¸ºå¼‚å¸¸
                    'volume_change_ratio': 10.0,  # æˆäº¤é‡å˜åŒ–è¶…è¿‡10å€è§†ä¸ºå¼‚å¸¸
                    'price_zscore': 3.0,  # ä»·æ ¼Z-scoreè¶…è¿‡3è§†ä¸ºå¼‚å¸¸
                    'volume_zscore': 3.0   # æˆäº¤é‡Z-scoreè¶…è¿‡3è§†ä¸ºå¼‚å¸¸
                }
            }
        
        def _get_fundamental_quality_rules(self) -> Dict[str, Any]:
            """èŽ·å–åŸºæœ¬é¢æ•°æ®è´¨é‡è§„åˆ™"""
            return {
                'numeric_columns': ['PE', 'PB', 'PS', 'PCF', 'ROE', 'ROA'],
                'ratio_ranges': {
                    'PE': (0, 1000),    # å¸‚ç›ˆçŽ‡åˆç†èŒƒå›´
                    'PB': (0, 100),     # å¸‚å‡€çŽ‡åˆç†èŒƒå›´
                    'ROE': (-1, 1),     # ROEåˆç†èŒƒå›´
                    'ROA': (-1, 1)      # ROAåˆç†èŒƒå›´
                }
            }
        
        def _get_general_quality_rules(self) -> Dict[str, Any]:
            """èŽ·å–é€šç”¨æ•°æ®è´¨é‡è§„åˆ™"""
            return {
                'max_missing_ratio': 0.1,  # æœ€å¤§ç¼ºå¤±å€¼æ¯”ä¾‹
                'min_data_points': 10,     # æœ€å°‘æ•°æ®ç‚¹æ•°é‡
                'duplicate_tolerance': 0.05  # é‡å¤æ•°æ®å®¹å¿åº¦
            }
        
        def check_data_quality(self, df: pd.DataFrame, 
                              data_type: str = 'price') -> Dict[str, Any]:
            """
            æ£€æŸ¥æ•°æ®è´¨é‡
            
            Args:
                df: å¾…æ£€æŸ¥çš„æ•°æ®
                data_type: æ•°æ®ç±»åž‹ ('price', 'fundamental', 'general')
                
            Returns:
                æ•°æ®è´¨é‡æŠ¥å‘Š
            """
            if df.empty:
                return {
                    'status': 'error',
                    'score': 0.0,
                    'issues': ['æ•°æ®ä¸ºç©º'],
                    'warnings': [],
                    'statistics': {}
                }
            
            issues = []
            warnings = []
            statistics = {}
            
            # åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯
            statistics.update(self._get_basic_statistics(df))
            
            # é€šç”¨æ£€æŸ¥
            general_issues, general_warnings = self._check_general_quality(df)
            issues.extend(general_issues)
            warnings.extend(general_warnings)
            
            # ç‰¹å®šç±»åž‹æ£€æŸ¥
            if data_type in self.quality_rules:
                type_issues, type_warnings = self._check_type_specific_quality(df, data_type)
                issues.extend(type_issues)
                warnings.extend(type_warnings)
            
            # è®¡ç®—è´¨é‡åˆ†æ•°
            score = self._calculate_quality_score(df, issues, warnings)
            
            # ç¡®å®šçŠ¶æ€
            if score >= 0.8:
                status = 'good'
            elif score >= 0.6:
                status = 'warning'
            else:
                status = 'error'
            
            return {
                'status': status,
                'score': score,
                'issues': issues,
                'warnings': warnings,
                'statistics': statistics
            }
        
        def _get_basic_statistics(self, df: pd.DataFrame) -> Dict[str, Any]:
            """èŽ·å–åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯"""
            stats = {
                'row_count': len(df),
                'column_count': len(df.columns),
                'missing_values': df.isnull().sum().to_dict(),
                'data_types': df.dtypes.astype(str).to_dict()
            }
            
            # æ•°å€¼åˆ—ç»Ÿè®¡
            numeric_columns = df.select_dtypes(include=[np.number]).columns
            if len(numeric_columns) > 0:
                stats['numeric_summary'] = df[numeric_columns].describe().to_dict()
            
            return stats
        
        def _check_general_quality(self, df: pd.DataFrame) -> Tuple[List[str], List[str]]:
            """æ£€æŸ¥é€šç”¨æ•°æ®è´¨é‡"""
            issues = []
            warnings = []
            rules = self.quality_rules['general']
            
            # æ£€æŸ¥æ•°æ®é‡
            if len(df) < rules['min_data_points']:
                issues.append(f"æ•°æ®ç‚¹æ•°é‡ä¸è¶³: {len(df)} < {rules['min_data_points']}")
            
            # æ£€æŸ¥ç¼ºå¤±å€¼æ¯”ä¾‹
            missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
            if missing_ratio > rules['max_missing_ratio']:
                issues.append(f"ç¼ºå¤±å€¼æ¯”ä¾‹è¿‡é«˜: {missing_ratio:.2%} > {rules['max_missing_ratio']:.2%}")
            
            # æ£€æŸ¥é‡å¤æ•°æ®
            duplicate_count = df.duplicated().sum()
            duplicate_ratio = duplicate_count / len(df)
            if duplicate_ratio > rules['duplicate_tolerance']:
                warnings.append(f"é‡å¤æ•°æ®æ¯”ä¾‹: {duplicate_ratio:.2%}")
            
            return issues, warnings
        
        def _check_type_specific_quality(self, df: pd.DataFrame, 
                                       data_type: str) -> Tuple[List[str], List[str]]:
            """æ£€æŸ¥ç‰¹å®šç±»åž‹çš„æ•°æ®è´¨é‡"""
            issues = []
            warnings = []
            rules = self.quality_rules[data_type]
            
            if data_type == 'price':
                issues_p, warnings_p = self._check_price_quality(df, rules)
                issues.extend(issues_p)
                warnings.extend(warnings_p)
            elif data_type == 'fundamental':
                issues_f, warnings_f = self._check_fundamental_quality(df, rules)
                issues.extend(issues_f)
                warnings.extend(warnings_f)
            
            return issues, warnings
        
        def _check_price_quality(self, df: pd.DataFrame, 
                               rules: Dict[str, Any]) -> Tuple[List[str], List[str]]:
            """æ£€æŸ¥ä»·æ ¼æ•°æ®è´¨é‡"""
            issues = []
            warnings = []
            
            # æ£€æŸ¥å¿…è¦åˆ—
            missing_columns = set(rules['required_columns']) - set(df.columns)
            if missing_columns:
                issues.append(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
                return issues, warnings
            
            # æ£€æŸ¥æ•°å€¼ç±»åž‹
            for col in rules['numeric_columns']:
                if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
                    issues.append(f"åˆ—{col}ä¸æ˜¯æ•°å€¼ç±»åž‹")
            
            # æ£€æŸ¥æ­£å€¼
            for col in rules['positive_columns']:
                if col in df.columns:
                    negative_count = (df[col] < 0).sum()
                    if negative_count > 0:
                        issues.append(f"åˆ—{col}å­˜åœ¨{negative_count}ä¸ªè´Ÿå€¼")
            
            # æ£€æŸ¥ä»·æ ¼å…³ç³»
            for col1, col2, operator in rules['price_relations']:
                if col1 in df.columns and col2 in df.columns:
                    if operator == '>=':
                        violation_count = (df[col1] < df[col2]).sum()
                    elif operator == '<=':
                        violation_count = (df[col1] > df[col2]).sum()
                    else:
                        continue
                    
                    if violation_count > 0:
                        issues.append(f"ä»·æ ¼å…³ç³»è¿è§„: {col1} {operator} {col2}, "
                                    f"è¿è§„æ•°é‡: {violation_count}")
            
            # æ£€æŸ¥å¼‚å¸¸å€¼
            outlier_issues, outlier_warnings = self._check_price_outliers(df, rules['outlier_thresholds'])
            issues.extend(outlier_issues)
            warnings.extend(outlier_warnings)
            
            return issues, warnings
        
        def _check_fundamental_quality(self, df: pd.DataFrame, 
                                     rules: Dict[str, Any]) -> Tuple[List[str], List[str]]:
            """æ£€æŸ¥åŸºæœ¬é¢æ•°æ®è´¨é‡"""
            issues = []
            warnings = []
            
            # æ£€æŸ¥æ•°å€¼ç±»åž‹
            for col in rules['numeric_columns']:
                if col in df.columns and not pd.api.types.is_numeric_dtype(df[col]):
                    warnings.append(f"åŸºæœ¬é¢åˆ—{col}ä¸æ˜¯æ•°å€¼ç±»åž‹")
            
            # æ£€æŸ¥æ¯”çŽ‡èŒƒå›´
            for col, (min_val, max_val) in rules['ratio_ranges'].items():
                if col in df.columns:
                    out_of_range = ((df[col] < min_val) | (df[col] > max_val)).sum()
                    if out_of_range > 0:
                        warnings.append(f"åˆ—{col}å­˜åœ¨{out_of_range}ä¸ªè¶…å‡ºåˆç†èŒƒå›´çš„å€¼")
            
            return issues, warnings
        
        def _check_price_outliers(self, df: pd.DataFrame, 
                                thresholds: Dict[str, float]) -> Tuple[List[str], List[str]]:
            """æ£€æŸ¥ä»·æ ¼å¼‚å¸¸å€¼"""
            issues = []
            warnings = []
            
            if 'close' in df.columns:
                # æ£€æŸ¥ä»·æ ¼å˜åŒ–å¼‚å¸¸
                price_change = df['close'].pct_change().abs()
                extreme_changes = (price_change > thresholds['price_change_ratio']).sum()
                if extreme_changes > 0:
                    warnings.append(f"å­˜åœ¨{extreme_changes}ä¸ªæžç«¯ä»·æ ¼å˜åŒ–")
                
                # æ£€æŸ¥ä»·æ ¼Z-scoreå¼‚å¸¸
                price_zscore = np.abs((df['close'] - df['close'].mean()) / df['close'].std())
                price_outliers = (price_zscore > thresholds['price_zscore']).sum()
                if price_outliers > 0:
                    warnings.append(f"å­˜åœ¨{price_outliers}ä¸ªä»·æ ¼å¼‚å¸¸å€¼")
            
            if 'volume' in df.columns:
                # æ£€æŸ¥æˆäº¤é‡å˜åŒ–å¼‚å¸¸
                volume_change = df['volume'].pct_change().abs()
                extreme_volume_changes = (volume_change > thresholds['volume_change_ratio']).sum()
                if extreme_volume_changes > 0:
                    warnings.append(f"å­˜åœ¨{extreme_volume_changes}ä¸ªæžç«¯æˆäº¤é‡å˜åŒ–")
                
                # æ£€æŸ¥æˆäº¤é‡Z-scoreå¼‚å¸¸
                volume_zscore = np.abs((df['volume'] - df['volume'].mean()) / df['volume'].std())
                volume_outliers = (volume_zscore > thresholds['volume_zscore']).sum()
                if volume_outliers > 0:
                    warnings.append(f"å­˜åœ¨{volume_outliers}ä¸ªæˆäº¤é‡å¼‚å¸¸å€¼")
            
            return issues, warnings
        
        def _calculate_quality_score(self, df: pd.DataFrame, 
                                   issues: List[str], warnings: List[str]) -> float:
            """è®¡ç®—æ•°æ®è´¨é‡åˆ†æ•°"""
            base_score = 1.0
            
            # æ ¹æ®é—®é¢˜æ•°é‡æ‰£åˆ†
            issue_penalty = len(issues) * 0.2
            warning_penalty = len(warnings) * 0.05
            
            # æ ¹æ®ç¼ºå¤±å€¼æ¯”ä¾‹æ‰£åˆ†
            missing_ratio = df.isnull().sum().sum() / (len(df) * len(df.columns))
            missing_penalty = missing_ratio * 0.3
            
            # è®¡ç®—æœ€ç»ˆåˆ†æ•°
            final_score = max(0.0, base_score - issue_penalty - warning_penalty - missing_penalty)
            
            return final_score
        
        def clean_data(self, df: pd.DataFrame, 
                       data_type: str = 'price',
                       strategy: str = 'conservative') -> pd.DataFrame:
            """
            æ¸…æ´—æ•°æ®
            
            Args:
                df: å¾…æ¸…æ´—çš„æ•°æ®
                data_type: æ•°æ®ç±»åž‹
                strategy: æ¸…æ´—ç­–ç•¥ ('conservative', 'aggressive')
                
            Returns:
                æ¸…æ´—åŽçš„æ•°æ®
            """
            if df.empty:
                return df
            
            cleaned_df = df.copy()
            
            # åˆ é™¤å®Œå…¨é‡å¤çš„è¡Œ
            cleaned_df = cleaned_df.drop_duplicates()
            
            # å¤„ç†ç¼ºå¤±å€¼
            if strategy == 'conservative':
                # ä¿å®ˆç­–ç•¥ï¼šåªåˆ é™¤å…¨éƒ¨ä¸ºç©ºçš„è¡Œ
                cleaned_df = cleaned_df.dropna(how='all')
            elif strategy == 'aggressive':
                # æ¿€è¿›ç­–ç•¥ï¼šåˆ é™¤ä»»ä½•åŒ…å«ç©ºå€¼çš„è¡Œ
                cleaned_df = cleaned_df.dropna()
            
            # ç‰¹å®šç±»åž‹çš„æ¸…æ´—
            if data_type == 'price':
                cleaned_df = self._clean_price_data(cleaned_df, strategy)
            elif data_type == 'fundamental':
                cleaned_df = self._clean_fundamental_data(cleaned_df, strategy)
            
            logger.info(f"æ•°æ®æ¸…æ´—å®Œæˆ: {len(df)} -> {len(cleaned_df)} è¡Œ")
            
            return cleaned_df
        
        def _clean_price_data(self, df: pd.DataFrame, strategy: str) -> pd.DataFrame:
            """æ¸…æ´—ä»·æ ¼æ•°æ®"""
            cleaned_df = df.copy()
            
            # åˆ é™¤è´Ÿä»·æ ¼
            price_columns = ['open', 'high', 'low', 'close']
            for col in price_columns:
                if col in cleaned_df.columns:
                    cleaned_df = cleaned_df[cleaned_df[col] > 0]
            
            # åˆ é™¤æˆäº¤é‡ä¸ºè´Ÿçš„è®°å½•
            if 'volume' in cleaned_df.columns:
                cleaned_df = cleaned_df[cleaned_df['volume'] >= 0]
            
            # ä¿®æ­£ä»·æ ¼å…³ç³»é”™è¯¯
            if all(col in cleaned_df.columns for col in ['high', 'low']):
                # åˆ é™¤æœ€é«˜ä»·ä½ŽäºŽæœ€ä½Žä»·çš„è®°å½•
                cleaned_df = cleaned_df[cleaned_df['high'] >= cleaned_df['low']]
            
            # å¤„ç†æžç«¯å¼‚å¸¸å€¼
            if strategy == 'aggressive':
                for col in price_columns:
                    if col in cleaned_df.columns:
                        # ä½¿ç”¨3Ïƒè§„åˆ™åˆ é™¤å¼‚å¸¸å€¼
                        mean_val = cleaned_df[col].mean()
                        std_val = cleaned_df[col].std()
                        lower_bound = mean_val - 3 * std_val
                        upper_bound = mean_val + 3 * std_val
                        cleaned_df = cleaned_df[
                            (cleaned_df[col] >= lower_bound) & 
                            (cleaned_df[col] <= upper_bound)
                        ]
            
            return cleaned_df
        
        def _clean_fundamental_data(self, df: pd.DataFrame, strategy: str) -> pd.DataFrame:
            """æ¸…æ´—åŸºæœ¬é¢æ•°æ®"""
            cleaned_df = df.copy()
            
            # å¤„ç†æžç«¯æ¯”çŽ‡å€¼
            ratio_columns = ['PE', 'PB', 'PS', 'PCF']
            for col in ratio_columns:
                if col in cleaned_df.columns:
                    # åˆ é™¤è´Ÿå€¼å’Œæžå¤§å€¼
                    cleaned_df = cleaned_df[
                        (cleaned_df[col] > 0) & 
                        (cleaned_df[col] < 1000)
                    ]
            
            return cleaned_df
    
    
    # å…¨å±€æ•°æ®è´¨é‡æ£€æŸ¥å™¨å®žä¾‹
    _global_quality_checker = None
    
    
    def get_global_quality_checker() -> DataQualityChecker:
        """èŽ·å–å…¨å±€æ•°æ®è´¨é‡æ£€æŸ¥å™¨å®žä¾‹"""
        global _global_quality_checker
        if _global_quality_checker is None:
            _global_quality_checker = DataQualityChecker()
        return _global_quality_checker
    ]]></file>
  <file path="src/rl_trading_system/data/data_processor.py"><![CDATA[
    """
    æ•°æ®é¢„å¤„ç†æ¨¡å—
    å®žçŽ°å®Œæ•´çš„æ•°æ®é¢„å¤„ç†æµæ°´çº¿ï¼ŒåŒ…æ‹¬æ•°æ®æ¸…æ´—ã€ç‰¹å¾è®¡ç®—ã€æ ‡å‡†åŒ–å’Œç¼“å­˜
    """
    
    import pandas as pd
    import numpy as np
    import logging
    from typing import Dict, List, Any, Optional, Union, Tuple
    from datetime import datetime
    from concurrent.futures import ThreadPoolExecutor, as_completed
    import hashlib
    import json
    
    from .data_models import FeatureVector, MarketData
    from .feature_engineer import FeatureEngineer
    from .data_quality import DataQualityChecker, get_global_quality_checker
    from .data_cache import DataCache, get_global_cache
    
    logger = logging.getLogger(__name__)
    
    
    class DataProcessor:
        """æ•°æ®é¢„å¤„ç†å™¨"""
        
        def __init__(self, 
                     feature_engineer: Optional[FeatureEngineer] = None,
                     quality_checker: Optional[DataQualityChecker] = None,
                     cache: Optional[DataCache] = None):
            """
            åˆå§‹åŒ–æ•°æ®é¢„å¤„ç†å™¨
            
            Args:
                feature_engineer: ç‰¹å¾å·¥ç¨‹å™¨å®žä¾‹
                quality_checker: æ•°æ®è´¨é‡æ£€æŸ¥å™¨å®žä¾‹
                cache: æ•°æ®ç¼“å­˜å®žä¾‹
            """
            self.feature_engineer = feature_engineer or FeatureEngineer()
            self.quality_checker = quality_checker or get_global_quality_checker()
            self.cache = cache or get_global_cache()
            
            # é»˜è®¤é…ç½®
            self.config = {
                'clean_strategy': 'conservative',
                'missing_value_method': 'ffill',
                'outlier_treatment': 'clip',
                'normalize': False,
                'normalization_method': 'zscore',
                'calculate_features': True,
                'feature_selection': False,
                'cache_enabled': True,
                'parallel_processing': True,
                'max_workers': 4
            }
            
            # å¤„ç†ç»Ÿè®¡ä¿¡æ¯
            self.stats = {
                'processed_count': 0,
                'cache_hits': 0,
                'cache_misses': 0,
                'processing_times': []
            }
        
        def configure_pipeline(self, config: Dict[str, Any]):
            """
            é…ç½®é¢„å¤„ç†æµæ°´çº¿
            
            Args:
                config: é…ç½®å­—å…¸
            """
            self.config.update(config)
            logger.info(f"æ•°æ®é¢„å¤„ç†æµæ°´çº¿é…ç½®å·²æ›´æ–°: {config}")
        
        def process_data(self, 
                        data: pd.DataFrame,
                        symbols: List[str],
                        data_type: str = 'price',
                        **kwargs) -> Dict[str, Any]:
            """
            å¤„ç†å•ä¸ªæ•°æ®é›†
            
            Args:
                data: è¾“å…¥æ•°æ®
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                data_type: æ•°æ®ç±»åž‹ ('price', 'fundamental')
                **kwargs: é¢å¤–å‚æ•°ï¼Œä¼šè¦†ç›–é»˜è®¤é…ç½®
                
            Returns:
                å¤„ç†ç»“æžœå­—å…¸ï¼ŒåŒ…å«processed_data, quality_report, feature_vectorsç­‰
            """
            import time
            start_time = time.time()
            
            # åˆå¹¶é…ç½®
            config = {**self.config, **kwargs}
            
            # æ£€æŸ¥ç¼“å­˜
            if config.get('use_cache', config['cache_enabled']):
                cache_key = self._generate_cache_key(data, symbols, data_type, config)
                cached_result = self.cache.get(cache_key)
                if cached_result is not None:
                    self.stats['cache_hits'] += 1
                    logger.debug(f"ç¼“å­˜å‘½ä¸­: {symbols}")
                    return cached_result
                else:
                    self.stats['cache_misses'] += 1
            
            try:
                # æ•°æ®éªŒè¯
                if not self.validate_data(data, data_type):
                    raise ValueError(f"æ•°æ®éªŒè¯å¤±è´¥: {data_type}")
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                quality_report = self.check_data_quality(data, data_type)
                
                # æ•°æ®æ¸…æ´—
                cleaned_data = self._clean_data(data, data_type, config)
                
                # å¤„ç†ç¼ºå¤±å€¼
                processed_data = self._handle_missing_values(cleaned_data, config)
                
                # å¼‚å¸¸å€¼å¤„ç†
                processed_data = self._handle_outliers(processed_data, config)
                
                # ç‰¹å¾å·¥ç¨‹
                feature_vectors = []
                if config.get('calculate_features', True):
                    feature_vectors = self._calculate_features(
                        processed_data, symbols, data_type, config
                    )
                
                # æ•°æ®æ ‡å‡†åŒ–
                if config.get('normalize', False):
                    processed_data = self._normalize_data(processed_data, config)
                
                # ç‰¹å¾é€‰æ‹©
                if config.get('feature_selection', False) and feature_vectors:
                    feature_vectors = self._select_features(feature_vectors, config)
                
                # æž„å»ºç»“æžœ
                result = {
                    'processed_data': processed_data,
                    'quality_report': quality_report,
                    'feature_vectors': feature_vectors,
                    'processing_info': {
                        'symbols': symbols,
                        'data_type': data_type,
                        'config': config,
                        'processing_time': time.time() - start_time,
                        'original_shape': data.shape,
                        'processed_shape': processed_data.shape
                    }
                }
                
                # ç¼“å­˜ç»“æžœ
                if config.get('use_cache', config['cache_enabled']):
                    self.cache.set(cache_key, result)
                
                # æ›´æ–°ç»Ÿè®¡ä¿¡æ¯
                self.stats['processed_count'] += 1
                self.stats['processing_times'].append(time.time() - start_time)
                
                logger.info(f"æ•°æ®å¤„ç†å®Œæˆ: {symbols}, è€—æ—¶: {time.time() - start_time:.2f}s")
                
                return result
                
            except Exception as e:
                logger.error(f"æ•°æ®å¤„ç†å¤±è´¥: {symbols}, é”™è¯¯: {str(e)}")
                raise
        
        def process_batch(self, 
                         batch_data: Dict[str, pd.DataFrame],
                         data_type: str = 'price',
                         parallel: bool = None,
                         **kwargs) -> Dict[str, Dict[str, Any]]:
            """
            æ‰¹é‡å¤„ç†å¤šä¸ªæ•°æ®é›†
            
            Args:
                batch_data: æ‰¹é‡æ•°æ®å­—å…¸ï¼Œé”®ä¸ºè‚¡ç¥¨ä»£ç ï¼Œå€¼ä¸ºæ•°æ®
                data_type: æ•°æ®ç±»åž‹
                parallel: æ˜¯å¦å¹¶è¡Œå¤„ç†
                **kwargs: é¢å¤–å‚æ•°
                
            Returns:
                æ‰¹é‡å¤„ç†ç»“æžœå­—å…¸
            """
            if parallel is None:
                parallel = self.config.get('parallel_processing', True)
            
            results = {}
            
            if parallel and len(batch_data) > 1:
                # å¹¶è¡Œå¤„ç†
                max_workers = min(self.config.get('max_workers', 4), len(batch_data))
                
                with ThreadPoolExecutor(max_workers=max_workers) as executor:
                    # æäº¤ä»»åŠ¡
                    future_to_symbol = {
                        executor.submit(
                            self.process_data, 
                            data, 
                            [symbol], 
                            data_type, 
                            **kwargs
                        ): symbol
                        for symbol, data in batch_data.items()
                    }
                    
                    # æ”¶é›†ç»“æžœ
                    for future in as_completed(future_to_symbol):
                        symbol = future_to_symbol[future]
                        try:
                            result = future.result()
                            results[symbol] = result
                        except Exception as e:
                            logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {symbol}, é”™è¯¯: {str(e)}")
                            results[symbol] = {
                                'error': str(e),
                                'processed_data': pd.DataFrame(),
                                'quality_report': {'status': 'error', 'score': 0.0},
                                'feature_vectors': []
                            }
            else:
                # ä¸²è¡Œå¤„ç†
                for symbol, data in batch_data.items():
                    try:
                        result = self.process_data(data, [symbol], data_type, **kwargs)
                        results[symbol] = result
                    except Exception as e:
                        logger.error(f"æ‰¹é‡å¤„ç†å¤±è´¥: {symbol}, é”™è¯¯: {str(e)}")
                        results[symbol] = {
                            'error': str(e),
                            'processed_data': pd.DataFrame(),
                            'quality_report': {'status': 'error', 'score': 0.0},
                            'feature_vectors': []
                        }
            
            logger.info(f"æ‰¹é‡å¤„ç†å®Œæˆ: {len(batch_data)}ä¸ªæ•°æ®é›†")
            return results
        
        def validate_data(self, data: pd.DataFrame, data_type: str) -> bool:
            """
            éªŒè¯æ•°æ®æœ‰æ•ˆæ€§
            
            Args:
                data: è¾“å…¥æ•°æ®
                data_type: æ•°æ®ç±»åž‹
                
            Returns:
                æ˜¯å¦æœ‰æ•ˆ
            """
            if data.empty:
                logger.warning("æ•°æ®ä¸ºç©º")
                return False
            
            if data_type == 'price':
                required_columns = ['open', 'high', 'low', 'close', 'volume', 'amount']
                missing_columns = set(required_columns) - set(data.columns)
                if missing_columns:
                    logger.warning(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
                    return False
                
                # æ£€æŸ¥ä»·æ ¼å…³ç³»
                if 'high' in data.columns and 'low' in data.columns:
                    invalid_relations = (data['high'] < data['low']).sum()
                    if invalid_relations > len(data) * 0.1:  # è¶…è¿‡10%çš„æ•°æ®æœ‰é—®é¢˜
                        logger.warning(f"ä»·æ ¼å…³ç³»é”™è¯¯è¿‡å¤š: {invalid_relations}")
                        return False
            
            return True
        
        def check_data_quality(self, data: pd.DataFrame, data_type: str) -> Dict[str, Any]:
            """
            æ£€æŸ¥æ•°æ®è´¨é‡
            
            Args:
                data: è¾“å…¥æ•°æ®
                data_type: æ•°æ®ç±»åž‹
                
            Returns:
                è´¨é‡æŠ¥å‘Š
            """
            return self.quality_checker.check_data_quality(data, data_type)
        
        def _clean_data(self, data: pd.DataFrame, data_type: str, config: Dict[str, Any]) -> pd.DataFrame:
            """æ¸…æ´—æ•°æ®"""
            strategy = config.get('clean_strategy', 'conservative')
            return self.quality_checker.clean_data(data, data_type, strategy)
        
        def _handle_missing_values(self, data: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
            """å¤„ç†ç¼ºå¤±å€¼"""
            method = config.get('missing_value_method', 'ffill')
            return self.feature_engineer.handle_missing_values(data, method)
        
        def _handle_outliers(self, data: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
            """å¤„ç†å¼‚å¸¸å€¼"""
            treatment = config.get('outlier_treatment', 'clip')
            if treatment == 'clip':
                return self.feature_engineer.treat_outliers(data, method='clip')
            elif treatment == 'remove':
                return self.feature_engineer.treat_outliers(data, method='remove')
            else:
                return data
        
        def _calculate_features(self, 
                              data: pd.DataFrame, 
                              symbols: List[str], 
                              data_type: str,
                              config: Dict[str, Any]) -> List[FeatureVector]:
            """è®¡ç®—ç‰¹å¾"""
            feature_vectors = []
            
            try:
                if data_type == 'price':
                    # è®¡ç®—æŠ€æœ¯æŒ‡æ ‡
                    technical_features = self.feature_engineer.calculate_technical_indicators(data)
                    
                    # è®¡ç®—å¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
                    microstructure_features = self.feature_engineer.calculate_microstructure_features(data)
                    
                    # åˆå¹¶ç‰¹å¾
                    all_features = self.feature_engineer.combine_features([
                        technical_features, microstructure_features
                    ])
                    
                    # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹å’Œè‚¡ç¥¨åˆ›å»ºç‰¹å¾å‘é‡
                    for timestamp in all_features.index:
                        if pd.isna(timestamp):
                            continue
                        
                        for symbol in symbols:
                            try:
                                feature_series = all_features.loc[timestamp]
                                if feature_series.isna().all():
                                    continue
                                
                                feature_vector = self.feature_engineer.create_feature_vector(
                                    timestamp=timestamp,
                                    symbol=symbol,
                                    normalized_features=feature_series
                                )
                                feature_vectors.append(feature_vector)
                            except Exception as e:
                                logger.warning(f"åˆ›å»ºç‰¹å¾å‘é‡å¤±è´¥: {symbol}, {timestamp}, {str(e)}")
                                continue
                
                elif data_type == 'fundamental':
                    # è®¡ç®—åŸºæœ¬é¢å› å­
                    fundamental_features = self.feature_engineer.calculate_fundamental_factors(data)
                    
                    # ä¸ºæ¯ä¸ªæ—¶é—´ç‚¹å’Œè‚¡ç¥¨åˆ›å»ºç‰¹å¾å‘é‡
                    for timestamp in fundamental_features.index:
                        if pd.isna(timestamp):
                            continue
                        
                        for symbol in symbols:
                            try:
                                feature_series = fundamental_features.loc[timestamp]
                                if feature_series.isna().all():
                                    continue
                                
                                # åˆ›å»ºåŸºæœ¬é¢ç‰¹å¾å‘é‡
                                feature_vector = FeatureVector(
                                    timestamp=timestamp,
                                    symbol=symbol,
                                    technical_indicators={'default_tech': 0.0},
                                    fundamental_factors=feature_series.to_dict(),
                                    market_microstructure={'default_micro': 0.0}
                                )
                                feature_vectors.append(feature_vector)
                            except Exception as e:
                                logger.warning(f"åˆ›å»ºåŸºæœ¬é¢ç‰¹å¾å‘é‡å¤±è´¥: {symbol}, {timestamp}, {str(e)}")
                                continue
                
            except Exception as e:
                logger.error(f"ç‰¹å¾è®¡ç®—å¤±è´¥: {str(e)}")
            
            logger.info(f"è®¡ç®—ç‰¹å¾å®Œæˆ: {len(feature_vectors)}ä¸ªç‰¹å¾å‘é‡")
            return feature_vectors
        
        def _normalize_data(self, data: pd.DataFrame, config: Dict[str, Any]) -> pd.DataFrame:
            """æ ‡å‡†åŒ–æ•°æ®"""
            method = config.get('normalization_method', 'zscore')
            return self.feature_engineer.normalize_features(data, method)
        
        def _select_features(self, feature_vectors: List[FeatureVector], 
                            config: Dict[str, Any]) -> List[FeatureVector]:
            """ç‰¹å¾é€‰æ‹©"""
            # è¿™é‡Œå¯ä»¥å®žçŽ°ç‰¹å¾é€‰æ‹©é€»è¾‘
            # æš‚æ—¶è¿”å›žåŽŸå§‹ç‰¹å¾å‘é‡
            return feature_vectors
        
        def _generate_cache_key(self, 
                              data: pd.DataFrame, 
                              symbols: List[str], 
                              data_type: str,
                              config: Dict[str, Any]) -> str:
            """ç”Ÿæˆç¼“å­˜é”®"""
            # åˆ›å»ºæ•°æ®æŒ‡çº¹
            data_hash = hashlib.md5(
                pd.util.hash_pandas_object(data, index=True).values
            ).hexdigest()[:16]
            
            # åˆ›å»ºé…ç½®æŒ‡çº¹
            config_str = json.dumps(config, sort_keys=True)
            config_hash = hashlib.md5(config_str.encode()).hexdigest()[:16]
            
            # ç»„åˆç¼“å­˜é”®
            cache_key = f"data_processor_{data_type}_{'-'.join(symbols)}_{data_hash}_{config_hash}"
            
            return cache_key
        
        def get_processing_stats(self) -> Dict[str, Any]:
            """èŽ·å–å¤„ç†ç»Ÿè®¡ä¿¡æ¯"""
            avg_time = np.mean(self.stats['processing_times']) if self.stats['processing_times'] else 0
            
            return {
                'processed_count': self.stats['processed_count'],
                'cache_hits': self.stats['cache_hits'],
                'cache_misses': self.stats['cache_misses'],
                'cache_hit_rate': self.stats['cache_hits'] / max(1, self.stats['cache_hits'] + self.stats['cache_misses']),
                'average_processing_time': avg_time,
                'total_processing_time': sum(self.stats['processing_times'])
            }
        
        def clear_cache(self):
            """æ¸…ç©ºç¼“å­˜"""
            self.cache.clear()
            logger.info("æ•°æ®å¤„ç†å™¨ç¼“å­˜å·²æ¸…ç©º")
        
        def reset_stats(self):
            """é‡ç½®ç»Ÿè®¡ä¿¡æ¯"""
            self.stats = {
                'processed_count': 0,
                'cache_hits': 0,
                'cache_misses': 0,
                'processing_times': []
            }
            logger.info("æ•°æ®å¤„ç†å™¨ç»Ÿè®¡ä¿¡æ¯å·²é‡ç½®")
    
    
    class BatchDataProcessor:
        """æ‰¹é‡æ•°æ®å¤„ç†å™¨"""
        
        def __init__(self, processor: DataProcessor):
            """
            åˆå§‹åŒ–æ‰¹é‡å¤„ç†å™¨
            
            Args:
                processor: æ•°æ®å¤„ç†å™¨å®žä¾‹
            """
            self.processor = processor
        
        def process_time_series_batch(self, 
                                     data: pd.DataFrame,
                                     symbols: List[str],
                                     window_size: int = 60,
                                     step_size: int = 1,
                                     data_type: str = 'price',
                                     **kwargs) -> List[Dict[str, Any]]:
            """
            å¤„ç†æ—¶é—´åºåˆ—æ‰¹é‡æ•°æ®
            
            Args:
                data: æ—¶é—´åºåˆ—æ•°æ®
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                window_size: çª—å£å¤§å°
                step_size: æ­¥é•¿
                data_type: æ•°æ®ç±»åž‹
                **kwargs: é¢å¤–å‚æ•°
                
            Returns:
                æ‰¹é‡å¤„ç†ç»“æžœåˆ—è¡¨
            """
            results = []
            
            # æŒ‰çª—å£åˆ‡åˆ†æ•°æ®
            for i in range(0, len(data) - window_size + 1, step_size):
                window_data = data.iloc[i:i + window_size]
                
                try:
                    result = self.processor.process_data(
                        data=window_data,
                        symbols=symbols,
                        data_type=data_type,
                        **kwargs
                    )
                    
                    # æ·»åŠ çª—å£ä¿¡æ¯
                    result['window_info'] = {
                        'start_index': i,
                        'end_index': i + window_size,
                        'start_date': window_data.index[0],
                        'end_date': window_data.index[-1]
                    }
                    
                    results.append(result)
                    
                except Exception as e:
                    logger.error(f"çª—å£å¤„ç†å¤±è´¥: {i}-{i + window_size}, é”™è¯¯: {str(e)}")
                    continue
            
            logger.info(f"æ—¶é—´åºåˆ—æ‰¹é‡å¤„ç†å®Œæˆ: {len(results)}ä¸ªçª—å£")
            return results
        
        def process_cross_sectional_batch(self,
                                        data_dict: Dict[str, pd.DataFrame],
                                        timestamp: datetime,
                                        data_type: str = 'price',
                                        **kwargs) -> Dict[str, Dict[str, Any]]:
            """
            å¤„ç†æ¨ªæˆªé¢æ‰¹é‡æ•°æ®
            
            Args:
                data_dict: è‚¡ç¥¨æ•°æ®å­—å…¸
                timestamp: æ—¶é—´æˆ³
                data_type: æ•°æ®ç±»åž‹
                **kwargs: é¢å¤–å‚æ•°
                
            Returns:
                æ¨ªæˆªé¢å¤„ç†ç»“æžœå­—å…¸
            """
            # æå–æŒ‡å®šæ—¶é—´æˆ³çš„æ•°æ®
            cross_sectional_data = {}
            
            for symbol, data in data_dict.items():
                if timestamp in data.index:
                    # æå–å•è¡Œæ•°æ®å¹¶è½¬æ¢ä¸ºDataFrame
                    row_data = data.loc[[timestamp]]
                    cross_sectional_data[symbol] = row_data
            
            # æ‰¹é‡å¤„ç†
            return self.processor.process_batch(
                batch_data=cross_sectional_data,
                data_type=data_type,
                **kwargs
            )
    
    
    # å…¨å±€æ•°æ®å¤„ç†å™¨å®žä¾‹
    _global_processor = None
    
    
    def get_global_processor() -> DataProcessor:
        """èŽ·å–å…¨å±€æ•°æ®å¤„ç†å™¨å®žä¾‹"""
        global _global_processor
        if _global_processor is None:
            _global_processor = DataProcessor()
        return _global_processor
    
    
    def set_global_processor(processor: DataProcessor):
        """è®¾ç½®å…¨å±€æ•°æ®å¤„ç†å™¨å®žä¾‹"""
        global _global_processor
        _global_processor = processor
    ]]></file>
  <file path="src/rl_trading_system/data/data_models.py"><![CDATA[
    """
    æ ¸å¿ƒæ•°æ®æ¨¡åž‹
    å®šä¹‰ç³»ç»Ÿä¸­ä½¿ç”¨çš„æ ¸å¿ƒæ•°æ®ç»“æž„ï¼ŒåŒ…æ‹¬å¸‚åœºæ•°æ®ã€ç‰¹å¾å‘é‡ã€äº¤æ˜“çŠ¶æ€ç­‰
    """
    
    from dataclasses import dataclass, field
    from datetime import datetime
    from typing import Dict, Any, Optional, List
    import numpy as np
    import json
    import math
    
    
    @dataclass
    class MarketData:
        """å¸‚åœºæ•°æ®ç»“æž„"""
        timestamp: datetime
        symbol: str
        open_price: float
        high_price: float
        low_price: float
        close_price: float
        volume: int
        amount: float
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            # éªŒè¯ä»·æ ¼é¡ºåº
            if self.high_price < self.low_price:
                raise ValueError("æœ€é«˜ä»·ä¸èƒ½ä½ŽäºŽæœ€ä½Žä»·")
            
            # éªŒè¯æˆäº¤é‡å’Œæˆäº¤é¢éžè´Ÿ
            if self.volume < 0:
                raise ValueError("æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            if self.amount < 0:
                raise ValueError("æˆäº¤é¢ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯ä»·æ ¼éžè´Ÿ
            if any(price < 0 for price in [self.open_price, self.high_price, 
                                          self.low_price, self.close_price]):
                raise ValueError("ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'timestamp': self.timestamp.isoformat(),
                'symbol': self.symbol,
                'open_price': self.open_price,
                'high_price': self.high_price,
                'low_price': self.low_price,
                'close_price': self.close_price,
                'volume': self.volume,
                'amount': self.amount
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'MarketData':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            data_copy = data.copy()
            data_copy['timestamp'] = datetime.fromisoformat(data_copy['timestamp'])
            return cls(**data_copy)
        
        def to_json(self) -> str:
            """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
            return json.dumps(self.to_dict())
        
        @classmethod
        def from_json(cls, json_str: str) -> 'MarketData':
            """ä»ŽJSONå­—ç¬¦ä¸²åˆ›å»ºå¯¹è±¡"""
            data = json.loads(json_str)
            return cls.from_dict(data)
    
    
    @dataclass
    class FeatureVector:
        """ç‰¹å¾å‘é‡ç»“æž„"""
        timestamp: datetime
        symbol: str
        technical_indicators: Dict[str, float]
        fundamental_factors: Dict[str, float]
        market_microstructure: Dict[str, float]
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            # éªŒè¯ç‰¹å¾å­—å…¸éžç©º
            if not self.technical_indicators:
                raise ValueError("æŠ€æœ¯æŒ‡æ ‡ä¸èƒ½ä¸ºç©º")
            
            if not self.fundamental_factors:
                raise ValueError("åŸºæœ¬é¢å› å­ä¸èƒ½ä¸ºç©º")
            
            if not self.market_microstructure:
                raise ValueError("å¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾ä¸èƒ½ä¸ºç©º")
            
            # éªŒè¯ç‰¹å¾å€¼ä¸åŒ…å«NaN
            all_features = {**self.technical_indicators, 
                           **self.fundamental_factors, 
                           **self.market_microstructure}
            
            for name, value in all_features.items():
                if math.isnan(value):
                    raise ValueError(f"ç‰¹å¾å€¼ä¸èƒ½åŒ…å«NaN: {name}")
                if math.isinf(value):
                    raise ValueError(f"ç‰¹å¾å€¼ä¸èƒ½åŒ…å«æ— ç©·å¤§: {name}")
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'timestamp': self.timestamp.isoformat(),
                'symbol': self.symbol,
                'technical_indicators': self.technical_indicators,
                'fundamental_factors': self.fundamental_factors,
                'market_microstructure': self.market_microstructure
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'FeatureVector':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            data_copy = data.copy()
            data_copy['timestamp'] = datetime.fromisoformat(data_copy['timestamp'])
            return cls(**data_copy)
        
        def to_json(self) -> str:
            """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
            return json.dumps(self.to_dict())
        
        @classmethod
        def from_json(cls, json_str: str) -> 'FeatureVector':
            """ä»ŽJSONå­—ç¬¦ä¸²åˆ›å»ºå¯¹è±¡"""
            data = json.loads(json_str)
            return cls.from_dict(data)
        
        def get_all_features(self) -> Dict[str, float]:
            """èŽ·å–æ‰€æœ‰ç‰¹å¾çš„åˆå¹¶å­—å…¸"""
            return {**self.technical_indicators, 
                    **self.fundamental_factors, 
                    **self.market_microstructure}
    
    
    @dataclass
    class TradingState:
        """äº¤æ˜“çŠ¶æ€ç»“æž„"""
        features: np.ndarray  # [lookback_window, n_stocks, n_features]
        positions: np.ndarray  # [n_stocks]
        market_state: np.ndarray  # [market_features]
        cash: float
        total_value: float
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            # éªŒè¯ç‰¹å¾æ•°ç»„ç»´åº¦
            if self.features.ndim != 3:
                raise ValueError("ç‰¹å¾æ•°ç»„å¿…é¡»æ˜¯3ç»´")
            
            # éªŒè¯æŒä»“æƒé‡å’Œ
            if abs(self.positions.sum() - 1.0) > 1e-6:
                raise ValueError("æŒä»“æƒé‡å’Œå¿…é¡»æŽ¥è¿‘1")
            
            # éªŒè¯æŒä»“æƒé‡éžè´Ÿ
            if (self.positions < 0).any():
                raise ValueError("æŒä»“æƒé‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯çŽ°é‡‘éžè´Ÿ
            if self.cash < 0:
                raise ValueError("çŽ°é‡‘ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯æ€»ä»·å€¼ä¸ºæ­£
            if self.total_value <= 0:
                raise ValueError("æ€»ä»·å€¼å¿…é¡»ä¸ºæ­£æ•°")
            
            # éªŒè¯æ•°ç»„ä¸åŒ…å«NaNæˆ–æ— ç©·å¤§
            if np.isnan(self.features).any():
                raise ValueError("ç‰¹å¾æ•°ç»„ä¸èƒ½åŒ…å«NaN")
            
            if np.isinf(self.features).any():
                raise ValueError("ç‰¹å¾æ•°ç»„ä¸èƒ½åŒ…å«æ— ç©·å¤§")
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'features': self.features.tolist(),
                'positions': self.positions.tolist(),
                'market_state': self.market_state.tolist(),
                'cash': self.cash,
                'total_value': self.total_value
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'TradingState':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            return cls(
                features=np.array(data['features']),
                positions=np.array(data['positions']),
                market_state=np.array(data['market_state']),
                cash=data['cash'],
                total_value=data['total_value']
            )
        
        def to_json(self) -> str:
            """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
            return json.dumps(self.to_dict())
        
        @classmethod
        def from_json(cls, json_str: str) -> 'TradingState':
            """ä»ŽJSONå­—ç¬¦ä¸²åˆ›å»ºå¯¹è±¡"""
            data = json.loads(json_str)
            return cls.from_dict(data)
        
        def get_portfolio_value(self) -> float:
            """èŽ·å–æŠ•èµ„ç»„åˆä»·å€¼ï¼ˆä¸åŒ…æ‹¬çŽ°é‡‘ï¼‰"""
            return self.total_value - self.cash
        
        def get_leverage(self) -> float:
            """èŽ·å–æ æ†çŽ‡"""
            portfolio_value = self.get_portfolio_value()
            if portfolio_value == 0:
                return 0.0
            return self.total_value / portfolio_value
    
    
    @dataclass
    class TradingAction:
        """äº¤æ˜“åŠ¨ä½œç»“æž„"""
        target_weights: np.ndarray  # [n_stocks]
        confidence: float
        timestamp: datetime
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            # éªŒè¯æƒé‡å’Œ
            if abs(self.target_weights.sum() - 1.0) > 1e-6:
                raise ValueError("ç›®æ ‡æƒé‡å’Œå¿…é¡»æŽ¥è¿‘1")
            
            # éªŒè¯æƒé‡éžè´Ÿ
            if (self.target_weights < 0).any():
                raise ValueError("ç›®æ ‡æƒé‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯ç½®ä¿¡åº¦èŒƒå›´
            if not (0 <= self.confidence <= 1):
                raise ValueError("ç½®ä¿¡åº¦å¿…é¡»åœ¨0åˆ°1ä¹‹é—´")
            
            # éªŒè¯æ•°ç»„ä¸åŒ…å«NaNæˆ–æ— ç©·å¤§
            if np.isnan(self.target_weights).any():
                raise ValueError("ç›®æ ‡æƒé‡ä¸èƒ½åŒ…å«NaN")
            
            if np.isinf(self.target_weights).any():
                raise ValueError("ç›®æ ‡æƒé‡ä¸èƒ½åŒ…å«æ— ç©·å¤§")
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'target_weights': self.target_weights.tolist(),
                'confidence': self.confidence,
                'timestamp': self.timestamp.isoformat()
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'TradingAction':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            return cls(
                target_weights=np.array(data['target_weights']),
                confidence=data['confidence'],
                timestamp=datetime.fromisoformat(data['timestamp'])
            )
        
        def to_json(self) -> str:
            """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
            return json.dumps(self.to_dict())
        
        @classmethod
        def from_json(cls, json_str: str) -> 'TradingAction':
            """ä»ŽJSONå­—ç¬¦ä¸²åˆ›å»ºå¯¹è±¡"""
            data = json.loads(json_str)
            return cls.from_dict(data)
        
        def get_concentration(self) -> float:
            """èŽ·å–æƒé‡é›†ä¸­åº¦ï¼ˆHerfindahlæŒ‡æ•°ï¼‰"""
            return np.sum(self.target_weights ** 2)
        
        def get_active_positions(self, threshold: float = 1e-6) -> int:
            """èŽ·å–æ´»è·ƒæŒä»“æ•°é‡"""
            return np.sum(self.target_weights > threshold)
    
    
    @dataclass
    class TransactionRecord:
        """äº¤æ˜“è®°å½•ç»“æž„"""
        timestamp: datetime
        symbol: str
        action_type: str  # 'buy' or 'sell'
        quantity: int
        price: float
        commission: float
        stamp_tax: float
        slippage: float
        total_cost: float
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            # éªŒè¯äº¤æ˜“ç±»åž‹
            if self.action_type not in ['buy', 'sell']:
                raise ValueError("äº¤æ˜“ç±»åž‹å¿…é¡»æ˜¯'buy'æˆ–'sell'")
            
            # éªŒè¯æ•°é‡ä¸ºæ­£
            if self.quantity < 0:
                raise ValueError("äº¤æ˜“æ•°é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯ä»·æ ¼ä¸ºæ­£
            if self.price < 0:
                raise ValueError("ä»·æ ¼ä¸èƒ½ä¸ºè´Ÿæ•°")
            
            # éªŒè¯æˆæœ¬é¡¹éžè´Ÿ
            if any(cost < 0 for cost in [self.commission, self.stamp_tax, 
                                        self.slippage, self.total_cost]):
                raise ValueError("æˆæœ¬é¡¹ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'timestamp': self.timestamp.isoformat(),
                'symbol': self.symbol,
                'action_type': self.action_type,
                'quantity': self.quantity,
                'price': self.price,
                'commission': self.commission,
                'stamp_tax': self.stamp_tax,
                'slippage': self.slippage,
                'total_cost': self.total_cost
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'TransactionRecord':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            data_copy = data.copy()
            data_copy['timestamp'] = datetime.fromisoformat(data_copy['timestamp'])
            return cls(**data_copy)
        
        def to_json(self) -> str:
            """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
            return json.dumps(self.to_dict())
        
        @classmethod
        def from_json(cls, json_str: str) -> 'TransactionRecord':
            """ä»ŽJSONå­—ç¬¦ä¸²åˆ›å»ºå¯¹è±¡"""
            data = json.loads(json_str)
            return cls.from_dict(data)
        
        def get_transaction_value(self) -> float:
            """èŽ·å–äº¤æ˜“ä»·å€¼"""
            return self.quantity * self.price
        
        def get_cost_ratio(self) -> float:
            """èŽ·å–æˆæœ¬æ¯”çŽ‡"""
            transaction_value = self.get_transaction_value()
            if transaction_value == 0:
                return 0.0
            return self.total_cost / transaction_value
    
    
    # å·¥å…·å‡½æ•°
    def validate_array_shape(array: np.ndarray, expected_shape: tuple, name: str):
        """éªŒè¯æ•°ç»„å½¢çŠ¶"""
        if array.shape != expected_shape:
            raise ValueError(f"{name}çš„å½¢çŠ¶åº”ä¸º{expected_shape}ï¼Œå®žé™…ä¸º{array.shape}")
    
    
    def validate_weights(weights: np.ndarray, name: str = "æƒé‡"):
        """éªŒè¯æƒé‡æ•°ç»„"""
        if (weights < 0).any():
            raise ValueError(f"{name}ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        if abs(weights.sum() - 1.0) > 1e-6:
            raise ValueError(f"{name}å’Œå¿…é¡»æŽ¥è¿‘1")
        
        if np.isnan(weights).any():
            raise ValueError(f"{name}ä¸èƒ½åŒ…å«NaN")
        
        if np.isinf(weights).any():
            raise ValueError(f"{name}ä¸èƒ½åŒ…å«æ— ç©·å¤§")
    
    
    def validate_price_data(prices: Dict[str, float]):
        """éªŒè¯ä»·æ ¼æ•°æ®"""
        required_fields = ['open', 'high', 'low', 'close']
        
        for field in required_fields:
            if field not in prices:
                raise ValueError(f"ç¼ºå°‘å¿…è¦çš„ä»·æ ¼å­—æ®µ: {field}")
            
            if prices[field] < 0:
                raise ValueError(f"ä»·æ ¼å­—æ®µ{field}ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        if prices['high'] < prices['low']:
            raise ValueError("æœ€é«˜ä»·ä¸èƒ½ä½ŽäºŽæœ€ä½Žä»·")
    
    
    def safe_divide(numerator: float, denominator: float, default: float = 0.0) -> float:
        """å®‰å…¨é™¤æ³•ï¼Œé¿å…é™¤é›¶é”™è¯¯"""
        if abs(denominator) < 1e-8:
            return default
        return numerator / denominator
    
    
    def normalize_weights(weights: np.ndarray) -> np.ndarray:
        """æ ‡å‡†åŒ–æƒé‡ï¼Œç¡®ä¿å’Œä¸º1"""
        total = weights.sum()
        if total == 0:
            return np.ones_like(weights) / len(weights)
        return weights / total
    
    
    def calculate_portfolio_metrics(weights: np.ndarray, returns: np.ndarray) -> Dict[str, float]:
        """è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡"""
        portfolio_return = np.dot(weights, returns)
        concentration = np.sum(weights ** 2)  # HerfindahlæŒ‡æ•°
        active_positions = np.sum(weights > 1e-6)
        
        return {
            'portfolio_return': portfolio_return,
            'concentration': concentration,
            'active_positions': active_positions,
            'max_weight': weights.max(),
            'min_weight': weights.min()
        }
    ]]></file>
  <file path="src/rl_trading_system/data/data_cache.py"><![CDATA[
    """
    æ•°æ®ç¼“å­˜ç®¡ç†æ¨¡å—
    å®žçŽ°æ•°æ®ç¼“å­˜æœºåˆ¶ï¼Œæé«˜æ•°æ®èŽ·å–æ•ˆçŽ‡
    """
    
    import os
    import pickle
    import hashlib
    import logging
    from typing import Any, Optional, Dict
    from datetime import datetime, timedelta
    import pandas as pd
    
    logger = logging.getLogger(__name__)
    
    
    class DataCache:
        """æ•°æ®ç¼“å­˜ç®¡ç†å™¨"""
        
        def __init__(self, cache_dir: str = "cache", 
                     default_ttl: int = 3600):
            """
            åˆå§‹åŒ–æ•°æ®ç¼“å­˜
            
            Args:
                cache_dir: ç¼“å­˜ç›®å½•
                default_ttl: é»˜è®¤ç¼“å­˜æ—¶é—´ï¼ˆç§’ï¼‰
            """
            self.cache_dir = cache_dir
            self.default_ttl = default_ttl
            
            # åˆ›å»ºç¼“å­˜ç›®å½•
            os.makedirs(cache_dir, exist_ok=True)
            
            # å†…å­˜ç¼“å­˜
            self._memory_cache: Dict[str, Dict[str, Any]] = {}
        
        def _get_cache_key(self, key: str) -> str:
            """ç”Ÿæˆç¼“å­˜é”®çš„å“ˆå¸Œå€¼"""
            return hashlib.md5(key.encode()).hexdigest()
        
        def _get_cache_path(self, cache_key: str) -> str:
            """èŽ·å–ç¼“å­˜æ–‡ä»¶è·¯å¾„"""
            return os.path.join(self.cache_dir, f"{cache_key}.pkl")
        
        def _is_expired(self, timestamp: datetime, ttl: int) -> bool:
            """æ£€æŸ¥ç¼“å­˜æ˜¯å¦è¿‡æœŸ"""
            return datetime.now() - timestamp > timedelta(seconds=ttl)
        
        def get(self, key: str, ttl: Optional[int] = None) -> Optional[Any]:
            """
            èŽ·å–ç¼“å­˜æ•°æ®
            
            Args:
                key: ç¼“å­˜é”®
                ttl: ç¼“å­˜æ—¶é—´ï¼ŒNoneä½¿ç”¨é»˜è®¤å€¼
                
            Returns:
                ç¼“å­˜çš„æ•°æ®ï¼Œå¦‚æžœä¸å­˜åœ¨æˆ–è¿‡æœŸè¿”å›žNone
            """
            if ttl is None:
                ttl = self.default_ttl
            
            cache_key = self._get_cache_key(key)
            
            # å…ˆæ£€æŸ¥å†…å­˜ç¼“å­˜
            if cache_key in self._memory_cache:
                cache_item = self._memory_cache[cache_key]
                if not self._is_expired(cache_item['timestamp'], ttl):
                    logger.debug(f"å†…å­˜ç¼“å­˜å‘½ä¸­: {key}")
                    return cache_item['data']
                else:
                    # è¿‡æœŸï¼Œåˆ é™¤å†…å­˜ç¼“å­˜
                    del self._memory_cache[cache_key]
            
            # æ£€æŸ¥æ–‡ä»¶ç¼“å­˜
            cache_path = self._get_cache_path(cache_key)
            if os.path.exists(cache_path):
                try:
                    with open(cache_path, 'rb') as f:
                        cache_item = pickle.load(f)
                    
                    if not self._is_expired(cache_item['timestamp'], ttl):
                        # åŠ è½½åˆ°å†…å­˜ç¼“å­˜
                        self._memory_cache[cache_key] = cache_item
                        logger.debug(f"æ–‡ä»¶ç¼“å­˜å‘½ä¸­: {key}")
                        return cache_item['data']
                    else:
                        # è¿‡æœŸï¼Œåˆ é™¤æ–‡ä»¶
                        os.remove(cache_path)
                        logger.debug(f"ç¼“å­˜è¿‡æœŸï¼Œå·²åˆ é™¤: {key}")
                        
                except Exception as e:
                    logger.error(f"è¯»å–ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
                    # åˆ é™¤æŸåçš„ç¼“å­˜æ–‡ä»¶
                    try:
                        os.remove(cache_path)
                    except:
                        pass
            
            return None
        
        def set(self, key: str, data: Any, ttl: Optional[int] = None):
            """
            è®¾ç½®ç¼“å­˜æ•°æ®
            
            Args:
                key: ç¼“å­˜é”®
                data: è¦ç¼“å­˜çš„æ•°æ®
                ttl: ç¼“å­˜æ—¶é—´ï¼ŒNoneä½¿ç”¨é»˜è®¤å€¼
            """
            if ttl is None:
                ttl = self.default_ttl
            
            cache_key = self._get_cache_key(key)
            cache_item = {
                'data': data,
                'timestamp': datetime.now(),
                'ttl': ttl
            }
            
            # è®¾ç½®å†…å­˜ç¼“å­˜
            self._memory_cache[cache_key] = cache_item
            
            # è®¾ç½®æ–‡ä»¶ç¼“å­˜
            cache_path = self._get_cache_path(cache_key)
            try:
                with open(cache_path, 'wb') as f:
                    pickle.dump(cache_item, f)
                logger.debug(f"æ•°æ®å·²ç¼“å­˜: {key}")
            except Exception as e:
                logger.error(f"å†™å…¥ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        
        def delete(self, key: str):
            """
            åˆ é™¤ç¼“å­˜æ•°æ®
            
            Args:
                key: ç¼“å­˜é”®
            """
            cache_key = self._get_cache_key(key)
            
            # åˆ é™¤å†…å­˜ç¼“å­˜
            if cache_key in self._memory_cache:
                del self._memory_cache[cache_key]
            
            # åˆ é™¤æ–‡ä»¶ç¼“å­˜
            cache_path = self._get_cache_path(cache_key)
            if os.path.exists(cache_path):
                try:
                    os.remove(cache_path)
                    logger.debug(f"ç¼“å­˜å·²åˆ é™¤: {key}")
                except Exception as e:
                    logger.error(f"åˆ é™¤ç¼“å­˜æ–‡ä»¶å¤±è´¥: {e}")
        
        def clear(self):
            """æ¸…ç©ºæ‰€æœ‰ç¼“å­˜"""
            # æ¸…ç©ºå†…å­˜ç¼“å­˜
            self._memory_cache.clear()
            
            # æ¸…ç©ºæ–‡ä»¶ç¼“å­˜
            try:
                for filename in os.listdir(self.cache_dir):
                    if filename.endswith('.pkl'):
                        file_path = os.path.join(self.cache_dir, filename)
                        os.remove(file_path)
                logger.info("æ‰€æœ‰ç¼“å­˜å·²æ¸…ç©º")
            except Exception as e:
                logger.error(f"æ¸…ç©ºç¼“å­˜å¤±è´¥: {e}")
        
        def get_cache_info(self) -> Dict[str, Any]:
            """
            èŽ·å–ç¼“å­˜ä¿¡æ¯
            
            Returns:
                ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯
            """
            memory_count = len(self._memory_cache)
            
            file_count = 0
            total_size = 0
            try:
                for filename in os.listdir(self.cache_dir):
                    if filename.endswith('.pkl'):
                        file_count += 1
                        file_path = os.path.join(self.cache_dir, filename)
                        total_size += os.path.getsize(file_path)
            except Exception as e:
                logger.error(f"èŽ·å–ç¼“å­˜ä¿¡æ¯å¤±è´¥: {e}")
            
            return {
                'memory_cache_count': memory_count,
                'file_cache_count': file_count,
                'total_size_bytes': total_size,
                'total_size_mb': total_size / (1024 * 1024),
                'cache_dir': self.cache_dir
            }
        
        def cleanup_expired(self):
            """æ¸…ç†è¿‡æœŸçš„ç¼“å­˜"""
            # æ¸…ç†å†…å­˜ç¼“å­˜
            expired_keys = []
            for cache_key, cache_item in self._memory_cache.items():
                if self._is_expired(cache_item['timestamp'], cache_item['ttl']):
                    expired_keys.append(cache_key)
            
            for key in expired_keys:
                del self._memory_cache[key]
            
            # æ¸…ç†æ–‡ä»¶ç¼“å­˜
            try:
                for filename in os.listdir(self.cache_dir):
                    if filename.endswith('.pkl'):
                        file_path = os.path.join(self.cache_dir, filename)
                        try:
                            with open(file_path, 'rb') as f:
                                cache_item = pickle.load(f)
                            
                            if self._is_expired(cache_item['timestamp'], cache_item['ttl']):
                                os.remove(file_path)
                        except:
                            # å¦‚æžœæ–‡ä»¶æŸåï¼Œç›´æŽ¥åˆ é™¤
                            os.remove(file_path)
                
                logger.info(f"æ¸…ç†äº†{len(expired_keys)}ä¸ªè¿‡æœŸç¼“å­˜")
            except Exception as e:
                logger.error(f"æ¸…ç†è¿‡æœŸç¼“å­˜å¤±è´¥: {e}")
    
    
    # å…¨å±€ç¼“å­˜å®žä¾‹
    _global_cache = None
    
    
    def get_global_cache() -> DataCache:
        """èŽ·å–å…¨å±€ç¼“å­˜å®žä¾‹"""
        global _global_cache
        if _global_cache is None:
            _global_cache = DataCache()
        return _global_cache
    
    
    def set_global_cache(cache: DataCache):
        """è®¾ç½®å…¨å±€ç¼“å­˜å®žä¾‹"""
        global _global_cache
        _global_cache = cache
    ]]></file>
  <file path="src/rl_trading_system/data/akshare_interface.py"><![CDATA[
    """
    Akshareæ•°æ®æŽ¥å£å®žçŽ°
    """
    
    from typing import List, Optional
    import pandas as pd
    import logging
    import time
    from .interfaces import DataInterface
    from .data_cache import get_global_cache
    from .data_quality import get_global_quality_checker
    
    logger = logging.getLogger(__name__)
    
    
    class AkshareDataInterface(DataInterface):
        """Akshareæ•°æ®æŽ¥å£å®žçŽ°"""
        
        def __init__(self, rate_limit: float = 0.1):
            """
            åˆå§‹åŒ–Akshareæ•°æ®æŽ¥å£
            
            Args:
                rate_limit: APIè°ƒç”¨é—´éš”ï¼ˆç§’ï¼‰ï¼Œç”¨äºŽé¿å…é¢‘çŽ‡é™åˆ¶
            """
            super().__init__()
            self.rate_limit = rate_limit
            self._last_call_time = 0
        
        def _rate_limit_wait(self):
            """ç­‰å¾…ä»¥é¿å…APIé¢‘çŽ‡é™åˆ¶"""
            current_time = time.time()
            time_since_last_call = current_time - self._last_call_time
            
            if time_since_last_call < self.rate_limit:
                wait_time = self.rate_limit - time_since_last_call
                time.sleep(wait_time)
            
            self._last_call_time = time.time()
        
        def get_stock_list(self, market: str = 'A') -> List[str]:
            """
            èŽ·å–è‚¡ç¥¨åˆ—è¡¨
            
            Args:
                market: å¸‚åœºä»£ç ï¼Œ'A'è¡¨ç¤ºAè‚¡
                
            Returns:
                è‚¡ç¥¨ä»£ç åˆ—è¡¨
            """
            cache_key = self._get_cache_key('get_stock_list', market=market)
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                return cached_result.tolist() if hasattr(cached_result, 'tolist') else cached_result
            
            try:
                import akshare as ak
                
                self._rate_limit_wait()
                
                if market == 'A':
                    # èŽ·å–Aè‚¡è‚¡ç¥¨åˆ—è¡¨
                    stock_info = ak.stock_info_a_code_name()
                    
                    # è½¬æ¢ä¸ºæ ‡å‡†æ ¼å¼ï¼ˆæ·»åŠ äº¤æ˜“æ‰€åŽç¼€ï¼‰
                    stock_list = []
                    for _, row in stock_info.iterrows():
                        code = row['code']
                        # æ ¹æ®ä»£ç åˆ¤æ–­äº¤æ˜“æ‰€
                        if code.startswith('6'):
                            stock_list.append(f"{code}.SH")
                        else:
                            stock_list.append(f"{code}.SZ")
                    
                    # ç¼“å­˜ç»“æžœ
                    self._set_cache(cache_key, pd.Series(stock_list))
                    
                    logger.info(f"æˆåŠŸèŽ·å–{len(stock_list)}åªAè‚¡è‚¡ç¥¨")
                    return stock_list
                
                else:
                    logger.warning(f"æš‚ä¸æ”¯æŒå¸‚åœº: {market}")
                    return []
                    
            except ImportError:
                logger.error("Akshareæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…akshare: pip install akshare")
                raise ImportError("Akshareæœªå®‰è£…")
            except Exception as e:
                logger.error(f"èŽ·å–è‚¡ç¥¨åˆ—è¡¨å¤±è´¥: {e}")
                raise
        
        def get_price_data(self, symbols: List[str], 
                          start_date: str, end_date: str) -> pd.DataFrame:
            """
            èŽ·å–ä»·æ ¼æ•°æ®
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                start_date: å¼€å§‹æ—¥æœŸ
                end_date: ç»“æŸæ—¥æœŸ
                
            Returns:
                ä»·æ ¼æ•°æ®DataFrame
            """
            # å‚æ•°éªŒè¯
            if not self.validate_symbols(symbols):
                raise ValueError("è‚¡ç¥¨ä»£ç åˆ—è¡¨æ— æ•ˆ")
            
            if not self.validate_date_range(start_date, end_date):
                raise ValueError("æ—¥æœŸèŒƒå›´æ— æ•ˆ")
            
            cache_key = self._get_cache_key('get_price_data',
                                           symbols=tuple(symbols),
                                           start_date=start_date,
                                           end_date=end_date)
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                return cached_result
            
            try:
                import akshare as ak
                
                all_data = []
                
                for symbol in symbols:
                    self._rate_limit_wait()
                    
                    # è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼ï¼ˆåŽ»æŽ‰äº¤æ˜“æ‰€åŽç¼€ï¼‰
                    clean_symbol = symbol.split('.')[0]
                    
                    try:
                        # èŽ·å–åŽ†å²æ•°æ®
                        data = ak.stock_zh_a_hist(
                            symbol=clean_symbol,
                            period="daily",
                            start_date=start_date.replace('-', ''),
                            end_date=end_date.replace('-', ''),
                            adjust=""
                        )
                        
                        if data.empty:
                            logger.warning(f"è‚¡ç¥¨{symbol}æ— æ•°æ®")
                            continue
                        
                        # æ ‡å‡†åŒ–åˆ—å
                        column_mapping = {
                            'æ—¥æœŸ': 'datetime',
                            'å¼€ç›˜': 'open',
                            'æœ€é«˜': 'high',
                            'æœ€ä½Ž': 'low',
                            'æ”¶ç›˜': 'close',
                            'æˆäº¤é‡': 'volume',
                            'æˆäº¤é¢': 'amount'
                        }
                        
                        # é‡å‘½ååˆ—
                        data = data.rename(columns=column_mapping)
                        
                        # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
                        data['instrument'] = symbol
                        
                        # è½¬æ¢æ—¥æœŸæ ¼å¼
                        data['datetime'] = pd.to_datetime(data['datetime'])
                        
                        # è®¾ç½®ç´¢å¼•
                        data = data.set_index(['datetime', 'instrument'])
                        
                        all_data.append(data)
                        
                    except Exception as e:
                        logger.error(f"èŽ·å–è‚¡ç¥¨{symbol}æ•°æ®å¤±è´¥: {e}")
                        continue
                
                if not all_data:
                    logger.warning("æœªèŽ·å–åˆ°ä»»ä½•æ•°æ®")
                    return pd.DataFrame()
                
                # åˆå¹¶æ‰€æœ‰æ•°æ®
                combined_data = pd.concat(all_data)
                
                # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
                combined_data = self.standardize_dataframe(combined_data, 'price')
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                quality_report = self.check_data_quality(combined_data, 'price')
                if quality_report['status'] == 'warning':
                    logger.warning(f"æ•°æ®è´¨é‡é—®é¢˜: {quality_report['issues']}")
                
                # ç¼“å­˜ç»“æžœ
                self._set_cache(cache_key, combined_data)
                
                logger.info(f"æˆåŠŸèŽ·å–ä»·æ ¼æ•°æ®: {len(combined_data)}æ¡è®°å½•")
                return combined_data
                
            except ImportError:
                logger.error("Akshareæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…akshare: pip install akshare")
                raise ImportError("Akshareæœªå®‰è£…")
            except Exception as e:
                logger.error(f"èŽ·å–ä»·æ ¼æ•°æ®å¤±è´¥: {e}")
                raise
        
        def get_fundamental_data(self, symbols: List[str], 
                               start_date: str, end_date: str) -> pd.DataFrame:
            """
            èŽ·å–åŸºæœ¬é¢æ•°æ®
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                start_date: å¼€å§‹æ—¥æœŸ
                end_date: ç»“æŸæ—¥æœŸ
                
            Returns:
                åŸºæœ¬é¢æ•°æ®DataFrame
            """
            # å‚æ•°éªŒè¯
            if not self.validate_symbols(symbols):
                raise ValueError("è‚¡ç¥¨ä»£ç åˆ—è¡¨æ— æ•ˆ")
            
            if not self.validate_date_range(start_date, end_date):
                raise ValueError("æ—¥æœŸèŒƒå›´æ— æ•ˆ")
            
            cache_key = self._get_cache_key('get_fundamental_data',
                                           symbols=tuple(symbols),
                                           start_date=start_date,
                                           end_date=end_date)
            cached_result = self._get_from_cache(cache_key)
            if cached_result is not None:
                return cached_result
            
            try:
                import akshare as ak
                
                all_data = []
                
                for symbol in symbols:
                    self._rate_limit_wait()
                    
                    # è½¬æ¢è‚¡ç¥¨ä»£ç æ ¼å¼
                    clean_symbol = symbol.split('.')[0]
                    
                    try:
                        # èŽ·å–åŸºæœ¬é¢æ•°æ®ï¼ˆè´¢åŠ¡æŒ‡æ ‡ï¼‰
                        data = ak.stock_financial_analysis_indicator(symbol=clean_symbol)
                        
                        if data.empty:
                            logger.warning(f"è‚¡ç¥¨{symbol}æ— åŸºæœ¬é¢æ•°æ®")
                            continue
                        
                        # æ·»åŠ è‚¡ç¥¨ä»£ç åˆ—
                        data['instrument'] = symbol
                        
                        # è½¬æ¢æ—¥æœŸæ ¼å¼
                        if 'æ—¥æœŸ' in data.columns:
                            data['datetime'] = pd.to_datetime(data['æ—¥æœŸ'])
                            data = data.set_index(['datetime', 'instrument'])
                        
                        all_data.append(data)
                        
                    except Exception as e:
                        logger.error(f"èŽ·å–è‚¡ç¥¨{symbol}åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
                        continue
                
                if not all_data:
                    logger.warning("æœªèŽ·å–åˆ°ä»»ä½•åŸºæœ¬é¢æ•°æ®")
                    return pd.DataFrame()
                
                # åˆå¹¶æ‰€æœ‰æ•°æ®
                combined_data = pd.concat(all_data)
                
                # æ ‡å‡†åŒ–æ•°æ®æ ¼å¼
                combined_data = self.standardize_dataframe(combined_data, 'fundamental')
                
                # æ•°æ®è´¨é‡æ£€æŸ¥
                quality_report = self.check_data_quality(combined_data, 'fundamental')
                if quality_report['status'] == 'warning':
                    logger.warning(f"åŸºæœ¬é¢æ•°æ®è´¨é‡é—®é¢˜: {quality_report['issues']}")
                
                # ç¼“å­˜ç»“æžœ
                self._set_cache(cache_key, combined_data)
                
                logger.info(f"æˆåŠŸèŽ·å–åŸºæœ¬é¢æ•°æ®: {len(combined_data)}æ¡è®°å½•")
                return combined_data
                
            except ImportError:
                logger.error("Akshareæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…akshare: pip install akshare")
                raise ImportError("Akshareæœªå®‰è£…")
            except Exception as e:
                logger.error(f"èŽ·å–åŸºæœ¬é¢æ•°æ®å¤±è´¥: {e}")
                raise
        
        def get_realtime_data(self, symbols: List[str]) -> pd.DataFrame:
            """
            èŽ·å–å®žæ—¶æ•°æ®
            
            Args:
                symbols: è‚¡ç¥¨ä»£ç åˆ—è¡¨
                
            Returns:
                å®žæ—¶æ•°æ®DataFrame
            """
            try:
                import akshare as ak
                
                all_data = []
                
                for symbol in symbols:
                    self._rate_limit_wait()
                    
                    clean_symbol = symbol.split('.')[0]
                    
                    try:
                        # èŽ·å–å®žæ—¶æ•°æ®
                        data = ak.stock_zh_a_spot_em()
                        
                        # ç­›é€‰æŒ‡å®šè‚¡ç¥¨
                        stock_data = data[data['ä»£ç '] == clean_symbol]
                        
                        if stock_data.empty:
                            continue
                        
                        # æ ‡å‡†åŒ–æ ¼å¼
                        stock_data['instrument'] = symbol
                        stock_data['datetime'] = pd.Timestamp.now()
                        
                        all_data.append(stock_data)
                        
                    except Exception as e:
                        logger.error(f"èŽ·å–è‚¡ç¥¨{symbol}å®žæ—¶æ•°æ®å¤±è´¥: {e}")
                        continue
                
                if not all_data:
                    return pd.DataFrame()
                
                combined_data = pd.concat(all_data)
                logger.info(f"æˆåŠŸèŽ·å–å®žæ—¶æ•°æ®: {len(combined_data)}æ¡è®°å½•")
                return combined_data
                
            except ImportError:
                logger.error("Akshareæœªå®‰è£…ï¼Œè¯·å…ˆå®‰è£…akshare: pip install akshare")
                raise ImportError("Akshareæœªå®‰è£…")
            except Exception as e:
                logger.error(f"èŽ·å–å®žæ—¶æ•°æ®å¤±è´¥: {e}")
                raise
        
        def get_index_data(self, index_code: str, 
                          start_date: str, end_date: str) -> pd.DataFrame:
            """
            èŽ·å–æŒ‡æ•°æ•°æ®
            
            Args:
                index_code: æŒ‡æ•°ä»£ç 
                start_date: å¼€å§‹æ—¥æœŸ
                end_date: ç»“æŸæ—¥æœŸ
                
            Returns:
                æŒ‡æ•°æ•°æ®DataFrame
            """
            try:
                import akshare as ak
                
                self._rate_limit_wait()
                
                # èŽ·å–æŒ‡æ•°æ•°æ®
                data = ak.stock_zh_index_daily(
                    symbol=index_code,
                    start_date=start_date.replace('-', ''),
                    end_date=end_date.replace('-', '')
                )
                
                if data.empty:
                    return pd.DataFrame()
                
                # æ ‡å‡†åŒ–æ ¼å¼
                data['datetime'] = pd.to_datetime(data['date'])
                data['instrument'] = index_code
                data = data.set_index(['datetime', 'instrument'])
                
                logger.info(f"æˆåŠŸèŽ·å–æŒ‡æ•°{index_code}æ•°æ®: {len(data)}æ¡è®°å½•")
                return data
                
            except Exception as e:
                logger.error(f"èŽ·å–æŒ‡æ•°æ•°æ®å¤±è´¥: {e}")
                raise
    ]]></file>
  <file path="src/rl_trading_system/data/__init__.py"><![CDATA[
    """æ•°æ®å¤„ç†æ¨¡å—"""
    
    from .interfaces import DataInterface
    from .qlib_interface import QlibDataInterface
    from .akshare_interface import AkshareDataInterface
    from .feature_engineer import FeatureEngineer
    from .data_processor import DataProcessor
    from .data_models import MarketData, FeatureVector, TradingState, TradingAction
    
    __all__ = [
        "DataInterface",
        "QlibDataInterface", 
        "AkshareDataInterface",
        "FeatureEngineer",
        "DataProcessor",
        "MarketData",
        "FeatureVector", 
        "TradingState",
        "TradingAction"
    ]
    ]]></file>
  <file path="src/rl_trading_system/config/schemas.py"><![CDATA[
    """
    é…ç½®éªŒè¯æ¨¡å¼å®šä¹‰
    
    å®šä¹‰å„ç§é…ç½®æ–‡ä»¶çš„éªŒè¯æ¨¡å¼ï¼ŒåŒ…æ‹¬ç±»åž‹æ£€æŸ¥ã€èŒƒå›´çº¦æŸå’Œé»˜è®¤å€¼
    éœ€æ±‚: 10.1
    """
    
    from typing import Dict, Any
    
    
    def validate_stock_code(field: str, value: str, error_callback) -> None:
        """éªŒè¯è‚¡ç¥¨ä»£ç æ ¼å¼"""
        if not isinstance(value, str):
            error_callback(field, "è‚¡ç¥¨ä»£ç å¿…é¡»æ˜¯å­—ç¬¦ä¸²")
            return
        
        if len(value) != 9:
            error_callback(field, "è‚¡ç¥¨ä»£ç é•¿åº¦å¿…é¡»æ˜¯9ä½")
            return
        
        if not (value.endswith('.SZ') or value.endswith('.SH')):
            error_callback(field, "è‚¡ç¥¨ä»£ç å¿…é¡»ä»¥.SZæˆ–.SHç»“å°¾")
    
    
    def validate_positive_float(field: str, value: float, error_callback) -> None:
        """éªŒè¯æ­£æµ®ç‚¹æ•°"""
        if not isinstance(value, (int, float)):
            error_callback(field, "å¿…é¡»æ˜¯æ•°å€¼ç±»åž‹")
            return
        
        if value <= 0:
            error_callback(field, "å¿…é¡»æ˜¯æ­£æ•°")
    
    
    def validate_probability(field: str, value: float, error_callback) -> None:
        """éªŒè¯æ¦‚çŽ‡å€¼ï¼ˆ0-1ä¹‹é—´ï¼‰"""
        if not isinstance(value, (int, float)):
            error_callback(field, "å¿…é¡»æ˜¯æ•°å€¼ç±»åž‹")
            return
        
        if not (0 <= value <= 1):
            error_callback(field, "æ¦‚çŽ‡å€¼å¿…é¡»åœ¨0-1ä¹‹é—´")
    
    
    # æ¨¡åž‹é…ç½®éªŒè¯æ¨¡å¼
    MODEL_CONFIG_SCHEMA: Dict[str, Any] = {
        'model': {
            'required': True,
            'type': dict,
            'schema': {
                'transformer': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'd_model': {
                            'required': True,
                            'type': int,
                            'min': 64,
                            'max': 2048,
                            'default': 256
                        },
                        'n_heads': {
                            'required': True,
                            'type': int,
                            'min': 1,
                            'max': 32,
                            'default': 8
                        },
                        'n_layers': {
                            'required': True,
                            'type': int,
                            'min': 1,
                            'max': 24,
                            'default': 6
                        },
                        'd_ff': {
                            'required': True,
                            'type': int,
                            'min': 128,
                            'max': 8192,
                            'default': 1024
                        },
                        'dropout': {
                            'required': True,
                            'type': float,
                            'min': 0.0,
                            'max': 0.9,
                            'default': 0.1,
                            'validator': validate_probability
                        },
                        'max_seq_len': {
                            'required': True,
                            'type': int,
                            'min': 10,
                            'max': 1000,
                            'default': 252
                        },
                        'n_features': {
                            'required': True,
                            'type': int,
                            'min': 1,
                            'max': 200,
                            'default': 50
                        }
                    }
                },
                'sac': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'state_dim': {
                            'required': True,
                            'type': int,
                            'min': 64,
                            'max': 2048,
                            'default': 256
                        },
                        'action_dim': {
                            'required': True,
                            'type': int,
                            'min': 1,
                            'max': 1000,
                            'default': 100
                        },
                        'hidden_dim': {
                            'required': True,
                            'type': int,
                            'min': 128,
                            'max': 2048,
                            'default': 512
                        },
                        'lr_actor': {
                            'required': True,
                            'type': float,
                            'min': 1e-6,
                            'max': 1e-1,
                            'default': 3e-4,
                            'validator': validate_positive_float
                        },
                        'lr_critic': {
                            'required': True,
                            'type': float,
                            'min': 1e-6,
                            'max': 1e-1,
                            'default': 3e-4,
                            'validator': validate_positive_float
                        },
                        'lr_alpha': {
                            'required': True,
                            'type': float,
                            'min': 1e-6,
                            'max': 1e-1,
                            'default': 3e-4,
                            'validator': validate_positive_float
                        },
                        'gamma': {
                            'required': True,
                            'type': float,
                            'min': 0.9,
                            'max': 0.999,
                            'default': 0.99,
                            'validator': validate_probability
                        },
                        'tau': {
                            'required': True,
                            'type': float,
                            'min': 0.001,
                            'max': 0.1,
                            'default': 0.005,
                            'validator': validate_positive_float
                        },
                        'alpha': {
                            'required': True,
                            'type': float,
                            'min': 0.01,
                            'max': 1.0,
                            'default': 0.2,
                            'validator': validate_positive_float
                        },
                        'target_entropy': {
                            'required': True,
                            'type': float,
                            'max': 0,
                            'default': -100
                        },
                        'buffer_size': {
                            'required': True,
                            'type': int,
                            'min': 10000,
                            'max': 10000000,
                            'default': 1000000
                        },
                        'batch_size': {
                            'required': True,
                            'type': int,
                            'min': 16,
                            'max': 1024,
                            'default': 256
                        }
                    }
                }
            }
        },
        'training': {
            'required': True,
            'type': dict,
            'schema': {
                'n_episodes': {
                    'required': True,
                    'type': int,
                    'min': 100,
                    'max': 100000,
                    'default': 10000
                },
                'eval_freq': {
                    'required': True,
                    'type': int,
                    'min': 10,
                    'max': 1000,
                    'default': 100
                },
                'patience': {
                    'required': True,
                    'type': int,
                    'min': 10,
                    'max': 500,
                    'default': 50
                },
                'min_delta': {
                    'required': True,
                    'type': float,
                    'min': 1e-6,
                    'max': 0.1,
                    'default': 0.001,
                    'validator': validate_positive_float
                },
                'checkpoint_dir': {
                    'required': True,
                    'type': str,
                    'default': "./checkpoints"
                }
            }
        }
    }
    
    
    # äº¤æ˜“é…ç½®éªŒè¯æ¨¡å¼
    TRADING_CONFIG_SCHEMA: Dict[str, Any] = {
        'trading': {
            'required': True,
            'type': dict,
            'schema': {
                'environment': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'stock_pool': {
                            'required': True,
                            'type': list,
                            'default': []
                        },
                        'lookback_window': {
                            'required': True,
                            'type': int,
                            'min': 5,
                            'max': 500,
                            'default': 60
                        },
                        'initial_cash': {
                            'required': True,
                            'type': float,
                            'min': 10000,
                            'max': 100000000,
                            'default': 1000000.0,
                            'validator': validate_positive_float
                        },
                        'commission_rate': {
                            'required': True,
                            'type': float,
                            'min': 0.0,
                            'max': 0.01,
                            'default': 0.001,
                            'validator': validate_probability
                        },
                        'stamp_tax_rate': {
                            'required': True,
                            'type': float,
                            'min': 0.0,
                            'max': 0.01,
                            'default': 0.001,
                            'validator': validate_probability
                        },
                        'risk_aversion': {
                            'required': True,
                            'type': float,
                            'min': 0.0,
                            'max': 10.0,
                            'default': 0.1,
                            'validator': validate_positive_float
                        },
                        'max_drawdown_penalty': {
                            'required': True,
                            'type': float,
                            'min': 0.0,
                            'max': 10.0,
                            'default': 1.0,
                            'validator': validate_positive_float
                        }
                    }
                },
                'cost_model': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'almgren_chriss': {
                            'required': True,
                            'type': dict,
                            'schema': {
                                'permanent_impact': {
                                    'required': True,
                                    'type': float,
                                    'min': 0.0,
                                    'max': 1.0,
                                    'default': 0.1,
                                    'validator': validate_positive_float
                                },
                                'temporary_impact': {
                                    'required': True,
                                    'type': float,
                                    'min': 0.0,
                                    'max': 1.0,
                                    'default': 0.05,
                                    'validator': validate_positive_float
                                }
                            }
                        }
                    }
                },
                'data': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'provider': {
                            'required': True,
                            'type': str,
                            'allowed': ['qlib', 'akshare'],
                            'default': 'qlib'
                        },
                        'cache_enabled': {
                            'required': True,
                            'type': bool,
                            'default': True
                        },
                        'cache_dir': {
                            'required': True,
                            'type': str,
                            'default': './data_cache'
                        }
                    }
                }
            }
        },
        'backtest': {
            'required': True,
            'type': dict,
            'schema': {
                'freq': {
                    'required': True,
                    'type': str,
                    'allowed': ['1d', '1h', '1min'],
                    'default': '1d'
                },
                'rebalance_freq': {
                    'required': True,
                    'type': str,
                    'allowed': ['1d', '1h', '1min'],
                    'default': '1d'
                },
                'start_date': {
                    'required': True,
                    'type': str,
                    'default': '2020-01-01'
                },
                'end_date': {
                    'required': True,
                    'type': str,
                    'default': '2023-12-31'
                },
                'benchmark': {
                    'required': True,
                    'type': str,
                    'default': '000300.SH',
                    'validator': validate_stock_code
                }
            }
        }
    }
    
    
    # åˆè§„é…ç½®éªŒè¯æ¨¡å¼
    COMPLIANCE_CONFIG_SCHEMA: Dict[str, Any] = {
        'compliance': {
            'required': True,
            'type': dict,
            'schema': {
                'audit': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'retention_years': {
                            'required': True,
                            'type': int,
                            'min': 1,
                            'max': 20,
                            'default': 5
                        },
                        'database_url': {
                            'required': True,
                            'type': str,
                            'default': 'influxdb://localhost:8086/trading_audit'
                        }
                    }
                },
                'risk_control': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'max_position_concentration': {
                            'required': True,
                            'type': float,
                            'min': 0.01,
                            'max': 1.0,
                            'default': 0.1,
                            'validator': validate_probability
                        },
                        'max_sector_exposure': {
                            'required': True,
                            'type': float,
                            'min': 0.1,
                            'max': 1.0,
                            'default': 0.3,
                            'validator': validate_probability
                        },
                        'stop_loss_threshold': {
                            'required': True,
                            'type': float,
                            'min': -0.5,
                            'max': 0.0,
                            'default': -0.05
                        }
                    }
                },
                'explainability': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'shap_enabled': {
                            'required': True,
                            'type': bool,
                            'default': True
                        },
                        'lime_enabled': {
                            'required': True,
                            'type': bool,
                            'default': True
                        },
                        'attention_visualization': {
                            'required': True,
                            'type': bool,
                            'default': True
                        }
                    }
                },
                'reporting': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'daily_report': {
                            'required': True,
                            'type': bool,
                            'default': True
                        },
                        'weekly_report': {
                            'required': True,
                            'type': bool,
                            'default': True
                        },
                        'monthly_report': {
                            'required': True,
                            'type': bool,
                            'default': True
                        },
                        'report_recipients': {
                            'required': True,
                            'type': list,
                            'default': []
                        }
                    }
                }
            }
        }
    }
    
    
    # ç›‘æŽ§é…ç½®éªŒè¯æ¨¡å¼
    MONITORING_CONFIG_SCHEMA: Dict[str, Any] = {
        'monitoring': {
            'required': True,
            'type': dict,
            'schema': {
                'prometheus': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'port': {
                            'required': True,
                            'type': int,
                            'min': 1024,
                            'max': 65535,
                            'default': 8000
                        },
                        'metrics_path': {
                            'required': True,
                            'type': str,
                            'default': '/metrics'
                        }
                    }
                },
                'thresholds': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'max_drawdown': {
                            'required': True,
                            'type': float,
                            'min': 0.01,
                            'max': 1.0,
                            'default': 0.15,
                            'validator': validate_probability
                        },
                        'min_sharpe_ratio': {
                            'required': True,
                            'type': float,
                            'min': -5.0,
                            'max': 10.0,
                            'default': 0.5
                        },
                        'max_var_95': {
                            'required': True,
                            'type': float,
                            'min': 0.001,
                            'max': 0.5,
                            'default': 0.03,
                            'validator': validate_probability
                        },
                        'lookback_days': {
                            'required': True,
                            'type': int,
                            'min': 7,
                            'max': 365,
                            'default': 90
                        }
                    }
                },
                'alerts': {
                    'required': True,
                    'type': dict,
                    'schema': {
                        'channels': {
                            'required': True,
                            'type': list,
                            'default': ['email']
                        },
                        'email': {
                            'required': False,
                            'type': dict,
                            'schema': {
                                'smtp_server': {
                                    'required': True,
                                    'type': str,
                                    'default': 'smtp.gmail.com'
                                },
                                'smtp_port': {
                                    'required': True,
                                    'type': int,
                                    'min': 1,
                                    'max': 65535,
                                    'default': 587
                                },
                                'username': {
                                    'required': True,
                                    'type': str,
                                    'default': ''
                                },
                                'password': {
                                    'required': True,
                                    'type': str,
                                    'default': ''
                                },
                                'recipients': {
                                    'required': True,
                                    'type': list,
                                    'default': []
                                }
                            }
                        },
                        'dingtalk': {
                            'required': False,
                            'type': dict,
                            'schema': {
                                'webhook_url': {
                                    'required': True,
                                    'type': str,
                                    'default': ''
                                }
                            }
                        }
                    }
                }
            }
        },
        'logging': {
            'required': True,
            'type': dict,
            'schema': {
                'level': {
                    'required': True,
                    'type': str,
                    'allowed': ['DEBUG', 'INFO', 'WARNING', 'ERROR', 'CRITICAL'],
                    'default': 'INFO'
                },
                'format': {
                    'required': True,
                    'type': str,
                    'default': '{time:YYYY-MM-DD HH:mm:ss} | {level} | {name}:{function}:{line} - {message}'
                },
                'rotation': {
                    'required': True,
                    'type': str,
                    'default': '1 day'
                },
                'retention': {
                    'required': True,
                    'type': str,
                    'default': '30 days'
                },
                'compression': {
                    'required': True,
                    'type': str,
                    'allowed': ['gz', 'bz2', 'xz'],
                    'default': 'gz'
                }
            }
        }
    }
    
    
    # æ‰€æœ‰é…ç½®æ¨¡å¼çš„æ˜ å°„
    CONFIG_SCHEMAS = {
        'model': MODEL_CONFIG_SCHEMA,
        'trading': TRADING_CONFIG_SCHEMA,
        'compliance': COMPLIANCE_CONFIG_SCHEMA,
        'monitoring': MONITORING_CONFIG_SCHEMA
    }
    ]]></file>
  <file path="src/rl_trading_system/config/config_manager.py"><![CDATA[
    """
    é…ç½®ç®¡ç†å™¨
    
    å®žçŽ°YAMLé…ç½®æ–‡ä»¶åŠ è½½ã€çŽ¯å¢ƒå˜é‡è¦†ç›–ã€é…ç½®éªŒè¯å’Œé»˜è®¤å€¼åº”ç”¨
    éœ€æ±‚: 10.1
    """
    
    import os
    import re
    from pathlib import Path
    from typing import Dict, Any, List, Union, Optional, Callable
    from copy import deepcopy
    import yaml
    from loguru import logger
    
    
    class ConfigLoadError(Exception):
        """é…ç½®åŠ è½½é”™è¯¯"""
        pass
    
    
    class ConfigValidationError(Exception):
        """é…ç½®éªŒè¯é”™è¯¯"""
        pass
    
    
    class ConfigManager:
        """é…ç½®ç®¡ç†å™¨
        
        æä¾›YAMLé…ç½®æ–‡ä»¶åŠ è½½ã€çŽ¯å¢ƒå˜é‡è¦†ç›–ã€é…ç½®éªŒè¯å’Œé»˜è®¤å€¼åº”ç”¨åŠŸèƒ½
        """
        
        def __init__(self):
            self._config_cache: Dict[str, Dict[str, Any]] = {}
            self._file_timestamps: Dict[str, float] = {}
        
        def load_config(self, config_path: Union[str, Path], 
                       enable_env_override: bool = True,
                       use_cache: bool = False) -> Dict[str, Any]:
            """åŠ è½½å•ä¸ªé…ç½®æ–‡ä»¶
            
            Args:
                config_path: é…ç½®æ–‡ä»¶è·¯å¾„
                enable_env_override: æ˜¯å¦å¯ç”¨çŽ¯å¢ƒå˜é‡è¦†ç›–
                use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
                
            Returns:
                é…ç½®å­—å…¸
                
            Raises:
                ConfigLoadError: é…ç½®åŠ è½½å¤±è´¥
            """
            config_path = Path(config_path)
            cache_key = str(config_path.absolute())
            
            # æ£€æŸ¥ç¼“å­˜
            if use_cache and self._is_cache_valid(cache_key, config_path):
                logger.debug(f"ä½¿ç”¨ç¼“å­˜é…ç½®: {config_path}")
                return deepcopy(self._config_cache[cache_key])
            
            try:
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å­˜åœ¨
                if not config_path.exists():
                    raise ConfigLoadError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
                
                # è¯»å–YAMLæ–‡ä»¶
                with open(config_path, 'r', encoding='utf-8') as f:
                    config = yaml.safe_load(f) or {}
                
                logger.info(f"æˆåŠŸåŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
                
                # åº”ç”¨çŽ¯å¢ƒå˜é‡è¦†ç›–
                if enable_env_override:
                    config = self._apply_env_overrides(config)
                
                # æ›´æ–°ç¼“å­˜
                if use_cache:
                    self._config_cache[cache_key] = deepcopy(config)
                    self._file_timestamps[cache_key] = config_path.stat().st_mtime
                
                return config
                
            except yaml.YAMLError as e:
                raise ConfigLoadError(f"YAMLè§£æžé”™è¯¯: {e}")
            except (OSError, IOError) as e:
                raise ConfigLoadError(f"æ–‡ä»¶è¯»å–é”™è¯¯: {e}")
        
        def load_configs(self, config_paths: List[Union[str, Path]], 
                        enable_env_override: bool = True,
                        use_cache: bool = False) -> Dict[str, Any]:
            """åŠ è½½å¤šä¸ªé…ç½®æ–‡ä»¶å¹¶åˆå¹¶
            
            Args:
                config_paths: é…ç½®æ–‡ä»¶è·¯å¾„åˆ—è¡¨
                enable_env_override: æ˜¯å¦å¯ç”¨çŽ¯å¢ƒå˜é‡è¦†ç›–
                use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
                
            Returns:
                åˆå¹¶åŽçš„é…ç½®å­—å…¸
            """
            merged_config = {}
            
            for config_path in config_paths:
                config = self.load_config(config_path, enable_env_override, use_cache)
                merged_config = self._deep_merge(merged_config, config)
            
            return merged_config
        
        def validate_config(self, config: Dict[str, Any], 
                           schema: Dict[str, Any]) -> None:
            """éªŒè¯é…ç½®
            
            Args:
                config: é…ç½®å­—å…¸
                schema: éªŒè¯æ¨¡å¼
                
            Raises:
                ConfigValidationError: é…ç½®éªŒè¯å¤±è´¥
            """
            self._validate_dict(config, schema, "")
        
        def apply_defaults(self, config: Dict[str, Any], 
                          schema: Dict[str, Any]) -> Dict[str, Any]:
            """åº”ç”¨é»˜è®¤å€¼
            
            Args:
                config: é…ç½®å­—å…¸
                schema: åŒ…å«é»˜è®¤å€¼çš„æ¨¡å¼
                
            Returns:
                åº”ç”¨é»˜è®¤å€¼åŽçš„é…ç½®å­—å…¸
            """
            result = deepcopy(config)
            self._apply_defaults_recursive(result, schema)
            return result
        
        def load_and_validate_config(self, config_path: Union[str, Path],
                                    schema: Dict[str, Any],
                                    enable_env_override: bool = True,
                                    use_cache: bool = False) -> Dict[str, Any]:
            """åŠ è½½ã€éªŒè¯å¹¶åº”ç”¨é»˜è®¤å€¼çš„å®Œæ•´æµç¨‹
            
            Args:
                config_path: é…ç½®æ–‡ä»¶è·¯å¾„
                schema: éªŒè¯æ¨¡å¼
                enable_env_override: æ˜¯å¦å¯ç”¨çŽ¯å¢ƒå˜é‡è¦†ç›–
                use_cache: æ˜¯å¦ä½¿ç”¨ç¼“å­˜
                
            Returns:
                å¤„ç†åŽçš„é…ç½®å­—å…¸
            """
            # åŠ è½½é…ç½®
            config = self.load_config(config_path, enable_env_override, use_cache)
            
            # åº”ç”¨é»˜è®¤å€¼
            config = self.apply_defaults(config, schema)
            
            # éªŒè¯é…ç½®
            self.validate_config(config, schema)
            
            return config
        
        def reload_config(self, config_path: Union[str, Path]) -> Dict[str, Any]:
            """å¼ºåˆ¶é‡æ–°åŠ è½½é…ç½®ï¼ˆå¿½ç•¥ç¼“å­˜ï¼‰
            
            Args:
                config_path: é…ç½®æ–‡ä»¶è·¯å¾„
                
            Returns:
                é‡æ–°åŠ è½½çš„é…ç½®å­—å…¸
            """
            cache_key = str(Path(config_path).absolute())
            
            # æ¸…é™¤ç¼“å­˜
            if cache_key in self._config_cache:
                del self._config_cache[cache_key]
            if cache_key in self._file_timestamps:
                del self._file_timestamps[cache_key]
            
            return self.load_config(config_path, use_cache=True)
        
        def _is_cache_valid(self, cache_key: str, config_path: Path) -> bool:
            """æ£€æŸ¥ç¼“å­˜æ˜¯å¦æœ‰æ•ˆ"""
            if cache_key not in self._config_cache:
                return False
            
            if cache_key not in self._file_timestamps:
                return False
            
            try:
                current_mtime = config_path.stat().st_mtime
                cached_mtime = self._file_timestamps[cache_key]
                return current_mtime <= cached_mtime
            except OSError:
                return False
        
        def _apply_env_overrides(self, config: Dict[str, Any]) -> Dict[str, Any]:
            """åº”ç”¨çŽ¯å¢ƒå˜é‡è¦†ç›–
            
            çŽ¯å¢ƒå˜é‡å‘½åè§„åˆ™ï¼š
            - ä½¿ç”¨ä¸‹åˆ’çº¿åˆ†éš”åµŒå¥—å±‚çº§
            - å…¨éƒ¨å¤§å†™
            - ä¾‹å¦‚ï¼šMODEL_TRANSFORMER_D_MODEL å¯¹åº” config['model']['transformer']['d_model']
            """
            result = deepcopy(config)
            
            # èŽ·å–æ‰€æœ‰ç›¸å…³çš„çŽ¯å¢ƒå˜é‡
            env_overrides = {}
            for key, value in os.environ.items():
                if self._is_config_env_var(key):
                    env_overrides[key] = value
            
            # åº”ç”¨çŽ¯å¢ƒå˜é‡è¦†ç›–
            for env_key, env_value in env_overrides.items():
                self._apply_single_env_override(result, env_key, env_value)
            
            return result
        
        def _is_config_env_var(self, env_key: str) -> bool:
            """åˆ¤æ–­æ˜¯å¦æ˜¯é…ç½®ç›¸å…³çš„çŽ¯å¢ƒå˜é‡"""
            # ç®€å•çš„å¯å‘å¼è§„åˆ™ï¼šåŒ…å«å¸¸è§çš„é…ç½®å‰ç¼€
            config_prefixes = ['MODEL_', 'TRADING_', 'DATA_', 'MONITORING_', 'COMPLIANCE_', 'FEATURE_']
            return any(env_key.startswith(prefix) for prefix in config_prefixes)
        
        def _apply_single_env_override(self, config: Dict[str, Any], 
                                      env_key: str, env_value: str) -> None:
            """åº”ç”¨å•ä¸ªçŽ¯å¢ƒå˜é‡è¦†ç›–"""
            # å°†çŽ¯å¢ƒå˜é‡é”®è½¬æ¢ä¸ºé…ç½®è·¯å¾„
            # éœ€è¦æ™ºèƒ½å¤„ç†ä¸‹åˆ’çº¿ï¼Œé¿å…å°† D_MODEL æ‹†åˆ†æˆ D å’Œ MODEL
            path_parts = self._parse_env_key_path(env_key)
            
            # å¯¼èˆªåˆ°ç›®æ ‡ä½ç½®
            current = config
            for part in path_parts[:-1]:
                if part not in current:
                    current[part] = {}
                elif not isinstance(current[part], dict):
                    # å¦‚æžœä¸æ˜¯å­—å…¸ï¼Œåˆ›å»ºæ–°çš„å­—å…¸
                    current[part] = {}
                current = current[part]
            
            # èŽ·å–åŽŸå§‹å€¼çš„ç±»åž‹ï¼ˆå¦‚æžœå­˜åœ¨ï¼‰
            final_key = path_parts[-1]
            original_type = type(current.get(final_key)) if final_key in current else str
            
            # è½¬æ¢çŽ¯å¢ƒå˜é‡å€¼
            converted_value = self._convert_env_value(env_value, original_type)
            
            # è®¾ç½®å€¼
            current[final_key] = converted_value
            
            logger.debug(f"çŽ¯å¢ƒå˜é‡è¦†ç›–: {env_key} = {converted_value}")
        
        def _parse_env_key_path(self, env_key: str) -> List[str]:
            """è§£æžçŽ¯å¢ƒå˜é‡é”®ä¸ºé…ç½®è·¯å¾„
            
            ä½¿ç”¨é¢„å®šä¹‰çš„æ˜ å°„æ¥æ­£ç¡®è§£æžçŽ¯å¢ƒå˜é‡è·¯å¾„
            """
            # è½¬æ¢ä¸ºå°å†™
            key_lower = env_key.lower()
            
            # é¢„å®šä¹‰çš„çŽ¯å¢ƒå˜é‡åˆ°é…ç½®è·¯å¾„çš„æ˜ å°„
            env_mappings = {
                # Model config mappings
                'model_transformer_d_model': ['model', 'transformer', 'd_model'],
                'model_transformer_n_heads': ['model', 'transformer', 'n_heads'],
                'model_transformer_n_layers': ['model', 'transformer', 'n_layers'],
                'model_transformer_d_ff': ['model', 'transformer', 'd_ff'],
                'model_transformer_dropout': ['model', 'transformer', 'dropout'],
                'model_transformer_max_seq_len': ['model', 'transformer', 'max_seq_len'],
                'model_transformer_n_features': ['model', 'transformer', 'n_features'],
                'model_sac_state_dim': ['model', 'sac', 'state_dim'],
                'model_sac_action_dim': ['model', 'sac', 'action_dim'],
                'model_sac_hidden_dim': ['model', 'sac', 'hidden_dim'],
                'model_sac_lr_actor': ['model', 'sac', 'lr_actor'],
                'model_sac_lr_critic': ['model', 'sac', 'lr_critic'],
                'model_sac_lr_alpha': ['model', 'sac', 'lr_alpha'],
                'model_sac_gamma': ['model', 'sac', 'gamma'],
                'model_sac_tau': ['model', 'sac', 'tau'],
                'model_sac_alpha': ['model', 'sac', 'alpha'],
                'model_sac_target_entropy': ['model', 'sac', 'target_entropy'],
                'model_sac_buffer_size': ['model', 'sac', 'buffer_size'],
                'model_sac_batch_size': ['model', 'sac', 'batch_size'],
                
                # Trading config mappings
                'trading_environment_stock_pool': ['trading', 'environment', 'stock_pool'],
                'trading_environment_lookback_window': ['trading', 'environment', 'lookback_window'],
                'trading_environment_initial_cash': ['trading', 'environment', 'initial_cash'],
                'trading_environment_commission_rate': ['trading', 'environment', 'commission_rate'],
                'trading_environment_stamp_tax_rate': ['trading', 'environment', 'stamp_tax_rate'],
                'trading_environment_risk_aversion': ['trading', 'environment', 'risk_aversion'],
                'trading_environment_max_drawdown_penalty': ['trading', 'environment', 'max_drawdown_penalty'],
                
                # Feature config mappings
                'feature_enabled': ['feature', 'enabled'],
                'feature_debug': ['feature', 'debug'],
                
                # Stock pool mapping
                'trading_stock_pool': ['trading', 'stock_pool']
            }
            
            # æ£€æŸ¥æ˜¯å¦æœ‰é¢„å®šä¹‰çš„æ˜ å°„
            if key_lower in env_mappings:
                return env_mappings[key_lower]
            
            # å¦‚æžœæ²¡æœ‰é¢„å®šä¹‰æ˜ å°„ï¼Œä½¿ç”¨ç®€å•çš„ä¸‹åˆ’çº¿åˆ†å‰²
            return key_lower.split('_')
        
        def _convert_env_value(self, env_value: str, target_type: type) -> Any:
            """è½¬æ¢çŽ¯å¢ƒå˜é‡å€¼åˆ°ç›®æ ‡ç±»åž‹"""
            if target_type == bool:
                return env_value.lower() in ('true', '1', 'yes', 'on')
            elif target_type == int:
                try:
                    return int(env_value)
                except ValueError:
                    return int(float(env_value))  # å¤„ç† "1.0" è¿™æ ·çš„æƒ…å†µ
            elif target_type == float:
                return float(env_value)
            elif target_type == list:
                return [item.strip() for item in env_value.split(',')]
            else:
                return env_value
        
        def _deep_merge(self, dict1: Dict[str, Any], dict2: Dict[str, Any]) -> Dict[str, Any]:
            """æ·±åº¦åˆå¹¶ä¸¤ä¸ªå­—å…¸"""
            result = deepcopy(dict1)
            
            for key, value in dict2.items():
                if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = self._deep_merge(result[key], value)
                else:
                    result[key] = deepcopy(value)
            
            return result
        
        def _validate_dict(self, config: Dict[str, Any], 
                          schema: Dict[str, Any], path: str) -> None:
            """é€’å½’éªŒè¯å­—å…¸"""
            for key, field_schema in schema.items():
                current_path = f"{path}.{key}" if path else key
                
                # æ£€æŸ¥å¿…éœ€å­—æ®µ
                if field_schema.get('required', False) and key not in config:
                    raise ConfigValidationError(f"å¿…éœ€å­—æ®µç¼ºå¤±: {current_path}")
                
                if key not in config:
                    continue
                
                value = config[key]
                
                # ç±»åž‹æ£€æŸ¥
                expected_type = field_schema.get('type')
                if expected_type and not isinstance(value, expected_type):
                    raise ConfigValidationError(
                        f"ç±»åž‹é”™è¯¯: {current_path}, æœŸæœ› {expected_type.__name__}, å®žé™… {type(value).__name__}"
                    )
                
                # èŒƒå›´æ£€æŸ¥
                if 'min' in field_schema and value < field_schema['min']:
                    raise ConfigValidationError(f"å€¼è¶…å‡ºèŒƒå›´: {current_path} < {field_schema['min']}")
                
                if 'max' in field_schema and value > field_schema['max']:
                    raise ConfigValidationError(f"å€¼è¶…å‡ºèŒƒå›´: {current_path} > {field_schema['max']}")
                
                # å…è®¸å€¼æ£€æŸ¥
                if 'allowed' in field_schema and value not in field_schema['allowed']:
                    raise ConfigValidationError(
                        f"ä¸å…è®¸çš„å€¼: {current_path} = {value}, å…è®¸çš„å€¼: {field_schema['allowed']}"
                    )
                
                # è‡ªå®šä¹‰éªŒè¯å™¨
                if 'validator' in field_schema:
                    validator = field_schema['validator']
                    try:
                        validator(current_path, value, self._validation_error)
                    except ConfigValidationError:
                        raise
                    except Exception as e:
                        raise ConfigValidationError(f"éªŒè¯å™¨é”™è¯¯: {current_path}, {e}")
                
                # é€’å½’éªŒè¯åµŒå¥—å­—å…¸
                if isinstance(value, dict) and 'schema' in field_schema:
                    self._validate_dict(value, field_schema['schema'], current_path)
        
        def _validation_error(self, field: str, message: str) -> None:
            """éªŒè¯é”™è¯¯å›žè°ƒ"""
            raise ConfigValidationError(f"{field}: {message}")
        
        def _apply_defaults_recursive(self, config: Dict[str, Any], 
                                     schema: Dict[str, Any]) -> None:
            """é€’å½’åº”ç”¨é»˜è®¤å€¼"""
            for key, field_schema in schema.items():
                # å¦‚æžœå­—æ®µä¸å­˜åœ¨ä¸”æœ‰é»˜è®¤å€¼ï¼Œåˆ™åº”ç”¨é»˜è®¤å€¼
                if key not in config and 'default' in field_schema:
                    config[key] = deepcopy(field_schema['default'])
                
                # é€’å½’å¤„ç†åµŒå¥—å­—å…¸
                if (key in config and isinstance(config[key], dict) and 
                    'schema' in field_schema):
                    self._apply_defaults_recursive(config[key], field_schema['schema'])
    
    
    # å…¨å±€é…ç½®ç®¡ç†å™¨å®žä¾‹
    config_manager = ConfigManager()
    
    
    def load_config(config_path: Union[str, Path], **kwargs) -> Dict[str, Any]:
        """ä¾¿æ·å‡½æ•°ï¼šåŠ è½½é…ç½®"""
        return config_manager.load_config(config_path, **kwargs)
    
    
    def load_configs(config_paths: List[Union[str, Path]], **kwargs) -> Dict[str, Any]:
        """ä¾¿æ·å‡½æ•°ï¼šåŠ è½½å¤šä¸ªé…ç½®"""
        return config_manager.load_configs(config_paths, **kwargs)
    
    
    def validate_config(config: Dict[str, Any], schema: Dict[str, Any]) -> None:
        """ä¾¿æ·å‡½æ•°ï¼šéªŒè¯é…ç½®"""
        return config_manager.validate_config(config, schema)
    ]]></file>
  <file path="src/rl_trading_system/config/__init__.py"><![CDATA[
    """é…ç½®ç®¡ç†æ¨¡å—"""
    
    from .config_manager import (
        ConfigManager, 
        ConfigLoadError, 
        ConfigValidationError,
        config_manager,
        load_config,
        load_configs,
        validate_config
    )
    from .schemas import (
        MODEL_CONFIG_SCHEMA,
        TRADING_CONFIG_SCHEMA,
        COMPLIANCE_CONFIG_SCHEMA,
        MONITORING_CONFIG_SCHEMA,
        CONFIG_SCHEMAS
    )
    
    __all__ = [
        "ConfigManager", 
        "ConfigLoadError", 
        "ConfigValidationError",
        "config_manager",
        "load_config",
        "load_configs", 
        "validate_config",
        "MODEL_CONFIG_SCHEMA",
        "TRADING_CONFIG_SCHEMA",
        "COMPLIANCE_CONFIG_SCHEMA",
        "MONITORING_CONFIG_SCHEMA",
        "CONFIG_SCHEMAS"
    ]
    ]]></file>
  <file path="src/rl_trading_system/backtest/multi_frequency_backtest.py"><![CDATA[
    """
    å¤šé¢‘çŽ‡å›žæµ‹å¼•æ“Žå®žçŽ°
    æ”¯æŒæ—¥é¢‘å’Œåˆ†é’Ÿé¢‘å›žæµ‹ï¼Œå¤šç§æˆäº¤ä»·æ ¼ï¼Œäº¤æ˜“æ‰§è¡Œæ¨¡æ‹Ÿï¼Œè€ƒè™‘å®žé™…æˆäº¤æƒ…å†µ
    ä¸¥æ ¼éµå¾ªTDDå¼€å‘ï¼Œä¸å…è®¸æ•èŽ·å¼‚å¸¸ï¼Œè®©å¼‚å¸¸æš´éœ²ä»¥å°½æ—©å‘çŽ°é”™è¯¯
    """
    import uuid
    import numpy as np
    import pandas as pd
    from datetime import datetime, date, time
    from typing import Dict, List, Tuple, Optional, Callable, Any, Union
    from decimal import Decimal, ROUND_HALF_UP
    from enum import Enum
    from dataclasses import dataclass, field
    from abc import ABC, abstractmethod
    
    
    class ExecutionMode(Enum):
        """äº¤æ˜“æ‰§è¡Œæ¨¡å¼"""
        NEXT_BAR = "next_bar"          # ä¸‹ä¸€ä¸ªbaræ‰§è¡Œ
        NEXT_CLOSE = "next_close"      # ä¸‹ä¸€ä¸ªæ”¶ç›˜ä»·æ‰§è¡Œ
        NEXT_OPEN = "next_open"        # ä¸‹ä¸€ä¸ªå¼€ç›˜ä»·æ‰§è¡Œ
        MARKET_ORDER = "market_order"  # å¸‚ä»·å•
        LIMIT_ORDER = "limit_order"    # é™ä»·å•
        
        def __eq__(self, other):
            if isinstance(other, str):
                return self.value == other
            return super().__eq__(other)
        
        def __hash__(self):
            return super().__hash__()
    
    
    class PriceMode(Enum):
        """ä»·æ ¼æ¨¡å¼"""
        CLOSE = "close"  # æ”¶ç›˜ä»·
        OPEN = "open"    # å¼€ç›˜ä»·
        HIGH = "high"    # æœ€é«˜ä»·
        LOW = "low"      # æœ€ä½Žä»·
        VWAP = "vwap"    # æˆäº¤é‡åŠ æƒå¹³å‡ä»·æ ¼
        TWAP = "twap"    # æ—¶é—´åŠ æƒå¹³å‡ä»·æ ¼
        
        def __eq__(self, other):
            if isinstance(other, str):
                return self.value == other
            return super().__eq__(other)
        
        def __hash__(self):
            return super().__hash__()
    
    
    class OrderType(Enum):
        """è®¢å•ç±»åž‹"""
        BUY = "buy"      # ä¹°å…¥
        SELL = "sell"    # å–å‡º
        SHORT = "short"  # åšç©º
        COVER = "cover"  # å¹³ä»“
        
        def __eq__(self, other):
            if isinstance(other, str):
                return self.value == other
            return super().__eq__(other)
        
        def __hash__(self):
            return super().__hash__()
    
    
    @dataclass
    class Order:
        """è®¢å•ç±»"""
        symbol: str
        order_type: OrderType
        quantity: int
        price: Decimal
        timestamp: datetime = field(default_factory=datetime.now)
        order_id: str = field(default_factory=lambda: str(uuid.uuid4()))
        status: str = field(default="pending")
        
        def __post_init__(self):
            """è®¢å•åˆ›å»ºåŽéªŒè¯"""
            if not self.symbol:
                raise ValueError("è‚¡ç¥¨ä»£ç ä¸èƒ½ä¸ºç©º")
            if self.quantity <= 0:
                raise ValueError("è®¢å•æ•°é‡å¿…é¡»ä¸ºæ­£æ•°")
            if self.price <= 0:
                raise ValueError("è®¢å•ä»·æ ¼å¿…é¡»ä¸ºæ­£æ•°")
        
        def get_execution_price(self, market_data: Dict[str, float], price_mode: PriceMode) -> Decimal:
            """æ ¹æ®ä»·æ ¼æ¨¡å¼èŽ·å–æ‰§è¡Œä»·æ ¼"""
            if price_mode == PriceMode.CLOSE:
                price = market_data.get('close', 0)
            elif price_mode == PriceMode.OPEN:
                price = market_data.get('open', 0)
            elif price_mode == PriceMode.HIGH:
                price = market_data.get('high', 0)
            elif price_mode == PriceMode.LOW:
                price = market_data.get('low', 0)
            elif price_mode == PriceMode.VWAP:
                price = market_data.get('vwap', market_data.get('close', 0))
            elif price_mode == PriceMode.TWAP:
                price = market_data.get('twap', market_data.get('close', 0))
            else:
                price = market_data.get('close', 0)
            
            return Decimal(str(price)).quantize(Decimal('0.01'), rounding=ROUND_HALF_UP)
    
    
    @dataclass
    class Trade:
        """äº¤æ˜“è®°å½•ç±»"""
        symbol: str
        trade_type: OrderType
        quantity: int
        price: Decimal
        timestamp: datetime = field(default_factory=datetime.now)
        commission: Decimal = field(default=Decimal('0'))
        trade_id: str = field(default_factory=lambda: str(uuid.uuid4()))
        
        def get_trade_value(self) -> Decimal:
            """è®¡ç®—äº¤æ˜“ä»·å€¼ï¼ˆä¹°å…¥ä¸ºè´Ÿï¼Œå–å‡ºä¸ºæ­£ï¼‰"""
            base_value = self.quantity * self.price
            
            if self.trade_type in [OrderType.BUY, OrderType.COVER]:
                # ä¹°å…¥ï¼šè´ŸçŽ°é‡‘æµ
                return -(base_value + self.commission)
            else:
                # å–å‡ºï¼šæ­£çŽ°é‡‘æµ
                return base_value - self.commission
    
    
    class Position:
        """æŒä»“ç±»"""
        
        def __init__(self, symbol: str):
            self.symbol = symbol
            self.quantity = 0
            self.avg_price = Decimal('0')
            self.market_value = Decimal('0')
            self.unrealized_pnl = Decimal('0')
            
        def update_position(self, trade_type: OrderType, quantity: int, price: Decimal):
            """æ›´æ–°æŒä»“"""
            if trade_type in [OrderType.BUY, OrderType.COVER]:
                # ä¹°å…¥æ“ä½œ
                if self.quantity == 0:
                    self.avg_price = price
                    self.quantity = quantity
                else:
                    # åŠ æƒå¹³å‡æˆæœ¬
                    total_cost = self.quantity * self.avg_price + quantity * price
                    total_quantity = self.quantity + quantity
                    self.avg_price = total_cost / total_quantity
                    self.quantity = total_quantity
            else:
                # å–å‡ºæ“ä½œ
                if quantity > self.quantity:
                    raise ValueError(f"æŒä»“æ•°é‡ä¸è¶³ï¼Œå½“å‰æŒä»“ï¼š{self.quantity}ï¼Œå–å‡ºæ•°é‡ï¼š{quantity}")
                self.quantity -= quantity
                
                # å¦‚æžœå…¨éƒ¨å–å‡ºï¼Œé‡ç½®å¹³å‡ä»·æ ¼
                if self.quantity == 0:
                    self.avg_price = Decimal('0')
        
        def update_market_value(self, current_price: Decimal):
            """æ›´æ–°å¸‚å€¼å’Œæœªå®žçŽ°ç›ˆäº"""
            self.market_value = self.quantity * current_price
            cost_basis = self.quantity * self.avg_price
            self.unrealized_pnl = self.market_value - cost_basis
    
    
    class Portfolio:
        """æŠ•èµ„ç»„åˆç±»"""
        
        def __init__(self, initial_cash: Decimal):
            self.initial_cash = initial_cash
            self.cash = initial_cash
            self.positions: Dict[str, Position] = {}
            self.trades: List[Trade] = []
            
        @property
        def total_value(self) -> Decimal:
            """æ€»ä»·å€¼"""
            market_value = sum(pos.market_value for pos in self.positions.values())
            return self.cash + market_value
        
        def execute_order(self, order: Order, market_data: pd.Series, 
                         price_mode: PriceMode, commission_rate: Decimal, stamp_tax_rate: Decimal = Decimal('0.001')) -> Trade:
            """æ‰§è¡Œè®¢å•"""
            # èŽ·å–æ‰§è¡Œä»·æ ¼
            execution_price = order.get_execution_price(market_data.to_dict(), price_mode)
            
            # è®¡ç®—ä½£é‡‘
            base_commission = order.quantity * execution_price * commission_rate
            
            # å¦‚æžœæ˜¯å–å‡ºï¼Œè¿˜éœ€è¦åŠ ä¸Šå°èŠ±ç¨Žï¼ˆAè‚¡ç‰¹æœ‰ï¼‰
            if order.order_type == OrderType.SELL:
                stamp_tax = order.quantity * execution_price * stamp_tax_rate
                total_commission = base_commission + stamp_tax
            else:
                total_commission = base_commission
                
            # æ£€æŸ¥èµ„é‡‘æ˜¯å¦è¶³å¤Ÿï¼ˆä¹°å…¥æ—¶ï¼‰
            if order.order_type == OrderType.BUY:
                required_cash = order.quantity * execution_price + total_commission
                if required_cash > self.cash:
                    raise ValueError(f"çŽ°é‡‘ä¸è¶³ï¼Œéœ€è¦ï¼š{required_cash}ï¼Œå¯ç”¨ï¼š{self.cash}")
            
            # åˆ›å»ºäº¤æ˜“è®°å½•
            trade = Trade(
                symbol=order.symbol,
                trade_type=order.order_type,
                quantity=order.quantity,
                price=execution_price,
                timestamp=order.timestamp,
                commission=total_commission
            )
            
            # æ›´æ–°æŒä»“
            if order.symbol not in self.positions:
                self.positions[order.symbol] = Position(order.symbol)
            
            self.positions[order.symbol].update_position(
                trade_type=order.order_type,
                quantity=order.quantity,
                price=execution_price
            )
            
            # æ›´æ–°çŽ°é‡‘
            self.cash += trade.get_trade_value()
            
            # è®°å½•äº¤æ˜“
            self.trades.append(trade)
            
            return trade
        
        def update_market_values(self, current_prices: Dict[str, Decimal]):
            """æ›´æ–°æ‰€æœ‰æŒä»“çš„å¸‚å€¼"""
            for symbol, position in self.positions.items():
                if symbol in current_prices:
                    position.update_market_value(current_prices[symbol])
        
        def get_performance_metrics(self) -> Dict[str, float]:
            """èŽ·å–ç»©æ•ˆæŒ‡æ ‡"""
            total_value = float(self.total_value)
            initial_value = float(self.initial_cash)
            
            total_return = (total_value / initial_value) - 1
            cash_ratio = float(self.cash) / total_value
            
            return {
                'total_value': total_value,
                'total_return': total_return,
                'cash_ratio': cash_ratio,
                'cash': float(self.cash),
                'market_value': total_value - float(self.cash)
            }
    
    
    @dataclass
    class BacktestConfig:
        """å›žæµ‹é…ç½®"""
        start_date: date
        end_date: date
        initial_capital: float
        frequency: str = "1d"
        execution_mode: ExecutionMode = ExecutionMode.NEXT_CLOSE
        price_mode: PriceMode = PriceMode.CLOSE
        commission_rate: float = 0.001
        stamp_tax_rate: float = 0.001
        
        def __post_init__(self):
            """é…ç½®éªŒè¯"""
            if self.end_date < self.start_date:
                raise ValueError("ç»“æŸæ—¥æœŸä¸èƒ½æ—©äºŽå¼€å§‹æ—¥æœŸ")
            if self.initial_capital <= 0:
                raise ValueError("åˆå§‹èµ„é‡‘å¿…é¡»ä¸ºæ­£æ•°")
            if self.commission_rate < 0:
                raise ValueError("ä½£é‡‘çŽ‡å¿…é¡»ä¸ºéžè´Ÿæ•°")
            if self.stamp_tax_rate < 0:
                raise ValueError("å°èŠ±ç¨ŽçŽ‡å¿…é¡»ä¸ºéžè´Ÿæ•°")
            
            # éªŒè¯é¢‘çŽ‡
            valid_frequencies = ["1d", "1h", "30min", "15min", "5min", "1min"]
            if self.frequency not in valid_frequencies:
                raise ValueError(f"ä¸æ”¯æŒçš„é¢‘çŽ‡ï¼š{self.frequency}")
    
    
    @dataclass
    class BacktestResult:
        """å›žæµ‹ç»“æžœ"""
        trades: List[Trade]
        portfolio_values: pd.Series
        positions: Dict[str, Position]
        final_cash: Decimal
        
        def calculate_performance_metrics(self) -> Dict[str, float]:
            """è®¡ç®—ç»©æ•ˆæŒ‡æ ‡"""
            if len(self.portfolio_values) < 2:
                return {
                    'total_return': 0.0,
                    'annualized_return': 0.0,
                    'volatility': 0.0,
                    'sharpe_ratio': 0.0,
                    'max_drawdown': 0.0,
                    'win_rate': 0.0
                }
            
            # è®¡ç®—æ”¶ç›ŠçŽ‡åºåˆ—
            returns = self.portfolio_values.pct_change().dropna()
            
            # åŸºæœ¬æŒ‡æ ‡
            total_return = (self.portfolio_values.iloc[-1] / self.portfolio_values.iloc[0]) - 1
            
            # å¹´åŒ–æ”¶ç›ŠçŽ‡ï¼ˆå‡è®¾252ä¸ªäº¤æ˜“æ—¥ï¼‰
            trading_days = len(self.portfolio_values)
            if trading_days > 1:
                annualized_return = (1 + total_return) ** (252 / trading_days) - 1
            else:
                annualized_return = 0.0
            
            # æ³¢åŠ¨çŽ‡
            volatility = returns.std() * np.sqrt(252) if len(returns) > 1 else 0.0
            
            # å¤æ™®æ¯”çŽ‡ï¼ˆå‡è®¾æ— é£Žé™©åˆ©çŽ‡ä¸º3%ï¼‰
            risk_free_rate = 0.03
            if volatility > 0:
                sharpe_ratio = (annualized_return - risk_free_rate) / volatility
            else:
                sharpe_ratio = 0.0
            
            # æœ€å¤§å›žæ’¤
            peak = self.portfolio_values.expanding().max()
            drawdown = (self.portfolio_values - peak) / peak
            max_drawdown = abs(drawdown.min())
            
            # èƒœçŽ‡
            if len(returns) > 0:
                win_rate = (returns > 0).mean()
            else:
                win_rate = 0.0
            
            return {
                'total_return': total_return,
                'annualized_return': annualized_return,
                'volatility': volatility,
                'sharpe_ratio': sharpe_ratio,
                'max_drawdown': max_drawdown,
                'win_rate': win_rate
            }
    
    
    class MultiFrequencyBacktest:
        """å¤šé¢‘çŽ‡å›žæµ‹å¼•æ“Ž"""
        
        def __init__(self, config: BacktestConfig):
            self.config = config
            self.portfolio = Portfolio(Decimal(str(config.initial_capital)))
            self.trades: List[Trade] = []
            self.portfolio_values: List[float] = []
            self.timestamps: List[pd.Timestamp] = []
            
        def _validate_data(self, data: pd.DataFrame):
            """éªŒè¯å›žæµ‹æ•°æ®"""
            if data.empty:
                raise ValueError("å›žæµ‹æ•°æ®ä¸èƒ½ä¸ºç©º")
            
            required_columns = ['open', 'high', 'low', 'close', 'volume']
            missing_columns = set(required_columns) - set(data.columns)
            if missing_columns:
                raise ValueError(f"æ•°æ®ç¼ºå°‘å¿…è¦çš„åˆ—ï¼š{missing_columns}")
        
        def _resample_data_if_needed(self, data: pd.DataFrame) -> pd.DataFrame:
            """æ ¹æ®å›žæµ‹é¢‘çŽ‡é‡é‡‡æ ·æ•°æ®"""
            if self.config.frequency == "1d":
                # å¦‚æžœæ˜¯æ—¥é¢‘å›žæµ‹ï¼Œä½†æ•°æ®æ˜¯é«˜é¢‘çš„ï¼Œéœ€è¦é‡é‡‡æ ·åˆ°æ—¥é¢‘
                if hasattr(data.index, 'levels'):
                    # MultiIndex case
                    time_index = data.index.get_level_values(0)
                else:
                    time_index = data.index
                    
                # æ£€æŸ¥æ˜¯å¦éœ€è¦é‡é‡‡æ ·
                if len(time_index) > 0:
                    time_diff = pd.Timedelta(days=1)
                    if len(time_index) > 1:
                        actual_freq = time_index[1] - time_index[0]
                        if actual_freq < time_diff:
                            # éœ€è¦é‡é‡‡æ ·åˆ°æ—¥é¢‘
                            if 'symbol' in data.index.names:
                                # MultiIndex with symbol
                                resampled_data = []
                                for symbol in data.index.get_level_values('symbol').unique():
                                    symbol_data = data.xs(symbol, level='symbol')
                                    daily_data = symbol_data.resample('1D').agg({
                                        'open': 'first',
                                        'high': 'max',
                                        'low': 'min',
                                        'close': 'last',
                                        'volume': 'sum'
                                    }).dropna()
                                    daily_data['symbol'] = symbol
                                    resampled_data.append(daily_data.reset_index().set_index(['datetime', 'symbol']))
                                
                                if resampled_data:
                                    return pd.concat(resampled_data).sort_index()
            
            return data
        
        def run(self, data: pd.DataFrame, strategy: Callable) -> BacktestResult:
            """è¿è¡Œå›žæµ‹"""
            # éªŒè¯æ•°æ®
            self._validate_data(data)
            
            # é‡é‡‡æ ·æ•°æ®ï¼ˆå¦‚æžœéœ€è¦ï¼‰
            data = self._resample_data_if_needed(data)
            
            # èŽ·å–æ—¶é—´ç´¢å¼•
            if hasattr(data.index, 'levels') and 'datetime' in data.index.names:
                # MultiIndex case
                timestamps = data.index.get_level_values('datetime').unique().sort_values()
            else:
                timestamps = data.index.unique() if hasattr(data.index, 'unique') else [data.index[0]]
            
            # è¿‡æ»¤æ—¶é—´èŒƒå›´
            start_ts = pd.Timestamp(self.config.start_date)
            end_ts = pd.Timestamp(self.config.end_date) + pd.Timedelta(days=1)
            timestamps = [ts for ts in timestamps if start_ts <= ts < end_ts]
            
            if not timestamps:
                timestamps = [start_ts]  # è‡³å°‘æœ‰ä¸€ä¸ªæ—¶é—´ç‚¹
            
            # åˆå§‹åŒ–ç»„åˆä»·å€¼è®°å½•
            self.portfolio_values = [float(self.portfolio.initial_cash)]
            self.timestamps = [timestamps[0] if timestamps else start_ts]
            
            # æŒ‰æ—¶é—´æ­¥æ‰§è¡Œå›žæµ‹
            for i, timestamp in enumerate(timestamps):
                # èŽ·å–å½“å‰æ—¶é—´çš„æ•°æ®
                try:
                    if hasattr(data.index, 'levels') and 'datetime' in data.index.names:
                        current_data = data.xs(timestamp, level='datetime')
                    else:
                        # æ‰¾åˆ°æœ€æŽ¥è¿‘çš„æ—¶é—´ç‚¹
                        closest_idx = np.abs(pd.to_datetime(data.index) - timestamp).argmin()
                        current_data = data.iloc[closest_idx:closest_idx+1]
                except (KeyError, IndexError):
                    continue
                
                # è°ƒç”¨ç­–ç•¥ç”Ÿæˆè®¢å•
                orders = strategy(current_data, self.portfolio, timestamp)
                
                # æ‰§è¡Œè®¢å•
                for order in orders:
                    if hasattr(current_data, 'xs'):
                        # MultiIndex case - get data for specific symbol
                        try:
                            symbol_data = current_data.xs(order.symbol) if order.symbol in current_data.index else current_data.iloc[0]
                        except (KeyError, IndexError):
                            symbol_data = current_data.iloc[0] if len(current_data) > 0 else pd.Series()
                    else:
                        symbol_data = current_data.iloc[0] if len(current_data) > 0 else pd.Series()
                    
                    if not symbol_data.empty:
                        trade = self.portfolio.execute_order(
                            order=order,
                            market_data=symbol_data,
                            price_mode=self.config.price_mode,
                            commission_rate=Decimal(str(self.config.commission_rate)),
                            stamp_tax_rate=Decimal(str(self.config.stamp_tax_rate))
                        )
                        self.trades.append(trade)
                
                # æ›´æ–°æŒä»“å¸‚å€¼
                if hasattr(current_data, 'index') and len(current_data) > 0:
                    current_prices = {}
                    if hasattr(current_data, 'xs'):
                        # MultiIndex case
                        for symbol in current_data.index:
                            try:
                                symbol_data = current_data.xs(symbol)
                                current_prices[symbol] = Decimal(str(symbol_data['close']))
                            except (KeyError, IndexError):
                                pass
                    else:
                        # Single index case
                        if 'close' in current_data.columns and len(current_data) > 0:
                            symbol = getattr(current_data.iloc[0], 'symbol', 'default')
                            current_prices[symbol] = Decimal(str(current_data.iloc[0]['close']))
                    
                    self.portfolio.update_market_values(current_prices)
                
                # è®°å½•ç»„åˆä»·å€¼
                current_value = float(self.portfolio.total_value)
                if i < len(timestamps) - 1 or len(self.portfolio_values) == 1:
                    self.portfolio_values.append(current_value)
                    if i < len(timestamps) - 1:
                        self.timestamps.append(timestamps[i + 1])
            
            # åˆ›å»ºæ—¶é—´åºåˆ—ï¼Œç¡®ä¿é•¿åº¦åŒ¹é…
            # å¦‚æžœportfolio_valuesæ¯”timestampsé•¿ï¼Œæˆªæ–­portfolio_values
            # å¦‚æžœtimestampsæ¯”portfolio_valuesé•¿ï¼Œæˆªæ–­timestamps
            min_length = min(len(self.portfolio_values), len(self.timestamps))
            portfolio_series = pd.Series(
                self.portfolio_values[:min_length],
                index=pd.DatetimeIndex(self.timestamps[:min_length])
            )
            
            # è¿”å›žå›žæµ‹ç»“æžœ
            return BacktestResult(
                trades=self.trades,
                portfolio_values=portfolio_series,
                positions=self.portfolio.positions.copy(),
                final_cash=self.portfolio.cash
            )
    ]]></file>
  <file path="src/rl_trading_system/backtest/__init__.py"><![CDATA[
    """å›žæµ‹å¼•æ“Žæ¨¡å—"""
    
    from .multi_frequency_backtest import (
        MultiFrequencyBacktest,
        BacktestConfig,
        BacktestResult,
        ExecutionMode,
        PriceMode
    )
    
    __all__ = [
        "MultiFrequencyBacktest",
        "BacktestConfig",
        "BacktestResult", 
        "ExecutionMode",
        "PriceMode"
    ]
    ]]></file>
  <file path="src/rl_trading_system/audit/audit_logger.py"><![CDATA[
    """
    å®¡è®¡æ—¥å¿—ç³»ç»Ÿ
    å®žçŽ°äº¤æ˜“å†³ç­–è®°å½•ã€å­˜å‚¨æœºåˆ¶ã€æŸ¥è¯¢æŽ¥å£å’Œæ•°æ®å®Œæ•´æ€§ç®¡ç†
    """
    
    import asyncio
    import json
    import uuid
    from datetime import datetime, timedelta
    from dataclasses import dataclass, field
    from typing import Dict, List, Any, Optional, Union
    import logging
    from abc import ABC, abstractmethod
    
    import numpy as np
    from influxdb_client import InfluxDBClient, Point
    from influxdb_client.client.write_api import SYNCHRONOUS
    import asyncpg
    
    from ..data.data_models import TradingState, TradingAction, TransactionRecord
    from ..utils.logger import get_logger
    
    logger = get_logger(__name__)
    
    
    @dataclass
    class AuditRecord:
        """å®¡è®¡è®°å½•åŸºç¡€ç»“æž„"""
        record_id: str
        timestamp: datetime
        event_type: str  # trading_decision, transaction_execution, model_update, etc.
        user_id: str
        session_id: str
        model_version: str
        data: Dict[str, Any]
        metadata: Dict[str, Any] = field(default_factory=dict)
        
        # æœ‰æ•ˆçš„äº‹ä»¶ç±»åž‹
        VALID_EVENT_TYPES = {
            'trading_decision', 'transaction_execution', 'model_update',
            'risk_violation', 'system_error', 'compliance_check'
        }
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            self._validate()
        
        def _validate(self):
            """éªŒè¯æ•°æ®æœ‰æ•ˆæ€§"""
            if self.event_type not in self.VALID_EVENT_TYPES:
                raise ValueError(f"æ— æ•ˆçš„äº‹ä»¶ç±»åž‹: {self.event_type}")
            
            if not self.record_id:
                raise ValueError("è®°å½•IDä¸èƒ½ä¸ºç©º")
            
            if not self.session_id:
                raise ValueError("ä¼šè¯IDä¸èƒ½ä¸ºç©º")
            
            if not self.model_version:
                raise ValueError("æ¨¡åž‹ç‰ˆæœ¬ä¸èƒ½ä¸ºç©º")
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'record_id': self.record_id,
                'timestamp': self.timestamp.isoformat(),
                'event_type': self.event_type,
                'user_id': self.user_id,
                'session_id': self.session_id,
                'model_version': self.model_version,
                'data': json.dumps(self.data),
                'metadata': json.dumps(self.metadata)
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'AuditRecord':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            return cls(
                record_id=data['record_id'],
                timestamp=datetime.fromisoformat(data['timestamp']),
                event_type=data['event_type'],
                user_id=data['user_id'],
                session_id=data['session_id'],
                model_version=data['model_version'],
                data=json.loads(data['data']) if isinstance(data['data'], str) else data['data'],
                metadata=json.loads(data['metadata']) if isinstance(data['metadata'], str) else data['metadata']
            )
        
        def to_json(self) -> str:
            """è½¬æ¢ä¸ºJSONå­—ç¬¦ä¸²"""
            return json.dumps(self.to_dict())
        
        @classmethod
        def from_json(cls, json_str: str) -> 'AuditRecord':
            """ä»ŽJSONå­—ç¬¦ä¸²åˆ›å»ºå¯¹è±¡"""
            data = json.loads(json_str)
            return cls.from_dict(data)
    
    
    @dataclass
    class DecisionRecord:
        """äº¤æ˜“å†³ç­–è®°å½•"""
        decision_id: str
        timestamp: datetime
        model_version: str
        input_state: TradingState
        output_action: TradingAction
        model_outputs: Dict[str, Any]
        feature_importance: Dict[str, float]
        risk_metrics: Dict[str, float]
        execution_time_ms: Optional[float] = None
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'decision_id': self.decision_id,
                'timestamp': self.timestamp.isoformat(),
                'model_version': self.model_version,
                'input_state': self.input_state.to_dict(),
                'output_action': self.output_action.to_dict(),
                'model_outputs': json.dumps(self.model_outputs),
                'feature_importance': json.dumps(self.feature_importance),
                'risk_metrics': json.dumps(self.risk_metrics),
                'execution_time_ms': self.execution_time_ms
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'DecisionRecord':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            return cls(
                decision_id=data['decision_id'],
                timestamp=datetime.fromisoformat(data['timestamp']),
                model_version=data['model_version'],
                input_state=TradingState.from_dict(data['input_state']),
                output_action=TradingAction.from_dict(data['output_action']),
                model_outputs=json.loads(data['model_outputs']) if isinstance(data['model_outputs'], str) else data['model_outputs'],
                feature_importance=json.loads(data['feature_importance']) if isinstance(data['feature_importance'], str) else data['feature_importance'],
                risk_metrics=json.loads(data['risk_metrics']) if isinstance(data['risk_metrics'], str) else data['risk_metrics'],
                execution_time_ms=data.get('execution_time_ms')
            )
    
    
    @dataclass
    class ComplianceReport:
        """åˆè§„æŠ¥å‘Š"""
        report_id: str
        generated_at: datetime
        period_start: datetime
        period_end: datetime
        total_decisions: int
        risk_violations: List[Dict[str, Any]]
        concentration_analysis: Dict[str, float]
        model_performance: Dict[str, float]
        compliance_score: float
        
        def __post_init__(self):
            """æ•°æ®éªŒè¯"""
            if not (0 <= self.compliance_score <= 1):
                raise ValueError("åˆè§„åˆ†æ•°å¿…é¡»åœ¨0åˆ°1ä¹‹é—´")
        
        def to_dict(self) -> Dict[str, Any]:
            """è½¬æ¢ä¸ºå­—å…¸"""
            return {
                'report_id': self.report_id,
                'generated_at': self.generated_at.isoformat(),
                'period_start': self.period_start.isoformat(),
                'period_end': self.period_end.isoformat(),
                'total_decisions': self.total_decisions,
                'risk_violations': self.risk_violations,
                'concentration_analysis': self.concentration_analysis,
                'model_performance': self.model_performance,
                'compliance_score': self.compliance_score
            }
        
        @classmethod
        def from_dict(cls, data: Dict[str, Any]) -> 'ComplianceReport':
            """ä»Žå­—å…¸åˆ›å»ºå¯¹è±¡"""
            return cls(
                report_id=data['report_id'],
                generated_at=datetime.fromisoformat(data['generated_at']),
                period_start=datetime.fromisoformat(data['period_start']),
                period_end=datetime.fromisoformat(data['period_end']),
                total_decisions=data['total_decisions'],
                risk_violations=data['risk_violations'],
                concentration_analysis=data['concentration_analysis'],
                model_performance=data['model_performance'],
                compliance_score=data['compliance_score']
            )
    
    
    class DatabaseInterface(ABC):
        """æ•°æ®åº“æŽ¥å£æŠ½è±¡ç±»"""
        
        @abstractmethod
        async def connect(self):
            """è¿žæŽ¥æ•°æ®åº“"""
            pass
        
        @abstractmethod
        async def disconnect(self):
            """æ–­å¼€æ•°æ®åº“è¿žæŽ¥"""
            pass
        
        @abstractmethod
        async def write_records(self, records: List[AuditRecord]):
            """å†™å…¥è®°å½•"""
            pass
        
        @abstractmethod
        async def query_records(self, **kwargs) -> List[AuditRecord]:
            """æŸ¥è¯¢è®°å½•"""
            pass
    
    
    class InfluxDBInterface(DatabaseInterface):
        """InfluxDBæ—¶åºæ•°æ®åº“æŽ¥å£"""
        
        def __init__(self, url: str, token: str, org: str, bucket: str):
            self.url = url
            self.token = token
            self.org = org
            self.bucket = bucket
            self.client = None
            self.write_api = None
        
        async def connect(self):
            """è¿žæŽ¥InfluxDB"""
            try:
                self.client = InfluxDBClient(url=self.url, token=self.token, org=self.org)
                self.write_api = self.client.write_api(write_options=SYNCHRONOUS)
                logger.info("InfluxDBè¿žæŽ¥æˆåŠŸ")
            except Exception as e:
                logger.error(f"InfluxDBè¿žæŽ¥å¤±è´¥: {e}")
                raise
        
        async def disconnect(self):
            """æ–­å¼€InfluxDBè¿žæŽ¥"""
            if self.client:
                self.client.close()
                logger.info("InfluxDBè¿žæŽ¥å·²æ–­å¼€")
        
        async def write_records(self, records: List[AuditRecord]):
            """å†™å…¥è®°å½•åˆ°InfluxDB"""
            try:
                points = []
                for record in records:
                    point = Point("audit_record") \
                        .tag("event_type", record.event_type) \
                        .tag("user_id", record.user_id) \
                        .tag("session_id", record.session_id) \
                        .tag("model_version", record.model_version) \
                        .field("record_id", record.record_id) \
                        .field("data", json.dumps(record.data)) \
                        .field("metadata", json.dumps(record.metadata)) \
                        .time(record.timestamp)
                    points.append(point)
                
                self.write_api.write(bucket=self.bucket, record=points)
                logger.debug(f"å†™å…¥{len(records)}æ¡è®°å½•åˆ°InfluxDB")
                
            except Exception as e:
                logger.error(f"å†™å…¥InfluxDBå¤±è´¥: {e}")
                raise
        
        async def query_records(self, **kwargs) -> List[AuditRecord]:
            """ä»ŽInfluxDBæŸ¥è¯¢è®°å½•"""
            # InfluxDBä¸»è¦ç”¨äºŽæ—¶åºæ•°æ®å­˜å‚¨ï¼Œå¤æ‚æŸ¥è¯¢é€šè¿‡PostgreSQLè¿›è¡Œ
            pass
    
    
    class PostgreSQLInterface(DatabaseInterface):
        """PostgreSQLå…³ç³»æ•°æ®åº“æŽ¥å£"""
        
        def __init__(self, connection_string: str):
            self.connection_string = connection_string
            self.pool = None
        
        async def connect(self):
            """è¿žæŽ¥PostgreSQL"""
            try:
                self.pool = await asyncpg.create_pool(self.connection_string)
                
                # åˆ›å»ºå®¡è®¡è¡¨
                await self._create_tables()
                logger.info("PostgreSQLè¿žæŽ¥æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"PostgreSQLè¿žæŽ¥å¤±è´¥: {e}")
                raise
        
        async def disconnect(self):
            """æ–­å¼€PostgreSQLè¿žæŽ¥"""
            if self.pool:
                await self.pool.close()
                logger.info("PostgreSQLè¿žæŽ¥å·²æ–­å¼€")
        
        async def _create_tables(self):
            """åˆ›å»ºå®¡è®¡ç›¸å…³è¡¨"""
            async with self.pool.acquire() as conn:
                # å®¡è®¡è®°å½•è¡¨
                await conn.execute('''
                    CREATE TABLE IF NOT EXISTS audit_records (
                        record_id VARCHAR(255) PRIMARY KEY,
                        timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
                        event_type VARCHAR(50) NOT NULL,
                        user_id VARCHAR(100) NOT NULL,
                        session_id VARCHAR(255) NOT NULL,
                        model_version VARCHAR(50) NOT NULL,
                        data JSONB NOT NULL,
                        metadata JSONB DEFAULT '{}',
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    );
                ''')
                
                # å†³ç­–è®°å½•è¡¨
                await conn.execute('''
                    CREATE TABLE IF NOT EXISTS decision_records (
                        decision_id VARCHAR(255) PRIMARY KEY,
                        timestamp TIMESTAMP WITH TIME ZONE NOT NULL,
                        model_version VARCHAR(50) NOT NULL,
                        input_state JSONB NOT NULL,
                        output_action JSONB NOT NULL,
                        model_outputs JSONB DEFAULT '{}',
                        feature_importance JSONB DEFAULT '{}',
                        risk_metrics JSONB DEFAULT '{}',
                        execution_time_ms FLOAT,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    );
                ''')
                
                # åˆè§„æŠ¥å‘Šè¡¨
                await conn.execute('''
                    CREATE TABLE IF NOT EXISTS compliance_reports (
                        report_id VARCHAR(255) PRIMARY KEY,
                        generated_at TIMESTAMP WITH TIME ZONE NOT NULL,
                        period_start TIMESTAMP WITH TIME ZONE NOT NULL,
                        period_end TIMESTAMP WITH TIME ZONE NOT NULL,
                        total_decisions INTEGER NOT NULL,
                        risk_violations JSONB DEFAULT '[]',
                        concentration_analysis JSONB DEFAULT '{}',
                        model_performance JSONB DEFAULT '{}',
                        compliance_score FLOAT NOT NULL,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    );
                ''')
                
                # åˆ›å»ºç´¢å¼•
                await conn.execute('CREATE INDEX IF NOT EXISTS idx_audit_timestamp ON audit_records(timestamp);')
                await conn.execute('CREATE INDEX IF NOT EXISTS idx_audit_event_type ON audit_records(event_type);')
                await conn.execute('CREATE INDEX IF NOT EXISTS idx_audit_session ON audit_records(session_id);')
                await conn.execute('CREATE INDEX IF NOT EXISTS idx_audit_model_version ON audit_records(model_version);')
                await conn.execute('CREATE INDEX IF NOT EXISTS idx_decision_timestamp ON decision_records(timestamp);')
                await conn.execute('CREATE INDEX IF NOT EXISTS idx_decision_model_version ON decision_records(model_version);')
        
        async def write_records(self, records: List[AuditRecord]):
            """å†™å…¥è®°å½•åˆ°PostgreSQL"""
            try:
                async with self.pool.acquire() as conn:
                    # æ‰¹é‡æ’å…¥å®¡è®¡è®°å½•
                    values = [
                        (r.record_id, r.timestamp, r.event_type, r.user_id, 
                         r.session_id, r.model_version, json.dumps(r.data), 
                         json.dumps(r.metadata))
                        for r in records
                    ]
                    
                    await conn.executemany('''
                        INSERT INTO audit_records 
                        (record_id, timestamp, event_type, user_id, session_id, 
                         model_version, data, metadata)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8)
                        ON CONFLICT (record_id) DO NOTHING
                    ''', values)
                    
                    logger.debug(f"å†™å…¥{len(records)}æ¡è®°å½•åˆ°PostgreSQL")
                    
            except Exception as e:
                logger.error(f"å†™å…¥PostgreSQLå¤±è´¥: {e}")
                raise
        
        async def query_records(self, **kwargs) -> List[AuditRecord]:
            """ä»ŽPostgreSQLæŸ¥è¯¢è®°å½•"""
            try:
                async with self.pool.acquire() as conn:
                    query = "SELECT * FROM audit_records WHERE 1=1"
                    params = []
                    param_count = 0
                    
                    # æž„å»ºæŸ¥è¯¢æ¡ä»¶
                    if 'start_time' in kwargs and 'end_time' in kwargs:
                        param_count += 2
                        query += f" AND timestamp BETWEEN ${param_count-1} AND ${param_count}"
                        params.extend([kwargs['start_time'], kwargs['end_time']])
                    
                    if 'event_type' in kwargs:
                        param_count += 1
                        query += f" AND event_type = ${param_count}"
                        params.append(kwargs['event_type'])
                    
                    if 'session_id' in kwargs:
                        param_count += 1
                        query += f" AND session_id = ${param_count}"
                        params.append(kwargs['session_id'])
                    
                    if 'model_version' in kwargs:
                        param_count += 1
                        query += f" AND model_version = ${param_count}"
                        params.append(kwargs['model_version'])
                    
                    query += " ORDER BY timestamp DESC"
                    
                    if 'limit' in kwargs:
                        param_count += 1
                        query += f" LIMIT ${param_count}"
                        params.append(kwargs['limit'])
                    
                    rows = await conn.fetch(query, *params)
                    
                    # è½¬æ¢ä¸ºAuditRecordå¯¹è±¡
                    records = []
                    for row in rows:
                        record = AuditRecord(
                            record_id=row['record_id'],
                            timestamp=row['timestamp'],
                            event_type=row['event_type'],
                            user_id=row['user_id'],
                            session_id=row['session_id'],
                            model_version=row['model_version'],
                            data=json.loads(row['data']) if isinstance(row['data'], str) else row['data'],
                            metadata=json.loads(row['metadata']) if isinstance(row['metadata'], str) else row['metadata']
                        )
                        records.append(record)
                    
                    return records
                    
            except Exception as e:
                logger.error(f"æŸ¥è¯¢PostgreSQLå¤±è´¥: {e}")
                raise
        
        async def write_decision_record(self, record: DecisionRecord):
            """å†™å…¥å†³ç­–è®°å½•"""
            try:
                async with self.pool.acquire() as conn:
                    await conn.execute('''
                        INSERT INTO decision_records 
                        (decision_id, timestamp, model_version, input_state, output_action,
                         model_outputs, feature_importance, risk_metrics, execution_time_ms)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                        ON CONFLICT (decision_id) DO NOTHING
                    ''', 
                        record.decision_id, record.timestamp, record.model_version,
                        json.dumps(record.input_state.to_dict()),
                        json.dumps(record.output_action.to_dict()),
                        json.dumps(record.model_outputs),
                        json.dumps(record.feature_importance),
                        json.dumps(record.risk_metrics),
                        record.execution_time_ms
                    )
                    
            except Exception as e:
                logger.error(f"å†™å…¥å†³ç­–è®°å½•å¤±è´¥: {e}")
                raise
        
        async def get_decision_record(self, decision_id: str) -> Optional[DecisionRecord]:
            """èŽ·å–å†³ç­–è®°å½•"""
            try:
                async with self.pool.acquire() as conn:
                    row = await conn.fetchrow(
                        "SELECT * FROM decision_records WHERE decision_id = $1",
                        decision_id
                    )
                    
                    if row:
                        return DecisionRecord(
                            decision_id=row['decision_id'],
                            timestamp=row['timestamp'],
                            model_version=row['model_version'],
                            input_state=TradingState.from_dict(json.loads(row['input_state'])),
                            output_action=TradingAction.from_dict(json.loads(row['output_action'])),
                            model_outputs=json.loads(row['model_outputs']),
                            feature_importance=json.loads(row['feature_importance']),
                            risk_metrics=json.loads(row['risk_metrics']),
                            execution_time_ms=row['execution_time_ms']
                        )
                    
                    return None
                    
            except Exception as e:
                logger.error(f"èŽ·å–å†³ç­–è®°å½•å¤±è´¥: {e}")
                raise
    
    
    class AuditLogger:
        """å®¡è®¡æ—¥å¿—å™¨"""
        
        def __init__(self, config: Dict[str, Any]):
            self.config = config
            self.batch_records: List[AuditRecord] = []
            self.batch_lock = asyncio.Lock()
            self.is_running = False
            self.flush_task = None
            
            # åˆå§‹åŒ–æ•°æ®åº“æŽ¥å£
            self._init_databases()
        
        def _init_databases(self):
            """åˆå§‹åŒ–æ•°æ®åº“æŽ¥å£"""
            # InfluxDBé…ç½®
            influx_config = self.config.get('influxdb', {})
            if influx_config:
                self.timeseries_db = InfluxDBInterface(
                    url=influx_config.get('url', 'http://localhost:8086'),
                    token=influx_config.get('token', ''),
                    org=influx_config.get('org', 'trading'),
                    bucket=influx_config.get('bucket', 'audit')
                )
            else:
                self.timeseries_db = None
            
            # PostgreSQLé…ç½®
            postgres_url = self.config.get('relational_db_url', 'postgresql://localhost:5432/audit')
            self.relational_db = PostgreSQLInterface(postgres_url)
        
        async def start(self):
            """å¯åŠ¨å®¡è®¡æ—¥å¿—å™¨"""
            if self.is_running:
                return
            
            try:
                # è¿žæŽ¥æ•°æ®åº“
                if self.timeseries_db:
                    await self.timeseries_db.connect()
                await self.relational_db.connect()
                
                self.is_running = True
                
                # å¯åŠ¨å®šæœŸåˆ·æ–°ä»»åŠ¡
                flush_interval = self.config.get('flush_interval', 60)
                self.flush_task = asyncio.create_task(self._periodic_flush(flush_interval))
                
                logger.info("å®¡è®¡æ—¥å¿—å™¨å¯åŠ¨æˆåŠŸ")
                
            except Exception as e:
                logger.error(f"å®¡è®¡æ—¥å¿—å™¨å¯åŠ¨å¤±è´¥: {e}")
                raise
        
        async def stop(self):
            """åœæ­¢å®¡è®¡æ—¥å¿—å™¨"""
            if not self.is_running:
                return
            
            self.is_running = False
            
            # åœæ­¢å®šæœŸåˆ·æ–°ä»»åŠ¡
            if self.flush_task:
                self.flush_task.cancel()
                try:
                    await self.flush_task
                except asyncio.CancelledError:
                    pass
            
            # åˆ·æ–°å‰©ä½™è®°å½•
            await self._flush_batch()
            
            # æ–­å¼€æ•°æ®åº“è¿žæŽ¥
            if self.timeseries_db:
                await self.timeseries_db.disconnect()
            await self.relational_db.disconnect()
            
            logger.info("å®¡è®¡æ—¥å¿—å™¨å·²åœæ­¢")
        
        async def log_trading_decision(self, session_id: str, model_version: str,
                                     input_state: TradingState, output_action: TradingAction,
                                     model_outputs: Dict[str, Any],
                                     feature_importance: Dict[str, float],
                                     execution_time_ms: Optional[float] = None):
            """è®°å½•äº¤æ˜“å†³ç­–"""
            try:
                # ç”Ÿæˆå†³ç­–ID
                decision_id = self._generate_record_id()
                
                # åˆ›å»ºå†³ç­–è®°å½•
                decision_record = DecisionRecord(
                    decision_id=decision_id,
                    timestamp=datetime.now(),
                    model_version=model_version,
                    input_state=input_state,
                    output_action=output_action,
                    model_outputs=model_outputs,
                    feature_importance=feature_importance,
                    risk_metrics=self._calculate_risk_metrics(output_action),
                    execution_time_ms=execution_time_ms
                )
                
                # å†™å…¥å†³ç­–è®°å½•è¡¨
                await self.relational_db.write_decision_record(decision_record)
                
                # åˆ›å»ºå®¡è®¡è®°å½•
                audit_record = AuditRecord(
                    record_id=self._generate_record_id(),
                    timestamp=datetime.now(),
                    event_type="trading_decision",
                    user_id="system",
                    session_id=session_id,
                    model_version=model_version,
                    data={
                        "decision_id": decision_id,
                        "target_weights": output_action.target_weights.tolist(),
                        "confidence": output_action.confidence,
                        "portfolio_value": input_state.total_value,
                        "cash": input_state.cash
                    },
                    metadata={
                        "execution_time_ms": execution_time_ms,
                        "feature_count": len(feature_importance),
                        "model_output_keys": list(model_outputs.keys())
                    }
                )
                
                # æ·»åŠ åˆ°æ‰¹æ¬¡
                async with self.batch_lock:
                    self.batch_records.append(audit_record)
                    
                    # å¦‚æžœæ‰¹æ¬¡æ»¡äº†ï¼Œç«‹å³åˆ·æ–°
                    if len(self.batch_records) >= self.config.get('batch_size', 100):
                        await self._flush_batch()
                
                logger.debug(f"è®°å½•äº¤æ˜“å†³ç­–: {decision_id}")
                
            except Exception as e:
                logger.error(f"è®°å½•äº¤æ˜“å†³ç­–å¤±è´¥: {e}")
                raise
        
        async def log_transaction_execution(self, session_id: str, transaction: TransactionRecord,
                                          execution_details: Dict[str, Any]):
            """è®°å½•äº¤æ˜“æ‰§è¡Œ"""
            try:
                audit_record = AuditRecord(
                    record_id=self._generate_record_id(),
                    timestamp=datetime.now(),
                    event_type="transaction_execution",
                    user_id="system",
                    session_id=session_id,
                    model_version="execution",  # äº¤æ˜“æ‰§è¡Œä½¿ç”¨ç‰¹æ®Šæ ‡è¯†
                    data={
                        "symbol": transaction.symbol,
                        "action_type": transaction.action_type,
                        "quantity": transaction.quantity,
                        "price": transaction.price,
                        "commission": transaction.commission,
                        "stamp_tax": transaction.stamp_tax,
                        "slippage": transaction.slippage,
                        "total_cost": transaction.total_cost,
                        **execution_details
                    },
                    metadata={
                        "transaction_value": transaction.get_transaction_value(),
                        "cost_ratio": transaction.get_cost_ratio()
                    }
                )
                
                async with self.batch_lock:
                    self.batch_records.append(audit_record)
                
                logger.debug(f"è®°å½•äº¤æ˜“æ‰§è¡Œ: {transaction.symbol} {transaction.action_type}")
                
            except Exception as e:
                logger.error(f"è®°å½•äº¤æ˜“æ‰§è¡Œå¤±è´¥: {e}")
                raise
        
        async def log_risk_violation(self, session_id: str, model_version: str,
                                   violation_type: str, violation_details: Dict[str, Any]):
            """è®°å½•é£Žé™©è¿è§„"""
            try:
                audit_record = AuditRecord(
                    record_id=self._generate_record_id(),
                    timestamp=datetime.now(),
                    event_type="risk_violation",
                    user_id="system",
                    session_id=session_id,
                    model_version=model_version,
                    data={
                        "violation_type": violation_type,
                        **violation_details
                    },
                    metadata={
                        "severity": violation_details.get("severity", "medium")
                    }
                )
                
                async with self.batch_lock:
                    self.batch_records.append(audit_record)
                
                logger.warning(f"è®°å½•é£Žé™©è¿è§„: {violation_type}")
                
            except Exception as e:
                logger.error(f"è®°å½•é£Žé™©è¿è§„å¤±è´¥: {e}")
                raise
        
        async def _flush_batch(self):
            """åˆ·æ–°æ‰¹æ¬¡è®°å½•"""
            if not self.batch_records:
                return
            
            try:
                records_to_flush = self.batch_records.copy()
                self.batch_records.clear()
                
                # å†™å…¥æ—¶åºæ•°æ®åº“
                if self.timeseries_db:
                    await self.timeseries_db.write_records(records_to_flush)
                
                # å†™å…¥å…³ç³»æ•°æ®åº“
                await self.relational_db.write_records(records_to_flush)
                
                logger.debug(f"åˆ·æ–°{len(records_to_flush)}æ¡å®¡è®¡è®°å½•")
                
            except Exception as e:
                logger.error(f"åˆ·æ–°æ‰¹æ¬¡è®°å½•å¤±è´¥: {e}")
                # å°†è®°å½•é‡æ–°åŠ å…¥æ‰¹æ¬¡
                async with self.batch_lock:
                    self.batch_records.extend(records_to_flush)
                raise
        
        async def _periodic_flush(self, interval: int):
            """å®šæœŸåˆ·æ–°"""
            while self.is_running:
                try:
                    await asyncio.sleep(interval)
                    async with self.batch_lock:
                        await self._flush_batch()
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"å®šæœŸåˆ·æ–°å¤±è´¥: {e}")
        
        def _generate_record_id(self) -> str:
            """ç”Ÿæˆè®°å½•ID"""
            return str(uuid.uuid4())
        
        def _calculate_risk_metrics(self, action: TradingAction) -> Dict[str, float]:
            """è®¡ç®—é£Žé™©æŒ‡æ ‡"""
            return {
                "concentration": action.get_concentration(),
                "active_positions": float(action.get_active_positions()),
                "max_weight": float(action.target_weights.max()),
                "min_weight": float(action.target_weights.min())
            }
    
    
    class AuditQueryInterface:
        """å®¡è®¡æŸ¥è¯¢æŽ¥å£"""
        
        def __init__(self, config: Dict[str, Any]):
            self.config = config
            postgres_url = config.get('relational_db_url', 'postgresql://localhost:5432/audit')
            self.relational_db = PostgreSQLInterface(postgres_url)
        
        async def connect(self):
            """è¿žæŽ¥æ•°æ®åº“"""
            await self.relational_db.connect()
        
        async def disconnect(self):
            """æ–­å¼€æ•°æ®åº“è¿žæŽ¥"""
            await self.relational_db.disconnect()
        
        async def query_by_time_range(self, start_time: datetime, end_time: datetime,
                                    event_type: Optional[str] = None,
                                    limit: Optional[int] = None) -> List[AuditRecord]:
            """æŒ‰æ—¶é—´èŒƒå›´æŸ¥è¯¢"""
            kwargs = {
                'start_time': start_time,
                'end_time': end_time
            }
            
            if event_type:
                kwargs['event_type'] = event_type
            
            if limit:
                kwargs['limit'] = limit
            
            return await self.relational_db.query_records(**kwargs)
        
        async def query_by_session(self, session_id: str,
                                 limit: Optional[int] = None) -> List[AuditRecord]:
            """æŒ‰ä¼šè¯æŸ¥è¯¢"""
            kwargs = {'session_id': session_id}
            if limit:
                kwargs['limit'] = limit
            
            return await self.relational_db.query_records(**kwargs)
        
        async def query_by_model_version(self, model_version: str,
                                       limit: Optional[int] = None) -> List[AuditRecord]:
            """æŒ‰æ¨¡åž‹ç‰ˆæœ¬æŸ¥è¯¢"""
            kwargs = {'model_version': model_version}
            if limit:
                kwargs['limit'] = limit
            
            return await self.relational_db.query_records(**kwargs)
        
        async def get_decision_details(self, decision_id: str) -> Optional[DecisionRecord]:
            """èŽ·å–å†³ç­–è¯¦æƒ…"""
            return await self.relational_db.get_decision_record(decision_id)
    
    
    class DataRetentionManager:
        """æ•°æ®ä¿ç•™ç®¡ç†å™¨"""
        
        def __init__(self, config: Dict[str, Any]):
            self.config = config
            self.retention_days = config.get('retention_days', 1825)  # é»˜è®¤5å¹´
            postgres_url = config.get('relational_db_url', 'postgresql://localhost:5432/audit')
            self.relational_db = PostgreSQLInterface(postgres_url)
            
            # åˆå§‹åŒ–æŸ¥è¯¢æŽ¥å£
            self.query_interface = AuditQueryInterface(config)
        
        async def start(self):
            """å¯åŠ¨æ•°æ®ä¿ç•™ç®¡ç†å™¨"""
            await self.relational_db.connect()
            await self.query_interface.connect()
            
            # å¯åŠ¨å®šæœŸæ¸…ç†ä»»åŠ¡
            cleanup_interval = self.config.get('cleanup_interval_hours', 24) * 3600
            self.cleanup_task = asyncio.create_task(self._periodic_cleanup(cleanup_interval))
            
            logger.info("æ•°æ®ä¿ç•™ç®¡ç†å™¨å¯åŠ¨æˆåŠŸ")
        
        async def stop(self):
            """åœæ­¢æ•°æ®ä¿ç•™ç®¡ç†å™¨"""
            if hasattr(self, 'cleanup_task'):
                self.cleanup_task.cancel()
                try:
                    await self.cleanup_task
                except asyncio.CancelledError:
                    pass
            
            await self.query_interface.disconnect()
            await self.relational_db.disconnect()
            
            logger.info("æ•°æ®ä¿ç•™ç®¡ç†å™¨å·²åœæ­¢")
        
        async def cleanup_expired_data(self):
            """æ¸…ç†è¿‡æœŸæ•°æ®"""
            try:
                retention_date = self._calculate_retention_date()
                
                async with self.relational_db.pool.acquire() as conn:
                    # æ¸…ç†è¿‡æœŸçš„å®¡è®¡è®°å½•
                    audit_deleted = await conn.execute(
                        "DELETE FROM audit_records WHERE timestamp < $1",
                        retention_date
                    )
                    
                    # æ¸…ç†è¿‡æœŸçš„å†³ç­–è®°å½•
                    decision_deleted = await conn.execute(
                        "DELETE FROM decision_records WHERE timestamp < $1",
                        retention_date
                    )
                    
                    # æ¸…ç†è¿‡æœŸçš„åˆè§„æŠ¥å‘Š
                    report_deleted = await conn.execute(
                        "DELETE FROM compliance_reports WHERE period_end < $1",
                        retention_date
                    )
                    
                    logger.info(f"æ¸…ç†è¿‡æœŸæ•°æ®å®Œæˆ: å®¡è®¡è®°å½•{audit_deleted}æ¡, "
                               f"å†³ç­–è®°å½•{decision_deleted}æ¡, åˆè§„æŠ¥å‘Š{report_deleted}æ¡")
                    
            except Exception as e:
                logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
                raise
        
        async def get_data_statistics(self) -> Dict[str, Any]:
            """èŽ·å–æ•°æ®ç»Ÿè®¡ä¿¡æ¯"""
            try:
                async with self.relational_db.pool.acquire() as conn:
                    # å®¡è®¡è®°å½•ç»Ÿè®¡
                    audit_stats = await conn.fetchrow('''
                        SELECT 
                            COUNT(*) as total_records,
                            MIN(timestamp) as oldest_record,
                            MAX(timestamp) as newest_record
                        FROM audit_records
                    ''')
                    
                    # å†³ç­–è®°å½•ç»Ÿè®¡
                    decision_stats = await conn.fetchrow('''
                        SELECT COUNT(*) as total_decisions
                        FROM decision_records
                    ''')
                    
                    # åˆè§„æŠ¥å‘Šç»Ÿè®¡
                    report_stats = await conn.fetchrow('''
                        SELECT COUNT(*) as total_reports
                        FROM compliance_reports
                    ''')
                    
                    # æ•°æ®åº“å¤§å°ç»Ÿè®¡ï¼ˆPostgreSQLç‰¹å®šï¼‰
                    size_stats = await conn.fetchrow('''
                        SELECT pg_size_pretty(pg_database_size(current_database())) as db_size
                    ''')
                    
                    return {
                        'total_records': audit_stats['total_records'],
                        'total_decisions': decision_stats['total_decisions'],
                        'total_reports': report_stats['total_reports'],
                        'oldest_record': audit_stats['oldest_record'].isoformat() if audit_stats['oldest_record'] else None,
                        'newest_record': audit_stats['newest_record'].isoformat() if audit_stats['newest_record'] else None,
                        'database_size': size_stats['db_size'],
                        'retention_days': self.retention_days
                    }
                    
            except Exception as e:
                logger.error(f"èŽ·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
                raise
        
        async def _periodic_cleanup(self, interval: int):
            """å®šæœŸæ¸…ç†ä»»åŠ¡"""
            while True:
                try:
                    await asyncio.sleep(interval)
                    await self.cleanup_expired_data()
                except asyncio.CancelledError:
                    break
                except Exception as e:
                    logger.error(f"å®šæœŸæ¸…ç†ä»»åŠ¡å¤±è´¥: {e}")
        
        def _calculate_retention_date(self) -> datetime:
            """è®¡ç®—æ•°æ®ä¿ç•™æˆªæ­¢æ—¥æœŸ"""
            return datetime.now() - timedelta(days=self.retention_days)
    
    
    class ComplianceReportGenerator:
        """åˆè§„æŠ¥å‘Šç”Ÿæˆå™¨"""
        
        def __init__(self, config: Dict[str, Any]):
            self.config = config
            self.query_interface = AuditQueryInterface(config)
        
        async def start(self):
            """å¯åŠ¨åˆè§„æŠ¥å‘Šç”Ÿæˆå™¨"""
            await self.query_interface.connect()
            logger.info("åˆè§„æŠ¥å‘Šç”Ÿæˆå™¨å¯åŠ¨æˆåŠŸ")
        
        async def stop(self):
            """åœæ­¢åˆè§„æŠ¥å‘Šç”Ÿæˆå™¨"""
            await self.query_interface.disconnect()
            logger.info("åˆè§„æŠ¥å‘Šç”Ÿæˆå™¨å·²åœæ­¢")
        
        async def generate_compliance_report(self, period_start: datetime, 
                                           period_end: datetime) -> ComplianceReport:
            """ç”Ÿæˆåˆè§„æŠ¥å‘Š"""
            try:
                # æŸ¥è¯¢æœŸé—´å†…çš„æ‰€æœ‰å†³ç­–è®°å½•
                decisions = await self.query_interface.query_by_time_range(
                    period_start, period_end, event_type="trading_decision"
                )
                
                # æŸ¥è¯¢æœŸé—´å†…çš„é£Žé™©è¿è§„è®°å½•
                violations = await self.query_interface.query_by_time_range(
                    period_start, period_end, event_type="risk_violation"
                )
                
                # åˆ†æžé›†ä¸­åº¦
                concentration_analysis = await self._analyze_concentration(decisions)
                
                # åˆ†æžæ¨¡åž‹æ€§èƒ½
                model_performance = await self._calculate_model_performance(decisions)
                
                # è®¡ç®—åˆè§„åˆ†æ•°
                compliance_score = self._calculate_compliance_score(
                    len(decisions), len(violations), concentration_analysis
                )
                
                # åˆ›å»ºåˆè§„æŠ¥å‘Š
                report = ComplianceReport(
                    report_id=str(uuid.uuid4()),
                    generated_at=datetime.now(),
                    period_start=period_start,
                    period_end=period_end,
                    total_decisions=len(decisions),
                    risk_violations=[v.data for v in violations],
                    concentration_analysis=concentration_analysis,
                    model_performance=model_performance,
                    compliance_score=compliance_score
                )
                
                # ä¿å­˜æŠ¥å‘Š
                await self._save_report(report)
                
                logger.info(f"ç”Ÿæˆåˆè§„æŠ¥å‘Š: {report.report_id}")
                return report
                
            except Exception as e:
                logger.error(f"ç”Ÿæˆåˆè§„æŠ¥å‘Šå¤±è´¥: {e}")
                raise
        
        async def _analyze_concentration(self, decisions: List[AuditRecord]) -> Dict[str, float]:
            """åˆ†æžæŒä»“é›†ä¸­åº¦"""
            if not decisions:
                return {}
            
            concentrations = []
            max_weights = []
            
            for decision in decisions:
                target_weights = decision.data.get('target_weights', [])
                if target_weights:
                    weights = np.array(target_weights)
                    concentration = np.sum(weights ** 2)  # HerfindahlæŒ‡æ•°
                    concentrations.append(concentration)
                    max_weights.append(weights.max())
            
            if concentrations:
                return {
                    'avg_concentration': np.mean(concentrations),
                    'max_concentration': np.max(concentrations),
                    'min_concentration': np.min(concentrations),
                    'avg_max_weight': np.mean(max_weights),
                    'max_single_weight': np.max(max_weights)
                }
            
            return {}
        
        async def _calculate_model_performance(self, decisions: List[AuditRecord]) -> Dict[str, float]:
            """è®¡ç®—æ¨¡åž‹æ€§èƒ½"""
            if not decisions:
                return {}
            
            confidences = []
            execution_times = []
            
            for decision in decisions:
                confidence = decision.data.get('confidence', 0)
                confidences.append(confidence)
                
                execution_time = decision.metadata.get('execution_time_ms', 0)
                if execution_time:
                    execution_times.append(execution_time)
            
            performance = {
                'avg_confidence': np.mean(confidences),
                'min_confidence': np.min(confidences),
                'max_confidence': np.max(confidences)
            }
            
            if execution_times:
                performance.update({
                    'avg_execution_time_ms': np.mean(execution_times),
                    'max_execution_time_ms': np.max(execution_times),
                    'min_execution_time_ms': np.min(execution_times)
                })
            
            return performance
        
        def _calculate_compliance_score(self, total_decisions: int, violation_count: int,
                                      concentration_analysis: Dict[str, float]) -> float:
            """è®¡ç®—åˆè§„åˆ†æ•°"""
            if total_decisions == 0:
                return 1.0
            
            # åŸºç¡€åˆ†æ•°
            base_score = 1.0
            
            # è¿è§„æƒ©ç½š
            violation_penalty = min(0.5, violation_count / total_decisions)
            base_score -= violation_penalty
            
            # é›†ä¸­åº¦æƒ©ç½š
            if concentration_analysis:
                max_concentration = concentration_analysis.get('max_concentration', 0)
                if max_concentration > 0.5:  # é›†ä¸­åº¦è¿‡é«˜
                    concentration_penalty = min(0.3, (max_concentration - 0.5) * 0.6)
                    base_score -= concentration_penalty
            
            return max(0.0, base_score)
        
        async def _save_report(self, report: ComplianceReport):
            """ä¿å­˜åˆè§„æŠ¥å‘Š"""
            try:
                async with self.query_interface.relational_db.pool.acquire() as conn:
                    await conn.execute('''
                        INSERT INTO compliance_reports 
                        (report_id, generated_at, period_start, period_end, total_decisions,
                         risk_violations, concentration_analysis, model_performance, compliance_score)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    ''',
                        report.report_id, report.generated_at, report.period_start,
                        report.period_end, report.total_decisions,
                        json.dumps(report.risk_violations),
                        json.dumps(report.concentration_analysis),
                        json.dumps(report.model_performance),
                        report.compliance_score
                    )
                    
            except Exception as e:
                logger.error(f"ä¿å­˜åˆè§„æŠ¥å‘Šå¤±è´¥: {e}")
                raiseal_db = PostgreSQLInterface(postgres_url)
        
        async def connect(self):
            """è¿žæŽ¥æ•°æ®åº“"""
            await self.relational_db.connect()
        
        async def disconnect(self):
            """æ–­å¼€æ•°æ®åº“è¿žæŽ¥"""
            await self.relational_db.disconnect()
        
        async def cleanup_expired_data(self):
            """æ¸…ç†è¿‡æœŸæ•°æ®"""
            try:
                retention_date = self._calculate_retention_date()
                
                async with self.relational_db.pool.acquire() as conn:
                    # åˆ é™¤è¿‡æœŸçš„å®¡è®¡è®°å½•
                    deleted_audit = await conn.execute(
                        "DELETE FROM audit_records WHERE timestamp < $1",
                        retention_date
                    )
                    
                    # åˆ é™¤è¿‡æœŸçš„å†³ç­–è®°å½•
                    deleted_decisions = await conn.execute(
                        "DELETE FROM decision_records WHERE timestamp < $1",
                        retention_date
                    )
                    
                    logger.info(f"æ¸…ç†è¿‡æœŸæ•°æ®å®Œæˆ: å®¡è®¡è®°å½•{deleted_audit}, å†³ç­–è®°å½•{deleted_decisions}")
                    
            except Exception as e:
                logger.error(f"æ¸…ç†è¿‡æœŸæ•°æ®å¤±è´¥: {e}")
                raise
        
        async def get_data_statistics(self) -> Dict[str, Any]:
            """èŽ·å–æ•°æ®ç»Ÿè®¡"""
            try:
                async with self.relational_db.pool.acquire() as conn:
                    # å®¡è®¡è®°å½•ç»Ÿè®¡
                    audit_stats = await conn.fetchrow('''
                        SELECT 
                            COUNT(*) as total_records,
                            MIN(timestamp) as oldest_record,
                            MAX(timestamp) as newest_record
                        FROM audit_records
                    ''')
                    
                    # å†³ç­–è®°å½•ç»Ÿè®¡
                    decision_stats = await conn.fetchrow('''
                        SELECT COUNT(*) as total_decisions
                        FROM decision_records
                    ''')
                    
                    # æ•°æ®åº“å¤§å°ç»Ÿè®¡
                    size_stats = await conn.fetchrow('''
                        SELECT 
                            pg_size_pretty(pg_total_relation_size('audit_records')) as audit_size,
                            pg_size_pretty(pg_total_relation_size('decision_records')) as decision_size
                    ''')
                    
                    return {
                        'total_records': audit_stats['total_records'],
                        'total_decisions': decision_stats['total_decisions'],
                        'oldest_record': audit_stats['oldest_record'].isoformat() if audit_stats['oldest_record'] else None,
                        'newest_record': audit_stats['newest_record'].isoformat() if audit_stats['newest_record'] else None,
                        'audit_table_size': size_stats['audit_size'],
                        'decision_table_size': size_stats['decision_size']
                    }
                    
            except Exception as e:
                logger.error(f"èŽ·å–æ•°æ®ç»Ÿè®¡å¤±è´¥: {e}")
                raise
        
        def _calculate_retention_date(self) -> datetime:
            """è®¡ç®—ä¿ç•™æ—¥æœŸ"""
            return datetime.now() - timedelta(days=self.retention_days)
    
    
    class ComplianceReportGenerator:
        """åˆè§„æŠ¥å‘Šç”Ÿæˆå™¨"""
        
        def __init__(self, config: Dict[str, Any]):
            self.config = config
            self.query_interface = AuditQueryInterface(config)
        
        async def generate_compliance_report(self, period_start: datetime, 
                                           period_end: datetime) -> ComplianceReport:
            """ç”Ÿæˆåˆè§„æŠ¥å‘Š"""
            try:
                await self.query_interface.connect()
                
                # æŸ¥è¯¢æœŸé—´å†…çš„æ‰€æœ‰è®°å½•
                records = await self.query_interface.query_by_time_range(
                    period_start, period_end
                )
                
                # ç»Ÿè®¡å†³ç­–æ•°é‡
                decision_records = [r for r in records if r.event_type == 'trading_decision']
                total_decisions = len(decision_records)
                
                # åˆ†æžé£Žé™©è¿è§„
                risk_violations = [r for r in records if r.event_type == 'risk_violation']
                violation_analysis = self._analyze_violations(risk_violations)
                
                # åˆ†æžæŒä»“é›†ä¸­åº¦
                concentration_analysis = await self._analyze_concentration(decision_records)
                
                # è®¡ç®—æ¨¡åž‹æ€§èƒ½
                model_performance = await self._calculate_model_performance(decision_records)
                
                # è®¡ç®—åˆè§„åˆ†æ•°
                compliance_score = self._calculate_compliance_score(
                    total_decisions, len(risk_violations), concentration_analysis
                )
                
                report = ComplianceReport(
                    report_id=str(uuid.uuid4()),
                    generated_at=datetime.now(),
                    period_start=period_start,
                    period_end=period_end,
                    total_decisions=total_decisions,
                    risk_violations=violation_analysis,
                    concentration_analysis=concentration_analysis,
                    model_performance=model_performance,
                    compliance_score=compliance_score
                )
                
                # ä¿å­˜æŠ¥å‘Š
                await self._save_report(report)
                
                return report
                
            finally:
                await self.query_interface.disconnect()
        
        def _analyze_violations(self, violations: List[AuditRecord]) -> List[Dict[str, Any]]:
            """åˆ†æžè¿è§„æƒ…å†µ"""
            violation_summary = {}
            
            for violation in violations:
                violation_type = violation.data.get('violation_type', 'unknown')
                if violation_type not in violation_summary:
                    violation_summary[violation_type] = {
                        'count': 0,
                        'severity_counts': {'low': 0, 'medium': 0, 'high': 0}
                    }
                
                violation_summary[violation_type]['count'] += 1
                severity = violation.metadata.get('severity', 'medium')
                violation_summary[violation_type]['severity_counts'][severity] += 1
            
            return [
                {
                    'type': vtype,
                    'count': vdata['count'],
                    'severity_distribution': vdata['severity_counts']
                }
                for vtype, vdata in violation_summary.items()
            ]
        
        async def _analyze_concentration(self, decisions: List[AuditRecord]) -> Dict[str, float]:
            """åˆ†æžæŒä»“é›†ä¸­åº¦"""
            if not decisions:
                return {}
            
            concentrations = []
            max_weights = []
            
            for decision in decisions:
                # ä»Žå†³ç­–è®°å½•ä¸­æå–é›†ä¸­åº¦ä¿¡æ¯
                decision_id = decision.data.get('decision_id')
                if decision_id:
                    decision_detail = await self.query_interface.get_decision_details(decision_id)
                    if decision_detail:
                        concentration = decision_detail.risk_metrics.get('concentration', 0)
                        max_weight = decision_detail.risk_metrics.get('max_weight', 0)
                        concentrations.append(concentration)
                        max_weights.append(max_weight)
            
            if concentrations:
                return {
                    'avg_concentration': np.mean(concentrations),
                    'max_concentration': np.max(concentrations),
                    'min_concentration': np.min(concentrations),
                    'avg_max_weight': np.mean(max_weights),
                    'max_single_weight': np.max(max_weights)
                }
            
            return {}
        
        async def _calculate_model_performance(self, decisions: List[AuditRecord]) -> Dict[str, float]:
            """è®¡ç®—æ¨¡åž‹æ€§èƒ½"""
            if not decisions:
                return {}
            
            confidences = []
            execution_times = []
            
            for decision in decisions:
                confidence = decision.data.get('confidence', 0)
                confidences.append(confidence)
                
                execution_time = decision.metadata.get('execution_time_ms', 0)
                if execution_time:
                    execution_times.append(execution_time)
            
            performance = {
                'avg_confidence': np.mean(confidences),
                'min_confidence': np.min(confidences),
                'max_confidence': np.max(confidences)
            }
            
            if execution_times:
                performance.update({
                    'avg_execution_time_ms': np.mean(execution_times),
                    'max_execution_time_ms': np.max(execution_times),
                    'min_execution_time_ms': np.min(execution_times)
                })
            
            return performance
        
        def _calculate_compliance_score(self, total_decisions: int, violation_count: int,
                                      concentration_analysis: Dict[str, float]) -> float:
            """è®¡ç®—åˆè§„åˆ†æ•°"""
            if total_decisions == 0:
                return 1.0
            
            # åŸºç¡€åˆ†æ•°
            base_score = 1.0
            
            # è¿è§„æƒ©ç½š
            violation_penalty = min(0.5, violation_count / total_decisions)
            base_score -= violation_penalty
            
            # é›†ä¸­åº¦æƒ©ç½š
            if concentration_analysis:
                max_concentration = concentration_analysis.get('max_concentration', 0)
                if max_concentration > 0.5:  # é›†ä¸­åº¦è¿‡é«˜
                    concentration_penalty = min(0.3, (max_concentration - 0.5) * 0.6)
                    base_score -= concentration_penalty
            
            return max(0.0, base_score)
        
        async def _save_report(self, report: ComplianceReport):
            """ä¿å­˜åˆè§„æŠ¥å‘Š"""
            try:
                async with self.query_interface.relational_db.pool.acquire() as conn:
                    await conn.execute('''
                        INSERT INTO compliance_reports 
                        (report_id, generated_at, period_start, period_end, total_decisions,
                         risk_violations, concentration_analysis, model_performance, compliance_score)
                        VALUES ($1, $2, $3, $4, $5, $6, $7, $8, $9)
                    ''',
                        report.report_id, report.generated_at, report.period_start,
                        report.period_end, report.total_decisions,
                        json.dumps(report.risk_violations),
                        json.dumps(report.concentration_analysis),
                        json.dumps(report.model_performance),
                        report.compliance_score
                    )
                    
            except Exception as e:
                logger.error(f"ä¿å­˜åˆè§„æŠ¥å‘Šå¤±è´¥: {e}")
                raise
    ]]></file>
  <file path="src/rl_trading_system/audit/__init__.py"><![CDATA[
    """
    å®¡è®¡æ—¥å¿—ç³»ç»Ÿæ¨¡å—
    æä¾›äº¤æ˜“å†³ç­–è®°å½•ã€å­˜å‚¨æœºåˆ¶ã€æŸ¥è¯¢æŽ¥å£å’Œæ•°æ®å®Œæ•´æ€§ç®¡ç†åŠŸèƒ½
    """
    
    from .audit_logger import (
        AuditLogger,
        AuditRecord,
        DecisionRecord,
        ComplianceReport,
        AuditQueryInterface,
        DataRetentionManager,
        ComplianceReportGenerator
    )
    
    __all__ = [
        'AuditLogger',
        'AuditRecord', 
        'DecisionRecord',
        'ComplianceReport',
        'AuditQueryInterface',
        'DataRetentionManager',
        'ComplianceReportGenerator'
    ]
    ]]></file>
  <file path="src/rl_trading_system/api/__init__.py"><![CDATA[
    """APIæŽ¥å£æ¨¡å—"""
    
    from .rest_api import TradingAPI
    from .auth import AuthManager
    from .rate_limiter import RateLimiter
    
    __all__ = [
        "TradingAPI",
        "AuthManager",
        "RateLimiter"
    ]
    ]]></file>
  <file path=".kiro/specs/qlib-trading-agent/tasks.md"><![CDATA[
    - [x] 1. é¡¹ç›®çŽ¯å¢ƒé…ç½®ä¸Žåˆå§‹åŒ–
      - åˆ›å»ºæ ‡å‡†Pythoné¡¹ç›®ç›®å½•ç»“æž„ï¼ˆsrc/, tests/, docs/, config/ï¼‰
      - å®‰è£…æ ¸å¿ƒä¾èµ–ï¼šPyTorch, Qlib, Akshare, OpenAI Gym, pytest
      - é…ç½®Gitä»“åº“ã€.gitignoreå’Œä»£ç è´¨é‡å·¥å…·ï¼ˆblack, flake8, mypyï¼‰
      - ç¼–å†™requirements.txtã€setup.pyå’Œé¡¹ç›®é…ç½®æ–‡ä»¶
      - é…ç½®æ—¥å¿—ç³»ç»Ÿï¼ˆä½¿ç”¨loguruï¼‰å’Œæµ‹è¯•æ¡†æž¶
      - _éœ€æ±‚: 10.1, 10.2_
    
    - [x] 2. é…ç½®ç®¡ç†ç³»ç»Ÿå®žçŽ°
      - [x] 2.1 ç¼–å†™é…ç½®ç®¡ç†æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•YAMLé…ç½®æ–‡ä»¶åŠ è½½å’ŒéªŒè¯
        - æµ‹è¯•çŽ¯å¢ƒå˜é‡è¦†ç›–æœºåˆ¶
        - æµ‹è¯•é…ç½®å‚æ•°ç±»åž‹æ£€æŸ¥å’Œé»˜è®¤å€¼
        - _éœ€æ±‚: 10.1_
    
      - [x] 2.2 å®žçŽ°é…ç½®ç®¡ç†å™¨
        - åˆ›å»ºYAMLé…ç½®æ–‡ä»¶æ¨¡æ¿ï¼ˆmodel_config.yaml, trading_config.yamlç­‰ï¼‰
        - å®žçŽ°ConfigManagerç±»ï¼Œæ”¯æŒçŽ¯å¢ƒå˜é‡è¦†ç›–
        - å®žçŽ°é…ç½®éªŒè¯æœºåˆ¶å’Œå‚æ•°ç±»åž‹æ£€æŸ¥
        - _éœ€æ±‚: 10.1_
    
    - [x] 3. æ ¸å¿ƒæ•°æ®æ¨¡åž‹å®žçŽ°
      - [x] 3.1 ç¼–å†™æ•°æ®ç»“æž„æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•MarketDataã€FeatureVectorã€TradingStateç­‰æ•°æ®ç±»
        - æµ‹è¯•æ•°æ®éªŒè¯ã€åºåˆ—åŒ–å’Œååºåˆ—åŒ–åŠŸèƒ½
        - æµ‹è¯•è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸æƒ…å†µå¤„ç†
        - _éœ€æ±‚: 1.1, 1.3_
    
      - [x] 3.2 å®žçŽ°æ ¸å¿ƒæ•°æ®ç±»
        - å®žçŽ°MarketDataã€FeatureVectorã€TradingStateã€TradingActionç­‰æ•°æ®ç±»
        - å®žçŽ°æ•°æ®éªŒè¯æ–¹æ³•å’Œç±»åž‹æ£€æŸ¥å‡½æ•°
        - å®žçŽ°æ•°æ®åºåˆ—åŒ–å’Œååºåˆ—åŒ–åŠŸèƒ½
        - _éœ€æ±‚: 1.1, 1.3, 1.4_
    
    - [x] 4. æ•°æ®æŽ¥å£å±‚å®žçŽ°
      - [x] 4.1 ç¼–å†™æ•°æ®æŽ¥å£æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•DataInterfaceæŠ½è±¡ç±»å’Œå…·ä½“å®žçŽ°
        - æµ‹è¯•Qlibå’ŒAkshareæ•°æ®èŽ·å–åŠŸèƒ½
        - æµ‹è¯•æ•°æ®æ ¼å¼ç»Ÿä¸€å’Œé”™è¯¯å¤„ç†
        - _éœ€æ±‚: 1.1, 1.5_
    
      - [x] 4.2 å®žçŽ°æ•°æ®æŽ¥å£
        - å®žçŽ°DataInterfaceæŠ½è±¡åŸºç±»
        - å®žçŽ°QlibDataInterfaceå’ŒAkshareDataInterface
        - å®žçŽ°ç»Ÿä¸€çš„æ•°æ®æ ¼å¼æ ‡å‡†å’Œç¼“å­˜æœºåˆ¶
        - å®žçŽ°æ•°æ®è´¨é‡æ£€æŸ¥å·¥å…·
        - _éœ€æ±‚: 1.1, 1.5_
    
    - [x] 5. ç‰¹å¾å·¥ç¨‹æ¨¡å—å®žçŽ°
      - [x] 5.1 ç¼–å†™ç‰¹å¾å·¥ç¨‹æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡½æ•°ï¼ˆRSI, MACD, Bollinger Bandsç­‰ï¼‰
        - æµ‹è¯•åŸºæœ¬é¢å› å­å’Œå¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾è®¡ç®—
        - æµ‹è¯•ç‰¹å¾æ ‡å‡†åŒ–å’Œç¼ºå¤±å€¼å¤„ç†é€»è¾‘
        - _éœ€æ±‚: 1.2, 1.3_
    
      - [x] 5.2 å®žçŽ°ç‰¹å¾å·¥ç¨‹å™¨
        - å®žçŽ°FeatureEngineerç±»å’ŒæŠ€æœ¯æŒ‡æ ‡è®¡ç®—å‡½æ•°
        - å®žçŽ°åŸºæœ¬é¢å› å­æå–å’Œå¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
        - å®žçŽ°ç‰¹å¾æ ‡å‡†åŒ–ã€ç¼ºå¤±å€¼å¤„ç†å’Œç‰¹å¾é€‰æ‹©å·¥å…·
        - _éœ€æ±‚: 1.2, 1.3_
    
    - [x] 6. æ•°æ®é¢„å¤„ç†ç®¡é“å®žçŽ°
      - [x] 6.1 ç¼–å†™æ•°æ®é¢„å¤„ç†æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ•°æ®æ¸…æ´—å’Œé¢„å¤„ç†æµæ°´çº¿
        - æµ‹è¯•æ•°æ®è´¨é‡æ£€æŸ¥å’Œå¼‚å¸¸æ•°æ®å¤„ç†
        - æµ‹è¯•æ•°æ®ç¼“å­˜å’Œæ‰¹å¤„ç†åŠŸèƒ½
        - _éœ€æ±‚: 1.4_
    
      - [x] 6.2 å®žçŽ°æ•°æ®é¢„å¤„ç†ç®¡é“
        - å®žçŽ°DataProcessorç±»å’Œå®Œæ•´çš„é¢„å¤„ç†æµç¨‹
        - å®žçŽ°æ•°æ®æ¸…æ´—ã€ç‰¹å¾è®¡ç®—å’Œæ ‡å‡†åŒ–æµç¨‹
        - å®žçŽ°æ•°æ®ç¼“å­˜æœºåˆ¶ï¼ˆRedis/æœ¬åœ°æ–‡ä»¶ï¼‰
        - _éœ€æ±‚: 1.4_
    
    - [x] 7. Transformerç¼–ç å™¨å®žçŽ°
      - [x] 7.1 ç¼–å†™ä½ç½®ç¼–ç æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•PositionalEncodingç±»çš„æ­£å¼¦ä½™å¼¦ç¼–ç 
        - æµ‹è¯•å¯å­¦ä¹ ä½ç½®ç¼–ç å’Œç›¸å¯¹ä½ç½®ç¼–ç 
        - æµ‹è¯•ä¸åŒåºåˆ—é•¿åº¦ä¸‹çš„ä½ç½®ç¼–ç æ•ˆæžœ
        - _éœ€æ±‚: 2.1, 2.2_
    
      - [x] 7.2 å®žçŽ°ä½ç½®ç¼–ç ç»„ä»¶
        - å®žçŽ°PositionalEncodingç±»ï¼Œæ”¯æŒå¤šç§ç¼–ç æ–¹å¼
        - å®žçŽ°æ­£å¼¦ä½™å¼¦ä½ç½®ç¼–ç å’Œå¯å­¦ä¹ ä½ç½®ç¼–ç 
        - æ”¯æŒç›¸å¯¹ä½ç½®ç¼–ç å’Œå¯å˜é•¿åº¦åºåˆ—
        - _éœ€æ±‚: 2.1, 2.2_
    
      - [x] 7.3 ç¼–å†™æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•TemporalAttentionç±»çš„æ³¨æ„åŠ›èšåˆ
        - æµ‹è¯•æ³¨æ„åŠ›æƒé‡çš„åˆç†æ€§å’Œèšåˆæ•ˆæžœ
        - æµ‹è¯•å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶çš„æ€§èƒ½
        - _éœ€æ±‚: 2.3_
    
      - [x] 7.4 å®žçŽ°æ—¶é—´æ³¨æ„åŠ›æœºåˆ¶
        - å®žçŽ°TemporalAttentionç±»å’Œæ—¶é—´ç»´åº¦èšåˆ
        - å®žçŽ°å¤šå¤´æ³¨æ„åŠ›æœºåˆ¶å’Œæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
        - ä¼˜åŒ–æ³¨æ„åŠ›è®¡ç®—çš„å†…å­˜ä½¿ç”¨å’Œè®¡ç®—æ•ˆçŽ‡
        - _éœ€æ±‚: 2.3_
    
      - [x] 7.5 ç¼–å†™Transformerç¼–ç å™¨æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•TimeSeriesTransformerç±»çš„å®Œæ•´åŠŸèƒ½
        - æµ‹è¯•å¤šå±‚ç¼–ç å™¨å’Œå‰å‘ä¼ æ’­é€»è¾‘
        - æµ‹è¯•æ¨¡åž‹åœ¨ä¸åŒè¾“å…¥ç»´åº¦ä¸‹çš„è¡¨çŽ°
        - _éœ€æ±‚: 2.1, 2.4_
    
      - [x] 7.6 å®žçŽ°å®Œæ•´çš„Transformerç¼–ç å™¨
        - å®žçŽ°TimeSeriesTransformerç±»å’Œå¤šå±‚ç¼–ç å™¨æž¶æž„
        - å®žçŽ°å‰å‘ä¼ æ’­é€»è¾‘å’Œæ¢¯åº¦è®¡ç®—
        - æ”¯æŒå¯å˜é•¿åº¦åºåˆ—å’Œæ‰¹å¤„ç†
        - _éœ€æ±‚: 2.1, 2.4, 2.5_
    
    - [x] 8. SACæ™ºèƒ½ä½“æ ¸å¿ƒç»„ä»¶å®žçŽ°
      - [x] 8.1 ç¼–å†™Actorç½‘ç»œæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•Actorç½‘ç»œçš„ç­–ç•¥è¾“å‡ºå’Œæƒé‡çº¦æŸ
        - æµ‹è¯•åŠ¨ä½œç”Ÿæˆçš„æœ‰æ•ˆæ€§å’Œæ¦‚çŽ‡åˆ†å¸ƒ
        - æµ‹è¯•é‡å‚æ•°åŒ–æŠ€å·§å’Œæ¢¯åº¦è®¡ç®—
        - _éœ€æ±‚: 4.2, 4.4_
    
      - [x] 8.2 å®žçŽ°Actorç½‘ç»œ
        - å®žçŽ°Actorç±»å’Œç­–ç•¥ç½‘ç»œæž¶æž„
        - å®žçŽ°æŠ•èµ„ç»„åˆæƒé‡åˆ†å¸ƒè¾“å‡ºå’ŒSoftmaxçº¦æŸ
        - å®žçŽ°é‡å‚æ•°åŒ–æŠ€å·§å’Œç¡®å®šæ€§/éšæœºåŠ¨ä½œç”Ÿæˆ
        - _éœ€æ±‚: 4.2, 4.4_
    
      - [x] 8.3 ç¼–å†™Criticç½‘ç»œæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•Criticç½‘ç»œçš„Qå€¼ä¼°è®¡åŠŸèƒ½
        - æµ‹è¯•åŒCriticæž¶æž„å’Œç›®æ ‡ç½‘ç»œæœºåˆ¶
        - æµ‹è¯•Qå€¼ä¼°è®¡çš„å‡†ç¡®æ€§å’Œç¨³å®šæ€§
        - _éœ€æ±‚: 4.1, 4.2_
    
      - [x] 8.4 å®žçŽ°Criticç½‘ç»œ
        - å®žçŽ°Criticç±»å’ŒåŒQç½‘ç»œæž¶æž„
        - å®žçŽ°çŠ¶æ€-åŠ¨ä½œä»·å€¼ä¼°è®¡å’Œç›®æ ‡ç½‘ç»œ
        - å®žçŽ°è½¯æ›´æ–°æœºåˆ¶å’Œç½‘ç»œå‚æ•°åŒæ­¥
        - _éœ€æ±‚: 4.1, 4.2_
    
      - [x] 8.5 ç¼–å†™ç»éªŒå›žæ”¾ç¼“å†²åŒºæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•ReplayBufferçš„å­˜å‚¨å’Œé‡‡æ ·åŠŸèƒ½
        - æµ‹è¯•ä¼˜å…ˆçº§å›žæ”¾å’Œå†…å­˜ç®¡ç†
        - æµ‹è¯•å¤šè¿›ç¨‹å¹¶è¡Œé‡‡æ ·å’Œæ•°æ®ä¸€è‡´æ€§
        - _éœ€æ±‚: 4.3_
    
      - [x] 8.6 å®žçŽ°ç»éªŒå›žæ”¾ç¼“å†²åŒº
        - å®žçŽ°ReplayBufferç±»å’Œç»éªŒå­˜å‚¨æœºåˆ¶
        - å®žçŽ°ä¼˜å…ˆçº§é‡‡æ ·å’Œé‡è¦æ€§æƒé‡è®¡ç®—
        - å®žçŽ°å†…å­˜æ•ˆçŽ‡ä¼˜åŒ–å’Œå¤šè¿›ç¨‹æ”¯æŒ
        - _éœ€æ±‚: 4.3_
    
      - [x] 8.7 ç¼–å†™å®Œæ•´SACæ™ºèƒ½ä½“æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•SACAgentçš„å®Œæ•´è®­ç»ƒå’ŒæŽ¨ç†æµç¨‹
        - æµ‹è¯•æ¸©åº¦å‚æ•°è‡ªåŠ¨è°ƒæ•´å’Œç†µæ­£åˆ™åŒ–
        - æµ‹è¯•æ™ºèƒ½ä½“çš„å­¦ä¹ èƒ½åŠ›å’Œç­–ç•¥ç¨³å®šæ€§
        - _éœ€æ±‚: 4.1, 4.2, 4.5_
    
      - [x] 8.8 å®žçŽ°å®Œæ•´çš„SACæ™ºèƒ½ä½“
        - å®žçŽ°SACAgentç±»å’Œå®Œæ•´çš„SACç®—æ³•
        - å®žçŽ°æ¸©åº¦å‚æ•°è‡ªåŠ¨è°ƒæ•´å’Œç†µæ­£åˆ™åŒ–
        - å®žçŽ°è®­ç»ƒå¾ªçŽ¯å’Œç½‘ç»œå‚æ•°æ›´æ–°é€»è¾‘
        - _éœ€æ±‚: 4.1, 4.2, 4.5_
    
    - [x] 9. äº¤æ˜“æˆæœ¬æ¨¡åž‹å®žçŽ°
      - [x] 9.1 ç¼–å†™Almgren-Chrissæ¨¡åž‹æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ°¸ä¹…å†²å‡»å’Œä¸´æ—¶å†²å‡»è®¡ç®—é€»è¾‘
        - æµ‹è¯•ä¸åŒäº¤æ˜“è§„æ¨¡ä¸‹çš„æˆæœ¬ä¼°ç®—å‡†ç¡®æ€§
        - æµ‹è¯•å¸‚åœºå‚ä¸Žåº¦å’ŒæµåŠ¨æ€§å½±å“
        - _éœ€æ±‚: 5.3, 5.4_
    
      - [x] 9.2 å®žçŽ°Almgren-Chrisså¸‚åœºå†²å‡»æ¨¡åž‹
        - å®žçŽ°AlmgrenChrissModelç±»å’Œå†²å‡»æˆæœ¬è®¡ç®—
        - å®žçŽ°æ°¸ä¹…å†²å‡»ï¼ˆçº¿æ€§ï¼‰å’Œä¸´æ—¶å†²å‡»ï¼ˆå¹³æ–¹æ ¹ï¼‰æ¨¡åž‹
        - æ”¯æŒä¸åŒå¸‚åœºæ¡ä»¶ä¸‹çš„æˆæœ¬å‚æ•°è°ƒæ•´
        - _éœ€æ±‚: 5.3, 5.4_
    
      - [x] 9.3 ç¼–å†™äº¤æ˜“æˆæœ¬è®¡ç®—æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ‰‹ç»­è´¹ã€å°èŠ±ç¨Žå’Œæ»‘ç‚¹æˆæœ¬è®¡ç®—
        - æµ‹è¯•æˆæœ¬æ¨¡åž‹åœ¨å„ç§äº¤æ˜“åœºæ™¯ä¸‹çš„è¡¨çŽ°
        - æµ‹è¯•Aè‚¡ç‰¹æœ‰çš„äº¤æ˜“è§„åˆ™å’Œæˆæœ¬ç»“æž„
        - _éœ€æ±‚: 5.1, 5.2, 5.5_
    
      - [x] 9.4 å®žçŽ°äº¤æ˜“æˆæœ¬è®¡ç®—æ¨¡å—
        - å®žçŽ°TransactionCostModelç±»å’Œæˆæœ¬è®¡ç®—é€»è¾‘
        - å®žçŽ°æ‰‹ç»­è´¹ï¼ˆåŒè¾¹ï¼‰ã€å°èŠ±ç¨Žï¼ˆä»…å–å‡ºï¼‰è®¡ç®—
        - é›†æˆAlmgren-Chrissæ¨¡åž‹å’Œæ»‘ç‚¹ä¼°è®¡
        - _éœ€æ±‚: 5.1, 5.2, 5.5_
    
    - [x] 10. æŠ•èµ„ç»„åˆçŽ¯å¢ƒå®žçŽ°
      - [x] 10.1 ç¼–å†™æŠ•èµ„ç»„åˆçŽ¯å¢ƒæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•PortfolioEnvironmentçš„GymæŽ¥å£å…¼å®¹æ€§
        - æµ‹è¯•çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±å‡½æ•°
        - æµ‹è¯•Aè‚¡äº¤æ˜“è§„åˆ™çº¦æŸï¼ˆT+1ã€æ¶¨è·Œåœç­‰ï¼‰
        - _éœ€æ±‚: 3.1, 3.2, 3.4_
    
      - [x] 10.2 å®žçŽ°æŠ•èµ„ç»„åˆçŽ¯å¢ƒ
        - å®žçŽ°PortfolioEnvironmentç±»ï¼Œç»§æ‰¿gym.Env
        - å®šä¹‰çŠ¶æ€ç©ºé—´ï¼ˆåŽ†å²ç‰¹å¾ã€æŒä»“ã€å¸‚åœºçŠ¶æ€ï¼‰
        - å®šä¹‰åŠ¨ä½œç©ºé—´ï¼ˆæŠ•èµ„ç»„åˆæƒé‡ï¼‰å’Œå¥–åŠ±å‡½æ•°
        - å®žçŽ°reset()å’Œstep()æ–¹æ³•ï¼Œæ·»åŠ Aè‚¡äº¤æ˜“è§„åˆ™
        - _éœ€æ±‚: 3.1, 3.2, 3.4, 3.5_
    
    - [x] 11. è®­ç»ƒæµç¨‹ç®¡ç†å®žçŽ°
      - [x] 11.1 ç¼–å†™æ•°æ®åˆ’åˆ†ç­–ç•¥æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ—¶åºæ•°æ®çš„è®­ç»ƒ/éªŒè¯/æµ‹è¯•åˆ’åˆ†
        - æµ‹è¯•æ»šåŠ¨çª—å£åˆ’åˆ†å’Œæ•°æ®æ³„éœ²é˜²æŠ¤
        - æµ‹è¯•ä¸åŒåˆ’åˆ†ç­–ç•¥çš„æœ‰æ•ˆæ€§
        - _éœ€æ±‚: 6.1_
    
      - [x] 11.2 å®žçŽ°æ•°æ®åˆ’åˆ†ç­–ç•¥
        - å®žçŽ°DataSplitStrategyç±»å’Œæ—¶åºæ•°æ®åˆ’åˆ†
        - å®žçŽ°æ»šåŠ¨çª—å£è®­ç»ƒå’Œå›ºå®šåˆ’åˆ†ç­–ç•¥
        - å®žçŽ°æ•°æ®æ³„éœ²æ£€æµ‹å’Œé˜²æŠ¤æœºåˆ¶
        - _éœ€æ±‚: 6.1_
    
      - [x] 11.3 ç¼–å†™è®­ç»ƒå™¨æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•RLTrainerçš„è®­ç»ƒå¾ªçŽ¯å’Œæ—©åœæœºåˆ¶
        - æµ‹è¯•è®­ç»ƒè¿‡ç¨‹çš„ç¨³å®šæ€§å’Œæ”¶æ•›æ€§
        - æµ‹è¯•å¤šGPUè®­ç»ƒå’Œåˆ†å¸ƒå¼æ”¯æŒ
        - _éœ€æ±‚: 4.5_
    
      - [x] 11.4 å®žçŽ°è®­ç»ƒå™¨å’Œæ—©åœæœºåˆ¶
        - å®žçŽ°RLTrainerç±»å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªçŽ¯
        - å®žçŽ°æ—©åœæœºåˆ¶ã€å­¦ä¹ çŽ‡è°ƒåº¦å’Œæ¢¯åº¦è£å‰ª
        - å®žçŽ°è®­ç»ƒè¿›åº¦ç›‘æŽ§å’Œå¯è§†åŒ–ï¼ˆtensorboardï¼‰
        - æ”¯æŒå¤šGPUæ•°æ®å¹¶è¡Œå’Œå¼‚æ­¥çŽ¯å¢ƒé‡‡æ ·
        - _éœ€æ±‚: 4.5_
    
    - [x] 12. æ¨¡åž‹ç®¡ç†ç³»ç»Ÿå®žçŽ°
      - [x] 12.1 ç¼–å†™æ£€æŸ¥ç‚¹ç®¡ç†æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ¨¡åž‹ä¿å­˜ã€åŠ è½½å’Œç‰ˆæœ¬ç®¡ç†åŠŸèƒ½
        - æµ‹è¯•æ£€æŸ¥ç‚¹çš„å®Œæ•´æ€§å’Œæ¢å¤èƒ½åŠ›
        - æµ‹è¯•æ¨¡åž‹åŽ‹ç¼©å’Œä¼˜åŒ–åŠŸèƒ½
        - _éœ€æ±‚: 8.5_
    
      - [x] 12.2 å®žçŽ°æ¨¡åž‹æ£€æŸ¥ç‚¹ç®¡ç†
        - å®žçŽ°CheckpointManagerç±»å’Œæ¨¡åž‹ç‰ˆæœ¬æŽ§åˆ¶
        - å®žçŽ°è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡åž‹å’Œè®­ç»ƒçŠ¶æ€æ¢å¤
        - å®žçŽ°æ¨¡åž‹åŽ‹ç¼©å’Œä¼˜åŒ–ï¼ˆONNXã€TorchScriptï¼‰
        - _éœ€æ±‚: 8.5_
    
    - [x] 13. å›žæµ‹å¼•æ“Žå®žçŽ°
      - [x] 13.1 ç¼–å†™å¤šé¢‘çŽ‡å›žæµ‹å¼•æ“Žæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ—¥é¢‘å’Œåˆ†é’Ÿé¢‘å›žæµ‹åŠŸèƒ½
        - æµ‹è¯•äº¤æ˜“æ‰§è¡Œæ¨¡æ‹Ÿå’Œæˆäº¤ä»·æ ¼å¤„ç†
        - æµ‹è¯•å›žæµ‹ç»“æžœçš„å‡†ç¡®æ€§å’Œæ€§èƒ½
        - _éœ€æ±‚: 6.1, 6.2_
    
      - [x] 13.2 å®žçŽ°å¤šé¢‘çŽ‡å›žæµ‹å¼•æ“Ž
        - å®žçŽ°MultiFrequencyBacktestç±»å’Œå›žæµ‹å¼•æ“Ž
        - æ”¯æŒæ—¥é¢‘å’Œåˆ†é’Ÿé¢‘å›žæµ‹ï¼Œå¤šç§æˆäº¤ä»·æ ¼
        - å®žçŽ°äº¤æ˜“æ‰§è¡Œæ¨¡æ‹Ÿï¼Œè€ƒè™‘å®žé™…æˆäº¤æƒ…å†µ
        - é›†æˆQlibå›žæµ‹å¼•æ“Žå’Œé«˜é¢‘äº¤æ˜“æ¨¡æ‹Ÿ
        - _éœ€æ±‚: 6.1, 6.2_
    
    - [x] 14. è¯„ä¼°æŒ‡æ ‡å’ŒæŠ¥å‘Šç³»ç»Ÿå®žçŽ°
      - [x] 14.1 ç¼–å†™ç»©æ•ˆæŒ‡æ ‡è®¡ç®—æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ”¶ç›ŠçŽ‡ã€å¤æ™®æ¯”çŽ‡ã€æœ€å¤§å›žæ’¤ç­‰æŒ‡æ ‡è®¡ç®—
        - æµ‹è¯•é£Žé™©æŒ‡æ ‡ï¼ˆVaRã€CVaRã€æ³¢åŠ¨çŽ‡ï¼‰è®¡ç®—
        - æµ‹è¯•äº¤æ˜“æŒ‡æ ‡ï¼ˆæ¢æ‰‹çŽ‡ã€æˆæœ¬åˆ†æžï¼‰è®¡ç®—
        - _éœ€æ±‚: 6.2, 6.3, 6.4_
    
      - [x] 14.2 å®žçŽ°ç»©æ•ˆæŒ‡æ ‡è®¡ç®—
        - å®žçŽ°æ”¶ç›ŠæŒ‡æ ‡ï¼ˆæ€»æ”¶ç›Šã€å¹´åŒ–æ”¶ç›Šã€æœˆåº¦æ”¶ç›Šï¼‰
        - å®žçŽ°é£Žé™©æŒ‡æ ‡ï¼ˆæ³¢åŠ¨çŽ‡ã€æœ€å¤§å›žæ’¤ã€VaRã€CVaRï¼‰
        - å®žçŽ°é£Žé™©è°ƒæ•´æŒ‡æ ‡ï¼ˆSharpeã€Sortinoã€Calmarï¼‰
        - å®žçŽ°äº¤æ˜“æŒ‡æ ‡ï¼ˆæ¢æ‰‹çŽ‡ã€æˆæœ¬åˆ†æžã€æŒä»“é›†ä¸­åº¦ï¼‰
        - _éœ€æ±‚: 6.2, 6.3, 6.4_
    
      - [x] 14.3 ç¼–å†™å›žæµ‹æŠ¥å‘Šç”Ÿæˆæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•HTMLæŠ¥å‘Šç”Ÿæˆå’Œå¯è§†åŒ–å›¾è¡¨
        - æµ‹è¯•æ”¶ç›Šæ›²çº¿ã€æŒä»“åˆ†æžå’Œé£Žé™©åˆ†è§£æŠ¥å‘Š
        - æµ‹è¯•æŠ¥å‘Šçš„å®Œæ•´æ€§å’Œå¯è¯»æ€§
        - _éœ€æ±‚: 6.5_
    
      - [x] 14.4 å®žçŽ°å›žæµ‹æŠ¥å‘Šç”Ÿæˆ
        - å®žçŽ°è‡ªåŠ¨ç”ŸæˆHTMLæŠ¥å‘Šå’Œå¯è§†åŒ–å›¾è¡¨
        - å®žçŽ°æ”¶ç›Šæ›²çº¿ã€æŒä»“åˆ†æžå’Œé£Žé™©åˆ†è§£å¯è§†åŒ–
        - å®žçŽ°å› å­æš´éœ²åˆ†æžå’Œç»©æ•ˆå½’å› æŠ¥å‘Š
        - _éœ€æ±‚: 6.5_
    
    - [x] 15. ç›‘æŽ§ç³»ç»Ÿå®žçŽ°
      - [x] 15.1 ç¼–å†™Prometheusç›‘æŽ§æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•ç›‘æŽ§æŒ‡æ ‡çš„å®šä¹‰å’Œæ”¶é›†åŠŸèƒ½
        - æµ‹è¯•æŒ‡æ ‡å¯¼å‡ºå’ŒGrafanaä»ªè¡¨æ¿é›†æˆ
        - æµ‹è¯•ç›‘æŽ§ç³»ç»Ÿçš„å®žæ—¶æ€§å’Œå‡†ç¡®æ€§
        - _éœ€æ±‚: 7.1, 7.4_
    
      - [x] 15.2 å®žçŽ°Prometheusç›‘æŽ§é›†æˆ
        - å®žçŽ°TradingSystemMonitorç±»å’ŒæŒ‡æ ‡æ”¶é›†
        - å®šä¹‰æ€§èƒ½ã€é£Žé™©å’Œç³»ç»Ÿç›‘æŽ§æŒ‡æ ‡
        - å®žçŽ°æŒ‡æ ‡é‡‡é›†ã€å¯¼å‡ºå’ŒGrafanaä»ªè¡¨æ¿é…ç½®
        - _éœ€æ±‚: 7.1, 7.4_
    
    - [x] 16. å‘Šè­¦ç³»ç»Ÿå®žçŽ°
      - [x] 16.1 ç¼–å†™åŠ¨æ€é˜ˆå€¼ç®¡ç†æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•åŸºäºŽåŽ†å²åˆ†ä½æ•°çš„åŠ¨æ€é˜ˆå€¼è®¡ç®—
        - æµ‹è¯•é˜ˆå€¼è°ƒæ•´çš„åˆç†æ€§å’Œå‘Šè­¦å‡†ç¡®æ€§
        - æµ‹è¯•å‘Šè­¦è§„åˆ™é…ç½®å’Œç®¡ç†åŠŸèƒ½
        - _éœ€æ±‚: 7.2_
    
      - [x] 16.2 å®žçŽ°åŠ¨æ€é˜ˆå€¼ç®¡ç†å’Œå‘Šè­¦ç³»ç»Ÿ
        - å®žçŽ°DynamicThresholdManagerç±»å’Œé˜ˆå€¼è®¡ç®—
        - å®žçŽ°å¤šçº§åˆ«å‘Šè­¦å’Œå‘Šè­¦è§„åˆ™é…ç½®
        - å®žçŽ°å¤šæ¸ é“é€šçŸ¥ï¼ˆé‚®ä»¶ã€é’‰é’‰ã€ä¼ä¸šå¾®ä¿¡ï¼‰
        - å®žçŽ°å‘Šè­¦èšåˆã€é™é»˜å’Œæ—¥å¿—è®°å½•
        - _éœ€æ±‚: 7.2, 7.3, 7.5_
    
    - [ ] 17. é‡‘ä¸é›€éƒ¨ç½²ç³»ç»Ÿå®žçŽ°
      - [x] 17.1 ç¼–å†™é‡‘ä¸é›€éƒ¨ç½²æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ–°æ¨¡åž‹çš„æ¸è¿›å¼éƒ¨ç½²å’Œè¯„ä¼°æœºåˆ¶
        - æµ‹è¯•A/Bæµ‹è¯•æ¡†æž¶å’Œæ€§èƒ½å¯¹æ¯”
        - æµ‹è¯•éƒ¨ç½²è¿‡ç¨‹çš„å®‰å…¨æ€§å’Œå›žæ»šèƒ½åŠ›
        - _éœ€æ±‚: 8.1, 8.2, 8.4_
    
      - [x] 17.2 å®žçŽ°é‡‘ä¸é›€éƒ¨ç½²ç³»ç»Ÿ
        - å®žçŽ°CanaryDeploymentç±»å’Œç°åº¦å‘å¸ƒæµç¨‹
        - å®žçŽ°A/Bæµ‹è¯•æ¡†æž¶å’Œæ¨¡åž‹æ€§èƒ½å¯¹æ¯”
        - å®žçŽ°è‡ªåŠ¨å›žæ»šæœºåˆ¶å’Œéƒ¨ç½²å®‰å…¨æŽ§åˆ¶
        - _éœ€æ±‚: 8.1, 8.2, 8.3, 8.4_
    
    - [-] 18. å®¹å™¨åŒ–éƒ¨ç½²å®žçŽ°
      - [ ] 18.1 ç¼–å†™å®¹å™¨åŒ–éƒ¨ç½²æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•Dockerå®¹å™¨æž„å»ºå’Œè¿è¡Œ
        - æµ‹è¯•Kuberneteséƒ¨ç½²å’ŒæœåŠ¡å‘çŽ°
        - æµ‹è¯•CI/CDæµæ°´çº¿å’Œè‡ªåŠ¨åŒ–éƒ¨ç½²
        - _éœ€æ±‚: 8.1, 8.4_
    
      - [ ] 18.2 å®žçŽ°å®¹å™¨åŒ–éƒ¨ç½²
        - ç¼–å†™Dockerfileå’ŒDocker Composeé…ç½®
        - ç¼–å†™Kuberneteséƒ¨ç½²æ–‡ä»¶å’ŒæœåŠ¡é…ç½®
        - é…ç½®CI/CDæµæ°´çº¿ï¼ˆGitHub Actions/Jenkinsï¼‰
        - å®žçŽ°ç”Ÿäº§çŽ¯å¢ƒä¼˜åŒ–å’ŒæœåŠ¡å¥åº·æ£€æŸ¥
        - _éœ€æ±‚: 8.1, 8.4_
    
    - [-] 19. å®¡è®¡æ—¥å¿—ç³»ç»Ÿå®žçŽ°
      - [x] 19.1 ç¼–å†™å®¡è®¡æ—¥å¿—æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•äº¤æ˜“å†³ç­–è®°å½•å’Œå­˜å‚¨æœºåˆ¶
        - æµ‹è¯•æ—¥å¿—æŸ¥è¯¢æŽ¥å£å’Œæ•°æ®å®Œæ•´æ€§
        - æµ‹è¯•æ—¶åºæ•°æ®åº“é›†æˆå’Œæ€§èƒ½
        - _éœ€æ±‚: 9.1, 9.4_
    
      - [-] 19.2 å®žçŽ°å®¡è®¡æ—¥å¿—ç³»ç»Ÿ
        - å®žçŽ°AuditLoggerç±»å’Œå†³ç­–è®°å½•å­˜å‚¨
        - å®žçŽ°æ—¶åºæ•°æ®åº“é›†æˆå’Œå®¡è®¡æŸ¥è¯¢æŽ¥å£
        - å®žçŽ°åˆè§„æŠ¥å‘Šè‡ªåŠ¨ç”Ÿæˆå’Œæ•°æ®ä¿ç•™ç­–ç•¥
        - _éœ€æ±‚: 9.1, 9.4, 9.5_
    
    - [ ] 20. æ¨¡åž‹å¯è§£é‡Šæ€§å®žçŽ°
      - [ ] 20.1 ç¼–å†™æ¨¡åž‹å¯è§£é‡Šæ€§æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•SHAPå’ŒLIMEè§£é‡Šå™¨é›†æˆ
        - æµ‹è¯•æ³¨æ„åŠ›æƒé‡å¯è§†åŒ–å’Œç‰¹å¾é‡è¦æ€§åˆ†æž
        - æµ‹è¯•è§£é‡Šç»“æžœçš„åˆç†æ€§å’Œå¯ç†è§£æ€§
        - _éœ€æ±‚: 9.2_
    
      - [ ] 20.2 å®žçŽ°æ¨¡åž‹å¯è§£é‡Šæ€§åˆ†æž
        - å®žçŽ°ModelExplainerç±»å’ŒSHAPå€¼è®¡ç®—
        - å®žçŽ°LIMEè§£é‡Šå™¨å’Œæ³¨æ„åŠ›æƒé‡å¯è§†åŒ–
        - å®žçŽ°ç‰¹å¾é‡è¦æ€§åˆ†æžå’Œå†³ç­–è§£é‡ŠæŠ¥å‘Š
        - _éœ€æ±‚: 9.2_
    
    - [ ] 21. é£Žé™©æŽ§åˆ¶æ¨¡å—å®žçŽ°
      - [ ] 21.1 ç¼–å†™é£Žé™©æŽ§åˆ¶æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æŒä»“é›†ä¸­åº¦é™åˆ¶å’Œè¡Œä¸šæš´éœ²æŽ§åˆ¶
        - æµ‹è¯•æ­¢æŸæœºåˆ¶å’Œå¼‚å¸¸äº¤æ˜“æ£€æµ‹
        - æµ‹è¯•é£Žé™©æŽ§åˆ¶è§„åˆ™çš„æœ‰æ•ˆæ€§
        - _éœ€æ±‚: 9.3_
    
      - [ ] 21.2 å®žçŽ°é£Žé™©æŽ§åˆ¶æ¨¡å—
        - å®žçŽ°æŒä»“é›†ä¸­åº¦é™åˆ¶å’Œè¡Œä¸šæš´éœ²æŽ§åˆ¶
        - å®žçŽ°æ­¢æŸæœºåˆ¶å’Œå¼‚å¸¸äº¤æ˜“æ£€æµ‹
        - å®žçŽ°é£Žé™©æŽ§åˆ¶è§„åˆ™é…ç½®å’ŒåŠ¨æ€è°ƒæ•´
        - _éœ€æ±‚: 9.3_
    
    - [ ] 22. APIæŽ¥å£ç³»ç»Ÿå®žçŽ°
      - [ ] 22.1 ç¼–å†™RESTful APIæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•APIæŽ¥å£çš„åŠŸèƒ½å®Œæ•´æ€§å’Œæ€§èƒ½
        - æµ‹è¯•æ ‡å‡†åŒ–æ•°æ®æ ¼å¼å’Œé”™è¯¯å¤„ç†
        - æµ‹è¯•APIæ–‡æ¡£å’Œä½¿ç”¨ç¤ºä¾‹
        - _éœ€æ±‚: 10.1, 10.3, 10.5_
    
      - [ ] 22.2 å®žçŽ°RESTful APIæŽ¥å£
        - å®žçŽ°æ ‡å‡†åŒ–çš„REST APIå’Œæ•°æ®æ ¼å¼
        - å®žçŽ°APIè·¯ç”±ã€è¯·æ±‚å¤„ç†å’Œå“åº”æ ¼å¼åŒ–
        - å®žçŽ°æ ‡å‡†åŒ–é”™è¯¯ç å’Œé”™è¯¯ä¿¡æ¯è¿”å›ž
        - _éœ€æ±‚: 10.1, 10.3, 10.5_
    
      - [ ] 22.3 ç¼–å†™è®¤è¯å’Œé™æµæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•APIå¯†é’¥å’ŒJWTä»¤ç‰Œè®¤è¯æœºåˆ¶
        - æµ‹è¯•è¯·æ±‚é™æµå’Œå¹¶å‘æŽ§åˆ¶
        - æµ‹è¯•è®¤è¯çš„å®‰å…¨æ€§å’Œè®¿é—®æŽ§åˆ¶
        - _éœ€æ±‚: 10.2, 10.4_
    
      - [ ] 22.4 å®žçŽ°è®¤è¯å’Œé™æµæœºåˆ¶
        - å®žçŽ°APIå¯†é’¥å’ŒJWTä»¤ç‰Œè®¤è¯ç³»ç»Ÿ
        - å®žçŽ°è¯·æ±‚é™æµã€å“åº”ç¼“å­˜å’Œå¹¶å‘æŽ§åˆ¶
        - å®žçŽ°è®¿é—®æŽ§åˆ¶å’Œæƒé™ç®¡ç†
        - _éœ€æ±‚: 10.2, 10.4_
    
    - [ ] 23. ç«¯åˆ°ç«¯ç³»ç»Ÿé›†æˆ
      - [ ] 23.1 ç¼–å†™ç«¯åˆ°ç«¯é›†æˆæµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•å®Œæ•´äº¤æ˜“æµç¨‹çš„åŠŸèƒ½æ­£ç¡®æ€§
        - æµ‹è¯•ç³»ç»Ÿå„ç»„ä»¶é—´çš„åè°ƒå’Œæ•°æ®æµ
        - æµ‹è¯•ç³»ç»Ÿåœ¨ä¸åŒåœºæ™¯ä¸‹çš„ç¨³å®šæ€§
        - _éœ€æ±‚: æ‰€æœ‰éœ€æ±‚_
    
      - [ ] 23.2 å®žçŽ°å®Œæ•´ç³»ç»Ÿé›†æˆ
        - é›†æˆæ•°æ®èŽ·å–ã€æ¨¡åž‹æŽ¨ç†ã€äº¤æ˜“æ‰§è¡Œç­‰æ‰€æœ‰ç»„ä»¶
        - å®žçŽ°å®Œæ•´çš„äº¤æ˜“å†³ç­–æµç¨‹å’Œæ•°æ®æµç®¡é“
        - å®žçŽ°ç³»ç»Ÿå¯åŠ¨ã€åœæ­¢å’ŒçŠ¶æ€ç®¡ç†
        - _éœ€æ±‚: æ‰€æœ‰éœ€æ±‚_
    
    - [ ] 24. æ€§èƒ½ä¼˜åŒ–å’Œç”Ÿäº§éƒ¨ç½²
      - [ ] 24.1 ç¼–å†™æ€§èƒ½ä¼˜åŒ–æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•æ¨¡åž‹æŽ¨ç†é€Ÿåº¦å’Œå†…å­˜ä½¿ç”¨æ•ˆçŽ‡
        - æµ‹è¯•ç³»ç»Ÿåœ¨ä¸åŒè´Ÿè½½ä¸‹çš„èµ„æºæ¶ˆè€—
        - æµ‹è¯•æ•°æ®åŠ è½½å’Œç¼“å­˜ç­–ç•¥ä¼˜åŒ–
        - _éœ€æ±‚: 2.4, 7.4_
    
      - [ ] 24.2 å®žçŽ°æ€§èƒ½ä¼˜åŒ–
        - ä¼˜åŒ–æ¨¡åž‹æŽ¨ç†é€Ÿåº¦ï¼ˆæ¨¡åž‹é‡åŒ–ã€å¹¶è¡Œè®¡ç®—ï¼‰
        - ä¼˜åŒ–å†…å­˜ä½¿ç”¨ï¼ˆå†…å­˜æ± ã€å¯¹è±¡å¤ç”¨ï¼‰
        - ä¼˜åŒ–æ•°æ®åŠ è½½ï¼ˆå¤šè¿›ç¨‹ã€é¢„å–ã€ç¼“å­˜ï¼‰
        - _éœ€æ±‚: 2.4, 7.4_
    
      - [ ] 24.3 ç¼–å†™ç”Ÿäº§éƒ¨ç½²æµ‹è¯•ç”¨ä¾‹
        - æµ‹è¯•ç”Ÿäº§çŽ¯å¢ƒé…ç½®å’Œéƒ¨ç½²æµç¨‹
        - æµ‹è¯•ç³»ç»Ÿç›‘æŽ§å’Œæ•…éšœæ¢å¤èƒ½åŠ›
        - æµ‹è¯•è´Ÿè½½å‡è¡¡å’Œé«˜å¯ç”¨æ€§
        - _éœ€æ±‚: 8.1, 8.4_
    
      - [ ] 24.4 å®žçŽ°ç”Ÿäº§çŽ¯å¢ƒéƒ¨ç½²
        - é…ç½®ç”Ÿäº§çŽ¯å¢ƒçš„å®¹å™¨åŒ–éƒ¨ç½²å’Œç›‘æŽ§
        - å®žçŽ°è´Ÿè½½å‡è¡¡ã€æœåŠ¡å‘çŽ°å’Œæ•…éšœæ¢å¤
        - å®žçŽ°ç³»ç»Ÿå¤‡ä»½ã€æ¢å¤å’Œç¾éš¾æ¢å¤è®¡åˆ’
        - _éœ€æ±‚: 8.1, 8.4_
    
    - [ ] 25. æ–‡æ¡£ç³»ç»Ÿå®Œå–„
      - [ ] 25.1 ç¼–å†™æŠ€æœ¯æ–‡æ¡£
        - åˆ›å»ºè¯¦ç»†çš„APIæ–‡æ¡£å’Œä»£ç ç¤ºä¾‹
        - ç¼–å†™ç³»ç»Ÿæž¶æž„æ–‡æ¡£å’Œè®¾è®¡è¯´æ˜Ž
        - ç¼–å†™å¼€å‘è€…æŒ‡å—å’Œè´¡çŒ®æŒ‡å—
        - _éœ€æ±‚: 10.5_
    
      - [ ] 25.2 ç¼–å†™ç”¨æˆ·æ–‡æ¡£
        - ç¼–å†™ç”¨æˆ·ä½¿ç”¨æ‰‹å†Œå’Œå¿«é€Ÿå…¥é—¨æŒ‡å—
        - ç¼–å†™ç³»ç»Ÿé…ç½®å’Œéƒ¨ç½²æŒ‡å—
        - ç¼–å†™æ•…éšœæŽ’é™¤å’Œç»´æŠ¤æ‰‹å†Œ
        - _éœ€æ±‚: 7.5, 10.5_
    
    - [ ] 26. æµ‹è¯•è¦†ç›–çŽ‡å’Œè´¨é‡ä¿è¯
      - [ ] 26.1 å®Œå–„å•å…ƒæµ‹è¯•
        - ç¡®ä¿æ‰€æœ‰æ ¸å¿ƒç»„ä»¶çš„å•å…ƒæµ‹è¯•è¦†ç›–çŽ‡ > 90%
        - ç¼–å†™è¾¹ç•Œæ¡ä»¶å’Œå¼‚å¸¸æƒ…å†µæµ‹è¯•
        - å®žçŽ°æµ‹è¯•æ•°æ®ç”Ÿæˆå’Œæ¨¡æ‹Ÿå·¥å…·
        - _éœ€æ±‚: æ‰€æœ‰éœ€æ±‚_
    
      - [ ] 26.2 å®Œå–„é›†æˆæµ‹è¯•
        - ç¼–å†™ç«¯åˆ°ç«¯äº¤æ˜“æµç¨‹é›†æˆæµ‹è¯•
        - ç¼–å†™æ€§èƒ½åŸºå‡†æµ‹è¯•å’ŒåŽ‹åŠ›æµ‹è¯•
        - ç¼–å†™éƒ¨ç½²æµç¨‹å’Œå›žæ»šæµ‹è¯•
        - _éœ€æ±‚: æ‰€æœ‰éœ€æ±‚_
    
    ]]></file>
  <file path=".kiro/specs/qlib-trading-agent/requirements.md"><![CDATA[
    # åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“éœ€æ±‚æ–‡æ¡£
    
    ## é¡¹ç›®ç®€ä»‹
    
    æœ¬é¡¹ç›®æ—¨åœ¨æž„å»ºä¸€ä¸ªåŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformeræž¶æž„çš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿã€‚ç³»ç»Ÿé‡‡ç”¨SACï¼ˆSoft Actor-Criticï¼‰/PPOï¼ˆProximal Policy Optimizationï¼‰ä½œä¸ºå†³ç­–å¼•æ“Žï¼Œä½¿ç”¨Transformer/Informeræž¶æž„æ•æ‰é•¿æœŸæ—¶åºä¾èµ–ï¼Œåœ¨è€ƒè™‘äº¤æ˜“æˆæœ¬çš„æƒ…å†µä¸‹ï¼Œå®žçŽ°å¹´åŒ–æ”¶ç›Š8%-12%ï¼Œæœ€å¤§å›žæ’¤æŽ§åˆ¶åœ¨15%ä»¥å†…çš„æŠ•èµ„ç›®æ ‡ã€‚
    
    ## éœ€æ±‚åˆ—è¡¨
    
    ### éœ€æ±‚1ï¼šæ•°æ®å¤„ç†ä¸Žç‰¹å¾å·¥ç¨‹ç³»ç»Ÿ
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºé‡åŒ–äº¤æ˜“ç³»ç»Ÿï¼Œæˆ‘éœ€è¦èƒ½å¤ŸèŽ·å–å’Œå¤„ç†Aè‚¡å¸‚åœºæ•°æ®ï¼Œä»¥ä¾¿ä¸ºæ¨¡åž‹æä¾›é«˜è´¨é‡çš„ç‰¹å¾è¾“å…¥ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN ç³»ç»Ÿå¯åŠ¨æ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿé€šè¿‡Qlibå’ŒAkshare APIèŽ·å–Aè‚¡åŽ†å²å’Œå®žæ—¶è¡Œæƒ…æ•°æ®
    2. WHEN èŽ·å–åˆ°åŽŸå§‹æ•°æ®æ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿè®¡ç®—æŠ€æœ¯æŒ‡æ ‡ã€åŸºæœ¬é¢å› å­å’Œå¸‚åœºå¾®è§‚ç»“æž„ç‰¹å¾
    3. WHEN ç‰¹å¾è®¡ç®—å®Œæˆæ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿå¯¹ç‰¹å¾è¿›è¡Œæ ‡å‡†åŒ–å’Œç¼ºå¤±å€¼å¤„ç†
    4. WHEN æ•°æ®è´¨é‡æ£€æŸ¥æ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿè¯†åˆ«å’Œå¤„ç†å¼‚å¸¸æ•°æ®ç‚¹
    5. IF æ•°æ®æºå‡ºçŽ°é—®é¢˜ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿè‡ªåŠ¨åˆ‡æ¢åˆ°å¤‡ç”¨æ•°æ®æº
    
    ### éœ€æ±‚2ï¼šTransformeræ—¶åºç¼–ç æ¨¡åž‹
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºäº¤æ˜“å†³ç­–å¼•æ“Žï¼Œæˆ‘éœ€è¦ä¸€ä¸ªèƒ½å¤Ÿæ•æ‰é•¿æœŸæ—¶åºä¾èµ–å…³ç³»çš„ç¥žç»ç½‘ç»œæ¨¡åž‹ï¼Œä»¥ä¾¿ç†è§£å¸‚åœºçš„å¤æ‚æ¨¡å¼ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN è¾“å…¥æ—¶åºæ•°æ®æ—¶ THEN Transformerç¼–ç å™¨åº”èƒ½å¤Ÿå¤„ç†å¤šç»´åº¦çš„è‚¡ç¥¨ç‰¹å¾åºåˆ—
    2. WHEN è¿›è¡Œæ—¶åºå»ºæ¨¡æ—¶ THEN ç³»ç»Ÿåº”ä½¿ç”¨ä½ç½®ç¼–ç å’Œå¤šå¤´æ³¨æ„åŠ›æœºåˆ¶
    3. WHEN å¤„ç†é•¿åºåˆ—æ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿæœ‰æ•ˆæ•æ‰é•¿æœŸä¾èµ–å…³ç³»
    4. WHEN æ¨¡åž‹æŽ¨ç†æ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿåœ¨åˆç†æ—¶é—´å†…å®Œæˆå‰å‘ä¼ æ’­
    5. IF åºåˆ—é•¿åº¦è¶…è¿‡æœ€å¤§é™åˆ¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿè¿›è¡Œé€‚å½“çš„æˆªæ–­æˆ–åˆ†æ®µå¤„ç†
    
    ### éœ€æ±‚3ï¼šå¼ºåŒ–å­¦ä¹ äº¤æ˜“çŽ¯å¢ƒ
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œæˆ‘éœ€è¦ä¸€ä¸ªç¬¦åˆOpenAI Gymè§„èŒƒçš„äº¤æ˜“çŽ¯å¢ƒï¼Œä»¥ä¾¿å­¦ä¹ æœ€ä¼˜çš„æŠ•èµ„ç»„åˆç­–ç•¥ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN çŽ¯å¢ƒåˆå§‹åŒ–æ—¶ THEN ç³»ç»Ÿåº”å®šä¹‰æ¸…æ™°çš„çŠ¶æ€ç©ºé—´ã€åŠ¨ä½œç©ºé—´å’Œå¥–åŠ±å‡½æ•°
    2. WHEN æ‰§è¡Œäº¤æ˜“åŠ¨ä½œæ—¶ THEN ç³»ç»Ÿåº”è€ƒè™‘Aè‚¡å¸‚åœºçš„äº¤æ˜“è§„åˆ™å’Œé™åˆ¶
    3. WHEN è®¡ç®—äº¤æ˜“æˆæœ¬æ—¶ THEN ç³»ç»Ÿåº”åŒ…å«æ‰‹ç»­è´¹ã€å°èŠ±ç¨Žå’Œå¸‚åœºå†²å‡»æˆæœ¬
    4. WHEN è®¡ç®—å¥–åŠ±æ—¶ THEN ç³»ç»Ÿåº”è€ƒè™‘æ”¶ç›Šã€é£Žé™©å’Œäº¤æ˜“æˆæœ¬çš„å¹³è¡¡
    5. WHEN çŽ¯å¢ƒé‡ç½®æ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿä»ŽæŒ‡å®šæ—¥æœŸå¼€å§‹æ–°çš„äº¤æ˜“å‘¨æœŸ
    
    ### éœ€æ±‚4ï¼šSAC/PPOå¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºäº¤æ˜“å†³ç­–ç³»ç»Ÿï¼Œæˆ‘éœ€è¦ä¸€ä¸ªåŸºäºŽSACæˆ–PPOç®—æ³•çš„å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“ï¼Œä»¥ä¾¿è‡ªåŠ¨å­¦ä¹ æœ€ä¼˜äº¤æ˜“ç­–ç•¥ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN æ™ºèƒ½ä½“è®­ç»ƒæ—¶ THEN ç³»ç»Ÿåº”ä½¿ç”¨Actor-Criticæž¶æž„è¿›è¡Œç­–ç•¥å­¦ä¹ 
    2. WHEN ç”Ÿæˆäº¤æ˜“åŠ¨ä½œæ—¶ THEN ç³»ç»Ÿåº”è¾“å‡ºæ ‡å‡†åŒ–çš„æŠ•èµ„ç»„åˆæƒé‡
    3. WHEN æ›´æ–°ç½‘ç»œå‚æ•°æ—¶ THEN ç³»ç»Ÿåº”ä½¿ç”¨ç»éªŒå›žæ”¾å’Œç›®æ ‡ç½‘ç»œæœºåˆ¶
    4. WHEN æŽ¢ç´¢ç­–ç•¥æ—¶ THEN ç³»ç»Ÿåº”å¹³è¡¡æŽ¢ç´¢å’Œåˆ©ç”¨çš„å…³ç³»
    5. IF è®­ç»ƒä¸ç¨³å®š THEN ç³»ç»Ÿåº”æä¾›æ¢¯åº¦è£å‰ªå’Œå­¦ä¹ çŽ‡è°ƒåº¦æœºåˆ¶
    
    ### éœ€æ±‚5ï¼šäº¤æ˜“æˆæœ¬å»ºæ¨¡ç³»ç»Ÿ
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºäº¤æ˜“æ‰§è¡Œæ¨¡å—ï¼Œæˆ‘éœ€è¦ç²¾ç¡®çš„äº¤æ˜“æˆæœ¬æ¨¡åž‹ï¼Œä»¥ä¾¿çœŸå®žåæ˜ å®žé™…äº¤æ˜“ä¸­çš„å„é¡¹è´¹ç”¨ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN è®¡ç®—æ‰‹ç»­è´¹æ—¶ THEN ç³»ç»Ÿåº”æŒ‰ç…§å®žé™…åˆ¸å•†è´¹çŽ‡è®¡ç®—åŒè¾¹æ‰‹ç»­è´¹
    2. WHEN è®¡ç®—å°èŠ±ç¨Žæ—¶ THEN ç³»ç»Ÿåº”ä»…å¯¹å–å‡ºäº¤æ˜“æ”¶å–å°èŠ±ç¨Ž
    3. WHEN è®¡ç®—å¸‚åœºå†²å‡»æ—¶ THEN ç³»ç»Ÿåº”ä½¿ç”¨Almgren-Chrissæ¨¡åž‹ä¼°ç®—æ»‘ç‚¹æˆæœ¬
    4. WHEN äº¤æ˜“é‡è¾ƒå¤§æ—¶ THEN ç³»ç»Ÿåº”è€ƒè™‘æµåŠ¨æ€§çº¦æŸå¯¹æˆæœ¬çš„å½±å“
    5. WHEN æˆæœ¬è®¡ç®—å®Œæˆæ—¶ THEN ç³»ç»Ÿåº”å°†æ€»æˆæœ¬åé¦ˆç»™å¥–åŠ±å‡½æ•°
    
    ### éœ€æ±‚6ï¼šå›žæµ‹ä¸Žè¯„ä¼°ç³»ç»Ÿ
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºç³»ç»ŸéªŒè¯å·¥å…·ï¼Œæˆ‘éœ€è¦ä¸€ä¸ªå…¨é¢çš„å›žæµ‹å¼•æ“Žï¼Œä»¥ä¾¿è¯„ä¼°ç­–ç•¥çš„åŽ†å²è¡¨çŽ°å’Œé£Žé™©ç‰¹å¾ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN è¿›è¡ŒåŽ†å²å›žæµ‹æ—¶ THEN ç³»ç»Ÿåº”æ”¯æŒæ—¥é¢‘å’Œåˆ†é’Ÿé¢‘çš„å¤šé¢‘çŽ‡å›žæµ‹
    2. WHEN è®¡ç®—ç»©æ•ˆæŒ‡æ ‡æ—¶ THEN ç³»ç»Ÿåº”æä¾›æ”¶ç›ŠçŽ‡ã€å¤æ™®æ¯”çŽ‡ã€æœ€å¤§å›žæ’¤ç­‰å…³é”®æŒ‡æ ‡
    3. WHEN åˆ†æžé£Žé™©æ—¶ THEN ç³»ç»Ÿåº”è®¡ç®—VaRã€CVaRå’Œæ³¢åŠ¨çŽ‡ç­‰é£Žé™©åº¦é‡
    4. WHEN è¯„ä¼°äº¤æ˜“è¡Œä¸ºæ—¶ THEN ç³»ç»Ÿåº”åˆ†æžæ¢æ‰‹çŽ‡ã€äº¤æ˜“æˆæœ¬å’ŒæŒä»“é›†ä¸­åº¦
    5. WHEN ç”ŸæˆæŠ¥å‘Šæ—¶ THEN ç³»ç»Ÿåº”æä¾›è¯¦ç»†çš„ç»©æ•ˆå½’å› åˆ†æž
    
    ### éœ€æ±‚7ï¼šå®žæ—¶ç›‘æŽ§ä¸Žå‘Šè­¦ç³»ç»Ÿ
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºç³»ç»Ÿè¿ç»´äººå‘˜ï¼Œæˆ‘éœ€è¦å®žæ—¶ç›‘æŽ§ç³»ç»Ÿçš„è¿è¡ŒçŠ¶æ€å’Œäº¤æ˜“è¡¨çŽ°ï¼Œä»¥ä¾¿åŠæ—¶å‘çŽ°å’Œå¤„ç†å¼‚å¸¸æƒ…å†µã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN ç³»ç»Ÿè¿è¡Œæ—¶ THEN ç›‘æŽ§ç³»ç»Ÿåº”å®žæ—¶æ”¶é›†æ€§èƒ½ã€é£Žé™©å’Œç³»ç»ŸæŒ‡æ ‡
    2. WHEN æŒ‡æ ‡å¼‚å¸¸æ—¶ THEN ç³»ç»Ÿåº”åŸºäºŽåŠ¨æ€é˜ˆå€¼è§¦å‘ç›¸åº”çº§åˆ«çš„å‘Šè­¦
    3. WHEN å‘ç”Ÿå‘Šè­¦æ—¶ THEN ç³»ç»Ÿåº”é€šè¿‡å¤šç§æ¸ é“å‘é€é€šçŸ¥
    4. WHEN æŸ¥çœ‹ç›‘æŽ§æ•°æ®æ—¶ THEN ç³»ç»Ÿåº”æä¾›PrometheusæŒ‡æ ‡å’ŒGrafanaä»ªè¡¨æ¿
    5. IF ç³»ç»Ÿå‡ºçŽ°ä¸¥é‡å¼‚å¸¸ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿè‡ªåŠ¨æ‰§è¡Œåº”æ€¥å¤„ç†æµç¨‹
    
    ### éœ€æ±‚8ï¼šæ¨¡åž‹éƒ¨ç½²ä¸Žç‰ˆæœ¬ç®¡ç†
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºç³»ç»Ÿç®¡ç†å‘˜ï¼Œæˆ‘éœ€è¦å®‰å…¨å¯é çš„æ¨¡åž‹éƒ¨ç½²æœºåˆ¶ï¼Œä»¥ä¾¿åœ¨ä¸å½±å“ç”Ÿäº§çŽ¯å¢ƒçš„æƒ…å†µä¸‹æ›´æ–°äº¤æ˜“ç­–ç•¥ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN éƒ¨ç½²æ–°æ¨¡åž‹æ—¶ THEN ç³»ç»Ÿåº”ä½¿ç”¨é‡‘ä¸é›€éƒ¨ç½²ç­–ç•¥é€æ­¥æŽ¨å¹¿
    2. WHEN è¯„ä¼°æ–°æ¨¡åž‹æ—¶ THEN ç³»ç»Ÿåº”å¹¶è¡Œè¿è¡Œæ–°æ—§æ¨¡åž‹è¿›è¡Œå¯¹æ¯”
    3. WHEN æ¨¡åž‹è¡¨çŽ°ä¸ä½³æ—¶ THEN ç³»ç»Ÿåº”èƒ½å¤Ÿå¿«é€Ÿå›žæ»šåˆ°ç¨³å®šç‰ˆæœ¬
    4. WHEN æ¨¡åž‹æŽ¨å¹¿æ—¶ THEN ç³»ç»Ÿåº”é€æ­¥å¢žåŠ æ–°æ¨¡åž‹çš„èµ„é‡‘åˆ†é…æ¯”ä¾‹
    5. WHEN ç‰ˆæœ¬ç®¡ç†æ—¶ THEN ç³»ç»Ÿåº”ç»´æŠ¤å®Œæ•´çš„æ¨¡åž‹ç‰ˆæœ¬åŽ†å²å’Œå˜æ›´è®°å½•
    
    ### éœ€æ±‚9ï¼šåˆè§„å®¡è®¡ä¸Žå¯è§£é‡Šæ€§
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºåˆè§„ç®¡ç†äººå‘˜ï¼Œæˆ‘éœ€è¦å®Œæ•´çš„å®¡è®¡æ—¥å¿—å’Œæ¨¡åž‹è§£é‡ŠåŠŸèƒ½ï¼Œä»¥ä¾¿æ»¡è¶³ç›‘ç®¡è¦æ±‚å’Œé£Žé™©ç®¡ç†éœ€è¦ã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN æ‰§è¡Œäº¤æ˜“å†³ç­–æ—¶ THEN ç³»ç»Ÿåº”è®°å½•å®Œæ•´çš„å†³ç­–è¿‡ç¨‹å’Œç›¸å…³æ•°æ®
    2. WHEN éœ€è¦è§£é‡Šæ¨¡åž‹å†³ç­–æ—¶ THEN ç³»ç»Ÿåº”æä¾›SHAPå’ŒLIMEç­‰å¯è§£é‡Šæ€§åˆ†æž
    3. WHEN ç”Ÿæˆå®¡è®¡æŠ¥å‘Šæ—¶ THEN ç³»ç»Ÿåº”åŒ…å«é£Žé™©è¿è§„ã€é›†ä¸­åº¦åˆ†æžç­‰åˆè§„æ£€æŸ¥
    4. WHEN æŸ¥è¯¢åŽ†å²è®°å½•æ—¶ THEN ç³»ç»Ÿåº”æ”¯æŒæŒ‰æ—¶é—´ã€æ¨¡åž‹ç‰ˆæœ¬ç­‰æ¡ä»¶æ£€ç´¢
    5. WHEN æ•°æ®ä¿å­˜æ—¶ THEN ç³»ç»Ÿåº”æ»¡è¶³è‡³å°‘5å¹´çš„æ•°æ®ä¿ç•™è¦æ±‚
    
    ### éœ€æ±‚10ï¼šç³»ç»Ÿé›†æˆä¸ŽAPIæŽ¥å£
    
    **ç”¨æˆ·æ•…äº‹ï¼š** ä½œä¸ºç¬¬ä¸‰æ–¹ç³»ç»Ÿï¼Œæˆ‘éœ€è¦æ ‡å‡†åŒ–çš„APIæŽ¥å£ï¼Œä»¥ä¾¿ä¸Žé‡åŒ–äº¤æ˜“ç³»ç»Ÿè¿›è¡Œæ•°æ®äº¤æ¢å’ŒåŠŸèƒ½é›†æˆã€‚
    
    #### éªŒæ”¶æ ‡å‡†
    
    1. WHEN å¤–éƒ¨ç³»ç»Ÿè°ƒç”¨æ—¶ THEN ç³»ç»Ÿåº”æä¾›RESTful APIæŽ¥å£
    2. WHEN è¿›è¡Œèº«ä»½éªŒè¯æ—¶ THEN ç³»ç»Ÿåº”æ”¯æŒAPIå¯†é’¥å’ŒJWTä»¤ç‰Œè®¤è¯
    3. WHEN è¿”å›žæ•°æ®æ—¶ THEN ç³»ç»Ÿåº”ä½¿ç”¨æ ‡å‡†çš„JSONæ ¼å¼
    4. WHEN å¤„ç†å¹¶å‘è¯·æ±‚æ—¶ THEN ç³»ç»Ÿåº”æ”¯æŒé€‚å½“çš„é™æµå’Œç¼“å­˜æœºåˆ¶
    5. WHEN APIå‡ºçŽ°é”™è¯¯æ—¶ THEN ç³»ç»Ÿåº”è¿”å›žæ ‡å‡†åŒ–çš„é”™è¯¯ç å’Œé”™è¯¯ä¿¡æ¯
    ]]></file>
  <file path=".kiro/specs/qlib-trading-agent/design.md"><![CDATA[
    # åŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformerçš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“è®¾è®¡æ–‡æ¡£
    
    ## æ¦‚è¿°
    
    æœ¬è®¾è®¡æ–‡æ¡£åŸºäºŽéœ€æ±‚è§„æ ¼ï¼Œè¯¦ç»†æè¿°äº†ä¸€ä¸ªåŸºäºŽå¼ºåŒ–å­¦ä¹ ä¸ŽTransformeræž¶æž„çš„Aè‚¡é‡åŒ–äº¤æ˜“æ™ºèƒ½ä½“ç³»ç»Ÿçš„æŠ€æœ¯å®žçŽ°æ–¹æ¡ˆã€‚ç³»ç»Ÿé‡‡ç”¨æ¨¡å—åŒ–è®¾è®¡ï¼Œæ”¯æŒæµ‹è¯•é©±åŠ¨å¼€å‘ï¼Œç¡®ä¿ä»£ç è´¨é‡å’Œç³»ç»Ÿç¨³å®šæ€§ã€‚
    
    ## æž¶æž„è®¾è®¡
    
    ### ç³»ç»Ÿæ•´ä½“æž¶æž„
    
    ```mermaid
    graph TB
        subgraph "æ•°æ®å±‚"
            A[Qlibæ•°æ®æŽ¥å£] --> B[æ•°æ®é¢„å¤„ç†å™¨]
            C[Akshare API] --> B
            D[å®žæ—¶è¡Œæƒ…] --> B
            B --> E[ç‰¹å¾å·¥ç¨‹æ¨¡å—]
        end
        
        subgraph "æ¨¡åž‹å±‚"
            E --> F[Transformerç¼–ç å™¨]
            F --> G[SACæ™ºèƒ½ä½“]
            G --> H[ç­–ç•¥ç½‘ç»œActor]
            G --> I[ä»·å€¼ç½‘ç»œCritic]
        end
        
        subgraph "äº¤æ˜“å±‚"
            H --> J[æŠ•èµ„ç»„åˆçŽ¯å¢ƒ]
            J --> K[äº¤æ˜“æˆæœ¬æ¨¡åž‹]
            K --> L[è®¢å•æ‰§è¡Œå™¨]
        end
        
        subgraph "ç›‘æŽ§å±‚"
            L --> M[æ€§èƒ½ç›‘æŽ§]
            M --> N[é£Žé™©ç›‘æŽ§]
            N --> O[å‘Šè­¦ç³»ç»Ÿ]
        end
        
        subgraph "éƒ¨ç½²å±‚"
            G --> P[æ¨¡åž‹ç‰ˆæœ¬ç®¡ç†]
            P --> Q[é‡‘ä¸é›€éƒ¨ç½²]
            Q --> R[ç”Ÿäº§çŽ¯å¢ƒ]
        end
    ```
    
    ### æ ¸å¿ƒç»„ä»¶è®¾è®¡
    
    #### 1. æ•°æ®å¤„ç†ç»„ä»¶
    
    **DataCollectorï¼ˆæ•°æ®æ”¶é›†å™¨ï¼‰**
    - èŒè´£ï¼šä»Žå¤šä¸ªæ•°æ®æºèŽ·å–Aè‚¡å¸‚åœºæ•°æ®
    - æŽ¥å£ï¼š
      - `collect_historical_data(symbols, start_date, end_date) -> pd.DataFrame`
      - `collect_realtime_data(symbols) -> pd.DataFrame`
      - `validate_data(data) -> bool`
    
    **FeatureEngineerï¼ˆç‰¹å¾å·¥ç¨‹å™¨ï¼‰**
    - èŒè´£ï¼šè®¡ç®—æŠ€æœ¯æŒ‡æ ‡å’ŒåŸºæœ¬é¢å› å­
    - æŽ¥å£ï¼š
      - `calculate_technical_indicators(data) -> pd.DataFrame`
      - `calculate_fundamental_factors(data) -> pd.DataFrame`
      - `normalize_features(features) -> pd.DataFrame`
    
    #### 2. æ¨¡åž‹ç»„ä»¶
    
    **TimeSeriesTransformerï¼ˆæ—¶åºTransformerï¼‰**
    - èŒè´£ï¼šç¼–ç æ—¶åºç‰¹å¾ï¼Œæ•æ‰é•¿æœŸä¾èµ–
    - æž¶æž„ï¼š
      - è¾“å…¥åµŒå…¥å±‚ï¼šå°†åŽŸå§‹ç‰¹å¾æ˜ å°„åˆ°é«˜ç»´ç©ºé—´
      - ä½ç½®ç¼–ç ï¼šä¸ºåºåˆ—æ·»åŠ æ—¶é—´ä½ç½®ä¿¡æ¯
      - å¤šå±‚Transformerç¼–ç å™¨ï¼šæ•æ‰å¤æ‚æ—¶åºæ¨¡å¼
      - æ—¶é—´æ³¨æ„åŠ›èšåˆï¼šå°†åºåˆ—ä¿¡æ¯åŽ‹ç¼©ä¸ºå›ºå®šç»´åº¦è¡¨ç¤º
    
    **SACAgentï¼ˆSACæ™ºèƒ½ä½“ï¼‰**
    - èŒè´£ï¼šåŸºäºŽçŠ¶æ€ç”Ÿæˆäº¤æ˜“å†³ç­–
    - ç»„ä»¶ï¼š
      - Actorç½‘ç»œï¼šç”ŸæˆæŠ•èµ„ç»„åˆæƒé‡åˆ†å¸ƒ
      - Criticç½‘ç»œï¼šè¯„ä¼°çŠ¶æ€-åŠ¨ä½œä»·å€¼
      - ç›®æ ‡ç½‘ç»œï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
      - ç»éªŒå›žæ”¾ç¼“å†²åŒºï¼šå­˜å‚¨å’Œé‡‡æ ·åŽ†å²ç»éªŒ
    
    #### 3. äº¤æ˜“çŽ¯å¢ƒç»„ä»¶
    
    **PortfolioEnvironmentï¼ˆæŠ•èµ„ç»„åˆçŽ¯å¢ƒï¼‰**
    - èŒè´£ï¼šæ¨¡æ‹ŸAè‚¡äº¤æ˜“çŽ¯å¢ƒï¼Œç¬¦åˆOpenAI Gymè§„èŒƒ
    - çŠ¶æ€ç©ºé—´ï¼š
      - åŽ†å²ç‰¹å¾çª—å£ï¼š[lookback_window, n_stocks, n_features]
      - å½“å‰æŒä»“ï¼š[n_stocks]
      - å¸‚åœºçŠ¶æ€ï¼š[market_features]
    - åŠ¨ä½œç©ºé—´ï¼šç›®æ ‡æŠ•èµ„ç»„åˆæƒé‡ [n_stocks]
    - å¥–åŠ±å‡½æ•°ï¼šé£Žé™©è°ƒæ•´åŽæ”¶ç›Š = å‡€æ”¶ç›Š - é£Žé™©æƒ©ç½š - å›žæ’¤æƒ©ç½š
    
    **TransactionCostModelï¼ˆäº¤æ˜“æˆæœ¬æ¨¡åž‹ï¼‰**
    - èŒè´£ï¼šç²¾ç¡®è®¡ç®—å„é¡¹äº¤æ˜“æˆæœ¬
    - æˆæœ¬ç»„æˆï¼š
      - æ‰‹ç»­è´¹ï¼šåŒè¾¹æ”¶å–ï¼Œè´¹çŽ‡0.1%
      - å°èŠ±ç¨Žï¼šä»…å–å‡ºæ”¶å–ï¼Œè´¹çŽ‡0.1%
      - å¸‚åœºå†²å‡»ï¼šä½¿ç”¨Almgren-Chrissæ¨¡åž‹
    
    ## æ•°æ®æ¨¡åž‹
    
    ### æ ¸å¿ƒæ•°æ®ç»“æž„
    
    ```python
    @dataclass
    class MarketData:
        """å¸‚åœºæ•°æ®ç»“æž„"""
        timestamp: datetime
        symbol: str
        open_price: float
        high_price: float
        low_price: float
        close_price: float
        volume: int
        amount: float
        
    @dataclass
    class FeatureVector:
        """ç‰¹å¾å‘é‡ç»“æž„"""
        timestamp: datetime
        symbol: str
        technical_indicators: Dict[str, float]
        fundamental_factors: Dict[str, float]
        market_microstructure: Dict[str, float]
        
    @dataclass
    class TradingState:
        """äº¤æ˜“çŠ¶æ€ç»“æž„"""
        features: np.ndarray  # [lookback_window, n_stocks, n_features]
        positions: np.ndarray  # [n_stocks]
        market_state: np.ndarray  # [market_features]
        cash: float
        total_value: float
        
    @dataclass
    class TradingAction:
        """äº¤æ˜“åŠ¨ä½œç»“æž„"""
        target_weights: np.ndarray  # [n_stocks]
        confidence: float
        timestamp: datetime
        
    @dataclass
    class TransactionRecord:
        """äº¤æ˜“è®°å½•ç»“æž„"""
        timestamp: datetime
        symbol: str
        action_type: str  # 'buy' or 'sell'
        quantity: int
        price: float
        commission: float
        stamp_tax: float
        slippage: float
        total_cost: float
    ```
    
    ### æ•°æ®åº“è®¾è®¡
    
    **æ—¶åºæ•°æ®å­˜å‚¨ï¼ˆInfluxDBï¼‰**
    - market_dataï¼šåŽŸå§‹å¸‚åœºæ•°æ®
    - featuresï¼šè®¡ç®—åŽçš„ç‰¹å¾æ•°æ®
    - portfolio_valuesï¼šæŠ•èµ„ç»„åˆä»·å€¼æ—¶åº
    - performance_metricsï¼šç»©æ•ˆæŒ‡æ ‡æ—¶åº
    
    **å…³ç³»æ•°æ®å­˜å‚¨ï¼ˆPostgreSQLï¼‰**
    - trading_sessionsï¼šäº¤æ˜“ä¼šè¯è®°å½•
    - model_versionsï¼šæ¨¡åž‹ç‰ˆæœ¬ä¿¡æ¯
    - audit_logsï¼šå®¡è®¡æ—¥å¿—
    - alert_recordsï¼šå‘Šè­¦è®°å½•
    
    ## æŽ¥å£è®¾è®¡
    
    ### æ•°æ®æŽ¥å£
    
    ```python
    class DataInterface:
        """æ•°æ®æŽ¥å£æŠ½è±¡ç±»"""
        
        @abstractmethod
        def get_stock_list(self, market: str = 'A') -> List[str]:
            """èŽ·å–è‚¡ç¥¨åˆ—è¡¨"""
            pass
        
        @abstractmethod
        def get_price_data(self, symbols: List[str], 
                          start_date: str, end_date: str) -> pd.DataFrame:
            """èŽ·å–ä»·æ ¼æ•°æ®"""
            pass
        
        @abstractmethod
        def get_fundamental_data(self, symbols: List[str], 
                               start_date: str, end_date: str) -> pd.DataFrame:
            """èŽ·å–åŸºæœ¬é¢æ•°æ®"""
            pass
    
    class QlibDataInterface(DataInterface):
        """Qlibæ•°æ®æŽ¥å£å®žçŽ°"""
        
        def __init__(self, provider_uri: str):
            qlib.init(provider_uri=provider_uri, region='cn')
        
        def get_stock_list(self, market: str = 'A') -> List[str]:
            return D.instruments(market=market)
        
        def get_price_data(self, symbols: List[str], 
                          start_date: str, end_date: str) -> pd.DataFrame:
            fields = ['$open', '$high', '$low', '$close', '$volume', '$amount']
            return D.features(symbols, fields, start_time=start_date, end_time=end_date)
    ```
    
    ### æ¨¡åž‹æŽ¥å£
    
    ```python
    class TradingAgent:
        """äº¤æ˜“æ™ºèƒ½ä½“æŽ¥å£"""
        
        @abstractmethod
        def predict(self, state: TradingState) -> TradingAction:
            """é¢„æµ‹äº¤æ˜“åŠ¨ä½œ"""
            pass
        
        @abstractmethod
        def train(self, experiences: List[Experience]) -> Dict[str, float]:
            """è®­ç»ƒæ¨¡åž‹"""
            pass
        
        @abstractmethod
        def save_model(self, path: str) -> None:
            """ä¿å­˜æ¨¡åž‹"""
            pass
        
        @abstractmethod
        def load_model(self, path: str) -> None:
            """åŠ è½½æ¨¡åž‹"""
            pass
    
    class SACTradingAgent(TradingAgent):
        """SACäº¤æ˜“æ™ºèƒ½ä½“å®žçŽ°"""
        
        def __init__(self, config: Dict):
            self.config = config
            self.model = self._build_model()
            self.replay_buffer = ReplayBuffer(config['buffer_size'])
        
        def predict(self, state: TradingState) -> TradingAction:
            with torch.no_grad():
                action, _ = self.model.get_action(state, deterministic=True)
                return TradingAction(
                    target_weights=action.cpu().numpy(),
                    confidence=self._calculate_confidence(state),
                    timestamp=datetime.now()
                )
    ```
    
    ### ç›‘æŽ§æŽ¥å£
    
    ```python
    class MonitoringInterface:
        """ç›‘æŽ§æŽ¥å£"""
        
        @abstractmethod
        def log_metric(self, name: str, value: float, timestamp: datetime) -> None:
            """è®°å½•æŒ‡æ ‡"""
            pass
        
        @abstractmethod
        def check_alert_conditions(self, metrics: Dict[str, float]) -> List[Alert]:
            """æ£€æŸ¥å‘Šè­¦æ¡ä»¶"""
            pass
        
        @abstractmethod
        def send_alert(self, alert: Alert) -> None:
            """å‘é€å‘Šè­¦"""
            pass
    
    class PrometheusMonitor(MonitoringInterface):
        """Prometheusç›‘æŽ§å®žçŽ°"""
        
        def __init__(self):
            self.metrics = self._initialize_metrics()
            start_http_server(8000)
        
        def log_metric(self, name: str, value: float, timestamp: datetime) -> None:
            if name in self.metrics:
                self.metrics[name].set(value)
    ```
    
    ## ç»„ä»¶å’ŒæŽ¥å£
    
    ### æ ¸å¿ƒç»„ä»¶è¯¦ç»†è®¾è®¡
    
    #### 1. Transformerç¼–ç å™¨ç»„ä»¶
    
    ```python
    class TransformerConfig:
        """Transformeré…ç½®"""
        d_model: int = 256
        n_heads: int = 8
        n_layers: int = 6
        d_ff: int = 1024
        dropout: float = 0.1
        max_seq_len: int = 252  # ä¸€å¹´äº¤æ˜“æ—¥
        n_features: int = 50
    
    class TimeSeriesTransformer(nn.Module):
        """æ—¶åºTransformerç¼–ç å™¨"""
        
        def __init__(self, config: TransformerConfig):
            super().__init__()
            self.config = config
            
            # è¾“å…¥æŠ•å½±å±‚
            self.input_projection = nn.Linear(config.n_features, config.d_model)
            
            # ä½ç½®ç¼–ç 
            self.pos_encoding = PositionalEncoding(config.d_model, config.max_seq_len)
            
            # Transformerç¼–ç å™¨
            encoder_layer = nn.TransformerEncoderLayer(
                d_model=config.d_model,
                nhead=config.n_heads,
                dim_feedforward=config.d_ff,
                dropout=config.dropout,
                activation='gelu',
                batch_first=True
            )
            self.transformer = nn.TransformerEncoder(encoder_layer, config.n_layers)
            
            # æ—¶é—´æ³¨æ„åŠ›èšåˆ
            self.temporal_attention = TemporalAttention(config.d_model)
        
        def forward(self, x: torch.Tensor, mask: Optional[torch.Tensor] = None) -> torch.Tensor:
            """
            å‰å‘ä¼ æ’­
            Args:
                x: [batch_size, seq_len, n_stocks, n_features]
                mask: [batch_size, seq_len] å¯é€‰çš„æŽ©ç 
            Returns:
                encoded: [batch_size, n_stocks, d_model]
            """
            batch_size, seq_len, n_stocks, n_features = x.shape
            
            # é‡å¡‘ä¸º [batch_size * n_stocks, seq_len, n_features]
            x = x.view(batch_size * n_stocks, seq_len, n_features)
            
            # è¾“å…¥æŠ•å½±
            x = self.input_projection(x)
            
            # æ·»åŠ ä½ç½®ç¼–ç 
            x = self.pos_encoding(x)
            
            # Transformerç¼–ç 
            if mask is not None:
                mask = mask.repeat_interleave(n_stocks, dim=0)
            x = self.transformer(x, src_key_padding_mask=mask)
            
            # æ—¶é—´æ³¨æ„åŠ›èšåˆ
            x = self.temporal_attention(x)
            
            # é‡å¡‘å›ž [batch_size, n_stocks, d_model]
            return x.view(batch_size, n_stocks, self.config.d_model)
    ```
    
    #### 2. SACæ™ºèƒ½ä½“ç»„ä»¶
    
    ```python
    class SACConfig:
        """SACé…ç½®"""
        state_dim: int = 256
        action_dim: int = 100  # è‚¡ç¥¨æ•°é‡
        hidden_dim: int = 512
        lr_actor: float = 3e-4
        lr_critic: float = 3e-4
        lr_alpha: float = 3e-4
        gamma: float = 0.99
        tau: float = 0.005
        alpha: float = 0.2
        target_entropy: float = -100  # -action_dim
    
    class SACAgent(nn.Module):
        """SACæ™ºèƒ½ä½“"""
        
        def __init__(self, config: SACConfig):
            super().__init__()
            self.config = config
            
            # å…±äº«ç¼–ç å™¨
            self.encoder = TimeSeriesTransformer(TransformerConfig())
            
            # Actorç½‘ç»œ
            self.actor = Actor(config.state_dim, config.action_dim, config.hidden_dim)
            
            # Criticç½‘ç»œ
            self.critic1 = Critic(config.state_dim, config.action_dim, config.hidden_dim)
            self.critic2 = Critic(config.state_dim, config.action_dim, config.hidden_dim)
            
            # ç›®æ ‡Criticç½‘ç»œ
            self.target_critic1 = Critic(config.state_dim, config.action_dim, config.hidden_dim)
            self.target_critic2 = Critic(config.state_dim, config.action_dim, config.hidden_dim)
            
            # åˆå§‹åŒ–ç›®æ ‡ç½‘ç»œ
            self.target_critic1.load_state_dict(self.critic1.state_dict())
            self.target_critic2.load_state_dict(self.critic2.state_dict())
            
            # æ¸©åº¦å‚æ•°
            self.log_alpha = nn.Parameter(torch.zeros(1))
            
            # ä¼˜åŒ–å™¨
            self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=config.lr_actor)
            self.critic_optimizer = torch.optim.Adam(
                list(self.critic1.parameters()) + list(self.critic2.parameters()),
                lr=config.lr_critic
            )
            self.alpha_optimizer = torch.optim.Adam([self.log_alpha], lr=config.lr_alpha)
        
        def get_action(self, state: Dict[str, torch.Tensor], 
                       deterministic: bool = False) -> Tuple[torch.Tensor, torch.Tensor]:
            """èŽ·å–åŠ¨ä½œ"""
            # ç¼–ç ç‰¹å¾
            encoded_features = self.encoder(state['features'])
            
            # æž„å»ºå®Œæ•´çŠ¶æ€
            batch_size = encoded_features.size(0)
            full_state = torch.cat([
                encoded_features.flatten(start_dim=1),
                state['positions'],
                state['market_state']
            ], dim=1)
            
            # ç”ŸæˆåŠ¨ä½œ
            return self.actor(full_state, deterministic)
    ```
    
    #### 3. æŠ•èµ„ç»„åˆçŽ¯å¢ƒç»„ä»¶
    
    ```python
    class PortfolioEnvConfig:
        """æŠ•èµ„ç»„åˆçŽ¯å¢ƒé…ç½®"""
        stock_pool: List[str] = field(default_factory=list)
        lookback_window: int = 60
        initial_cash: float = 1000000.0
        commission_rate: float = 0.001
        stamp_tax_rate: float = 0.001
        risk_aversion: float = 0.1
        max_drawdown_penalty: float = 1.0
    
    class PortfolioEnvironment(gym.Env):
        """æŠ•èµ„ç»„åˆçŽ¯å¢ƒ"""
        
        def __init__(self, config: PortfolioEnvConfig):
            super().__init__()
            self.config = config
            self.n_stocks = len(config.stock_pool)
            
            # å®šä¹‰è§‚å¯Ÿç©ºé—´
            self.observation_space = spaces.Dict({
                'features': spaces.Box(
                    low=-np.inf, high=np.inf,
                    shape=(config.lookback_window, self.n_stocks, 50),  # 50ä¸ªç‰¹å¾
                    dtype=np.float32
                ),
                'positions': spaces.Box(
                    low=0, high=1,
                    shape=(self.n_stocks,),
                    dtype=np.float32
                ),
                'market_state': spaces.Box(
                    low=-np.inf, high=np.inf,
                    shape=(10,),  # 10ä¸ªå¸‚åœºçŠ¶æ€ç‰¹å¾
                    dtype=np.float32
                )
            })
            
            # å®šä¹‰åŠ¨ä½œç©ºé—´
            self.action_space = spaces.Box(
                low=0, high=1,
                shape=(self.n_stocks,),
                dtype=np.float32
            )
            
            # åˆå§‹åŒ–çŠ¶æ€
            self.reset()
        
        def step(self, action: np.ndarray) -> Tuple[Dict, float, bool, Dict]:
            """æ‰§è¡Œä¸€æ­¥äº¤æ˜“"""
            # æ ‡å‡†åŒ–åŠ¨ä½œï¼ˆç¡®ä¿æƒé‡å’Œä¸º1ï¼‰
            target_weights = action / (action.sum() + 1e-8)
            
            # è®¡ç®—äº¤æ˜“æˆæœ¬
            transaction_cost = self._calculate_transaction_cost(
                self.current_positions, target_weights
            )
            
            # æ‰§è¡Œäº¤æ˜“
            self.current_positions = target_weights.copy()
            
            # è®¡ç®—æ”¶ç›Š
            returns = self._get_next_returns()
            portfolio_return = np.dot(self.current_positions, returns)
            
            # è®¡ç®—å¥–åŠ±
            reward = self._calculate_reward(portfolio_return, transaction_cost, target_weights)
            
            # æ›´æ–°çŠ¶æ€
            self.current_step += 1
            next_observation = self._get_observation()
            done = self.current_step >= self.max_steps
            
            # ä¿¡æ¯å­—å…¸
            info = {
                'portfolio_return': portfolio_return,
                'transaction_cost': transaction_cost,
                'positions': self.current_positions.copy(),
                'total_value': self.total_value
            }
            
            return next_observation, reward, done, info
        
        def _calculate_transaction_cost(self, current_weights: np.ndarray, 
                                      target_weights: np.ndarray) -> float:
            """è®¡ç®—äº¤æ˜“æˆæœ¬"""
            trade_weights = np.abs(target_weights - current_weights)
            
            # æ‰‹ç»­è´¹
            commission = np.sum(trade_weights) * self.config.commission_rate
            
            # å°èŠ±ç¨Žï¼ˆä»…å–å‡ºï¼‰
            sell_weights = np.maximum(current_weights - target_weights, 0)
            stamp_tax = np.sum(sell_weights) * self.config.stamp_tax_rate
            
            # å¸‚åœºå†²å‡»ï¼ˆç®€åŒ–æ¨¡åž‹ï¼‰
            slippage = np.sum(trade_weights ** 1.5) * 0.001
            
            return commission + stamp_tax + slippage
        
        def _calculate_reward(self, portfolio_return: float, 
                             transaction_cost: float, weights: np.ndarray) -> float:
            """è®¡ç®—å¥–åŠ±"""
            # å‡€æ”¶ç›Š
            net_return = portfolio_return - transaction_cost
            
            # é£Žé™©æƒ©ç½šï¼ˆåŸºäºŽæƒé‡é›†ä¸­åº¦ï¼‰
            concentration = np.sum(weights ** 2)  # HerfindahlæŒ‡æ•°
            risk_penalty = self.config.risk_aversion * concentration
            
            # å›žæ’¤æƒ©ç½š
            current_drawdown = self._calculate_current_drawdown()
            drawdown_penalty = self.config.max_drawdown_penalty * max(0, current_drawdown - 0.1)
            
            return net_return - risk_penalty - drawdown_penalty
    ```
    
    ## é”™è¯¯å¤„ç†
    
    ### å¼‚å¸¸å¤„ç†ç­–ç•¥
    
    ç³»ç»Ÿé‡‡ç”¨"å¿«é€Ÿå¤±è´¥"åŽŸåˆ™ï¼Œä¸æ•èŽ·å¼‚å¸¸ï¼Œè€Œæ˜¯é€šè¿‡ä»¥ä¸‹æ–¹å¼ç¡®ä¿ä»£ç å¥å£®æ€§ï¼š
    
    1. **è¾“å…¥éªŒè¯**ï¼šåœ¨å‡½æ•°å…¥å£è¿›è¡Œä¸¥æ ¼çš„å‚æ•°éªŒè¯
    2. **ç±»åž‹æ£€æŸ¥**ï¼šä½¿ç”¨ç±»åž‹æ³¨è§£å’Œè¿è¡Œæ—¶ç±»åž‹æ£€æŸ¥
    3. **è¾¹ç•Œæ¡ä»¶å¤„ç†**ï¼šæ˜Žç¡®å¤„ç†è¾¹ç•Œæƒ…å†µ
    4. **èµ„æºç®¡ç†**ï¼šä½¿ç”¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨ç¡®ä¿èµ„æºæ­£ç¡®é‡Šæ”¾
    
    ```python
    def validate_trading_data(data: pd.DataFrame) -> pd.DataFrame:
        """éªŒè¯äº¤æ˜“æ•°æ®"""
        # æ£€æŸ¥å¿…è¦åˆ—
        required_columns = ['open', 'high', 'low', 'close', 'volume']
        missing_columns = set(required_columns) - set(data.columns)
        if missing_columns:
            raise ValueError(f"ç¼ºå°‘å¿…è¦åˆ—: {missing_columns}")
        
        # æ£€æŸ¥æ•°æ®ç±»åž‹
        for col in required_columns:
            if not pd.api.types.is_numeric_dtype(data[col]):
                raise TypeError(f"åˆ— {col} å¿…é¡»æ˜¯æ•°å€¼ç±»åž‹")
        
        # æ£€æŸ¥æ•°æ®èŒƒå›´
        if (data['high'] < data['low']).any():
            raise ValueError("æœ€é«˜ä»·ä¸èƒ½ä½ŽäºŽæœ€ä½Žä»·")
        
        if (data['volume'] < 0).any():
            raise ValueError("æˆäº¤é‡ä¸èƒ½ä¸ºè´Ÿæ•°")
        
        return data
    
    def safe_divide(numerator: float, denominator: float, default: float = 0.0) -> float:
        """å®‰å…¨é™¤æ³•"""
        if abs(denominator) < 1e-8:
            return default
        return numerator / denominator
    ```
    
    ## æµ‹è¯•ç­–ç•¥
    
    ### æµ‹è¯•æž¶æž„
    
    é‡‡ç”¨åˆ†å±‚æµ‹è¯•ç­–ç•¥ï¼š
    
    1. **å•å…ƒæµ‹è¯•**ï¼šæµ‹è¯•å•ä¸ªå‡½æ•°å’Œç±»çš„åŠŸèƒ½
    2. **é›†æˆæµ‹è¯•**ï¼šæµ‹è¯•ç»„ä»¶é—´çš„äº¤äº’
    3. **ç«¯åˆ°ç«¯æµ‹è¯•**ï¼šæµ‹è¯•å®Œæ•´çš„äº¤æ˜“æµç¨‹
    4. **æ€§èƒ½æµ‹è¯•**ï¼šæµ‹è¯•ç³»ç»Ÿæ€§èƒ½å’Œèµ„æºä½¿ç”¨
    
    ### æµ‹è¯•å·¥å…·å’Œæ¡†æž¶
    
    - **pytest**ï¼šä¸»è¦æµ‹è¯•æ¡†æž¶
    - **pytest-mock**ï¼šæ¨¡æ‹Ÿå¤–éƒ¨ä¾èµ–
    - **pytest-cov**ï¼šä»£ç è¦†ç›–çŽ‡
    - **hypothesis**ï¼šå±žæ€§æµ‹è¯•
    - **pytest-benchmark**ï¼šæ€§èƒ½æµ‹è¯•
    
    ```python
    # ç¤ºä¾‹å•å…ƒæµ‹è¯•
    class TestPortfolioEnvironment:
        """æŠ•èµ„ç»„åˆçŽ¯å¢ƒæµ‹è¯•"""
        
        @pytest.fixture
        def env_config(self):
            return PortfolioEnvConfig(
                stock_pool=['000001.SZ', '000002.SZ'],
                lookback_window=30,
                initial_cash=100000.0
            )
        
        @pytest.fixture
        def portfolio_env(self, env_config):
            return PortfolioEnvironment(env_config)
        
        def test_environment_initialization(self, portfolio_env):
            """æµ‹è¯•çŽ¯å¢ƒåˆå§‹åŒ–"""
            assert portfolio_env.n_stocks == 2
            assert portfolio_env.config.initial_cash == 100000.0
            assert portfolio_env.observation_space is not None
            assert portfolio_env.action_space is not None
        
        def test_step_function(self, portfolio_env):
            """æµ‹è¯•æ­¥è¿›å‡½æ•°"""
            action = np.array([0.6, 0.4])
            obs, reward, done, info = portfolio_env.step(action)
            
            assert isinstance(obs, dict)
            assert 'features' in obs
            assert 'positions' in obs
            assert 'market_state' in obs
            assert isinstance(reward, float)
            assert isinstance(done, bool)
            assert isinstance(info, dict)
        
        @pytest.mark.parametrize("weights", [
            np.array([0.5, 0.5]),
            np.array([1.0, 0.0]),
            np.array([0.0, 1.0])
        ])
        def test_different_weight_allocations(self, portfolio_env, weights):
            """æµ‹è¯•ä¸åŒæƒé‡åˆ†é…"""
            obs, reward, done, info = portfolio_env.step(weights)
            assert not np.isnan(reward)
            assert np.allclose(info['positions'].sum(), 1.0, atol=1e-6)
    ```
    
    è¿™ä¸ªè®¾è®¡æ–‡æ¡£æä¾›äº†ç³»ç»Ÿçš„è¯¦ç»†æŠ€æœ¯æž¶æž„ï¼ŒåŒ…æ‹¬æ ¸å¿ƒç»„ä»¶è®¾è®¡ã€æ•°æ®æ¨¡åž‹ã€æŽ¥å£å®šä¹‰ã€é”™è¯¯å¤„ç†ç­–ç•¥å’Œæµ‹è¯•æ–¹æ³•ã€‚æŽ¥ä¸‹æ¥æˆ‘å°†è¯¢é—®æ‚¨å¯¹è®¾è®¡æ–‡æ¡£çš„æ„è§ã€‚
    ]]></file>
</files>
