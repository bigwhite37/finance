#!/usr/bin/env python3
"""
å¢å¼ºç³»ç»Ÿå¿«é€Ÿæµ‹è¯•
åŸºäºç°æœ‰ç³»ç»Ÿï¼Œåº”ç”¨å…³é”®æ”¹è¿›ï¼ŒéªŒè¯8%ç›®æ ‡
"""

import os
import sys
import argparse
import numpy as np
import pandas as pd
import logging
import torch
import warnings
import yaml
from datetime import datetime
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥åŸºç¡€ç»„ä»¶
from data.data_manager import DataManager
from config.config_manager import ConfigManager
from factors.factor_engine import FactorEngine
from rl_agent.cvar_ppo_agent import CVaRPPOAgent
from rl_agent.trading_environment import TradingEnvironment
from rl_agent.safety_shield import SafetyShield
from risk_control.risk_controller import RiskController
from backtest.backtest_engine import BacktestEngine
from utils.logger import setup_logger

logger = logging.getLogger(__name__)


class EnhancedTradingSystem:
    """å¢å¼ºäº¤æ˜“ç³»ç»Ÿ"""
    
    def __init__(self, config_file: str):
        self.config = self._load_enhanced_config(config_file)
        self._initialize_systems()
    
    def _load_enhanced_config(self, config_file: str) -> dict:
        """åŠ è½½å¹¶å¢å¼ºé…ç½®"""
        with open(config_file, 'r', encoding='utf-8') as f:
            base_config = yaml.safe_load(f)
        
        # åº”ç”¨å¢å¼ºé…ç½®
        enhanced_config = {
            'data': base_config.get('data', {}),
            'agent': {
                'learning_rate': 1e-4,      # é™ä½å­¦ä¹ ç‡æå‡ç¨³å®šæ€§
                'hidden_dim': 256,
                'clip_epsilon': 0.2,
                'ppo_epochs': 8,             # å¢åŠ è®­ç»ƒè½®æ¬¡
                'batch_size': 64,
                'gamma': 0.99,
                'lambda_gae': 0.95,
                'cvar_alpha': 0.05,
                'cvar_lambda': 0.1,
                'cvar_threshold': -0.02
            },
            'safety_shield': {
                'max_position': 0.25,        # æå‡ä»“ä½é™åˆ¶
                'max_leverage': 2.0,         # æå‡æ æ†é™åˆ¶
                'var_threshold': 0.05,       # æ”¾å®½VaRé˜ˆå€¼
                'max_drawdown_threshold': 0.12,
                'volatility_threshold': 0.30
            },
            'environment': {
                'lookback_window': 30,
                'transaction_cost': 0.001,
                'max_position': 0.25,
                'max_leverage': 2.0,
                'lambda1': 0.5,              # é™ä½å›æ’¤æƒ©ç½š
                'lambda2': 0.3,              # é™ä½CVaRæƒ©ç½š
                'reward_amplification': 25.0  # æå‡å¥–åŠ±æ”¾å¤§å€æ•°
            },
            'training': {
                'total_episodes': 300,       # é€‚ä¸­çš„è®­ç»ƒè½®æ•°
                'max_steps_per_episode': 300,
                'save_frequency': 50,
                'evaluation_frequency': 25
            },
            'model': base_config.get('model', {})
        }
        
        return enhanced_config
    
    def _initialize_systems(self):
        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""
        self.data_manager = DataManager(self.config['data'])
        self.factor_engine = FactorEngine(self.config)
        self.risk_controller = RiskController(self.config)
        self.backtest_engine = BacktestEngine(self.config)
        
        logger.info("å¢å¼ºäº¤æ˜“ç³»ç»Ÿåˆå§‹åŒ–å®Œæˆ")
    
    def prepare_data(self):
        """å‡†å¤‡è®­ç»ƒæ•°æ®"""
        logger.info("æ­£åœ¨è·å–è‚¡ç¥¨æ•°æ®...")
        
        stock_data = self.data_manager.get_stock_data(
            start_time=self.config['data']['start_date'],
            end_time=self.config['data']['end_date'],
            include_fundamentals=True
        )
        
        if stock_data.empty:
            raise ValueError("æœªèƒ½è·å–åˆ°è‚¡ç¥¨æ•°æ®")
        
        # è·å–ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®
        price_data = stock_data['$close'].unstack().T
        volume_data = stock_data['$volume'].unstack().T
        
        logger.info(f"ä»·æ ¼æ•°æ®å½¢çŠ¶: {price_data.shape}")
        
        # è‚¡ç¥¨æ± ç­›é€‰ï¼ˆä½¿ç”¨ç°æœ‰é€»è¾‘æˆ–åˆ›å»ºæ–°çš„ï¼‰
        if os.path.exists('./models/selected_stocks.pkl'):
            import pickle
            with open('./models/selected_stocks.pkl', 'rb') as f:
                selected_stocks = pickle.load(f)
            logger.info(f"å·²åŠ è½½è®­ç»ƒæ—¶çš„è‚¡ç¥¨æ± ï¼ŒåŒ…å« {len(selected_stocks)} åªè‚¡ç¥¨")
            
            available_stocks = list(set(selected_stocks) & set(price_data.columns))
            logger.info(f"å½“å‰æœŸé—´å¯ç”¨è‚¡ç¥¨: {len(available_stocks)} åª")
            
            price_data = price_data[available_stocks]
            volume_data = volume_data[available_stocks]
        else:
            # é¦–æ¬¡è®­ç»ƒï¼Œè¿›è¡Œç®€å•ç­›é€‰
            logger.info("è¿›è¡Œè‚¡ç¥¨ç­›é€‰...")
            returns = price_data.pct_change().dropna()
            volatility = returns.std()
            
            # é€‰æ‹©æ³¢åŠ¨ç‡é€‚ä¸­çš„è‚¡ç¥¨
            vol_threshold = volatility.quantile(0.7)
            selected_stocks = volatility[volatility <= vol_threshold].index.tolist()
            
            # è¿›ä¸€æ­¥ç­›é€‰æœ‰æ•ˆè‚¡ç¥¨
            min_data_points = len(price_data) * 0.8
            valid_stocks = []
            for stock in selected_stocks:
                if price_data[stock].notna().sum() >= min_data_points:
                    valid_stocks.append(stock)
            
            selected_stocks = valid_stocks[:800]
            logger.info(f"ç­›é€‰å‡º {len(selected_stocks)} åªè‚¡ç¥¨")
            
            # ä¿å­˜è‚¡ç¥¨æ± 
            os.makedirs('./models', exist_ok=True)
            import pickle
            with open('./models/selected_stocks.pkl', 'wb') as f:
                pickle.dump(selected_stocks, f)
            
            price_data = price_data[selected_stocks]
            volume_data = volume_data[selected_stocks]
        
        # è®¡ç®—å› å­ï¼ˆä½¿ç”¨å¢å¼ºçš„å› å­åˆ—è¡¨ï¼‰
        logger.info("æ­£åœ¨è®¡ç®—å¢å¼ºå› å­...")
        
        # ä½¿ç”¨ç°æœ‰ç³»ç»Ÿä¸­å¯ç”¨çš„ç®€å•å› å­
        enhanced_factors = [
            "return_5d", "return_20d", "return_60d", "price_momentum",
            "rsi_14d", "ma_ratio_10d", "ma_ratio_20d", "price_reversal"
        ]
        
        factor_data = self.factor_engine.calculate_all_factors(
            price_data, volume_data, factors=enhanced_factors
        )
        
        logger.info(f"å› å­è®¡ç®—å®Œæˆ! å› å­æ•°æ®å½¢çŠ¶: {factor_data.shape}")
        
        return price_data, factor_data
    
    def create_enhanced_environment(self, price_data, factor_data):
        """åˆ›å»ºå¢å¼ºäº¤æ˜“ç¯å¢ƒ"""
        # ä¿®æ”¹ç¯å¢ƒé…ç½®ä»¥åº”ç”¨å¢å¼ºå¥–åŠ±å‡½æ•°
        env_config = self.config['environment'].copy()
        
        # ä½¿ç”¨ç°æœ‰ç¯å¢ƒä½†ä¿®æ”¹å¥–åŠ±å‚æ•°
        env = TradingEnvironment(
            factor_data=factor_data,
            price_data=price_data,
            config=env_config,
            train_universe=list(price_data.columns)
        )
        
        # åŠ¨æ€ä¿®æ”¹å¥–åŠ±å‡½æ•°
        original_calculate_reward = env._calculate_reward
        
        def enhanced_calculate_reward(returns: float, transaction_costs: float) -> float:
            """å¢å¼ºå¥–åŠ±å‡½æ•°"""
            # åŸºç¡€æ”¶ç›Šå¥–åŠ± - å¤§å¹…å¢å¼º
            base_reward = returns * env_config.get('reward_amplification', 25.0)
            
            # åŠ¨é‡å¥–åŠ±
            momentum_bonus = 0.0
            if len(env.portfolio_returns) >= 5:
                recent_returns = np.array(env.portfolio_returns[-5:])
                if np.mean(recent_returns) > 0:
                    momentum_bonus = np.mean(recent_returns) * 8.0
            
            # å¤æ™®æ¯”ç‡å¥–åŠ±
            sharpe_bonus = 0.0
            if len(env.portfolio_returns) >= 20:
                returns_array = np.array(env.portfolio_returns)
                mean_return = np.mean(returns_array)
                std_return = np.std(returns_array)
                if std_return > 1e-8 and mean_return > 0:
                    sharpe_ratio = mean_return / std_return * np.sqrt(252)
                    sharpe_bonus = sharpe_ratio * 3.0
            
            # å‡å°‘é£é™©æƒ©ç½š
            risk_penalty = 0.0
            current_drawdown = (env.peak_value - env.portfolio_value) / env.peak_value
            if current_drawdown > 0.15:  # åªåœ¨15%ä»¥ä¸Šå›æ’¤æ—¶æƒ©ç½š
                risk_penalty = env_config.get('lambda1', 0.5) * (current_drawdown - 0.15)
            
            # è½»å¾®æˆæœ¬æƒ©ç½š
            cost_penalty = transaction_costs * env_config.get('lambda2', 0.3)
            
            # ä¸€è‡´æ€§å¥–åŠ±
            consistency_bonus = 0.0
            if len(env.portfolio_returns) >= 10:
                recent_returns = np.array(env.portfolio_returns[-10:])
                if np.mean(recent_returns) > 0:
                    win_rate = np.mean(recent_returns > 0)
                    consistency_bonus = win_rate * np.mean(recent_returns) * 5.0
            
            total_reward = (base_reward + momentum_bonus + sharpe_bonus + 
                           consistency_bonus - risk_penalty - cost_penalty)
            
            return total_reward
        
        # æ›¿æ¢å¥–åŠ±å‡½æ•°
        env._calculate_reward = enhanced_calculate_reward
        
        return env
    
    def create_enhanced_agent(self, env):
        """åˆ›å»ºå¢å¼ºæ™ºèƒ½ä½“"""
        agent_config = self.config['agent'].copy()
        
        # è®¡ç®—çŠ¶æ€å’ŒåŠ¨ä½œç»´åº¦
        # è·å–ä¸€ä¸ªç¤ºä¾‹è§‚å¯Ÿæ¥ç¡®å®šç»´åº¦
        try:
            sample_obs = env.reset()[0]
            state_dim = len(sample_obs) if isinstance(sample_obs, np.ndarray) else sample_obs.shape[0]
        except Exception as e:
            logger.warning(f"æ— æ³•è·å–çŠ¶æ€ç»´åº¦ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            state_dim = 100  # é»˜è®¤ç»´åº¦
        
        action_dim = len(env.train_universe)
        
        # åˆ›å»ºæ™ºèƒ½ä½“
        agent = CVaRPPOAgent(
            state_dim=state_dim,
            action_dim=action_dim,
            config=agent_config
        )
        
        # åº”ç”¨æ•°å€¼ç¨³å®šæ€§ä¿®å¤
        for param in agent.network.parameters():
            if param.dim() > 1:
                torch.nn.init.xavier_uniform_(param, gain=0.5)
            else:
                torch.nn.init.constant_(param, 0.0)
        
        return agent
    
    def create_enhanced_safety_shield(self):
        """åˆ›å»ºå¢å¼ºå®‰å…¨ä¿æŠ¤å±‚"""
        shield_config = self.config['safety_shield']
        
        safety_shield = SafetyShield(shield_config)
        
        # å¢å¼ºå®‰å…¨æ£€æŸ¥æ–¹æ³•
        original_shield_action = safety_shield.shield_action
        
        def enhanced_shield_action(action, state, price_data=None, current_portfolio=None):
            """å¢å¼ºå®‰å…¨æ£€æŸ¥"""
            safe_action = action.copy()
            
            # æ›´å®½æ¾çš„ä»“ä½é™åˆ¶
            max_pos = shield_config.get('max_position', 0.25)
            position_sizes = np.abs(safe_action)
            violations = position_sizes > max_pos
            
            if violations.any():
                # æ¸è¿›å¼è°ƒæ•´è€Œéç¡¬æˆªæ–­
                scale_factor = max_pos / np.max(position_sizes[violations])
                scale_factor = max(scale_factor, 0.8)  # ä¿ç•™80%çš„ä¿¡å·å¼ºåº¦
                safe_action[violations] *= scale_factor
            
            # æ›´å®½æ¾çš„æ æ†é™åˆ¶
            max_lev = shield_config.get('max_leverage', 2.0)
            total_leverage = np.sum(np.abs(safe_action))
            
            if total_leverage > max_lev:
                scale_factor = max_lev / total_leverage
                scale_factor = max(scale_factor, 0.7)  # ä¿ç•™70%çš„ä¿¡å·å¼ºåº¦
                safe_action *= scale_factor
            
            return safe_action
        
        safety_shield.shield_action = enhanced_shield_action
        
        return safety_shield
    
    def train(self):
        """è®­ç»ƒå¢å¼ºç³»ç»Ÿ"""
        logger.info("ğŸš€ å¼€å§‹è®­ç»ƒå¢å¼ºç³»ç»Ÿ")
        
        # å‡†å¤‡æ•°æ®
        price_data, factor_data = self.prepare_data()
        
        # åˆ›å»ºå¢å¼ºç»„ä»¶
        env = self.create_enhanced_environment(price_data, factor_data)
        agent = self.create_enhanced_agent(env)
        safety_shield = self.create_enhanced_safety_shield()
        
        # è®°å½•åŠ¨ä½œç»´åº¦ä»¥å¤‡åç”¨
        action_dim = len(env.train_universe)
        
        # è®­ç»ƒå‚æ•°
        training_config = self.config['training']
        total_episodes = training_config.get('total_episodes', 300)
        save_frequency = training_config.get('save_frequency', 50)
        
        logger.info(f"å¼€å§‹è®­ç»ƒ: æ€»å…± {total_episodes} ä¸ªepisode")
        
        best_performance = {
            'episode': 0,
            'annual_return': -np.inf,
            'sharpe_ratio': -np.inf,
            'model_path': None
        }
        
        for episode in range(total_episodes):
            state_info = env.reset()
            if isinstance(state_info, tuple):
                state = state_info[0]
            else:
                state = state_info
            
            # ç¡®ä¿çŠ¶æ€æ˜¯æœ‰æ•ˆçš„numpyæ•°ç»„
            if not isinstance(state, np.ndarray):
                state = np.array(state, dtype=np.float32)
            state = np.nan_to_num(state, nan=0.0, posinf=1.0, neginf=-1.0)
            
            episode_reward = 0.0
            episode_steps = 0
            
            while True:
                # è·å–åŠ¨ä½œ
                try:
                    action_result = agent.get_action(state)
                    if len(action_result) == 3:
                        action, log_prob, value = action_result
                    else:
                        logger.warning(f"get_actionè¿”å›äº†{len(action_result)}ä¸ªå€¼ï¼ŒæœŸæœ›3ä¸ª")
                        action = action_result[0] if len(action_result) > 0 else np.zeros(action_dim)
                        log_prob = action_result[1] if len(action_result) > 1 else 0.0
                        value = action_result[2] if len(action_result) > 2 else 0.0
                except Exception as e:
                    logger.error(f"è·å–åŠ¨ä½œæ—¶å‡ºé”™: {e}")
                    action = np.zeros(action_dim)
                    log_prob = 0.0
                    value = 0.0
                
                # åº”ç”¨å®‰å…¨ä¿æŠ¤
                safe_action = safety_shield.shield_action(action, {})
                
                # æ‰§è¡ŒåŠ¨ä½œ
                step_result = env.step(safe_action)
                if len(step_result) == 4:
                    next_state, reward, done, info = step_result
                    truncated = False
                else:
                    next_state, reward, done, truncated, info = step_result
                
                # ç¡®ä¿ä¸‹ä¸€ä¸ªçŠ¶æ€æœ‰æ•ˆ
                if not isinstance(next_state, np.ndarray):
                    next_state = np.array(next_state, dtype=np.float32)
                next_state = np.nan_to_num(next_state, nan=0.0, posinf=1.0, neginf=-1.0)
                
                # å­˜å‚¨ç»éªŒ
                agent.store_experience(state, safe_action, reward, log_prob, value, done or truncated)
                
                episode_reward += reward
                episode_steps += 1
                state = next_state
                
                if done or truncated:
                    break
            
            # æ›´æ–°æ™ºèƒ½ä½“
            if len(agent.memory['states']) >= agent.batch_size:
                update_info = agent.update()
            
            # å®šæœŸè¯„ä¼°å’Œä¿å­˜
            if (episode + 1) % save_frequency == 0:
                # è®¡ç®—æ€§èƒ½æŒ‡æ ‡
                if len(env.portfolio_returns) > 10:
                    returns_array = np.array(env.portfolio_returns)
                    total_return = (env.portfolio_value / env.initial_capital) - 1.0
                    
                    # å¹´åŒ–æ”¶ç›Š
                    trading_days = len(returns_array)
                    annual_return = ((1 + total_return) ** (252 / trading_days)) - 1 if trading_days > 0 else 0.0
                    
                    # å¤æ™®æ¯”ç‡
                    if np.std(returns_array) > 1e-8:
                        sharpe_ratio = np.mean(returns_array) / np.std(returns_array) * np.sqrt(252)
                    else:
                        sharpe_ratio = 0.0
                    
                    logger.info(
                        f"Episode {episode + 1}/{total_episodes}: "
                        f"å¥–åŠ±={episode_reward:.2f}, "
                        f"å¹´åŒ–æ”¶ç›Š={annual_return:.2%}, "
                        f"å¤æ™®æ¯”ç‡={sharpe_ratio:.2f}"
                    )
                    
                    # æ›´æ–°æœ€ä½³æ¨¡å‹
                    if annual_return > best_performance['annual_return']:
                        best_performance.update({
                            'episode': episode + 1,
                            'annual_return': annual_return,
                            'sharpe_ratio': sharpe_ratio
                        })
                        
                        # ä¿å­˜æœ€ä½³æ¨¡å‹
                        model_dir = self.config.get('model', {}).get('save_dir', './models/enhanced_csi300_2020_2022')
                        os.makedirs(model_dir, exist_ok=True)
                        model_path = f"{model_dir}/best_enhanced_agent_episode_{episode + 1}.pth"
                        agent.save_model(model_path)
                        best_performance['model_path'] = model_path
                        
                        logger.info(f"ä¿å­˜æœ€ä½³æ¨¡å‹: {model_path}, å¹´åŒ–æ”¶ç›Š: {annual_return:.2%}")
        
        logger.info("âœ… è®­ç»ƒå®Œæˆ")
        logger.info(f"æœ€ä½³æ€§èƒ½: Episode {best_performance['episode']}, "
                   f"å¹´åŒ–æ”¶ç›Š: {best_performance['annual_return']:.2%}, "
                   f"å¤æ™®æ¯”ç‡: {best_performance['sharpe_ratio']:.2f}")
        
        return best_performance
    
    def backtest(self, model_path: str):
        """å›æµ‹å¢å¼ºç³»ç»Ÿ"""
        logger.info("ğŸ“Š å¼€å§‹å›æµ‹å¢å¼ºç³»ç»Ÿ")
        
        # å‡†å¤‡å›æµ‹æ•°æ®
        price_data, factor_data = self.prepare_data()
        
        # åˆ›å»ºå›æµ‹ç¯å¢ƒå’Œæ™ºèƒ½ä½“
        env = self.create_enhanced_environment(price_data, factor_data)
        agent = self.create_enhanced_agent(env)
        safety_shield = self.create_enhanced_safety_shield()
        
        # åŠ è½½æ¨¡å‹
        logger.info(f"åŠ è½½æ¨¡å‹: {model_path}")
        agent.load_model(model_path)
        
        # è¿è¡Œå›æµ‹
        logger.info("å¼€å§‹è¿è¡Œå›æµ‹...")
        state = env.reset()
        
        step_count = 0
        while True:
            # è·å–åŠ¨ä½œï¼ˆç¡®å®šæ€§ï¼‰
            action, _, _ = agent.get_action(state, deterministic=True)
            
            # åº”ç”¨å®‰å…¨ä¿æŠ¤
            safe_action = safety_shield.shield_action(action, {})
            
            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done, info = env.step(safe_action)
            
            state = next_state
            step_count += 1
            
            # å®šæœŸè¾“å‡ºè¿›åº¦
            if step_count % 50 == 0:
                current_return = (env.portfolio_value / env.initial_capital) - 1.0
                current_drawdown = (env.peak_value - env.portfolio_value) / env.peak_value
                logger.info(f"å›æµ‹è¿›åº¦: {step_count} æ­¥, ç´¯ç§¯æ”¶ç›Š: {current_return:.2%}, å›æ’¤: {current_drawdown:.2%}")
            
            if done:
                break
        
        # è®¡ç®—æœ€ç»ˆæ€§èƒ½
        returns_array = np.array(env.portfolio_returns)
        total_return = (env.portfolio_value / env.initial_capital) - 1.0
        
        # å¹´åŒ–æ”¶ç›Š
        trading_days = len(returns_array)
        annual_return = ((1 + total_return) ** (252 / trading_days)) - 1 if trading_days > 0 else 0.0
        
        # å¤æ™®æ¯”ç‡
        if np.std(returns_array) > 1e-8:
            sharpe_ratio = np.mean(returns_array) / np.std(returns_array) * np.sqrt(252)
        else:
            sharpe_ratio = 0.0
        
        # æœ€å¤§å›æ’¤
        max_drawdown = (env.peak_value - env.portfolio_value) / env.peak_value
        
        # èƒœç‡
        win_rate = np.mean(returns_array > 0) if len(returns_array) > 0 else 0.0
        
        # å¡å°”ç›æ¯”ç‡
        calmar_ratio = annual_return / max_drawdown if max_drawdown > 1e-8 else 0.0
        
        # è¾“å‡ºç»“æœ
        logger.info("ğŸ“ˆ å›æµ‹å®Œæˆ")
        logger.info("=" * 60)
        logger.info("å¢å¼ºç³»ç»Ÿå›æµ‹æŠ¥å‘Š")
        logger.info("=" * 60)
        
        print(f"\nã€å›æµ‹æ¦‚è¦ã€‘")
        print(f"å›æµ‹æœŸé—´: {self.config['data']['start_date']} è‡³ {self.config['data']['end_date']}")
        print(f"å›æµ‹æ­¥æ•°: {step_count}")
        print(f"æœ€ç»ˆç»„åˆä»·å€¼: {env.portfolio_value:,.0f}")
        
        print(f"\nã€æ ¸å¿ƒæŒ‡æ ‡ã€‘")
        print(f"å¹´åŒ–æ”¶ç›Šç‡: {annual_return:.2%}")
        print(f"å¤æ™®æ¯”ç‡: {sharpe_ratio:.2f}")
        print(f"æœ€å¤§å›æ’¤: {max_drawdown:.2%}")
        print(f"èƒœç‡: {win_rate:.1%}")
        print(f"å¡å°”ç›æ¯”ç‡: {calmar_ratio:.2f}")
        
        print(f"\nã€ç›®æ ‡è¾¾æˆè¯„ä¼°ã€‘")
        target_return = 0.08
        target_achieved = annual_return >= target_return
        
        print(f"ç›®æ ‡å¹´åŒ–æ”¶ç›Š: {target_return:.1%}")
        print(f"å®é™…å¹´åŒ–æ”¶ç›Š: {annual_return:.2%}")
        print(f"ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if target_achieved else 'âŒ å¦'}")
        print(f"ç›®æ ‡å®Œæˆåº¦: {annual_return / target_return:.1%}")
        
        if target_achieved:
            print(f"ğŸ‰ æ­å–œï¼å¢å¼ºç³»ç»ŸæˆåŠŸè¾¾åˆ°8%å¹´åŒ–æ”¶ç›Šç›®æ ‡ï¼")
        else:
            gap = target_return - annual_return
            print(f"ğŸ“ˆ è·ç¦»ç›®æ ‡è¿˜éœ€æå‡: {gap:.2%}")
        
        return {
            'annual_return': annual_return,
            'sharpe_ratio': sharpe_ratio,
            'max_drawdown': max_drawdown,
            'win_rate': win_rate,
            'calmar_ratio': calmar_ratio,
            'target_achieved': target_achieved
        }


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='å¢å¼ºç³»ç»Ÿå¿«é€Ÿæµ‹è¯•')
    parser.add_argument('--mode', choices=['train', 'backtest', 'full'], 
                       default='full', help='è¿è¡Œæ¨¡å¼')
    parser.add_argument('--train_config', default='config_enhanced_train.yaml',
                       help='è®­ç»ƒé…ç½®æ–‡ä»¶')
    parser.add_argument('--backtest_config', default='config_enhanced_backtest.yaml',
                       help='å›æµ‹é…ç½®æ–‡ä»¶')
    parser.add_argument('--model', help='æ¨¡å‹è·¯å¾„ï¼ˆå›æµ‹æ¨¡å¼éœ€è¦ï¼‰')
    
    args = parser.parse_args()
    
    # è®¾ç½®æ—¥å¿—
    setup_logger()
    
    logger.info("ğŸš€ å¢å¼ºç³»ç»Ÿå¿«é€Ÿæµ‹è¯•å¼€å§‹")
    
    best_model_path = None
    
    if args.mode in ['train', 'full']:
        logger.info("=" * 60)
        logger.info("ç¬¬ä¸€æ­¥ï¼šè®­ç»ƒå¢å¼ºç³»ç»Ÿ (2020-2022)")
        logger.info("=" * 60)
        
        train_system = EnhancedTradingSystem(args.train_config)
        best_performance = train_system.train()
        best_model_path = best_performance['model_path']
        
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œæœ€ä½³æ¨¡å‹: {best_model_path}")
    
    if args.mode in ['backtest', 'full']:
        logger.info("=" * 60)
        logger.info("ç¬¬äºŒæ­¥ï¼šå›æµ‹å¢å¼ºç³»ç»Ÿ (2023)")
        logger.info("=" * 60)
        
        # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹è·¯å¾„
        if args.model:
            model_path = args.model
        elif best_model_path:
            model_path = best_model_path
        else:
            # æŸ¥æ‰¾æœ€æ–°çš„æ¨¡å‹
            model_dir = './models/enhanced_csi300_2020_2022'
            if os.path.exists(model_dir):
                model_files = [f for f in os.listdir(model_dir) if f.endswith('.pth')]
                if model_files:
                    model_path = os.path.join(model_dir, sorted(model_files)[-1])
                else:
                    raise ValueError("æœªæ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶")
            else:
                raise ValueError("æœªæ‰¾åˆ°æ¨¡å‹ç›®å½•ï¼Œè¯·å…ˆè¿è¡Œè®­ç»ƒ")
        
        logger.info(f"ä½¿ç”¨æ¨¡å‹: {model_path}")
        
        # è¿è¡Œå›æµ‹
        backtest_system = EnhancedTradingSystem(args.backtest_config)
        backtest_results = backtest_system.backtest(model_path)
        
        logger.info("å›æµ‹å®Œæˆ")
    
    logger.info("âœ… å¢å¼ºç³»ç»Ÿå¿«é€Ÿæµ‹è¯•å®Œæˆ")


if __name__ == "__main__":
    main()