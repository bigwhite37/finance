#!/usr/bin/env python3
"""
ç»¼åˆæ”¹è¿›å®æ–½å’Œé‡æ–°è®­ç»ƒæµ‹è¯•
é›†æˆæ‰€æœ‰ä¼˜åŒ–æ”¹è¿›ï¼Œè¿›è¡Œå®Œæ•´çš„è®­ç»ƒ-å›æµ‹éªŒè¯æµç¨‹
"""

import sys
import os
import argparse
import logging
import pickle
import pandas as pd
import numpy as np
from datetime import datetime
from typing import Dict, List, Tuple
import matplotlib.pyplot as plt
import seaborn as sns

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config import ConfigManager
from data import DataManager
from factors import FactorEngine
from rl_agent import TradingEnvironment, CVaRPPOAgent, SafetyShield
from risk_control import RiskController
from backtest import BacktestEngine


class ComprehensiveImprovementTester:
    """ç»¼åˆæ”¹è¿›æµ‹è¯•å™¨"""
    
    def __init__(self, config_path: str = None, verbose: bool = True):
        """
        åˆå§‹åŒ–æµ‹è¯•å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
            verbose: æ˜¯å¦è¯¦ç»†è¾“å‡º
        """
        self.verbose = verbose
        self.setup_logging()
        
        # åˆå§‹åŒ–é…ç½®ç®¡ç†å™¨
        self.config_manager = ConfigManager(config_path or 'config_train.yaml')
        
        # æµ‹è¯•ç»“æœå­˜å‚¨
        self.results = {
            'baseline_performance': None,
            'improved_performance': None,
            'comparison_metrics': None,
            'improvement_summary': None
        }
        
        # æ”¹è¿›è·Ÿè¸ª
        self.improvement_stages = {
            'stage_1_reward_function': False,
            'stage_2_factor_enhancement': False, 
            'stage_3_risk_optimization': False,
            'stage_4_architecture_upgrade': False
        }
        
    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—ç³»ç»Ÿ"""
        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        
        if self.verbose:
            logging.basicConfig(
                level=logging.INFO,
                format=log_format,
                handlers=[
                    logging.FileHandler(f'comprehensive_test_{datetime.now().strftime("%Y%m%d_%H%M%S")}.log'),
                    logging.StreamHandler(sys.stdout)
                ]
            )
        else:
            logging.basicConfig(level=logging.WARNING, format=log_format)
            
        self.logger = logging.getLogger(__name__)
    
    def validate_improvements(self) -> Dict[str, bool]:
        """éªŒè¯æ‰€æœ‰æ”¹è¿›æ˜¯å¦æ­£ç¡®å®æ–½"""
        self.logger.info("å¼€å§‹éªŒè¯æ”¹è¿›å®æ–½çŠ¶æ€...")
        
        validation_results = {}
        
        # 1. éªŒè¯å¥–åŠ±å‡½æ•°ä¼˜åŒ–
        try:
            from rl_agent.trading_environment import TradingEnvironment
            # æ£€æŸ¥å¥–åŠ±å‡½æ•°æ˜¯å¦åŒ…å«ä¼˜åŒ–çš„æ”¶ç›Šæ”¾å¤§é€»è¾‘
            env_config = self.config_manager.get_environment_config()
            test_env = TradingEnvironment(None, None, env_config)
            
            # æ£€æŸ¥_calculate_rewardæ–¹æ³•æ˜¯å¦å­˜åœ¨ä¼˜åŒ–
            import inspect
            reward_code = inspect.getsource(test_env._calculate_reward)
            if "10.0" in reward_code and "æ”¾å¤§æ”¶ç›Šä¿¡å·" in reward_code:
                validation_results['reward_function'] = True
                self.improvement_stages['stage_1_reward_function'] = True
            else:
                validation_results['reward_function'] = False
            
        except Exception as e:
            self.logger.warning(f"å¥–åŠ±å‡½æ•°éªŒè¯å¤±è´¥: {e}")
            validation_results['reward_function'] = False
        
        # 2. éªŒè¯å› å­åº“å¢å¼º
        try:
            from factors import FactorEngine
            factor_config = self.config_manager.get_config('factors')
            factor_engine = FactorEngine(factor_config)
            
            # æ£€æŸ¥æ˜¯å¦æœ‰17ä¸ªå¢å¼ºå› å­
            enhanced_factors = factor_engine.default_factors
            if len(enhanced_factors) >= 17:
                validation_results['factor_enhancement'] = True
                self.improvement_stages['stage_2_factor_enhancement'] = True
            else:
                validation_results['factor_enhancement'] = False
                
        except Exception as e:
            self.logger.warning(f"å› å­åº“éªŒè¯å¤±è´¥: {e}")
            validation_results['factor_enhancement'] = False
        
        # 3. éªŒè¯é£é™©æ§åˆ¶ä¼˜åŒ–
        try:
            config = self.config_manager.get_config('safety_shield')
            
            # æ£€æŸ¥å…³é”®å‚æ•°æ˜¯å¦ä¼˜åŒ–
            optimized_params = {
                'max_position': 0.17,
                'max_leverage': 1.4,
                'var_threshold': 0.035,
                'max_drawdown_threshold': 0.12
            }
            
            all_optimized = True
            for param, expected in optimized_params.items():
                actual = config.get(param, 0)
                if abs(actual - expected) > 0.01:  # å…è®¸å°è¯¯å·®
                    all_optimized = False
                    break
            
            validation_results['risk_optimization'] = all_optimized
            if all_optimized:
                self.improvement_stages['stage_3_risk_optimization'] = True
                
        except Exception as e:
            self.logger.warning(f"é£é™©æ§åˆ¶éªŒè¯å¤±è´¥: {e}")
            validation_results['risk_optimization'] = False
        
        # 4. éªŒè¯ç½‘ç»œæ¶æ„å‡çº§ï¼ˆæ£€æŸ¥æ˜¯å¦æœ‰å¢å¼ºæ¶æ„æ–‡ä»¶ï¼‰
        try:
            enhanced_arch_exists = os.path.exists('rl_agent/enhanced_architecture.py')
            training_strategy_exists = os.path.exists('rl_agent/enhanced_training_strategy.py')
            
            validation_results['architecture_upgrade'] = enhanced_arch_exists and training_strategy_exists
            if validation_results['architecture_upgrade']:
                self.improvement_stages['stage_4_architecture_upgrade'] = True
                
        except Exception as e:
            self.logger.warning(f"æ¶æ„å‡çº§éªŒè¯å¤±è´¥: {e}")
            validation_results['architecture_upgrade'] = False
        
        # è¾“å‡ºéªŒè¯ç»“æœ
        self.logger.info("æ”¹è¿›éªŒè¯ç»“æœ:")
        for improvement, status in validation_results.items():
            status_str = "âœ… å·²å®æ–½" if status else "âŒ æœªå®æ–½"
            self.logger.info(f"  {improvement}: {status_str}")
        
        return validation_results
    
    def run_baseline_test(self) -> Dict[str, float]:
        """è¿è¡ŒåŸºçº¿æµ‹è¯•ï¼ˆä½¿ç”¨å†å²æœ€ä½³æ¨¡å‹ï¼‰"""
        self.logger.info("å¼€å§‹åŸºçº¿æ€§èƒ½æµ‹è¯•...")
        
        try:
            # æ£€æŸ¥æ˜¯å¦æœ‰å†å²æœ€ä½³æ¨¡å‹
            model_files = []
            if os.path.exists('./models'):
                for file in os.listdir('./models'):
                    if file.endswith('.pth') and 'best_agent' in file:
                        model_files.append(file)
            
            if not model_files:
                self.logger.warning("æœªæ‰¾åˆ°å†å²æœ€ä½³æ¨¡å‹ï¼Œè·³è¿‡åŸºçº¿æµ‹è¯•")
                return {'baseline_annual_return': -0.2232}  # ä½¿ç”¨å·²çŸ¥çš„å†å²ç»“æœ
            
            # ä½¿ç”¨æœ€æ–°çš„æœ€ä½³æ¨¡å‹
            latest_model = sorted(model_files)[-1]
            model_path = os.path.join('./models', latest_model)
            
            self.logger.info(f"ä½¿ç”¨åŸºçº¿æ¨¡å‹: {model_path}")
            
            # è¿è¡Œå›æµ‹
            baseline_results = self._run_backtest_with_model(model_path, test_name="baseline")
            
            self.results['baseline_performance'] = baseline_results
            return baseline_results
            
        except Exception as e:
            self.logger.error(f"åŸºçº¿æµ‹è¯•å¤±è´¥: {e}")
            return {'baseline_annual_return': -0.2232}  # ä½¿ç”¨å·²çŸ¥ç»“æœ
    
    def run_comprehensive_training(self) -> Dict[str, float]:
        """è¿è¡Œç»¼åˆæ”¹è¿›çš„å®Œæ•´è®­ç»ƒ"""
        self.logger.info("å¼€å§‹ç»¼åˆæ”¹è¿›è®­ç»ƒ...")
        
        try:
            # åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶
            systems = self._initialize_systems()
            
            # å‡†å¤‡è®­ç»ƒæ•°æ®
            price_data, factor_data = self._prepare_training_data(systems)
            
            # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
            env_config = self.config_manager.get_environment_config()
            environment = TradingEnvironment(factor_data, price_data, env_config)
            
            # åˆ›å»ºæ™ºèƒ½ä½“ï¼ˆä½¿ç”¨ä¼˜åŒ–åçš„é…ç½®ï¼‰
            agent_config = self.config_manager.get_agent_config()
            state_dim = environment.observation_space.shape[0]
            action_dim = environment.action_space.shape[0]
            agent = CVaRPPOAgent(state_dim, action_dim, agent_config)
            
            # åˆ›å»ºå®‰å…¨ä¿æŠ¤å±‚
            shield_config = self.config_manager.get_config('safety_shield')
            safety_shield = SafetyShield(shield_config)
            
            # è®­ç»ƒé…ç½®
            training_config = self.config_manager.get_training_config()
            total_episodes = training_config['total_episodes']
            
            self.logger.info(f"å¼€å§‹è®­ç»ƒ {total_episodes} ä¸ªepisodes...")
            
            # è®­ç»ƒå¾ªç¯
            best_sharpe = float('-inf')
            training_metrics = {
                'episode_rewards': [],
                'sharpe_ratios': [],
                'annual_returns': [],
                'max_drawdowns': []
            }
            
            for episode in range(total_episodes):
                if episode % 20 == 0:
                    self.logger.info(f"è®­ç»ƒè¿›åº¦: {episode}/{total_episodes} ({episode/total_episodes*100:.1f}%)")
                
                # è¿è¡Œä¸€ä¸ªepisode
                episode_metrics = self._run_training_episode(
                    environment, agent, safety_shield, episode
                )
                
                # è®°å½•æŒ‡æ ‡
                for key, value in episode_metrics.items():
                    if key in training_metrics:
                        training_metrics[key].append(value)
                
                # ä¿å­˜æœ€ä½³æ¨¡å‹
                current_sharpe = episode_metrics.get('sharpe_ratio', 0)
                if current_sharpe > best_sharpe and episode > 50:  # è‡³å°‘è®­ç»ƒ50ä¸ªepisodes
                    best_sharpe = current_sharpe
                    model_path = f'./models/improved_best_agent_episode_{episode}.pth'
                    os.makedirs('./models', exist_ok=True)
                    agent.save_model(model_path)
                    self.logger.info(f"ä¿å­˜æ”¹è¿›æ¨¡å‹: {model_path}, Sharpe: {current_sharpe:.4f}")
            
            # è®­ç»ƒå®Œæˆç»Ÿè®¡
            final_metrics = {
                'training_episodes': total_episodes,
                'best_sharpe_ratio': best_sharpe,
                'final_episode_return': training_metrics['episode_rewards'][-1] if training_metrics['episode_rewards'] else 0,
                'avg_recent_return': np.mean(training_metrics['episode_rewards'][-20:]) if len(training_metrics['episode_rewards']) >= 20 else 0
            }
            
            self.logger.info("è®­ç»ƒå®Œæˆ!")
            self.logger.info(f"æœ€ä½³Sharpeæ¯”ç‡: {best_sharpe:.4f}")
            
            return final_metrics
            
        except Exception as e:
            self.logger.error(f"ç»¼åˆè®­ç»ƒå¤±è´¥: {e}")
            import traceback
            self.logger.error(traceback.format_exc())
            return {}\n    \n    def run_improved_backtest(self) -> Dict[str, float]:\n        """è¿è¡Œæ”¹è¿›åçš„å›æµ‹"""\n        self.logger.info("å¼€å§‹æ”¹è¿›æ¨¡å‹å›æµ‹...")\n        \n        try:\n            # æŸ¥æ‰¾æœ€æ–°çš„æ”¹è¿›æ¨¡å‹\n            improved_models = []\n            if os.path.exists('./models'):\n                for file in os.listdir('./models'):\n                    if file.endswith('.pth') and 'improved_best_agent' in file:\n                        improved_models.append(file)\n            \n            if not improved_models:\n                self.logger.error("æœªæ‰¾åˆ°æ”¹è¿›åçš„æ¨¡å‹æ–‡ä»¶")\n                return {}\n            \n            # ä½¿ç”¨æœ€æ–°çš„æ”¹è¿›æ¨¡å‹\n            latest_improved_model = sorted(improved_models)[-1]\n            model_path = os.path.join('./models', latest_improved_model)\n            \n            self.logger.info(f"ä½¿ç”¨æ”¹è¿›æ¨¡å‹: {model_path}")\n            \n            # è¿è¡Œå›æµ‹\n            improved_results = self._run_backtest_with_model(model_path, test_name="improved")\n            \n            self.results['improved_performance'] = improved_results\n            return improved_results\n            \n        except Exception as e:\n            self.logger.error(f"æ”¹è¿›å›æµ‹å¤±è´¥: {e}")\n            return {}\n    \n    def _initialize_systems(self) -> Dict:\n        """åˆå§‹åŒ–ç³»ç»Ÿç»„ä»¶"""\n        # æ•°æ®ç®¡ç†å™¨\n        data_config = self.config_manager.get_data_config()\n        data_manager = DataManager(data_config)\n        \n        # å› å­å¼•æ“\n        factor_config = self.config_manager.get_config('factors')\n        factor_engine = FactorEngine(factor_config)\n        \n        # é£é™©æ§åˆ¶å™¨\n        risk_config = self.config_manager.get_risk_control_config()\n        risk_controller = RiskController(risk_config)\n        \n        # å›æµ‹å¼•æ“\n        backtest_config = self.config_manager.get_backtest_config()\n        backtest_engine = BacktestEngine(backtest_config)\n        \n        return {\n            'data_manager': data_manager,\n            'factor_engine': factor_engine,\n            'risk_controller': risk_controller,\n            'backtest_engine': backtest_engine\n        }\n    \n    def _prepare_training_data(self, systems: Dict) -> Tuple[pd.DataFrame, pd.DataFrame]:\n        """å‡†å¤‡è®­ç»ƒæ•°æ®"""\n        data_manager = systems['data_manager']\n        factor_engine = systems['factor_engine']\n        \n        # è·å–è®­ç»ƒæ•°æ®é…ç½®\n        data_config = self.config_manager.get_data_config()\n        \n        # è·å–è‚¡ç¥¨æ•°æ®\n        stock_data = data_manager.get_stock_data(\n            start_time=data_config['start_date'],\n            end_time=data_config['end_date']\n        )\n        \n        # å¤„ç†ä»·æ ¼å’Œæˆäº¤é‡æ•°æ®\n        price_data = stock_data['$close'].unstack().T\n        volume_data = stock_data['$volume'].unstack().T\n        \n        # ç­›é€‰è‚¡ç¥¨æ± \n        low_vol_stocks = factor_engine.filter_low_volatility_universe(\n            price_data,\n            threshold=self.config_manager.get_value('factors.low_vol_threshold', 0.5),\n            window=self.config_manager.get_value('factors.low_vol_window', 60)\n        )\n        \n        if low_vol_stocks:\n            price_data = price_data[low_vol_stocks]\n            volume_data = volume_data[low_vol_stocks]\n        else:\n            price_data = price_data.iloc[:, :100]\n            volume_data = volume_data.iloc[:, :100]\n            low_vol_stocks = list(price_data.columns)\n        \n        # ä¿å­˜è‚¡ç¥¨æ± \n        os.makedirs('./models', exist_ok=True)\n        with open('./models/selected_stocks.pkl', 'wb') as f:\n            pickle.dump(low_vol_stocks, f)\n        \n        # è®¡ç®—å› å­\n        factor_data = factor_engine.calculate_all_factors(price_data, volume_data)\n        \n        # æ•°æ®å¯¹é½\n        if isinstance(factor_data.index, pd.MultiIndex):\n            factor_data = factor_data.unstack()\n            aligned_price, aligned_factor = price_data.align(factor_data, join='inner', axis=0)\n            price_data = aligned_price\n            factor_data = aligned_factor.stack().reorder_levels(['datetime', 'instrument'])\n        \n        self.logger.info(f"è®­ç»ƒæ•°æ®å‡†å¤‡å®Œæˆ - ä»·æ ¼æ•°æ®: {price_data.shape}, å› å­æ•°æ®: {factor_data.shape}")\n        \n        return price_data, factor_data\n    \n    def _run_training_episode(self, environment, agent, safety_shield, episode: int) -> Dict[str, float]:\n        """è¿è¡Œä¸€ä¸ªè®­ç»ƒepisode"""\n        state, info = environment.reset()\n        episode_reward = 0\n        step_count = 0\n        \n        while True:\n            # è·å–åŠ¨ä½œ\n            action, log_prob, value, cvar_estimate = agent.get_action(state)\n            \n            # åº”ç”¨å®‰å…¨ä¿æŠ¤\n            safe_action = safety_shield.shield_action(\n                action, info, environment.price_data, environment.portfolio_weights\n            )\n            \n            # æ‰§è¡ŒåŠ¨ä½œ\n            next_state, reward, terminated, truncated, info = environment.step(safe_action)\n            \n            # å­˜å‚¨ç»éªŒ\n            agent.store_transition(state, safe_action, reward, value, log_prob, terminated or truncated, cvar_estimate)\n            \n            # æ›´æ–°çŠ¶æ€\n            state = next_state\n            episode_reward += reward\n            step_count += 1\n            \n            if terminated or truncated:\n                break\n        \n        # å®šæœŸæ›´æ–°æ™ºèƒ½ä½“\n        if episode % 10 == 0 and episode > 0:\n            agent.update()\n        \n        # è®¡ç®—episodeæŒ‡æ ‡\n        portfolio_returns = environment.portfolio_returns\n        if len(portfolio_returns) > 1:\n            returns_array = np.array(portfolio_returns)\n            annual_return = np.mean(returns_array) * 252\n            volatility = np.std(returns_array) * np.sqrt(252)\n            sharpe_ratio = annual_return / volatility if volatility > 0 else 0\n            \n            # è®¡ç®—æœ€å¤§å›æ’¤\n            cumulative = np.cumprod(1 + returns_array)\n            running_max = np.maximum.accumulate(cumulative)\n            drawdown = (cumulative - running_max) / running_max\n            max_drawdown = np.min(drawdown)\n        else:\n            annual_return = 0\n            sharpe_ratio = 0\n            max_drawdown = 0\n        \n        return {\n            'episode_reward': episode_reward,\n            'annual_return': annual_return,\n            'sharpe_ratio': sharpe_ratio,\n            'max_drawdown': max_drawdown,\n            'steps': step_count\n        }\n    \n    def _run_backtest_with_model(self, model_path: str, test_name: str) -> Dict[str, float]:\n        """ä½¿ç”¨æŒ‡å®šæ¨¡å‹è¿è¡Œå›æµ‹"""\n        try:\n            # åˆ‡æ¢åˆ°å›æµ‹é…ç½®\n            backtest_config_manager = ConfigManager('config_backtest.yaml')\n            \n            # åˆå§‹åŒ–å›æµ‹ç³»ç»Ÿ\n            systems = self._initialize_systems()\n            \n            # å‡†å¤‡å›æµ‹æ•°æ®\n            backtest_data_config = backtest_config_manager.get_data_config()\n            stock_data = systems['data_manager'].get_stock_data(\n                start_time=backtest_data_config['start_date'],\n                end_time=backtest_data_config['end_date']\n            )\n            \n            # åŠ è½½è®­ç»ƒæ—¶çš„è‚¡ç¥¨æ± \n            with open('./models/selected_stocks.pkl', 'rb') as f:\n                selected_stocks = pickle.load(f)\n            \n            # å¤„ç†å›æµ‹æ•°æ®\n            price_data = stock_data['$close'].unstack().T\n            volume_data = stock_data['$volume'].unstack().T\n            \n            # ä½¿ç”¨ç›¸åŒçš„è‚¡ç¥¨æ± \n            available_stocks = [s for s in selected_stocks if s in price_data.columns]\n            price_data = price_data[available_stocks]\n            volume_data = volume_data[available_stocks]\n            \n            # è®¡ç®—å› å­\n            factor_data = systems['factor_engine'].calculate_all_factors(price_data, volume_data)\n            \n            # æ•°æ®å¯¹é½\n            if isinstance(factor_data.index, pd.MultiIndex):\n                factor_data = factor_data.unstack()\n                aligned_price, aligned_factor = price_data.align(factor_data, join='inner', axis=0)\n                price_data = aligned_price\n                factor_data = aligned_factor.stack().reorder_levels(['datetime', 'instrument'])\n            \n            # åˆ›å»ºç¯å¢ƒå’Œæ™ºèƒ½ä½“\n            env_config = backtest_config_manager.get_environment_config()\n            environment = TradingEnvironment(factor_data, price_data, env_config)\n            \n            agent_config = backtest_config_manager.get_agent_config()\n            state_dim = environment.observation_space.shape[0]\n            action_dim = environment.action_space.shape[0]\n            agent = CVaRPPOAgent(state_dim, action_dim, agent_config)\n            \n            # åŠ è½½æ¨¡å‹\n            agent.load_model(model_path)\n            \n            # åˆ›å»ºå®‰å…¨ä¿æŠ¤å±‚\n            shield_config = backtest_config_manager.get_config('safety_shield')\n            safety_shield = SafetyShield(shield_config)\n            \n            # è¿è¡Œå›æµ‹\n            backtest_results = systems['backtest_engine'].run_backtest(\n                agent=agent,\n                env=environment,\n                start_date=backtest_data_config['start_date'],\n                end_date=backtest_data_config['end_date'],\n                safety_shield=safety_shield\n            )\n            \n            # ç”ŸæˆæŠ¥å‘Š\n            report = systems['backtest_engine'].generate_backtest_report(backtest_results)\n            self.logger.info(f"{test_name}å›æµ‹æŠ¥å‘Š:\\n{report}")\n            \n            # æå–å…³é”®æŒ‡æ ‡\n            key_metrics = {\n                'annual_return': backtest_results.get('annual_return', 0),\n                'sharpe_ratio': backtest_results.get('sharpe_ratio', 0),\n                'max_drawdown': backtest_results.get('max_drawdown', 0),\n                'volatility': backtest_results.get('volatility', 0),\n                'total_days': backtest_results.get('total_days', 0),\n                'win_rate': backtest_results.get('win_rate', 0)\n            }\n            \n            return key_metrics\n            \n        except Exception as e:\n            self.logger.error(f"{test_name}å›æµ‹å¤±è´¥: {e}")\n            import traceback\n            self.logger.error(traceback.format_exc())\n            return {}\n    \n    def generate_comparison_report(self) -> Dict[str, any]:\n        """ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š"""\n        self.logger.info("ç”Ÿæˆæ€§èƒ½å¯¹æ¯”æŠ¥å‘Š...")\n        \n        baseline = self.results.get('baseline_performance', {})\n        improved = self.results.get('improved_performance', {})\n        \n        if not baseline or not improved:\n            self.logger.warning("ç¼ºå°‘åŸºçº¿æˆ–æ”¹è¿›åçš„æ€§èƒ½æ•°æ®")\n            return {}\n        \n        # è®¡ç®—æ”¹è¿›æŒ‡æ ‡\n        comparison = {}\n        \n        # å¹´åŒ–æ”¶ç›Šç‡å¯¹æ¯”\n        baseline_return = baseline.get('annual_return', -0.2232)\n        improved_return = improved.get('annual_return', 0)\n        return_improvement = improved_return - baseline_return\n        return_improvement_pct = (return_improvement / abs(baseline_return)) * 100 if baseline_return != 0 else 0\n        \n        comparison['annual_return'] = {\n            'baseline': baseline_return,\n            'improved': improved_return,\n            'absolute_improvement': return_improvement,\n            'percentage_improvement': return_improvement_pct\n        }\n        \n        # Sharpeæ¯”ç‡å¯¹æ¯”\n        baseline_sharpe = baseline.get('sharpe_ratio', 0)\n        improved_sharpe = improved.get('sharpe_ratio', 0)\n        sharpe_improvement = improved_sharpe - baseline_sharpe\n        \n        comparison['sharpe_ratio'] = {\n            'baseline': baseline_sharpe,\n            'improved': improved_sharpe,\n            'absolute_improvement': sharpe_improvement\n        }\n        \n        # æœ€å¤§å›æ’¤å¯¹æ¯”\n        baseline_dd = baseline.get('max_drawdown', 0)\n        improved_dd = improved.get('max_drawdown', 0)\n        dd_improvement = baseline_dd - improved_dd  # å›æ’¤å‡å°‘æ˜¯å¥½çš„\n        \n        comparison['max_drawdown'] = {\n            'baseline': baseline_dd,\n            'improved': improved_dd,\n            'improvement': dd_improvement\n        }\n        \n        # ç›®æ ‡è¾¾æˆè¯„ä¼°\n        target_return_achieved = improved_return >= 0.08  # 8%ç›®æ ‡\n        target_return_range = 0.08 <= improved_return <= 0.12  # 8-12%ç›®æ ‡åŒºé—´\n        \n        comparison['target_achievement'] = {\n            'target_8pct_achieved': target_return_achieved,\n            'target_range_achieved': target_return_range,\n            'distance_to_target': max(0, 0.08 - improved_return)\n        }\n        \n        self.results['comparison_metrics'] = comparison\n        \n        return comparison\n    \n    def print_final_summary(self):\n        """æ‰“å°æœ€ç»ˆæ€»ç»“"""\n        print("\\n" + "="*60)\n        print("         ç»¼åˆæ”¹è¿›å®æ–½å’Œæµ‹è¯•æ€»ç»“æŠ¥å‘Š")\n        print("="*60)\n        \n        # æ”¹è¿›å®æ–½çŠ¶æ€\n        print("\\nğŸ“‹ æ”¹è¿›å®æ–½çŠ¶æ€:")\n        for stage, status in self.improvement_stages.items():\n            status_icon = "âœ…" if status else "âŒ"\n            stage_name = {\n                'stage_1_reward_function': 'å¥–åŠ±å‡½æ•°ä¼˜åŒ–',\n                'stage_2_factor_enhancement': 'å› å­åº“å¢å¼º',\n                'stage_3_risk_optimization': 'é£é™©æ§åˆ¶ä¼˜åŒ–', \n                'stage_4_architecture_upgrade': 'ç½‘ç»œæ¶æ„å‡çº§'\n            }.get(stage, stage)\n            print(f"  {status_icon} {stage_name}")\n        \n        # æ€§èƒ½å¯¹æ¯”\n        comparison = self.results.get('comparison_metrics', {})\n        if comparison:\n            print("\\nğŸ“Š æ€§èƒ½å¯¹æ¯”ç»“æœ:")\n            \n            # å¹´åŒ–æ”¶ç›Šç‡\n            return_data = comparison.get('annual_return', {})\n            print(f"  å¹´åŒ–æ”¶ç›Šç‡:")\n            print(f"    åŸºçº¿: {return_data.get('baseline', 0)*100:.2f}%")\n            print(f"    æ”¹è¿›å: {return_data.get('improved', 0)*100:.2f}%")\n            print(f"    æ”¹è¿›å¹…åº¦: {return_data.get('absolute_improvement', 0)*100:+.2f}% ")\n            print(f"             ({return_data.get('percentage_improvement', 0):+.1f}%)")\n            \n            # Sharpeæ¯”ç‡\n            sharpe_data = comparison.get('sharpe_ratio', {})\n            print(f"  Sharpeæ¯”ç‡:")\n            print(f"    åŸºçº¿: {sharpe_data.get('baseline', 0):.3f}")\n            print(f"    æ”¹è¿›å: {sharpe_data.get('improved', 0):.3f}")\n            print(f"    æ”¹è¿›å¹…åº¦: {sharpe_data.get('absolute_improvement', 0):+.3f}")\n            \n            # æœ€å¤§å›æ’¤\n            dd_data = comparison.get('max_drawdown', {})\n            print(f"  æœ€å¤§å›æ’¤:")\n            print(f"    åŸºçº¿: {dd_data.get('baseline', 0)*100:.2f}%")\n            print(f"    æ”¹è¿›å: {dd_data.get('improved', 0)*100:.2f}%")\n            print(f"    å›æ’¤å‡å°‘: {dd_data.get('improvement', 0)*100:+.2f}%")\n            \n            # ç›®æ ‡è¾¾æˆ\n            target_data = comparison.get('target_achievement', {})\n            print(f"\\nğŸ¯ ç›®æ ‡è¾¾æˆæƒ…å†µ:")\n            target_8_icon = "âœ…" if target_data.get('target_8pct_achieved', False) else "âŒ"\n            target_range_icon = "âœ…" if target_data.get('target_range_achieved', False) else "âŒ"\n            print(f"  {target_8_icon} 8%å¹´åŒ–æ”¶ç›Šç‡ç›®æ ‡")\n            print(f"  {target_range_icon} 8-12%ç›®æ ‡åŒºé—´")\n            \n            distance = target_data.get('distance_to_target', 0)\n            if distance > 0:\n                print(f"  ğŸ“ è·ç¦»8%ç›®æ ‡è¿˜éœ€: {distance*100:.2f}%")\n        \n        print("\\n" + "="*60)\n        \n        # æ€»ä½“è¯„ä¼°\n        implemented_count = sum(self.improvement_stages.values())\n        total_improvements = len(self.improvement_stages)\n        \n        if comparison and implemented_count > 0:\n            improved_return = comparison.get('annual_return', {}).get('improved', 0)\n            if improved_return >= 0.08:\n                print("ğŸ‰ ç»¼åˆè¯„ä¼°: ä¼˜ç§€ - å·²è¾¾æˆæ”¶ç›Šç‡ç›®æ ‡!")\n            elif improved_return >= 0.05:\n                print("ğŸ‘ ç»¼åˆè¯„ä¼°: è‰¯å¥½ - æ˜¾è‘—æ”¹å–„ï¼Œæ¥è¿‘ç›®æ ‡")\n            elif improved_return >= 0:\n                print("ğŸ“ˆ ç»¼åˆè¯„ä¼°: æ”¹å–„ - æ‰­äºä¸ºç›ˆï¼Œç»§ç»­ä¼˜åŒ–")\n            else:\n                print("âš ï¸  ç»¼åˆè¯„ä¼°: éœ€è¦è¿›ä¸€æ­¥ä¼˜åŒ–")\n        else:\n            print("â„¹ï¸  ç»¼åˆè¯„ä¼°: æ”¹è¿›å®æ–½å®Œæˆï¼Œç­‰å¾…æ€§èƒ½éªŒè¯")\n        \n        print("="*60)\n\n\ndef main():\n    """ä¸»å‡½æ•°"""\n    parser = argparse.ArgumentParser(description='ç»¼åˆæ”¹è¿›å®æ–½å’Œæµ‹è¯•')\n    parser.add_argument('--config', type=str, help='é…ç½®æ–‡ä»¶è·¯å¾„')\n    parser.add_argument('--skip-baseline', action='store_true', help='è·³è¿‡åŸºçº¿æµ‹è¯•')\n    parser.add_argument('--skip-training', action='store_true', help='è·³è¿‡é‡æ–°è®­ç»ƒ')\n    parser.add_argument('--only-validate', action='store_true', help='ä»…éªŒè¯æ”¹è¿›å®æ–½')\n    parser.add_argument('--verbose', '-v', action='store_true', help='è¯¦ç»†è¾“å‡º')\n    \n    args = parser.parse_args()\n    \n    # åˆ›å»ºæµ‹è¯•å™¨\n    tester = ComprehensiveImprovementTester(\n        config_path=args.config,\n        verbose=args.verbose\n    )\n    \n    try:\n        # éªŒè¯æ”¹è¿›å®æ–½\n        validation_results = tester.validate_improvements()\n        \n        if args.only_validate:\n            tester.print_final_summary()\n            return\n        \n        # è¿è¡ŒåŸºçº¿æµ‹è¯•\n        if not args.skip_baseline:\n            baseline_results = tester.run_baseline_test()\n        \n        # è¿è¡Œç»¼åˆè®­ç»ƒ\n        if not args.skip_training:\n            training_results = tester.run_comprehensive_training()\n        \n        # è¿è¡Œæ”¹è¿›åå›æµ‹\n        improved_results = tester.run_improved_backtest()\n        \n        # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š\n        comparison = tester.generate_comparison_report()\n        \n        # æ‰“å°æœ€ç»ˆæ€»ç»“\n        tester.print_final_summary()\n        \n    except KeyboardInterrupt:\n        tester.logger.info("ç”¨æˆ·ä¸­æ–­æµ‹è¯•")\n    except Exception as e:\n        tester.logger.error(f"æµ‹è¯•è¿‡ç¨‹å‘ç”Ÿé”™è¯¯: {e}")\n        import traceback\n        tester.logger.error(traceback.format_exc())\n\n\nif __name__ == "__main__":\n    main()