"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨å®ç°
å®ç°RLTrainerç±»å’Œå¼ºåŒ–å­¦ä¹ è®­ç»ƒå¾ªç¯ï¼ŒåŒ…æ‹¬æ—©åœæœºåˆ¶ã€å­¦ä¹ ç‡è°ƒåº¦å’Œæ¢¯åº¦è£å‰ª
"""

import os
import numpy as np
import pandas as pd
import torch
import torch.nn as nn
import torch.multiprocessing as mp
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union
from dataclasses import dataclass, field
import logging
import pickle
from pathlib import Path
import time
import multiprocessing

from .data_split_strategy import SplitResult
from stable_baselines3.common.callbacks import EvalCallback, CheckpointCallback, StopTrainingOnRewardThreshold, BaseCallback
import os
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import SubprocVecEnv, DummyVecEnv

logger = logging.getLogger(__name__)


class TrainingProgressCallback(BaseCallback):
    """
    æ”¹è¿›çš„è®­ç»ƒè¿›åº¦å›è°ƒå‡½æ•°
    
    ä¿®å¤é—®é¢˜ï¼š
    1. ä½¿ç”¨model.num_timestepsæ›¿ä»£n_callsï¼ˆå¦‚sb3.mdå»ºè®®ï¼‰
    2. æ·»åŠ å¤šè¿›ç¨‹rankåˆ¤æ–­é¿å…é‡å¤æ—¥å¿—
    3. è€ƒè™‘å¤šç¯å¢ƒä¸‹çš„æ­¥æ•°è°ƒæ•´
    """
    
    def __init__(self, log_freq: int = 1000, verbose: int = 0):
        super().__init__(verbose)
        self.log_freq = log_freq
        self.logger = logging.getLogger(__name__)
        self.last_logged_timestep = 0
        
        # è·å–å½“å‰è¿›ç¨‹rankï¼ˆç”¨äºå¤šè¿›ç¨‹ç¯å¢ƒï¼‰
        self.rank = os.environ.get('RANK', '0')
        self.is_main_process = self.rank == '0' or self.rank == 0
        
    def _on_step(self) -> bool:
        """æ¯æ­¥è°ƒç”¨çš„å›è°ƒ"""
        # åªåœ¨ä¸»è¿›ç¨‹ä¸­è®°å½•æ—¥å¿—
        if not self.is_main_process:
            return True
            
        # ä½¿ç”¨model.num_timestepsè€Œä¸æ˜¯n_callsï¼ˆå¦‚sb3.mdå»ºè®®ï¼‰
        current_timesteps = self.model.num_timesteps
        
        # æ¯éš”log_freqä¸ªtimestepsæ‰“å°ä¸€æ¬¡ç»Ÿè®¡ä¿¡æ¯  
        if current_timesteps - self.last_logged_timestep >= self.log_freq:
            self.last_logged_timestep = current_timesteps
            
            # è·å–è®­ç»ƒç»Ÿè®¡ä¿¡æ¯
            if hasattr(self.model, 'logger') and self.model.logger is not None:
                # ä»SB3çš„loggerä¸­è·å–ç»Ÿè®¡ä¿¡æ¯
                record = self.model.logger.name_to_value
                
                # æå–å…³é”®æŒ‡æ ‡
                stats = {}
                if 'train/actor_loss' in record:
                    stats['actor_loss'] = record['train/actor_loss']
                if 'train/critic_loss' in record:
                    stats['critic_loss'] = record['train/critic_loss']
                if 'train/ent_coef' in record:
                    stats['entropy_coef'] = record['train/ent_coef']
                if 'train/ent_coef_loss' in record:
                    stats['entropy_loss'] = record['train/ent_coef_loss']
                if 'train/learning_rate' in record:
                    stats['learning_rate'] = record['train/learning_rate']
                
                # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
                if stats:
                    stats_str = " | ".join([f"{k}: {v:.4f}" for k, v in stats.items()])
                    self.logger.info(f"Timestep {current_timesteps}: {stats_str}")
        
        return True

# å°è¯•å¯¼å…¥å¢å¼ºæŒ‡æ ‡æ¨¡å—ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è·³è¿‡
try:
    from ..metrics.portfolio_metrics import (
        PortfolioMetricsCalculator,
        PortfolioMetrics,
        AgentBehaviorMetrics,
        RiskControlMetrics
    )
    ENHANCED_METRICS_AVAILABLE = True
except ImportError:
    logger.warning("å¢å¼ºæŒ‡æ ‡æ¨¡å—ä¸å¯ç”¨ï¼Œå°†è·³è¿‡ç›¸å…³åŠŸèƒ½")
    ENHANCED_METRICS_AVAILABLE = False






@dataclass
class TrainingConfig:
    """
    è®­ç»ƒé…ç½® - ä½œä¸ºé…ç½®ä¼˜å…ˆçº§çš„ä¸»é…ç½®ç±»
    
    é…ç½®ä¼˜å…ˆçº§ï¼š
    1. TrainingConfigä¸­çš„å‚æ•°ï¼ˆæœ€é«˜ä¼˜å…ˆçº§ï¼‰
    2. SACConfigä¸­çš„å‚æ•°ï¼ˆå¦‚æœTrainingConfigä¸­æ²¡æœ‰å¯¹åº”å‚æ•°ï¼‰
    3. é»˜è®¤å€¼ï¼ˆæœ€ä½ä¼˜å…ˆçº§ï¼‰
    
    æ³¨æ„ï¼šé‡å¤å­—æ®µå°†ä»SACConfigä¸­ç§»é™¤ï¼Œç”±æ­¤ç±»ç»Ÿä¸€ç®¡ç†
    """
    # === SB3æ ¸å¿ƒå‚æ•°ï¼ˆé«˜ä¼˜å…ˆçº§ç»Ÿä¸€ç®¡ç†ï¼‰ ===
    total_timesteps: int = 1000000  # æ€»è®­ç»ƒæ­¥æ•°
    n_envs: int = 1  # å¹¶è¡Œç¯å¢ƒæ•°é‡
    batch_size: int = 256           # æ‰¹æ¬¡å¤§å°
    learning_rate: float = 3e-4     # å­¦ä¹ ç‡
    buffer_size: int = 1000000      # ç»éªŒå›æ”¾ç¼“å†²åŒºå¤§å°
    
    # === SACç®—æ³•å‚æ•° ===
    gamma: float = 0.99  # æŠ˜æ‰£å› å­
    tau: float = 0.005   # è½¯æ›´æ–°å‚æ•°

    # === SB3å›è°ƒé¢‘ç‡ï¼ˆtimestepså•ä½ï¼‰ ===
    eval_freq: int = 10000          # è¯„ä¼°é¢‘ç‡ï¼ˆä»¥timestepsè®¡ï¼‰
    save_freq: int = 50000          # ä¿å­˜é¢‘ç‡ï¼ˆä»¥timestepsè®¡ï¼‰
    n_eval_episodes: int = 5        # è¯„ä¼°æ—¶è¿è¡Œçš„episodeæ•°

    # === å‘åå…¼å®¹çš„episodeå‚æ•°ï¼ˆç”¨äºè®¡ç®—total_timestepsï¼‰ ===
    n_episodes: int = 5000
    max_steps_per_episode: int = 180  # é™ä½ä»¥åŒ¹é…å®é™…æ•°æ®é•¿åº¦
    
    # å‘åå…¼å®¹çš„episodeé¢‘ç‡ï¼ˆå°†è¢«è½¬æ¢ä¸ºtimestepé¢‘ç‡ï¼‰
    validation_frequency: int = 50
    save_frequency: int = 100

    # === æ—©åœå‚æ•°ï¼ˆtimestepså•ä½ï¼‰ ===
    early_stopping_patience: int = 20000    # æ—©åœè€å¿ƒï¼ˆä»episodeè½¬ä¸ºtimestepsï¼‰
    early_stopping_min_delta: float = 0.001
    early_stopping_mode: str = 'max'  # 'max' or 'min'

    # === å­¦ä¹ ç‡è°ƒåº¦ ===
    lr_scheduler_step_size: int = 1000
    lr_scheduler_gamma: float = 0.95

    # === æ¢¯åº¦è£å‰ª ===
    gradient_clip_norm: Optional[float] = 1.0

    # === å·²åºŸå¼ƒå­—æ®µï¼ˆç”±SB3æ¥ç®¡ï¼Œä¿ç•™ä»…ä¸ºå‘åå…¼å®¹ï¼‰ ===
    warmup_episodes: int = 1  # DEPRECATED: SB3ä½¿ç”¨learning_starts
    update_frequency: int = 1  # DEPRECATED: SB3ä½¿ç”¨train_freq
    target_update_frequency: int = 1  # DEPRECATED: SB3ä½¿ç”¨target_update_interval

    # ä¿å­˜è·¯å¾„
    save_dir: str = "./checkpoints"

    # éšæœºç§å­
    random_seed: Optional[int] = None

    # è®¾å¤‡
    device: str = "cuda" if torch.cuda.is_available() else "cpu"

    # å›æ’¤æ§åˆ¶å’Œå¥–åŠ±ä¼˜åŒ–å‚æ•°
    enable_drawdown_monitoring: bool = False    # å¯ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›æ’¤ç›‘æ§
    drawdown_early_stopping: bool = False      # åŸºäºå›æ’¤çš„æ—©åœ
    max_training_drawdown: float = 0.3         # è®­ç»ƒè¿‡ç¨‹æœ€å¤§å…è®¸å›æ’¤
    reward_enhancement_progress: float = 0.5   # å¥–åŠ±å¢å¼ºè¿›åº¦ï¼ˆ0-1ï¼‰

    # è‡ªé€‚åº”è®­ç»ƒå‚æ•°
    enable_adaptive_learning: bool = False     # å¯ç”¨è‡ªé€‚åº”å­¦ä¹ å‚æ•°è°ƒæ•´
    lr_adaptation_factor: float = 0.8          # å­¦ä¹ ç‡è‡ªé€‚åº”å› å­
    min_lr_factor: float = 0.01                # æœ€å°å­¦ä¹ ç‡å› å­
    max_lr_factor: float = 1.0                 # æœ€å¤§å­¦ä¹ ç‡å› å­
    lr_recovery_factor: float = 1.25           # å­¦ä¹ ç‡æ¢å¤å› å­
    performance_threshold_down: float = 0.85   # æ€§èƒ½ä¸‹é™é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
    performance_threshold_up: float = 1.15     # æ€§èƒ½æå‡é˜ˆå€¼ï¼ˆæ›´ä¸¥æ ¼ï¼‰
    batch_size_adaptation: bool = False        # æ‰¹æ¬¡å¤§å°è‡ªé€‚åº”
    exploration_decay_by_performance: bool = False  # åŸºäºæ€§èƒ½çš„æ¢ç´¢è¡°å‡

    # å¤šæ ¸å¹¶è¡Œä¼˜åŒ–å‚æ•°ï¼ˆä¿ç•™ä»¥å‘åå…¼å®¹ï¼‰
    enable_multiprocessing: bool = True            # å¯ç”¨å¤šè¿›ç¨‹ä¼˜åŒ–
    num_workers: int = field(default_factory=lambda: min(8, multiprocessing.cpu_count()))  # æ•°æ®åŠ è½½å·¥ä½œè¿›ç¨‹æ•°
    parallel_environments: int = field(default_factory=lambda: min(4, multiprocessing.cpu_count() // 2))  # å¹¶è¡Œç¯å¢ƒæ•°é‡ï¼ˆå·²è¿ç§»åˆ°n_envsï¼‰
    data_loader_workers: int = field(default_factory=lambda: min(4, multiprocessing.cpu_count() // 2))    # DataLoaderå·¥ä½œçº¿ç¨‹æ•°
    pin_memory: bool = True                        # GPUå†…å­˜å›ºå®š
    persistent_workers: bool = True                # æŒä¹…åŒ–å·¥ä½œè¿›ç¨‹
    prefetch_factor: int = 2                       # é¢„å–å› å­

    # å¥–åŠ±é˜ˆå€¼ï¼ˆç”¨äºæ—©åœï¼‰
    reward_threshold: Optional[float] = None       # å¥–åŠ±é˜ˆå€¼ï¼Œè¾¾åˆ°æ—¶åœæ­¢è®­ç»ƒ

    # GPUä¼˜åŒ–å‚æ•°
    enable_mixed_precision: bool = False         # DEPRECATED: SB3 >= 2.2ä¼šè‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦ï¼Œæ‰‹åŠ¨å¯ç”¨å¯èƒ½å†²çª
    enable_cudnn_benchmark: bool = True          # å¯ç”¨cuDNNåŸºå‡†æµ‹è¯•
    non_blocking_transfer: bool = True           # éé˜»å¡æ•°æ®ä¼ è¾“

    # å¢å¼ºæŒ‡æ ‡é…ç½®
    enable_portfolio_metrics: bool = True          # å¯ç”¨æŠ•èµ„ç»„åˆæŒ‡æ ‡è®¡ç®—
    enable_agent_behavior_metrics: bool = True     # å¯ç”¨æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡è®¡ç®—
    enable_risk_control_metrics: bool = True       # å¯ç”¨é£é™©æ§åˆ¶æŒ‡æ ‡è®¡ç®—

    # æŒ‡æ ‡è®¡ç®—é¢‘ç‡ï¼ˆtimestepså•ä½ï¼‰
    metrics_calculation_frequency: int = 3600      # æ¯Nä¸ªtimestepsè®¡ç®—ä¸€æ¬¡æŒ‡æ ‡ï¼ˆä»episodeæ¢ç®—ï¼‰

    # åŸºå‡†æ•°æ®é…ç½®
    benchmark_data_path: Optional[str] = None      # åŸºå‡†æ•°æ®è·¯å¾„
    risk_free_rate: float = 0.03                   # æ— é£é™©åˆ©ç‡

    # ç¯å¢ƒé…ç½®ï¼ˆç”¨äºæŒ‡æ ‡è®¡ç®—çš„é»˜è®¤å€¼ï¼‰
    initial_cash: float = 1000000.0                # åˆå§‹èµ„é‡‘ï¼ˆç”¨äºæŒ‡æ ‡è®¡ç®—é»˜è®¤å€¼ï¼‰

    # æ—¥å¿—é…ç½®
    detailed_metrics_logging: bool = True          # è¯¦ç»†æŒ‡æ ‡æ—¥å¿—
    metrics_log_level: str = 'INFO'                # æŒ‡æ ‡æ—¥å¿—çº§åˆ«

    def __post_init__(self):
        """é…ç½®éªŒè¯"""
        if self.n_episodes <= 0:
            raise ValueError("n_episodeså¿…é¡»ä¸ºæ­£æ•°")

        if self.learning_rate <= 0:
            raise ValueError("learning_rateå¿…é¡»ä¸ºæ­£æ•°")

        if self.batch_size <= 0:
            raise ValueError("batch_sizeå¿…é¡»ä¸ºæ­£æ•°")

        if self.max_steps_per_episode <= 0:
            raise ValueError("max_steps_per_episodeå¿…é¡»ä¸ºæ­£æ•°")

        if self.early_stopping_mode not in ['max', 'min']:
            raise ValueError("early_stopping_modeå¿…é¡»æ˜¯'max'æˆ–'min'")

        # è‡ªåŠ¨è®¡ç®—total_timestepsï¼ˆå¦‚æœæœªè®¾ç½®æˆ–ä¸º0ï¼‰
        if self.total_timesteps == 1000000 or self.total_timesteps == 0:  # ä½¿ç”¨é»˜è®¤å€¼æˆ–å ä½ç¬¦
            self.total_timesteps = self.n_episodes * self.max_steps_per_episode * self.n_envs

        # è‡ªåŠ¨è®¡ç®—SB3å›è°ƒé¢‘ç‡ï¼ˆä»episodeè½¬æ¢ä¸ºtimestepsï¼‰
        if self.eval_freq == 10000:
            self.eval_freq = self.validation_frequency * self.max_steps_per_episode * self.n_envs
        if self.save_freq == 50000:
            self.save_freq = self.save_frequency * self.max_steps_per_episode * self.n_envs
            
        # è½¬æ¢æ—©åœè€å¿ƒä»episodeåˆ°timestepsï¼ˆå¦‚æœä½¿ç”¨é»˜è®¤å€¼ï¼‰
        if self.early_stopping_patience == 20000:  # æ–°çš„é»˜è®¤å€¼
            # åŸºäºåŸå§‹episodeçš„early_stopping_patienceè®¡ç®—
            episode_patience = 20  # åŸå§‹çš„episodeè€å¿ƒ
            self.early_stopping_patience = episode_patience * self.max_steps_per_episode * self.n_envs
            
        # è½¬æ¢æŒ‡æ ‡è®¡ç®—é¢‘ç‡ä»episodeåˆ°timestepsï¼ˆå¦‚æœä½¿ç”¨é»˜è®¤å€¼ï¼‰  
        if self.metrics_calculation_frequency == 3600:  # æ–°çš„é»˜è®¤å€¼
            # åŸºäºåŸå§‹episodeçš„metrics_calculation_frequencyè®¡ç®—
            episode_freq = 20  # åŸå§‹çš„episodeé¢‘ç‡
            self.metrics_calculation_frequency = episode_freq * self.max_steps_per_episode * self.n_envs
            
        # æ£€æŸ¥åºŸå¼ƒå­—æ®µçš„ä½¿ç”¨å¹¶å‘å‡ºè­¦å‘Š
        self._check_deprecated_fields()

        if self.min_lr_factor <= 0 or self.min_lr_factor >= self.max_lr_factor:
            raise ValueError("min_lr_factorå¿…é¡»å¤§äº0ä¸”å°äºmax_lr_factor")

        if self.lr_recovery_factor <= 1.0:
            raise ValueError("lr_recovery_factorå¿…é¡»å¤§äº1.0")

        if self.performance_threshold_down >= 1.0 or self.performance_threshold_up <= 1.0:
            raise ValueError("performance_threshold_downå¿…é¡»å°äº1.0ï¼Œperformance_threshold_upå¿…é¡»å¤§äº1.0")

        # å¤šæ ¸é…ç½®éªŒè¯
        if self.num_workers < 0:
            raise ValueError("num_workerså¿…é¡»ä¸ºéè´Ÿæ•°")

        if self.parallel_environments < 1:
            raise ValueError("parallel_environmentså¿…é¡»ä¸ºæ­£æ•°")

        if self.data_loader_workers < 0:
            raise ValueError("data_loader_workerså¿…é¡»ä¸ºéè´Ÿæ•°")

        # è‡ªåŠ¨è°ƒæ•´å¤šæ ¸é…ç½®ä»¥é¿å…èµ„æºè¿‡åº¦ä½¿ç”¨
        max_workers = multiprocessing.cpu_count()
        if self.num_workers > max_workers:
            self.num_workers = max_workers
        if self.parallel_environments > max_workers:
            self.parallel_environments = max_workers

        # åŒæ­¥n_envsä¸parallel_environments
        if self.n_envs == 1 and self.parallel_environments > 1:
            self.n_envs = self.parallel_environments

        # å¢å¼ºæŒ‡æ ‡é…ç½®éªŒè¯
        if self.metrics_calculation_frequency <= 0:
            raise ValueError("metrics_calculation_frequencyå¿…é¡»ä¸ºæ­£æ•°")

        if self.enable_portfolio_metrics and self.benchmark_data_path == "":
            raise ValueError("å¯ç”¨æŠ•èµ„ç»„åˆæŒ‡æ ‡æ—¶ï¼Œbenchmark_data_pathä¸èƒ½ä¸ºç©º")

        if self.risk_free_rate < 0:
            raise ValueError("risk_free_rateä¸èƒ½ä¸ºè´Ÿæ•°")
            
    def _check_deprecated_fields(self):
        """æ£€æŸ¥åºŸå¼ƒå­—æ®µçš„ä½¿ç”¨å¹¶å‘å‡ºè­¦å‘Š"""
        import warnings
        
        # æ£€æŸ¥warmup_episodesï¼ˆç°åœ¨ä½¿ç”¨learning_startsï¼‰
        if self.warmup_episodes != 1:  # éé»˜è®¤å€¼
            warnings.warn(
                f"warmup_episodeså·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨SACConfig.learning_startsã€‚"
                f"å½“å‰å€¼ï¼š{self.warmup_episodes}",
                DeprecationWarning,
                stacklevel=3
            )
            
        # æ£€æŸ¥update_frequencyï¼ˆç°åœ¨ä½¿ç”¨train_freqï¼‰
        if self.update_frequency != 1:  # éé»˜è®¤å€¼
            warnings.warn(
                f"update_frequencyå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨SACConfig.train_freqã€‚"
                f"å½“å‰å€¼ï¼š{self.update_frequency}",
                DeprecationWarning,
                stacklevel=3
            )
            
        # æ£€æŸ¥target_update_frequencyï¼ˆç°åœ¨ä½¿ç”¨target_update_intervalï¼‰
        if self.target_update_frequency != 1:  # éé»˜è®¤å€¼
            warnings.warn(
                f"target_update_frequencyå·²åºŸå¼ƒï¼Œè¯·ä½¿ç”¨SACConfig.target_update_intervalã€‚"
                f"å½“å‰å€¼ï¼š{self.target_update_frequency}",
                DeprecationWarning,
                stacklevel=3
            )
            
        # æ£€æŸ¥enable_mixed_precisionï¼ˆç°åœ¨ç”±SB3è‡ªåŠ¨å¤„ç†ï¼‰
        if self.enable_mixed_precision:
            warnings.warn(
                "enable_mixed_precisionå·²åºŸå¼ƒï¼ŒSB3 >= 2.2ä¼šè‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦è®­ç»ƒã€‚"
                "æ‰‹åŠ¨å¯ç”¨å¯èƒ½å¯¼è‡´å†²çªï¼Œå»ºè®®è®¾ç½®ä¸ºFalseã€‚",
                DeprecationWarning,
                stacklevel=3
            )


class EarlyStopping:
    """æ—©åœæœºåˆ¶"""

    def __init__(self, patience: int = 20, min_delta: float = 0.001, mode: str = 'max'):
        """
        åˆå§‹åŒ–æ—©åœæœºåˆ¶

        Args:
            patience: è€å¿ƒå€¼ï¼Œå³å…è®¸çš„æ— æ”¹è¿›epochæ•°
            min_delta: æœ€å°æ”¹è¿›å¹…åº¦
            mode: 'max'è¡¨ç¤ºåˆ†æ•°è¶Šé«˜è¶Šå¥½ï¼Œ'min'è¡¨ç¤ºåˆ†æ•°è¶Šä½è¶Šå¥½
        """
        self.patience = patience
        self.min_delta = min_delta
        self.mode = mode
        self.best_score = None
        self.counter = 0
        self.early_stop = False

        if mode == 'max':
            self.is_better = lambda score, best: score > best + min_delta
        else:
            self.is_better = lambda score, best: score < best - min_delta

    def step(self, score: float) -> bool:
        """
        æ›´æ–°æ—©åœçŠ¶æ€

        Args:
            score: å½“å‰åˆ†æ•°

        Returns:
            bool: æ˜¯å¦åº”è¯¥æ—©åœ
        """
        if self.best_score is None:
            self.best_score = score
            return False

        if self.is_better(score, self.best_score):
            self.best_score = score
            self.counter = 0
        else:
            self.counter += 1

        if self.counter > self.patience:
            self.early_stop = True
            return True

        return False

    def reset(self):
        """é‡ç½®æ—©åœçŠ¶æ€"""
        self.best_score = None
        self.counter = 0
        self.early_stop = False


class DrawdownEarlyStoppingCallback(BaseCallback):
    """åŸºäºå›æ’¤çš„æ—©åœå›è°ƒ"""

    def __init__(self, max_drawdown: float = 0.3, patience: int = 10, verbose: int = 0):
        super().__init__(verbose)
        self.max_drawdown = max_drawdown
        self.patience = patience
        self.peak_value = None
        self.counter = 0
        self.drawdown_history = []

    def _on_step(self) -> bool:
        """æ¯æ­¥æ£€æŸ¥å›æ’¤"""
        # ä»ç¯å¢ƒè·å–æŠ•èµ„ç»„åˆä»·å€¼
        if hasattr(self.training_env, 'get_attr'):
            try:
                portfolio_values = self.training_env.get_attr('total_value')
                if portfolio_values:
                    current_value = portfolio_values[0]  # ç¬¬ä¸€ä¸ªç¯å¢ƒçš„å€¼

                    if self.peak_value is None:
                        self.peak_value = current_value
                        return True

                    if current_value > self.peak_value:
                        self.peak_value = current_value

                    current_drawdown = (self.peak_value - current_value) / self.peak_value if self.peak_value > 0 else 0.0
                    current_drawdown = max(current_drawdown, 0.0)

                    self.drawdown_history.append(current_drawdown)

                    if current_drawdown > self.max_drawdown:
                        self.counter += 1
                    else:
                        self.counter = 0

                    if self.counter >= self.patience:
                        if self.verbose > 0:
                            print(f"\nè§¦å‘å›æ’¤æ—©åœ: å½“å‰å›æ’¤ {current_drawdown:.4f} > é˜ˆå€¼ {self.max_drawdown:.4f}")
                        return False
            except:
                pass
        return True

    def get_current_drawdown(self) -> float:
        return self.drawdown_history[-1] if self.drawdown_history else 0.0


class DrawdownEarlyStopping:
    """åŸºäºå›æ’¤çš„æ—©åœæœºåˆ¶ï¼ˆå…¼å®¹æ€§ç±»ï¼‰"""

    def __init__(self, max_drawdown: float = 0.3, patience: int = 10):
        """
        åˆå§‹åŒ–åŸºäºå›æ’¤çš„æ—©åœæœºåˆ¶

        Args:
            max_drawdown: æœ€å¤§å…è®¸å›æ’¤é˜ˆå€¼
            patience: è¶…è¿‡é˜ˆå€¼åçš„è€å¿ƒå€¼
        """
        self.max_drawdown = max_drawdown
        self.patience = patience
        self.peak_value = None
        self.counter = 0
        self.early_stop = False
        self.drawdown_history = []

    def step(self, current_value: float) -> bool:
        """
        æ›´æ–°å›æ’¤æ—©åœçŠ¶æ€

        Args:
            current_value: å½“å‰æŠ•èµ„ç»„åˆä»·å€¼

        Returns:
            bool: æ˜¯å¦åº”è¯¥æ—©åœ
        """
        if self.peak_value is None:
            # åˆå§‹åŒ–å³°å€¼ä¸ºå½“å‰å€¼
            self.peak_value = current_value
            self.drawdown_history.append(0.0)
            return False

        # æ›´æ–°å³°å€¼ï¼šå½“å‰å€¼å¤§äºå†å²å³°å€¼æ—¶æ›´æ–°
        if current_value > self.peak_value:
            self.peak_value = current_value

        # æ ‡å‡†å›æ’¤è®¡ç®—ï¼šç›¸å¯¹äºå†å²æœ€é«˜ç‚¹çš„æŸå¤±ç™¾åˆ†æ¯”
        if self.peak_value > 0:
            current_drawdown = (self.peak_value - current_value) / self.peak_value
            # ç¡®ä¿å›æ’¤ä¸ºéè´Ÿæ•°
            current_drawdown = max(current_drawdown, 0.0)
        else:
            # å¦‚æœå³°å€¼ä¸º0æˆ–è´Ÿæ•°ï¼Œæ— æ³•è®¡ç®—æœ‰æ„ä¹‰çš„å›æ’¤
            current_drawdown = 0.0

        self.drawdown_history.append(current_drawdown)

        # æ£€æŸ¥æ˜¯å¦è¶…è¿‡å›æ’¤é˜ˆå€¼
        if current_drawdown > self.max_drawdown:
            self.counter += 1
        else:
            self.counter = 0

        # åˆ¤æ–­æ˜¯å¦éœ€è¦æ—©åœ
        if self.counter >= self.patience:
            self.early_stop = True
            return True

        return False

    def get_current_drawdown(self) -> float:
        """è·å–å½“å‰å›æ’¤"""
        return self.drawdown_history[-1] if self.drawdown_history else 0.0

    def reset(self):
        """é‡ç½®æ—©åœçŠ¶æ€"""
        self.peak_value = None
        self.counter = 0
        self.early_stop = False
        self.drawdown_history = []




class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(self, config: TrainingConfig, environment, agent, data_split: SplitResult, env_factory=None):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: è®­ç»ƒé…ç½®
            environment: äº¤æ˜“ç¯å¢ƒ
            agent: å¼ºåŒ–å­¦ä¹ æ™ºèƒ½ä½“
            data_split: æ•°æ®åˆ’åˆ†ç»“æœ
            env_factory: ç¯å¢ƒå·¥å‚å‡½æ•°ï¼ˆç”¨äºåˆ›å»ºå¤šä¸ªç¯å¢ƒå®ä¾‹ï¼‰
        """
        self.config = config
        self.environment = environment
        self.agent = agent
        self.data_split = data_split
        self.env_factory = env_factory

        # åˆå§‹åŒ–è®­ç»ƒç»„ä»¶ï¼ˆSB3è‡ªå¸¦æŒ‡æ ‡æ”¶é›†ï¼‰
        self.early_stopping = EarlyStopping(
            patience=config.early_stopping_patience,
            min_delta=config.early_stopping_min_delta,
            mode=config.early_stopping_mode
        )

        # åˆå§‹åŒ–å›æ’¤æ§åˆ¶ç›¸å…³ç»„ä»¶ï¼ˆä¿ç•™ä»¥å‘åå…¼å®¹ï¼‰
        if config.enable_drawdown_monitoring:
            self.drawdown_early_stopping = DrawdownEarlyStopping(
                max_drawdown=config.max_training_drawdown,
                patience=config.early_stopping_patience // 2  # å›æ’¤æ—©åœè€å¿ƒå€¼æ›´å°
            )
            self.drawdown_metrics = []
            logger.info("å›æ’¤ç›‘æ§å·²å¯ç”¨ï¼ˆå°†é€šè¿‡SB3å›è°ƒå®ç°ï¼‰")
        else:
            self.drawdown_early_stopping = None
            self.drawdown_metrics = []

        # è‡ªé€‚åº”è®­ç»ƒå‚æ•°
        self.adaptive_learning_enabled = config.enable_adaptive_learning
        self.current_lr_factor = 1.0
        self.performance_history = []

        # åˆå§‹åŒ–å¢å¼ºæŒ‡æ ‡ç»„ä»¶
        if ENHANCED_METRICS_AVAILABLE:
            self.metrics_calculator = PortfolioMetricsCalculator()

            # å†å²æ•°æ®å­˜å‚¨
            self.portfolio_values_history: List[float] = []
            self.benchmark_values_history: List[float] = []
            self.dates_history: List[datetime] = []

            # æ™ºèƒ½ä½“è¡Œä¸ºæ•°æ®
            self.entropy_history: List[float] = []
            self.position_weights_history: List[np.ndarray] = []

            # é£é™©æ§åˆ¶æ•°æ®
            self.risk_budget_history: List[float] = []
            self.risk_usage_history: List[float] = []
            self.control_signals_history: List[Dict[str, Any]] = []
            self.market_regime_history: List[str] = []

            logger.info(f"æŒ‡æ ‡è®¡ç®—é…ç½®: æŠ•èµ„ç»„åˆæŒ‡æ ‡={config.enable_portfolio_metrics}, "
                       f"æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡={config.enable_agent_behavior_metrics}, "
                       f"é£é™©æ§åˆ¶æŒ‡æ ‡={config.enable_risk_control_metrics}")
        else:
            self.metrics_calculator = None
            self.portfolio_values_history = []
            self.benchmark_values_history = []
            self.dates_history = []
            self.entropy_history = []
            self.position_weights_history = []
            self.risk_budget_history = []
            self.risk_usage_history = []
            self.control_signals_history = []
            self.market_regime_history = []

        # è®¾ç½®éšæœºç§å­
        if config.random_seed is not None:
            self._set_random_seed(config.random_seed)

        # åˆ›å»ºä¿å­˜ç›®å½•
        self.save_dir = Path(config.save_dir)
        self.save_dir.mkdir(parents=True, exist_ok=True)

        # åˆå§‹åŒ–å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚æœæ™ºèƒ½ä½“æ”¯æŒï¼‰
        self._setup_lr_scheduler()

        # åˆå§‹åŒ–å¤šæ ¸ä¼˜åŒ–ç»„ä»¶
        self._setup_multicore_optimization()

        logger.info("è®­ç»ƒå™¨åˆå§‹åŒ–å®Œæˆ")
        logger.debug(f"è®­ç»ƒé…ç½®: {config}")

    def _set_random_seed(self, seed: int):
        """è®¾ç½®éšæœºç§å­"""
        np.random.seed(seed)
        torch.manual_seed(seed)
        if torch.cuda.is_available():
            torch.cuda.manual_seed(seed)
            torch.cuda.manual_seed_all(seed)

    def _setup_lr_scheduler(self):
        """è®¾ç½®å­¦ä¹ ç‡è°ƒåº¦å™¨"""
        # åœ¨å®é™…å®ç°ä¸­ï¼Œè¿™é‡Œä¼šè®¾ç½®PyTorchçš„å­¦ä¹ ç‡è°ƒåº¦å™¨
        # ç”±äºä½¿ç”¨æ¨¡æ‹Ÿæ™ºèƒ½ä½“ï¼Œè¿™é‡Œåªæ˜¯å ä½ç¬¦
        self.lr_scheduler = None

    def _setup_multicore_optimization(self):
        """è®¾ç½®å¤šæ ¸ä¼˜åŒ–"""
        if not self.config.enable_multiprocessing:
            logger.info("å¤šè¿›ç¨‹ä¼˜åŒ–å·²ç¦ç”¨")
            self.parallel_env_manager = None
            self.data_loader = None
            return

        logger.info(f"é…ç½®å¤šæ ¸ä¼˜åŒ–: {self.config.num_workers} ä¸ªæ•°æ®å·¥ä½œè¿›ç¨‹, "
                   f"{self.config.parallel_environments} ä¸ªå¹¶è¡Œç¯å¢ƒ")

        # è®¾ç½®PyTorchå¤šè¿›ç¨‹ä¸Šä¸‹æ–‡
        try:
            mp.set_start_method('spawn', force=True)
        except RuntimeError:
            # å¦‚æœå·²ç»è®¾ç½®äº†start methodï¼Œå¿½ç•¥é”™è¯¯
            pass

        # è®¾ç½®cuDNNåŸºå‡†æµ‹è¯•ï¼ˆå¦‚æœå¯ç”¨GPUä¼˜åŒ–ï¼‰
        if self.config.enable_cudnn_benchmark and torch.cuda.is_available():
            torch.backends.cudnn.benchmark = True
            logger.info("cuDNNåŸºå‡†æµ‹è¯•å·²å¯ç”¨")

        # æ··åˆç²¾åº¦ç°åœ¨ç”±SB3è‡ªåŠ¨å¤„ç†ï¼Œæ— éœ€æ‰‹åŠ¨åˆå§‹åŒ–
        if self.config.enable_mixed_precision:
            logger.warning("enable_mixed_precisionå·²å¯ç”¨ä½†å°†è¢«å¿½ç•¥ï¼ŒSB3ä¼šè‡ªåŠ¨å¤„ç†æ··åˆç²¾åº¦")

        # SB3çš„VecEnvä¼šè‡ªåŠ¨å¤„ç†å¹¶è¡Œç¯å¢ƒ
        logger.info(f"å°†ä½¿ç”¨SB3 VecEnvå¤„ç†{self.config.parallel_environments}ä¸ªå¹¶è¡Œç¯å¢ƒ")

    def _get_current_learning_rate(self, episode: int) -> float:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        # ç®€å•çš„æŒ‡æ•°è¡°å‡
        decay_rate = self.config.lr_scheduler_gamma
        decay_steps = self.config.lr_scheduler_step_size

        decay_factor = decay_rate ** (episode // decay_steps)
        return self.config.learning_rate * decay_factor


    def save_checkpoint(self, filepath: str, timesteps: int):
        """ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆSB3å…¼å®¹ç‰ˆæœ¬ï¼‰"""
        checkpoint = {
            'timesteps': timesteps,
            'config': self.config,
            'early_stopping_state': {
                'best_score': self.early_stopping.best_score,
                'counter': self.early_stopping.counter,
                'early_stop': self.early_stopping.early_stop
            }
        }

        # ä¿å­˜æ™ºèƒ½ä½“çŠ¶æ€ï¼ˆå¦‚æœæ”¯æŒï¼‰
        if hasattr(self.agent, 'save'):
            agent_path = filepath.replace('.pth', '_agent.pth')
            self.agent.save(agent_path)
            checkpoint['agent_path'] = agent_path

        with open(filepath, 'wb') as f:
            pickle.dump(checkpoint, f)

        logger.debug(f"æ£€æŸ¥ç‚¹å·²ä¿å­˜åˆ°: {filepath}")

    def load_checkpoint(self, filepath: str) -> int:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        with open(filepath, 'rb') as f:
            checkpoint = pickle.load(f)

        timesteps = checkpoint.get('timesteps', checkpoint.get('episode', 0))  # å‘åå…¼å®¹

        # æ¢å¤æ—©åœçŠ¶æ€
        early_stopping_state = checkpoint['early_stopping_state']
        self.early_stopping.best_score = early_stopping_state['best_score']
        self.early_stopping.counter = early_stopping_state['counter']
        self.early_stopping.early_stop = early_stopping_state['early_stop']

        # åŠ è½½æ™ºèƒ½ä½“çŠ¶æ€ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if 'agent_path' in checkpoint and hasattr(self.agent, 'load'):
            self.agent.load(checkpoint['agent_path'])

        logger.debug(f"æ£€æŸ¥ç‚¹å·²ä» {filepath} åŠ è½½ï¼Œtimesteps: {timesteps}")
        return timesteps



    def train(self):
        """æ‰§è¡Œè®­ç»ƒ - ä½¿ç”¨SB3çš„.learn()æ–¹æ³•"""
        logger.info(f"å¼€å§‹SB3è®­ç»ƒï¼Œæ€»æ­¥æ•°: {self.config.total_timesteps}")
        logger.info(f"å¹¶è¡Œç¯å¢ƒæ•°: {self.config.n_envs}")

        start_time = time.time()

        # è®¾ç½®æ™ºèƒ½ä½“çš„ç¯å¢ƒï¼ˆå¦‚æœè¿˜æ²¡è®¾ç½®ï¼‰
        if hasattr(self.agent, 'set_env'):
            if self.env_factory and self.config.n_envs > 1:
                # ä½¿ç”¨ç¯å¢ƒå·¥å‚åˆ›å»ºå¤šç¯å¢ƒ
                self.agent.set_env(None, env_factory=self.env_factory, n_envs=self.config.n_envs)
            else:
                # ä½¿ç”¨å•ç¯å¢ƒ
                self.agent.set_env(self.environment)

        # åˆ›å»ºå›è°ƒåˆ—è¡¨
        callbacks = self._create_callbacks()

        # æ‰§è¡ŒSB3è®­ç»ƒ
        try:
            self.agent.learn(
                total_timesteps=self.config.total_timesteps,
                callback=callbacks,
                log_interval=100,  # æ¯100æ¬¡æ›´æ–°æ‰“å°ä¸€æ¬¡æ—¥å¿—
                reset_num_timesteps=True
            )
        except KeyboardInterrupt:
            logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
            raise RuntimeError(f"SB3è®­ç»ƒå¤±è´¥: {e}") from e

        training_time = time.time() - start_time
        logger.info(f"SB3è®­ç»ƒå®Œæˆï¼Œæ€»ç”¨æ—¶: {training_time:.2f}ç§’")

        # ä¿å­˜æœ€ç»ˆæ¨¡å‹
        final_model_path = self.save_dir / "final_model"
        self.agent.save(final_model_path)
        logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜åˆ°: {final_model_path}")

        # è·å–è®­ç»ƒç»Ÿè®¡
        stats = self.agent.get_training_stats() if hasattr(self.agent, 'get_training_stats') else {}
        stats['training_time'] = training_time
        stats['total_timesteps'] = self.config.total_timesteps

        return stats

    def _create_callbacks(self):
        """åˆ›å»ºSB3å›è°ƒåˆ—è¡¨"""
        callbacks = []

        # 1. æ·»åŠ è¿›åº¦å›è°ƒï¼ˆæœ¬åœ°å®ç°ï¼Œé›†ä¸­ç®¡ç†ï¼‰
        progress_callback = TrainingProgressCallback(
            log_freq=max(1000, self.config.total_timesteps // 50)
        )
        callbacks.append(progress_callback)

        # 2. æ·»åŠ è¯„ä¼°å›è°ƒ
        if self.config.eval_freq > 0:
            # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
            eval_env = self.environment
            if self.env_factory:
                # å¦‚æœæœ‰ç¯å¢ƒå·¥å‚ï¼Œåˆ›å»ºå•ç‹¬çš„è¯„ä¼°ç¯å¢ƒ
                eval_env = self.env_factory()

            eval_callback = EvalCallback(
                eval_env,
                best_model_save_path=str(self.save_dir),
                log_path=str(self.save_dir),
                eval_freq=self.config.eval_freq,
                n_eval_episodes=self.config.n_eval_episodes,
                deterministic=True,
                render=False,
                verbose=1
            )
            callbacks.append(eval_callback)

        # 3. æ·»åŠ æ£€æŸ¥ç‚¹å›è°ƒ
        if self.config.save_freq > 0:
            checkpoint_callback = CheckpointCallback(
                save_freq=self.config.save_freq,
                save_path=str(self.save_dir),
                name_prefix="sac_checkpoint",
                verbose=1
            )
            callbacks.append(checkpoint_callback)

        # 4. æ·»åŠ å¥–åŠ±é˜ˆå€¼æ—©åœå›è°ƒ
        if hasattr(self.config, 'reward_threshold') and self.config.reward_threshold is not None:
            early_stop_callback = StopTrainingOnRewardThreshold(
                reward_threshold=self.config.reward_threshold,
                verbose=1
            )
            callbacks.append(early_stop_callback)

        # 5. æ·»åŠ å›æ’¤æ—©åœå›è°ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.config.enable_drawdown_monitoring:
            drawdown_callback = DrawdownEarlyStoppingCallback(
                max_drawdown=self.config.max_training_drawdown,
                patience=self.config.early_stopping_patience // 2,
                verbose=1
            )
            callbacks.append(drawdown_callback)

        # 6. æ·»åŠ å¢å¼ºæŒ‡æ ‡è®°å½•å›è°ƒï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if (ENHANCED_METRICS_AVAILABLE and
            (self.config.enable_portfolio_metrics or
             self.config.enable_agent_behavior_metrics or
             self.config.enable_risk_control_metrics)):
            metrics_callback = EnhancedMetricsCallback(
                trainer=self,
                log_freq=self.config.metrics_calculation_frequency * self.config.max_steps_per_episode
            )
            callbacks.append(metrics_callback)

        logger.info(f"åˆ›å»ºäº†{len(callbacks)}ä¸ªå›è°ƒ: {[type(cb).__name__ for cb in callbacks]}")
        return callbacks


class EnhancedMetricsCallback(BaseCallback):
    """å¢å¼ºæŒ‡æ ‡è®°å½•å›è°ƒ"""

    def __init__(self, trainer: 'RLTrainer', log_freq: int = 1000, verbose: int = 0):
        super().__init__(verbose)
        self.trainer = trainer
        self.log_freq = log_freq

    def _on_step(self) -> bool:
        """æ¯æ­¥è®°å½•å¢å¼ºæŒ‡æ ‡"""
        if self.n_calls % self.log_freq == 0:
            try:
                # ä» trainer ä¸­è®¡ç®—å¢å¼ºæŒ‡æ ‡
                if hasattr(self.trainer, '_calculate_and_log_enhanced_metrics'):
                    # æ¨¡æ‹Ÿ episode ç¼–å·ï¼ˆåŸºäº timestepsï¼‰
                    episode_num = self.n_calls // self.trainer.config.max_steps_per_episode
                    self.trainer._calculate_and_log_enhanced_metrics(episode_num)
            except Exception as e:
                if self.verbose > 0:
                    print(f"è®¡ç®—å¢å¼ºæŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")
        return True

    def _cleanup_multicore_resources(self):
        """æ¸…ç†å¤šæ ¸èµ„æºï¼ˆSB3è‡ªåŠ¨å¤„ç†ï¼‰"""
        # SB3 VecEnv ä¼šè‡ªåŠ¨æ¸…ç†èµ„æº
        logger.debug("SB3ä¼šè‡ªåŠ¨æ¸…ç†å¤šæ ¸èµ„æº")

    def evaluate(self, n_episodes: int = 10) -> Dict[str, float]:
        """è¯„ä¼°æ™ºèƒ½ä½“æ€§èƒ½ï¼ˆä½¿ç”¨SB3å†…ç½®è¯„ä¼°ï¼‰"""
        logger.info(f"å¼€å§‹è¯„ä¼°ï¼Œepisodes: {n_episodes}")

        # SB3çš„è¯„ä¼°ç°åœ¨é€šè¿‡EvalCallbackè‡ªåŠ¨å¤„ç†
        # è¿™é‡Œä¿ç•™æ–¹æ³•ä»¥å‘åå…¼å®¹ï¼Œä½†å»ºè®®ä½¿ç”¨SB3çš„è¯„ä¼°æœºåˆ¶
        logger.info("è¯„ä¼°ç°åœ¨é€šè¿‡SB3çš„EvalCallbackè‡ªåŠ¨è¿›è¡Œ")

        # è¿”å›åŸºæœ¬ç»Ÿè®¡ä¿¡æ¯ï¼ˆä»agentè·å–ï¼‰
        if hasattr(self.agent, 'get_training_stats'):
            return self.agent.get_training_stats()
        else:
            return {'note': 'evaluation_handled_by_sb3_evalcallback'}

    def _monitor_drawdown(self, episode_reward: float, episode: int):
        """ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„å›æ’¤"""
        try:
            # ä»ç¯å¢ƒè·å–çœŸå®çš„æŠ•èµ„ç»„åˆä»·å€¼æ¥è®¡ç®—å›æ’¤
            if hasattr(self.environment, 'total_value'):
                portfolio_value = self.environment.total_value
            else:
                # å¦‚æœç¯å¢ƒä¸æ”¯æŒï¼Œè®°å½•è­¦å‘Šå¹¶è·³è¿‡å›æ’¤ç›‘æ§
                logger.warning("ç¯å¢ƒä¸æ”¯æŒæŠ•èµ„ç»„åˆä»·å€¼è·å–ï¼Œè·³è¿‡å›æ’¤ç›‘æ§")
                return

            # æ›´æ–°å›æ’¤æ—©åœçŠ¶æ€ï¼ˆåŸºäºæŠ•èµ„ç»„åˆä»·å€¼è€Œéç´¯ç§¯å¥–åŠ±ï¼‰
            self.drawdown_early_stopping.step(portfolio_value)

            # è·å–æ›´æ–°åçš„å½“å‰å›æ’¤
            current_drawdown = self.drawdown_early_stopping.get_current_drawdown()

            # ä»ç¯å¢ƒè·å–å‡†ç¡®çš„å›æ’¤å€¼ï¼ˆå¦‚æœæ”¯æŒï¼‰
            env_drawdown = 0.0
            if hasattr(self.environment, '_calculate_current_drawdown'):
                env_drawdown = self.environment._calculate_current_drawdown()

            self.drawdown_metrics.append({
                'episode': episode,
                'portfolio_value': portfolio_value,
                'episode_reward': episode_reward,
                'training_drawdown': current_drawdown,  # åŸºäºæŠ•èµ„ç»„åˆä»·å€¼çš„è®­ç»ƒå›æ’¤
                'env_drawdown': env_drawdown  # ç¯å¢ƒå†…éƒ¨è®¡ç®—çš„å›æ’¤
            })

            # ä»ç¯å¢ƒè·å–å…¶ä»–å›æ’¤æŒ‡æ ‡ï¼ˆå¦‚æœç¯å¢ƒæ”¯æŒï¼‰
            if hasattr(self.environment, 'get_drawdown_metrics'):
                try:
                    env_metrics = self.environment.get_drawdown_metrics()
                    if env_metrics and isinstance(env_metrics, dict):
                        self.drawdown_metrics[-1].update(env_metrics)
                except Exception as e:
                    logger.debug(f"è·å–ç¯å¢ƒå›æ’¤æŒ‡æ ‡å¤±è´¥: {e}")

            # å®šæœŸè®°å½•å›æ’¤ä¿¡æ¯å’Œå¼‚å¸¸æ£€æµ‹
            if episode % 50 == 0:
                logger.info(f"Episode {episode}: æŠ•èµ„ç»„åˆä»·å€¼ {portfolio_value:.4f}, "
                          f"è®­ç»ƒå›æ’¤ {current_drawdown:.4f}, ç¯å¢ƒå›æ’¤ {env_drawdown:.4f}")

                # å›æ’¤å¼‚å¸¸æ£€æµ‹
                if current_drawdown == 0.0 and episode > 50:
                    logger.warning(f"å›æ’¤è®¡ç®—å¼‚å¸¸: è¿ç»­{episode}ä¸ªepisodeå›æ’¤ä¸º0ï¼Œå¯èƒ½å­˜åœ¨è®¡ç®—ç¼ºé™·")
                elif current_drawdown > self.config.max_training_drawdown * 0.8:
                    logger.warning(f"å›æ’¤é£é™©è¾ƒé«˜: {current_drawdown:.4f}, æ¥è¿‘é˜ˆå€¼ {self.config.max_training_drawdown:.4f}")

        except Exception as e:
            logger.error(f"å›æ’¤ç›‘æ§å¤±è´¥: {e}")
            raise RuntimeError(f"è®­ç»ƒå›æ’¤ç›‘æ§å‡ºç°é”™è¯¯: {e}") from e

    def _adapt_training_parameters(self, episode_reward: float, episode: int):
        """åŸºäºæ€§èƒ½è‡ªé€‚åº”è°ƒæ•´è®­ç»ƒå‚æ•°"""
        try:
            self.performance_history.append(episode_reward)

            # åªåœ¨æœ‰è¶³å¤Ÿå†å²æ•°æ®æ—¶è¿›è¡Œè°ƒæ•´
            if len(self.performance_history) < 50:
                return

            # è®¡ç®—æœ€è¿‘æ€§èƒ½
            recent_performance = np.mean(self.performance_history[-20:])
            long_term_performance = np.mean(self.performance_history[-50:])

            # æ”¹è¿›çš„è‡ªé€‚åº”å­¦ä¹ ç‡è°ƒæ•´ï¼ˆä¿®å¤è´Ÿå¥–åŠ±ç¯å¢ƒä¸‹çš„é€»è¾‘é”™è¯¯ï¼‰
            if self.config.enable_adaptive_learning:
                old_lr_factor = self.current_lr_factor

                # è®¡ç®—æ€§èƒ½å˜åŒ–çš„ç»å¯¹å€¼å’Œç›¸å¯¹å€¼ï¼Œé€‚åº”æ­£è´Ÿå¥–åŠ±ç¯å¢ƒ
                performance_diff = recent_performance - long_term_performance
                performance_change_ratio = abs(performance_diff) / max(abs(long_term_performance), 1.0)

                # æ€§èƒ½æ˜¾è‘—æ¶åŒ–çš„åˆ¤æ–­ï¼ˆé€‚åº”è´Ÿå¥–åŠ±ï¼‰
                is_performance_worse = False
                if long_term_performance >= 0:
                    # æ­£å¥–åŠ±ç¯å¢ƒï¼šæœ€è¿‘è¡¨ç°ä½äºé•¿æœŸè¡¨ç°çš„é˜ˆå€¼
                    is_performance_worse = recent_performance < long_term_performance * self.config.performance_threshold_down
                else:
                    # è´Ÿå¥–åŠ±ç¯å¢ƒï¼šæœ€è¿‘è¡¨ç°æ›´è´Ÿï¼ˆç»å¯¹å€¼æ›´å¤§ï¼‰
                    is_performance_worse = recent_performance < long_term_performance / self.config.performance_threshold_down

                # æ€§èƒ½æ˜¾è‘—æ”¹å–„çš„åˆ¤æ–­
                is_performance_better = False
                if long_term_performance >= 0:
                    # æ­£å¥–åŠ±ç¯å¢ƒï¼šæœ€è¿‘è¡¨ç°è¶…è¿‡é•¿æœŸè¡¨ç°çš„é˜ˆå€¼
                    is_performance_better = recent_performance > long_term_performance * self.config.performance_threshold_up
                else:
                    # è´Ÿå¥–åŠ±ç¯å¢ƒï¼šæœ€è¿‘è¡¨ç°æ›´å¥½ï¼ˆç»å¯¹å€¼æ›´å°ï¼‰
                    is_performance_better = recent_performance > long_term_performance / self.config.performance_threshold_up

                # é¿å…è¿‡äºé¢‘ç¹çš„å­¦ä¹ ç‡è°ƒæ•´
                significant_change = performance_change_ratio > 0.05  # è‡³å°‘5%çš„å˜åŒ–

                if is_performance_worse and significant_change:
                    # æ€§èƒ½ä¸‹é™ï¼Œé™ä½å­¦ä¹ ç‡ä½†æœ‰æœ€å°å€¼é™åˆ¶
                    self.current_lr_factor = max(
                        self.config.min_lr_factor,
                        self.current_lr_factor * self.config.lr_adaptation_factor
                    )
                    if self.current_lr_factor != old_lr_factor:
                        logger.info(f"Episode {episode}: æ£€æµ‹åˆ°æ€§èƒ½ä¸‹é™ï¼Œé™ä½å­¦ä¹ ç‡å› å­åˆ° {self.current_lr_factor:.4f}")

                elif is_performance_better and significant_change:
                    # æ€§èƒ½æå‡ï¼Œå¿«é€Ÿæ¢å¤å­¦ä¹ ç‡
                    self.current_lr_factor = min(
                        self.config.max_lr_factor,
                        self.current_lr_factor * self.config.lr_recovery_factor
                    )
                    if self.current_lr_factor != old_lr_factor:
                        logger.info(f"Episode {episode}: æ£€æµ‹åˆ°æ€§èƒ½æå‡ï¼Œè°ƒæ•´å­¦ä¹ ç‡å› å­åˆ° {self.current_lr_factor:.4f}")

            # åŸºäºå›æ’¤çš„æ¢ç´¢è°ƒæ•´
            if (self.config.exploration_decay_by_performance and
                hasattr(self.agent, 'exploration_rate')):
                if self.drawdown_early_stopping:
                    current_drawdown = self.drawdown_early_stopping.get_current_drawdown()
                    if current_drawdown > 0.2:  # å›æ’¤è¾ƒå¤§æ—¶å‡å°‘æ¢ç´¢
                        exploration_decay = 0.95
                        self.agent.exploration_rate *= exploration_decay
                        logger.info(f"Episode {episode}: å¤§å›æ’¤ï¼Œå‡å°‘æ¢ç´¢ç‡åˆ° {self.agent.exploration_rate:.4f}")

        except Exception as e:
            logger.error(f"è‡ªé€‚åº”å‚æ•°è°ƒæ•´å¤±è´¥: {e}")
            raise RuntimeError(f"è®­ç»ƒå‚æ•°è‡ªé€‚åº”è°ƒæ•´å‡ºç°é”™è¯¯: {e}") from e

    def collect_drawdown_metrics(self) -> Dict[str, Any]:
        """æ”¶é›†å›æ’¤æŒ‡æ ‡"""
        if not self.drawdown_metrics:
            return {'drawdown_monitoring_enabled': False}

        try:
            training_drawdowns = [m['training_drawdown'] for m in self.drawdown_metrics]
            env_drawdowns = [m['env_drawdown'] for m in self.drawdown_metrics]
            portfolio_values = [m['portfolio_value'] for m in self.drawdown_metrics]

            return {
                'drawdown_monitoring_enabled': True,
                'max_training_drawdown': max(training_drawdowns) if training_drawdowns else 0.0,
                'avg_training_drawdown': np.mean(training_drawdowns) if training_drawdowns else 0.0,
                'max_env_drawdown': max(env_drawdowns) if env_drawdowns else 0.0,
                'avg_env_drawdown': np.mean(env_drawdowns) if env_drawdowns else 0.0,
                'final_portfolio_value': portfolio_values[-1] if portfolio_values else 0.0,
                'peak_portfolio_value': max(portfolio_values) if portfolio_values else 0.0,
                'significant_drawdown_episodes': len([d for d in training_drawdowns if d > 0.1]),
                'total_monitored_episodes': len(self.drawdown_metrics)
            }

        except Exception as e:
            logger.error(f"æ”¶é›†å›æ’¤æŒ‡æ ‡å¤±è´¥: {e}")
            raise RuntimeError(f"å›æ’¤æŒ‡æ ‡æ”¶é›†å‡ºç°é”™è¯¯: {e}") from e

    # ==================== å¢å¼ºæŒ‡æ ‡ç›¸å…³æ–¹æ³• ====================

    def _should_calculate_metrics(self, timesteps: int) -> bool:
        """
        åˆ¤æ–­æ˜¯å¦åº”è¯¥è®¡ç®—æŒ‡æ ‡ï¼ˆåŸºäºtimestepsï¼‰

        Args:
            timesteps: å½“å‰timestepsæ•°

        Returns:
            æ˜¯å¦åº”è¯¥è®¡ç®—æŒ‡æ ‡
        """
        return timesteps % self.config.metrics_calculation_frequency == 0

    def _calculate_and_log_enhanced_metrics(self, episode_num: int):
        """
        è®¡ç®—å¹¶è®°å½•å¢å¼ºæŒ‡æ ‡

        Args:
            episode_num: episodeç¼–å·
        """
        if not ENHANCED_METRICS_AVAILABLE:
            return

        try:
            # è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡
            portfolio_metrics = None
            if self.config.enable_portfolio_metrics:
                portfolio_metrics = self._calculate_portfolio_metrics()

            # è®¡ç®—æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡
            agent_metrics = None
            if self.config.enable_agent_behavior_metrics:
                agent_metrics = self._calculate_agent_behavior_metrics()

            # è®¡ç®—é£é™©æ§åˆ¶æŒ‡æ ‡
            risk_metrics = None
            if self.config.enable_risk_control_metrics:
                risk_metrics = self._calculate_risk_control_metrics()

            # è®°å½•æŒ‡æ ‡æ—¥å¿—
            if self.config.detailed_metrics_logging:
                self._log_enhanced_metrics(episode_num, portfolio_metrics, agent_metrics, risk_metrics)

        except Exception as e:
            logger.error(f"è®¡ç®—å¢å¼ºæŒ‡æ ‡æ—¶å‘ç”Ÿé”™è¯¯: {e}")

    def _calculate_portfolio_metrics(self) -> Optional['PortfolioMetrics']:
        """
        è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡

        Returns:
            æŠ•èµ„ç»„åˆæŒ‡æ ‡æˆ–Noneï¼ˆå¦‚æœæ•°æ®ä¸è¶³ï¼‰
        """
        if not ENHANCED_METRICS_AVAILABLE or len(self.portfolio_values_history) <= 1:
            logger.debug("æŠ•èµ„ç»„åˆä»·å€¼å†å²æ•°æ®ä¸è¶³ï¼Œè·³è¿‡æŒ‡æ ‡è®¡ç®—")
            return None

        try:
            # ç¡®ä¿åŸºå‡†æ•°æ®é•¿åº¦åŒ¹é…
            if len(self.benchmark_values_history) != len(self.portfolio_values_history):
                logger.warning(f"åŸºå‡†æ•°æ®é•¿åº¦({len(self.benchmark_values_history)})ä¸æŠ•èµ„ç»„åˆæ•°æ®é•¿åº¦"
                             f"({len(self.portfolio_values_history)})ä¸åŒ¹é…")
                # æˆªå–åˆ°è¾ƒçŸ­çš„é•¿åº¦
                min_len = min(len(self.benchmark_values_history), len(self.portfolio_values_history))
                portfolio_values = self.portfolio_values_history[:min_len]
                benchmark_values = self.benchmark_values_history[:min_len]
                dates = self.dates_history[:min_len] if len(self.dates_history) >= min_len else [datetime.now()] * min_len
            else:
                portfolio_values = self.portfolio_values_history
                benchmark_values = self.benchmark_values_history
                dates = self.dates_history if len(self.dates_history) == len(portfolio_values) else [datetime.now()] * len(portfolio_values)

            metrics = self.metrics_calculator.calculate_portfolio_metrics(
                portfolio_values=portfolio_values,
                benchmark_values=benchmark_values,
                dates=dates,
                risk_free_rate=self.config.risk_free_rate
            )

            return metrics

        except Exception as e:
            logger.error(f"è®¡ç®—æŠ•èµ„ç»„åˆæŒ‡æ ‡å¤±è´¥: {e}")
            return None

    def _calculate_agent_behavior_metrics(self) -> Optional['AgentBehaviorMetrics']:
        """
        è®¡ç®—æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡

        Returns:
            æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡æˆ–Noneï¼ˆå¦‚æœæ•°æ®ä¸è¶³ï¼‰
        """
        if not ENHANCED_METRICS_AVAILABLE or len(self.entropy_history) == 0:
            logger.debug("ç†µå€¼å†å²æ•°æ®ä¸ºç©ºï¼Œè·³è¿‡æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡è®¡ç®—")
            return None

        try:
            metrics = self.metrics_calculator.calculate_agent_behavior_metrics(
                entropy_values=self.entropy_history,
                position_weights_history=self.position_weights_history
            )

            return metrics

        except Exception as e:
            logger.error(f"è®¡ç®—æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡å¤±è´¥: {e}")
            return None

    def _calculate_risk_control_metrics(self) -> Optional['RiskControlMetrics']:
        """
        è®¡ç®—é£é™©æ§åˆ¶æŒ‡æ ‡

        Returns:
            é£é™©æ§åˆ¶æŒ‡æ ‡æˆ–Noneï¼ˆå¦‚æœæ•°æ®ä¸è¶³ï¼‰
        """
        if not ENHANCED_METRICS_AVAILABLE:
            return None

        # æ£€æŸ¥ç¯å¢ƒæ˜¯å¦æœ‰å›æ’¤æ§åˆ¶å™¨
        if not hasattr(self.environment, 'drawdown_controller') or self.environment.drawdown_controller is None:
            logger.debug("ç¯å¢ƒä¸­æ²¡æœ‰å›æ’¤æ§åˆ¶å™¨ï¼Œè·³è¿‡é£é™©æ§åˆ¶æŒ‡æ ‡è®¡ç®—")
            return None

        try:
            drawdown_controller = self.environment.drawdown_controller

            # ä»å›æ’¤æ§åˆ¶å™¨è·å–æ•°æ®
            risk_budget_history = getattr(drawdown_controller.adaptive_risk_budget, 'risk_budget_history', [])
            risk_usage_history = getattr(drawdown_controller.adaptive_risk_budget, 'risk_usage_history', [])
            control_signals = getattr(drawdown_controller, 'control_signal_queue', [])

            # è·å–å¸‚åœºçŠ¶æ€å†å²
            market_regime_history = []
            if hasattr(drawdown_controller, 'market_regime_detector') and drawdown_controller.market_regime_detector:
                market_regime_history = getattr(drawdown_controller.market_regime_detector, 'regime_history', [])

            # è½¬æ¢æ§åˆ¶ä¿¡å·ä¸ºå­—å…¸æ ¼å¼
            control_signals_dict = []
            for signal in control_signals:
                if hasattr(signal, 'to_dict'):
                    control_signals_dict.append(signal.to_dict())
                elif isinstance(signal, dict):
                    control_signals_dict.append(signal)

            metrics = self.metrics_calculator.calculate_risk_control_metrics(
                risk_budget_history=risk_budget_history,
                risk_usage_history=risk_usage_history,
                control_signals=control_signals_dict,
                market_regime_history=market_regime_history
            )

            return metrics

        except Exception as e:
            logger.error(f"è®¡ç®—é£é™©æ§åˆ¶æŒ‡æ ‡å¤±è´¥: {e}")
            return None

    def _log_enhanced_metrics(self, episode: int,
                            portfolio_metrics: Optional['PortfolioMetrics'],
                            agent_metrics: Optional['AgentBehaviorMetrics'],
                            risk_metrics: Optional['RiskControlMetrics']):
        """
        è®°å½•å¢å¼ºæŒ‡æ ‡æ—¥å¿—

        Args:
            episode: episodeç¼–å·
            portfolio_metrics: æŠ•èµ„ç»„åˆæŒ‡æ ‡
            agent_metrics: æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡
            risk_metrics: é£é™©æ§åˆ¶æŒ‡æ ‡
        """
        log_lines = [f"=== Episode {episode} å¢å¼ºæŒ‡æ ‡æŠ¥å‘Š ==="]

        # æŠ•èµ„ç»„åˆæŒ‡æ ‡
        if portfolio_metrics:
            log_lines.append("ğŸ“Š æŠ•èµ„ç»„åˆä¸å¸‚åœºè¡¨ç°å¯¹æ¯”æŒ‡æ ‡:")
            log_lines.append(f"  â€¢ å¤æ™®æ¯”ç‡ (Sharpe Ratio): {portfolio_metrics.sharpe_ratio:.4f}")
            log_lines.append(f"  â€¢ æœ€å¤§å›æ’¤ (Max Drawdown): {portfolio_metrics.max_drawdown:.4f}")
            log_lines.append(f"  â€¢ Alpha (ç›¸å¯¹åŸºå‡†è¶…é¢æ”¶ç›Š): {portfolio_metrics.alpha:.4f}")
            log_lines.append(f"  â€¢ Beta (ç³»ç»Ÿæ€§é£é™©): {portfolio_metrics.beta:.4f}")
            log_lines.append(f"  â€¢ å¹´åŒ–æ”¶ç›Šç‡ (Annualized Return): {portfolio_metrics.annualized_return:.4f}")
        else:
            log_lines.append("ğŸ“Š æŠ•èµ„ç»„åˆæŒ‡æ ‡: æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®¡ç®—")

        # æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡
        if agent_metrics:
            log_lines.append("ğŸ¤– æ™ºèƒ½ä½“è¡Œä¸ºåˆ†ææŒ‡æ ‡:")
            log_lines.append(f"  â€¢ å¹³å‡ç†µå€¼ (Mean Entropy): {agent_metrics.mean_entropy:.4f}")
            log_lines.append(f"  â€¢ ç†µå€¼è¶‹åŠ¿ (Entropy Trend): {agent_metrics.entropy_trend:.4f}")
            log_lines.append(f"  â€¢ å¹³å‡æŒä»“é›†ä¸­åº¦ (Position Concentration): {agent_metrics.mean_position_concentration:.4f}")
            log_lines.append(f"  â€¢ æ¢æ‰‹ç‡ (Turnover Rate): {agent_metrics.turnover_rate:.4f}")
        else:
            log_lines.append("ğŸ¤– æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡: æ•°æ®ä¸è¶³ï¼Œè·³è¿‡è®¡ç®—")

        # é£é™©æ§åˆ¶æŒ‡æ ‡
        if risk_metrics:
            log_lines.append("ğŸ›¡ï¸ é£é™©ä¸å›æ’¤æ§åˆ¶æŒ‡æ ‡:")
            log_lines.append(f"  â€¢ å¹³å‡é£é™©é¢„ç®—ä½¿ç”¨ç‡: {risk_metrics.avg_risk_budget_utilization:.4f}")
            log_lines.append(f"  â€¢ é£é™©é¢„ç®—æ•ˆç‡: {risk_metrics.risk_budget_efficiency:.4f}")
            log_lines.append(f"  â€¢ æ§åˆ¶ä¿¡å·é¢‘ç‡: {risk_metrics.control_signal_frequency:.4f}")
            log_lines.append(f"  â€¢ å¸‚åœºçŠ¶æ€ç¨³å®šæ€§: {risk_metrics.market_regime_stability:.4f}")
        else:
            log_lines.append("ğŸ›¡ï¸ é£é™©æ§åˆ¶æŒ‡æ ‡: å›æ’¤æ§åˆ¶å™¨æœªå¯ç”¨æˆ–æ•°æ®ä¸è¶³")

        log_lines.append("=" * 50)

        # è¾“å‡ºæ—¥å¿—
        for line in log_lines:
            logger.info(line)

    def _update_metrics_histories(self, episode_info: Dict[str, Any], update_info: Dict[str, Any]):
        """
        æ›´æ–°æŒ‡æ ‡å†å²æ•°æ®

        Args:
            episode_info: episodeä¿¡æ¯
            update_info: æ™ºèƒ½ä½“æ›´æ–°ä¿¡æ¯
        """
        if not ENHANCED_METRICS_AVAILABLE:
            return

        # æ›´æ–°æŠ•èµ„ç»„åˆä»·å€¼å†å²
        if 'portfolio_value' in episode_info:
            self.portfolio_values_history.append(episode_info['portfolio_value'])

        # æ›´æ–°åŸºå‡†ä»·å€¼å†å²ï¼ˆå¦‚æœæœ‰ï¼‰
        if 'benchmark_value' in episode_info:
            self.benchmark_values_history.append(episode_info['benchmark_value'])
        elif len(self.portfolio_values_history) > len(self.benchmark_values_history):
            # å¦‚æœæ²¡æœ‰åŸºå‡†æ•°æ®ï¼Œä½¿ç”¨é»˜è®¤å¢é•¿ç‡
            if len(self.benchmark_values_history) == 0:
                self.benchmark_values_history.append(self.config.initial_cash)
            else:
                # å‡è®¾åŸºå‡†å¹´åŒ–æ”¶ç›Šç‡ä¸º8%
                daily_return = 0.08 / 252
                last_value = self.benchmark_values_history[-1]
                self.benchmark_values_history.append(last_value * (1 + daily_return))

        # æ›´æ–°æ—¥æœŸå†å²
        self.dates_history.append(datetime.now())

        # æ›´æ–°æ™ºèƒ½ä½“è¡Œä¸ºæ•°æ®
        if 'policy_entropy' in update_info:
            self.entropy_history.append(update_info['policy_entropy'])

        if 'positions' in episode_info:
            positions = episode_info['positions']
            if isinstance(positions, np.ndarray):
                self.position_weights_history.append(positions.copy())

    def get_enhanced_training_stats(self) -> Dict[str, Any]:
        """
        è·å–å¢å¼ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯

        Returns:
            å¢å¼ºè®­ç»ƒç»Ÿè®¡ä¿¡æ¯
        """
        # è·å–åŸºç¡€ç»Ÿè®¡ï¼ˆä»agentæˆ–åˆ›å»ºç©ºå­—å…¸ï¼‰
        base_stats = self.agent.get_training_stats() if hasattr(self.agent, 'get_training_stats') else {}

        if not ENHANCED_METRICS_AVAILABLE:
            return base_stats

        # æ·»åŠ å¢å¼ºç»Ÿè®¡
        enhanced_stats = {
            'portfolio_values_count': len(self.portfolio_values_history),
            'entropy_values_count': len(self.entropy_history),
            'position_weights_count': len(self.position_weights_history),
            'latest_portfolio_value': self.portfolio_values_history[-1] if self.portfolio_values_history else 0,
            'latest_entropy': self.entropy_history[-1] if self.entropy_history else 0,
        }

        # å¦‚æœæœ‰è¶³å¤Ÿæ•°æ®ï¼Œè®¡ç®—æœ€æ–°æŒ‡æ ‡
        if len(self.portfolio_values_history) > 1:
            try:
                latest_portfolio_metrics = self._calculate_portfolio_metrics()
                if latest_portfolio_metrics:
                    enhanced_stats.update({
                        'latest_sharpe_ratio': latest_portfolio_metrics.sharpe_ratio,
                        'latest_max_drawdown': latest_portfolio_metrics.max_drawdown,
                        'latest_alpha': latest_portfolio_metrics.alpha,
                        'latest_beta': latest_portfolio_metrics.beta,
                        'latest_annualized_return': latest_portfolio_metrics.annualized_return
                    })
            except Exception as e:
                logger.debug(f"è®¡ç®—æœ€æ–°æŠ•èµ„ç»„åˆæŒ‡æ ‡å¤±è´¥: {e}")

        if len(self.entropy_history) > 0:
            try:
                latest_agent_metrics = self._calculate_agent_behavior_metrics()
                if latest_agent_metrics:
                    enhanced_stats.update({
                        'latest_mean_entropy': latest_agent_metrics.mean_entropy,
                        'latest_entropy_trend': latest_agent_metrics.entropy_trend,
                        'latest_position_concentration': latest_agent_metrics.mean_position_concentration,
                        'latest_turnover_rate': latest_agent_metrics.turnover_rate
                    })
            except Exception as e:
                logger.debug(f"è®¡ç®—æœ€æ–°æ™ºèƒ½ä½“è¡Œä¸ºæŒ‡æ ‡å¤±è´¥: {e}")

        # åˆå¹¶ç»Ÿè®¡ä¿¡æ¯
        base_stats.update(enhanced_stats)
        return base_stats

    def reset_enhanced_histories(self):
        """é‡ç½®å¢å¼ºå†å²æ•°æ®"""
        if not ENHANCED_METRICS_AVAILABLE:
            return

        self.portfolio_values_history.clear()
        self.benchmark_values_history.clear()
        self.dates_history.clear()
        self.entropy_history.clear()
        self.position_weights_history.clear()
        self.risk_budget_history.clear()
        self.risk_usage_history.clear()
        self.control_signals_history.clear()
        self.market_regime_history.clear()

        logger.info("å¢å¼ºå†å²æ•°æ®å·²é‡ç½®")


# ä¸ºäº†å‘åå…¼å®¹ï¼Œåˆ›å»ºåˆ«å
EnhancedRLTrainer = RLTrainer
EnhancedTrainingConfig = TrainingConfig