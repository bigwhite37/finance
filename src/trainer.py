"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµæ°´çº¿
é›†æˆQlibæ•°æ®åŠ è½½ã€ç¯å¢ƒåˆ›å»ºã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œå›æµ‹
"""
import os
import yaml
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import pandas as pd
import torch
from stable_baselines3 import SAC, PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import (
    EvalCallback, StopTrainingOnRewardThreshold,
    CheckpointCallback, BaseCallback
)
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed
import gymnasium as gym
from tqdm import tqdm

from data_loader import QlibDataLoader, split_data
from env import PortfolioEnv, DrawdownEarlyStoppingCallback
from model import TradingPolicy, RiskAwareRewardWrapper, PortfolioMetrics

logger = logging.getLogger(__name__)


class TrainingQualityAnalyzer:
    """è®­ç»ƒè´¨é‡åˆ†æå™¨ï¼Œç”¨äºè¯„ä¼°è®­ç»ƒå‚æ•°ã€ä»£ç ã€æ¨¡å‹è´¨é‡"""
    
    def __init__(self):
        self.metrics_history = []
        self.quality_scores = {}
        
    def analyze_training_stability(self, rewards: List[float], window_size: int = 100) -> Dict[str, float]:
        """
        åˆ†æè®­ç»ƒç¨³å®šæ€§
        
        Args:
            rewards: å¥–åŠ±å†å²
            window_size: æ»‘åŠ¨çª—å£å¤§å°
            
        Returns:
            ç¨³å®šæ€§æŒ‡æ ‡å­—å…¸
        """
        if len(rewards) < window_size:
            return {
                'stability_score': 0.0, 
                'trend_score': 0.0, 
                'variance_score': 0.0,
                'slope': 0.0,
                'variance': 0.0
            }
        
        # è®¡ç®—æ»‘åŠ¨å¹³å‡
        moving_avg = []
        for i in range(window_size, len(rewards)):
            window_rewards = rewards[i-window_size:i]
            moving_avg.append(np.mean(window_rewards))
        
        if len(moving_avg) < 2:
            return {
                'stability_score': 0.0, 
                'trend_score': 0.0, 
                'variance_score': 0.0,
                'slope': 0.0,
                'variance': 0.0
            }
        
        # è¶‹åŠ¿åˆ†æ - çº¿æ€§å›å½’æ–œç‡
        x = np.arange(len(moving_avg))
        slope = np.polyfit(x, moving_avg, 1)[0]
        trend_score = max(0, min(1, (slope + 0.01) / 0.02))  # å½’ä¸€åŒ–åˆ°[0,1]
        
        # æ–¹å·®åˆ†æ
        variance = np.var(moving_avg)
        variance_score = max(0, min(1, 1 - variance / (np.mean(moving_avg) ** 2 + 1e-8)))
        
        # ç»¼åˆç¨³å®šæ€§å¾—åˆ†
        stability_score = 0.6 * trend_score + 0.4 * variance_score
        
        return {
            'stability_score': stability_score,
            'trend_score': trend_score,
            'variance_score': variance_score,
            'slope': slope,
            'variance': variance
        }
    
    def analyze_convergence_quality(self, losses: List[float]) -> Dict[str, float]:
        """
        åˆ†ææ”¶æ•›è´¨é‡
        
        Args:
            losses: æŸå¤±å†å²
            
        Returns:
            æ”¶æ•›è´¨é‡æŒ‡æ ‡
        """
        if len(losses) < 100:
            return {
                'convergence_score': 0.0, 
                'oscillation_score': 0.0,
                'improvement_ratio': 0.0,
                'oscillation_ratio': 0.0
            }
        
        # è®¡ç®—æŸå¤±ä¸‹é™è¶‹åŠ¿
        recent_losses = losses[-100:]
        early_losses = losses[:100] if len(losses) >= 200 else losses[:len(losses)//2]
        
        improvement = (np.mean(early_losses) - np.mean(recent_losses)) / (np.mean(early_losses) + 1e-8)
        convergence_score = max(0, min(1, improvement))
        
        # åˆ†æéœ‡è¡ç¨‹åº¦
        loss_diff = np.diff(recent_losses)
        oscillation = np.std(loss_diff) / (np.mean(np.abs(loss_diff)) + 1e-8)
        oscillation_score = max(0, min(1, 1 - oscillation / 10))
        
        return {
            'convergence_score': convergence_score,
            'oscillation_score': oscillation_score,
            'improvement_ratio': improvement,
            'oscillation_ratio': oscillation
        }
    
    def analyze_hyperparameter_quality(self, config: Dict[str, Any]) -> Dict[str, float]:
        """
        åˆ†æè¶…å‚æ•°è´¨é‡
        
        Args:
            config: é…ç½®å­—å…¸
            
        Returns:
            è¶…å‚æ•°è´¨é‡è¯„åˆ†
        """
        scores = {}
        
        # å­¦ä¹ ç‡è¯„ä¼°
        lr = config.get('model', {}).get('learning_rate', 3e-4)
        if 1e-5 <= lr <= 1e-2:
            scores['learning_rate_score'] = 1.0
        elif 1e-6 <= lr <= 1e-1:
            scores['learning_rate_score'] = 0.7
        else:
            scores['learning_rate_score'] = 0.3
        
        # æ‰¹æ¬¡å¤§å°è¯„ä¼°
        batch_size = config.get('model', {}).get('batch_size', 256)
        if 64 <= batch_size <= 512:
            scores['batch_size_score'] = 1.0
        elif 32 <= batch_size <= 1024:
            scores['batch_size_score'] = 0.8
        else:
            scores['batch_size_score'] = 0.5
        
        # ç½‘ç»œæ¶æ„è¯„ä¼°
        net_arch = config.get('model', {}).get('net_arch', [256, 256])
        if isinstance(net_arch, list) and 2 <= len(net_arch) <= 4:
            if all(64 <= size <= 512 for size in net_arch):
                scores['network_arch_score'] = 1.0
            else:
                scores['network_arch_score'] = 0.7
        else:
            scores['network_arch_score'] = 0.5
        
        # ç¯å¢ƒå‚æ•°è¯„ä¼°
        env_config = config.get('environment', {})
        transaction_cost = env_config.get('transaction_cost', 0.003)
        if 0.001 <= transaction_cost <= 0.01:
            scores['transaction_cost_score'] = 1.0
        else:
            scores['transaction_cost_score'] = 0.6
        
        # ç»¼åˆè¯„åˆ†
        scores['overall_hyperparameter_score'] = np.mean(list(scores.values()))
        
        return scores
    
    def generate_quality_report(self, 
                              rewards: List[float],
                              losses: List[float],
                              config: Dict[str, Any],
                              portfolio_values: List[float]) -> str:
        """
        ç”Ÿæˆè®­ç»ƒè´¨é‡æŠ¥å‘Š
        
        Args:
            rewards: å¥–åŠ±å†å²
            losses: æŸå¤±å†å²  
            config: é…ç½®
            portfolio_values: ç»„åˆä»·å€¼å†å²
            
        Returns:
            è´¨é‡æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = []
        report.append("=" * 80)
        report.append("è®­ç»ƒè´¨é‡åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)
        
        # ç¨³å®šæ€§åˆ†æ
        if rewards:
            stability = self.analyze_training_stability(rewards)
            report.append(f"\nğŸ“Š è®­ç»ƒç¨³å®šæ€§åˆ†æ:")
            report.append(f"  ç»¼åˆç¨³å®šæ€§å¾—åˆ†: {stability['stability_score']:.3f}")
            report.append(f"  è¶‹åŠ¿å¾—åˆ†: {stability['trend_score']:.3f}")
            report.append(f"  æ–¹å·®å¾—åˆ†: {stability['variance_score']:.3f}")
            report.append(f"  å¥–åŠ±è¶‹åŠ¿æ–œç‡: {stability['slope']:.6f}")
            
            if stability['stability_score'] > 0.8:
                report.append("  âœ… è®­ç»ƒç¨³å®šæ€§è‰¯å¥½")
            elif stability['stability_score'] > 0.6:
                report.append("  âš ï¸  è®­ç»ƒç¨³å®šæ€§ä¸€èˆ¬ï¼Œå»ºè®®è°ƒæ•´å­¦ä¹ ç‡")
            else:
                report.append("  âŒ è®­ç»ƒä¸ç¨³å®šï¼Œå»ºè®®æ£€æŸ¥è¶…å‚æ•°è®¾ç½®")
        
        # æ”¶æ•›è´¨é‡åˆ†æ
        if losses:
            convergence = self.analyze_convergence_quality(losses)
            report.append(f"\nğŸ¯ æ”¶æ•›è´¨é‡åˆ†æ:")
            report.append(f"  æ”¶æ•›å¾—åˆ†: {convergence['convergence_score']:.3f}")
            report.append(f"  éœ‡è¡å¾—åˆ†: {convergence['oscillation_score']:.3f}")
            report.append(f"  æ”¹è¿›æ¯”ç‡: {convergence['improvement_ratio']:.3f}")
            
            if convergence['convergence_score'] > 0.7:
                report.append("  âœ… æ¨¡å‹æ”¶æ•›è‰¯å¥½")
            elif convergence['convergence_score'] > 0.4:
                report.append("  âš ï¸  æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯è€ƒè™‘è°ƒæ•´å­¦ä¹ ç‡")
            else:
                report.append("  âŒ æ”¶æ•›å›°éš¾ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹æ¶æ„æˆ–æ•°æ®è´¨é‡")
        
        # è¶…å‚æ•°è´¨é‡åˆ†æ
        hyperparams = self.analyze_hyperparameter_quality(config)
        report.append(f"\nâš™ï¸  è¶…å‚æ•°è´¨é‡åˆ†æ:")
        report.append(f"  å­¦ä¹ ç‡å¾—åˆ†: {hyperparams['learning_rate_score']:.3f}")
        report.append(f"  æ‰¹æ¬¡å¤§å°å¾—åˆ†: {hyperparams['batch_size_score']:.3f}")
        report.append(f"  ç½‘ç»œæ¶æ„å¾—åˆ†: {hyperparams['network_arch_score']:.3f}")
        report.append(f"  äº¤æ˜“æˆæœ¬å¾—åˆ†: {hyperparams['transaction_cost_score']:.3f}")
        report.append(f"  ç»¼åˆè¶…å‚æ•°å¾—åˆ†: {hyperparams['overall_hyperparameter_score']:.3f}")
        
        # ç»„åˆè¡¨ç°åˆ†æ
        if portfolio_values and len(portfolio_values) > 1:
            initial_value = portfolio_values[0]
            final_value = portfolio_values[-1]
            total_return = (final_value / initial_value - 1) * 100
            
            # è®¡ç®—æœ€å¤§å›æ’¤
            peak = portfolio_values[0]
            max_drawdown = 0
            for value in portfolio_values:
                if value > peak:
                    peak = value
                drawdown = (peak - value) / peak
                if drawdown > max_drawdown:
                    max_drawdown = drawdown
            
            report.append(f"\nğŸ’° ç»„åˆè¡¨ç°åˆ†æ:")
            report.append(f"  æ€»æ”¶ç›Šç‡: {total_return:+.2f}%")
            report.append(f"  æœ€å¤§å›æ’¤: {max_drawdown:.2%}")
            
            # é£é™©è°ƒæ•´æ”¶ç›Š
            if max_drawdown > 0:
                risk_adjusted_return = total_return / (max_drawdown * 100)
                report.append(f"  é£é™©è°ƒæ•´æ”¶ç›Š: {risk_adjusted_return:.3f}")
                
                if risk_adjusted_return > 2.0:
                    report.append("  âœ… é£é™©è°ƒæ•´æ”¶ç›Šä¼˜ç§€")
                elif risk_adjusted_return > 1.0:
                    report.append("  âš ï¸  é£é™©è°ƒæ•´æ”¶ç›Šä¸€èˆ¬")
                else:
                    report.append("  âŒ é£é™©è°ƒæ•´æ”¶ç›Šè¾ƒå·®ï¼Œéœ€è¦ä¼˜åŒ–é£é™©æ§åˆ¶")
        
        # æ€»ä½“å»ºè®®
        report.append(f"\nğŸ” æ€»ä½“å»ºè®®:")
        
        if rewards and len(rewards) > 100:
            recent_performance = np.mean(rewards[-100:])
            early_performance = np.mean(rewards[:100])
            
            if recent_performance > early_performance * 1.1:
                report.append("  âœ… æ¨¡å‹æŒç»­æ”¹è¿›ï¼Œè®­ç»ƒæ•ˆæœè‰¯å¥½")
            elif recent_performance > early_performance * 0.9:
                report.append("  âš ï¸  æ¨¡å‹æ€§èƒ½è¶‹äºç¨³å®šï¼Œå¯è€ƒè™‘è°ƒæ•´æ¢ç´¢ç­–ç•¥")
            else:
                report.append("  âŒ æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œå»ºè®®æ£€æŸ¥è¿‡æ‹Ÿåˆæˆ–æ•°æ®æ³„éœ²é—®é¢˜")
        
        report.append("=" * 80)
        
        return "\n".join(report)


class DrawdownStoppingCallback(BaseCallback):
    """å›æ’¤æ—©åœå›è°ƒï¼Œç”¨äºStable-Baselines3"""

    def __init__(self, max_drawdown: float = 0.15, patience: int = 100, verbose: int = 0):
        super().__init__(verbose)
        self.max_drawdown = max_drawdown
        self.patience = patience
        self.violation_count = 0
        self.best_reward = -np.inf

    def _on_step(self) -> bool:
        """æ¯æ­¥æ£€æŸ¥å›æ’¤"""
        # è·å–ç¯å¢ƒä¿¡æ¯
        if hasattr(self.training_env, 'get_attr'):
            try:
                current_drawdowns = self.training_env.get_attr('current_drawdown')
                max_drawdown = max(current_drawdowns)

                if max_drawdown > self.max_drawdown:
                    self.violation_count += 1
                    if self.violation_count >= self.patience:
                        if self.verbose > 0:
                            print(f"\nå›æ’¤{max_drawdown:.2%}è¶…è¿‡é˜ˆå€¼{self.max_drawdown:.2%}ï¼Œ"
                                 f"è¿ç»­{self.patience}æ­¥ï¼Œè§¦å‘æ—©åœ")
                        return False
                else:
                    self.violation_count = 0

            except Exception as e:
                if self.verbose > 1:
                    print(f"è·å–å›æ’¤ä¿¡æ¯å¤±è´¥: {e}")

        return True


class TrainingMetricsCallback(BaseCallback):
    """è®­ç»ƒæŒ‡æ ‡ç›‘æ§å›è°ƒï¼Œå®šæœŸè¾“å‡ºè®­ç»ƒç›¸å…³æŒ‡æ ‡"""

    def __init__(self, 
                 log_interval: int = 1000,
                 eval_interval: int = 10000,
                 verbose: int = 1):
        """
        åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡å›è°ƒ
        
        Args:
            log_interval: æŒ‡æ ‡è¾“å‡ºé—´éš”ï¼ˆtimestepsï¼‰
            eval_interval: è¯¦ç»†è¯„ä¼°é—´éš”ï¼ˆtimestepsï¼‰
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.log_interval = log_interval
        self.eval_interval = eval_interval
        
        # æŒ‡æ ‡å†å²è®°å½•
        self.episode_rewards = []
        self.episode_lengths = []
        self.portfolio_values = []
        self.drawdowns = []
        self.actions_history = []
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.last_log_step = 0
        self.last_eval_step = 0
        self.episode_count = 0
        
        # æ€§èƒ½æŒ‡æ ‡
        self.best_mean_reward = -np.inf
        self.best_portfolio_value = 0
        self.worst_drawdown = 0
        
        # è´¨é‡åˆ†æå™¨
        self.quality_analyzer = TrainingQualityAnalyzer()
        self.losses_history = []
        
        logger.info(f"è®­ç»ƒæŒ‡æ ‡ç›‘æ§å·²å¯ç”¨ - æ—¥å¿—é—´éš”: {log_interval}, è¯„ä¼°é—´éš”: {eval_interval}")

    def _on_step(self) -> bool:
        """æ¯æ­¥æ‰§è¡Œçš„ç›‘æ§é€»è¾‘"""
        current_step = self.num_timesteps
        
        # æ”¶é›†ç¯å¢ƒæŒ‡æ ‡
        self._collect_env_metrics()
        
        # å®šæœŸè¾“å‡ºåŸºç¡€æŒ‡æ ‡
        if current_step - self.last_log_step >= self.log_interval:
            self._log_basic_metrics(current_step)
            self.last_log_step = current_step
        
        # å®šæœŸè¿›è¡Œè¯¦ç»†è¯„ä¼°
        if current_step - self.last_eval_step >= self.eval_interval:
            self._log_detailed_metrics(current_step)
            self._log_quality_analysis(current_step)
            self.last_eval_step = current_step
        
        return True

    def _collect_env_metrics(self):
        """æ”¶é›†ç¯å¢ƒæŒ‡æ ‡"""
        if not hasattr(self.training_env, 'get_attr'):
            return
            
        try:
            # è·å–ç¯å¢ƒçŠ¶æ€
            total_values = self.training_env.get_attr('total_value')
            drawdowns = self.training_env.get_attr('current_drawdown')
            
            if len(total_values) > 0:
                self.portfolio_values.extend(total_values)
                self.drawdowns.extend(drawdowns)
                
                # è®°å½•åˆ°TensorBoard
                self.logger.record('env/mean_total_value', np.mean(total_values))
                self.logger.record('env/max_total_value', np.max(total_values))
                self.logger.record('env/mean_drawdown', np.mean(drawdowns))
                self.logger.record('env/max_drawdown', np.max(drawdowns))
                
                # æ›´æ–°æœ€ä½³æŒ‡æ ‡
                current_max_value = np.max(total_values)
                current_worst_drawdown = np.max(drawdowns)
                
                if current_max_value > self.best_portfolio_value:
                    self.best_portfolio_value = current_max_value
                    
                if current_worst_drawdown > self.worst_drawdown:
                    self.worst_drawdown = current_worst_drawdown
                    
        except Exception as e:
            if self.verbose > 1:
                logger.warning(f"æ”¶é›†ç¯å¢ƒæŒ‡æ ‡å¤±è´¥: {e}")

    def _log_basic_metrics(self, current_step: int):
        """è¾“å‡ºåŸºç¡€è®­ç»ƒæŒ‡æ ‡"""
        if self.verbose < 1:
            return
            
        try:
            # è·å–æœ€è¿‘çš„å¥–åŠ±ä¿¡æ¯
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                recent_episodes = list(self.model.ep_info_buffer)[-10:]  # æœ€è¿‘10ä¸ªå›åˆ
                recent_rewards = [ep['r'] for ep in recent_episodes]
                recent_lengths = [ep['l'] for ep in recent_episodes]
                
                mean_reward = np.mean(recent_rewards)
                mean_length = np.mean(recent_lengths)
                
                # æ›´æ–°æœ€ä½³å¥–åŠ±
                if mean_reward > self.best_mean_reward:
                    self.best_mean_reward = mean_reward
                
                # è¾“å‡ºåŸºç¡€æŒ‡æ ‡
                logger.info(f"[æ­¥éª¤ {current_step:,}] "
                           f"å¹³å‡å¥–åŠ±: {mean_reward:.4f} | "
                           f"å¹³å‡å›åˆé•¿åº¦: {mean_length:.1f} | "
                           f"æœ€ä½³å¥–åŠ±: {self.best_mean_reward:.4f}")
                
                # å¦‚æœæœ‰ç»„åˆä»·å€¼æ•°æ®
                if self.portfolio_values:
                    recent_values = self.portfolio_values[-100:]  # æœ€è¿‘100ä¸ªå€¼
                    current_value = recent_values[-1] if recent_values else 0
                    value_change = ((current_value / recent_values[0]) - 1) * 100 if len(recent_values) > 1 else 0
                    
                    logger.info(f"[æ­¥éª¤ {current_step:,}] "
                               f"å½“å‰ç»„åˆä»·å€¼: {current_value:,.0f} | "
                               f"ä»·å€¼å˜åŒ–: {value_change:+.2f}% | "
                               f"æœ€å¤§å›æ’¤: {self.worst_drawdown:.2%}")
                
        except Exception as e:
            logger.warning(f"è¾“å‡ºåŸºç¡€æŒ‡æ ‡å¤±è´¥: {e}")

    def _log_detailed_metrics(self, current_step: int):
        """è¾“å‡ºè¯¦ç»†è®­ç»ƒæŒ‡æ ‡"""
        if self.verbose < 1:
            return
            
        try:
            logger.info("=" * 80)
            logger.info(f"è¯¦ç»†è®­ç»ƒæŠ¥å‘Š - æ­¥éª¤ {current_step:,}")
            logger.info("-" * 80)
            
            # æ¨¡å‹å­¦ä¹ ç‡ç­‰å‚æ•°
            if hasattr(self.model, 'learning_rate'):
                current_lr = self.model.learning_rate
                if callable(current_lr):
                    current_lr = current_lr(1.0)  # è·å–å½“å‰å­¦ä¹ ç‡
                logger.info(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")
            
            # å›åˆç»Ÿè®¡
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                episodes = list(self.model.ep_info_buffer)
                if len(episodes) >= 10:
                    rewards = [ep['r'] for ep in episodes[-50:]]  # æœ€è¿‘50ä¸ªå›åˆ
                    lengths = [ep['l'] for ep in episodes[-50:]]
                    
                    logger.info(f"å›åˆç»Ÿè®¡ (æœ€è¿‘50å›åˆ):")
                    logger.info(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.4f} Â± {np.std(rewards):.4f}")
                    logger.info(f"  å¥–åŠ±èŒƒå›´: [{np.min(rewards):.4f}, {np.max(rewards):.4f}]")
                    logger.info(f"  å¹³å‡é•¿åº¦: {np.mean(lengths):.1f} Â± {np.std(lengths):.1f}")
            
            # ç»„åˆæ€§èƒ½ç»Ÿè®¡
            if self.portfolio_values:
                recent_values = self.portfolio_values[-1000:]  # æœ€è¿‘1000ä¸ªå€¼
                if len(recent_values) > 1:
                    initial_value = recent_values[0]
                    current_value = recent_values[-1]
                    total_return = (current_value / initial_value - 1) * 100
                    
                    # è®¡ç®—æ³¢åŠ¨ç‡
                    returns = np.diff(recent_values) / recent_values[:-1]
                    volatility = np.std(returns) * np.sqrt(252) * 100  # å¹´åŒ–æ³¢åŠ¨ç‡
                    
                    logger.info(f"ç»„åˆæ€§èƒ½ç»Ÿè®¡:")
                    logger.info(f"  åˆå§‹ä»·å€¼: {initial_value:,.0f}")
                    logger.info(f"  å½“å‰ä»·å€¼: {current_value:,.0f}")
                    logger.info(f"  æ€»æ”¶ç›Šç‡: {total_return:+.2f}%")
                    logger.info(f"  å¹´åŒ–æ³¢åŠ¨ç‡: {volatility:.2f}%")
                    logger.info(f"  æœ€å¤§ä»·å€¼: {self.best_portfolio_value:,.0f}")
            
            # å›æ’¤ç»Ÿè®¡
            if self.drawdowns:
                recent_drawdowns = self.drawdowns[-1000:]
                logger.info(f"å›æ’¤ç»Ÿè®¡:")
                logger.info(f"  å½“å‰å›æ’¤: {recent_drawdowns[-1]:.2%}")
                logger.info(f"  å¹³å‡å›æ’¤: {np.mean(recent_drawdowns):.2%}")
                logger.info(f"  æœ€å¤§å›æ’¤: {np.max(recent_drawdowns):.2%}")
            
            # è®­ç»ƒç¨³å®šæ€§æŒ‡æ ‡
            if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
                metrics = self.model.logger.name_to_value
                if 'train/loss' in metrics:
                    logger.info(f"è®­ç»ƒæŸå¤±: {metrics['train/loss']:.6f}")
                if 'train/policy_gradient_loss' in metrics:
                    logger.info(f"ç­–ç•¥æ¢¯åº¦æŸå¤±: {metrics['train/policy_gradient_loss']:.6f}")
                if 'train/value_loss' in metrics:
                    logger.info(f"ä»·å€¼å‡½æ•°æŸå¤±: {metrics['train/value_loss']:.6f}")
            
            logger.info("=" * 80)
            
        except Exception as e:
            logger.warning(f"è¾“å‡ºè¯¦ç»†æŒ‡æ ‡å¤±è´¥: {e}")

    def _log_quality_analysis(self, current_step: int):
        """è¾“å‡ºè®­ç»ƒè´¨é‡åˆ†æ"""
        try:
            # æ”¶é›†è®­ç»ƒæŸå¤±
            if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
                metrics = self.model.logger.name_to_value
                if 'train/loss' in metrics:
                    self.losses_history.append(metrics['train/loss'])
            
            # æ”¶é›†å¥–åŠ±å†å²
            rewards_history = []
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                episodes = list(self.model.ep_info_buffer)
                rewards_history = [ep['r'] for ep in episodes]
            
            # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
            if len(rewards_history) > 50 and len(self.portfolio_values) > 50:
                # è·å–é…ç½®ä¿¡æ¯ï¼ˆéœ€è¦ä»çˆ¶ç±»ä¼ é€’ï¼‰
                config = getattr(self, 'config', {})
                
                quality_report = self.quality_analyzer.generate_quality_report(
                    rewards=rewards_history,
                    losses=self.losses_history,
                    config=config,
                    portfolio_values=self.portfolio_values
                )
                
                logger.info("\n" + quality_report)
                
                # ä¿å­˜è´¨é‡æŠ¥å‘Šåˆ°æ–‡ä»¶
                report_path = f"logs/quality_report_{current_step}.txt"
                with open(report_path, 'w', encoding='utf-8') as f:
                    f.write(quality_report)
                    
        except Exception as e:
            logger.warning(f"ç”Ÿæˆè´¨é‡åˆ†æå¤±è´¥: {e}")

    def set_config(self, config: Dict[str, Any]):
        """è®¾ç½®é…ç½®ä¿¡æ¯ä¾›è´¨é‡åˆ†æä½¿ç”¨"""
        self.config = config

    def _on_training_end(self) -> None:
        """è®­ç»ƒç»“æŸæ—¶çš„æ€»ç»“"""
        logger.info("=" * 80)
        logger.info("è®­ç»ƒå®Œæˆ - æœ€ç»ˆç»Ÿè®¡")
        logger.info("-" * 80)
        
        try:
            if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
                all_episodes = list(self.model.ep_info_buffer)
                all_rewards = [ep['r'] for ep in all_episodes]
                all_lengths = [ep['l'] for ep in all_episodes]
                
                logger.info(f"æ€»å›åˆæ•°: {len(all_episodes)}")
                logger.info(f"å¹³å‡å¥–åŠ±: {np.mean(all_rewards):.4f}")
                logger.info(f"æœ€ä½³å¥–åŠ±: {np.max(all_rewards):.4f}")
                logger.info(f"æœ€å·®å¥–åŠ±: {np.min(all_rewards):.4f}")
                logger.info(f"å¥–åŠ±æ ‡å‡†å·®: {np.std(all_rewards):.4f}")
                
            if self.portfolio_values:
                logger.info(f"æœ€ç»ˆç»„åˆä»·å€¼: {self.portfolio_values[-1]:,.0f}")
                logger.info(f"æœ€ä½³ç»„åˆä»·å€¼: {self.best_portfolio_value:,.0f}")
                
            if self.drawdowns:
                logger.info(f"æœ€å¤§å›æ’¤: {self.worst_drawdown:.2%}")
                
        except Exception as e:
            logger.warning(f"è¾“å‡ºæœ€ç»ˆç»Ÿè®¡å¤±è´¥: {e}")
            
        logger.info("=" * 80)


class TensorBoardCallback(BaseCallback):
    """è‡ªå®šä¹‰TensorBoardå›è°ƒï¼Œè®°å½•é¢å¤–æŒ‡æ ‡"""

    def __init__(self, verbose: int = 0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        """è®°å½•è‡ªå®šä¹‰æŒ‡æ ‡"""
        if hasattr(self.training_env, 'get_attr'):
            try:
                # è·å–ç¯å¢ƒæŒ‡æ ‡
                total_values = self.training_env.get_attr('total_value')
                drawdowns = self.training_env.get_attr('current_drawdown')

                # è®°å½•åˆ°TensorBoard
                if len(total_values) > 0:
                    self.logger.record('env/mean_total_value', np.mean(total_values))
                    self.logger.record('env/max_total_value', np.max(total_values))
                    self.logger.record('env/mean_drawdown', np.mean(drawdowns))
                    self.logger.record('env/max_drawdown', np.max(drawdowns))

            except Exception as e:
                if self.verbose > 1:
                    print(f"è®°å½•æŒ‡æ ‡å¤±è´¥: {e}")

        return True


class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
        """
        self.config = config
        self.setup_logging()
        self.setup_directories()

        # åˆå§‹åŒ–ç»„ä»¶
        self.data_loader = None
        self.train_env = None
        self.eval_env = None
        self.model = None

        # è®­ç»ƒçŠ¶æ€
        self.training_start_time = None
        self.best_reward = -np.inf
        self.training_history = []

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_level = getattr(logging, self.config.get('log_level', 'INFO').upper())
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"logs/training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                logging.StreamHandler()
            ]
        )

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        dirs = ['models', 'logs', 'tensorboard', 'results']
        for dir_name in dirs:
            os.makedirs(dir_name, exist_ok=True)

    def initialize_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å¹¶è·å–è®­ç»ƒæ•°æ®

        Returns:
            è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
        """
        logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")

        data_config = self.config['data']
        self.data_loader = QlibDataLoader(data_config.get('data_root'))

        # åˆå§‹åŒ–Qlib
        provider_uri = data_config.get('provider_uri')
        self.data_loader.initialize_qlib(provider_uri)

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        market = data_config.get('market', 'csi300')
        stock_limit = data_config.get('stock_limit', 50)
        stock_list = self.data_loader.get_stock_list(market, stock_limit)

        logger.info(f"è·å–{len(stock_list)}åªè‚¡ç¥¨ç”¨äºè®­ç»ƒ")

        # åŠ è½½æ•°æ®
        data = self.data_loader.load_data(
            instruments=stock_list,
            start_time=data_config['start_time'],
            end_time=data_config['end_time'],
            freq=data_config.get('freq', 'day'),
            fields=data_config.get('fields')
        )

        # åˆ†å‰²æ•°æ®
        train_data, valid_data, test_data = split_data(
            data,
            data_config['train_start'], data_config['train_end'],
            data_config['valid_start'], data_config['valid_end'],
            data_config['test_start'], data_config['test_end']
        )

        logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ - è®­ç»ƒ: {train_data.shape}, éªŒè¯: {valid_data.shape}, æµ‹è¯•: {test_data.shape}")

        return train_data, valid_data, test_data

    def create_environments(self, train_data: pd.DataFrame, valid_data: pd.DataFrame) -> None:
        """
        åˆ›å»ºè®­ç»ƒå’Œè¯„ä¼°ç¯å¢ƒ

        Args:
            train_data: è®­ç»ƒæ•°æ®
            valid_data: éªŒè¯æ•°æ®
        """
        logger.info("åˆ›å»ºè®­ç»ƒå’Œè¯„ä¼°ç¯å¢ƒ...")

        env_config = self.config['environment']

        def make_env(data: pd.DataFrame, rank: int = 0) -> gym.Env:
            """åˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°"""
            def _init():
                env = PortfolioEnv(
                    data=data,
                    initial_cash=env_config.get('initial_cash', 1000000),
                    lookback_window=env_config.get('lookback_window', 30),
                    transaction_cost=env_config.get('transaction_cost', 0.003),
                    max_drawdown_threshold=env_config.get('max_drawdown_threshold', 0.15),
                    reward_penalty=env_config.get('reward_penalty', 2.0),
                    features=env_config.get('features')
                )

                # æ·»åŠ MonitoråŒ…è£…å™¨ç”¨äºè®°å½•
                log_dir = f"logs/env_logs/rank_{rank}"
                os.makedirs(log_dir, exist_ok=True)
                env = Monitor(env, log_dir)

                # è®¾ç½®éšæœºç§å­
                env.reset(seed=rank)
                return env

            return _init

        # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        train_config = self.config['training']
        n_envs = train_config.get('n_envs', 4)

        if n_envs > 1:
            self.train_env = SubprocVecEnv([
                make_env(train_data, i) for i in range(n_envs)
            ])
        else:
            self.train_env = DummyVecEnv([make_env(train_data, 0)])

        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        self.eval_env = DummyVecEnv([make_env(valid_data, 999)])

        logger.info(f"ç¯å¢ƒåˆ›å»ºå®Œæˆ - è®­ç»ƒç¯å¢ƒæ•°é‡: {n_envs}")

    def create_model(self) -> None:
        """åˆ›å»ºå¼ºåŒ–å­¦ä¹ æ¨¡å‹"""
        logger.info("åˆ›å»ºå¼ºåŒ–å­¦ä¹ æ¨¡å‹...")

        model_config = self.config['model']
        algorithm = model_config.get('algorithm', 'SAC').upper()

        # è·å–ç¯å¢ƒä¿¡æ¯
        sample_env = self.train_env.envs[0] if hasattr(self.train_env, 'envs') else self.train_env.unwrapped

        # æ¨¡å‹å‚æ•°
        model_kwargs = {
            'learning_rate': model_config.get('learning_rate', 3e-4),
            'batch_size': model_config.get('batch_size', 256),
            'gamma': model_config.get('gamma', 0.99),
            'verbose': 1,
            'tensorboard_log': "tensorboard/",
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

        # ç­–ç•¥å‚æ•°
        net_arch = model_config.get('net_arch', [256, 256])
        
        # å¦‚æœä½¿ç”¨è‡ªå®šä¹‰ç­–ç•¥
        if model_config.get('use_custom_policy', False):
            env_config = self.config['environment']
            policy_kwargs = {
                'lookback_window': env_config.get('lookback_window', 30),
                'num_stocks': len(sample_env.stock_list) if hasattr(sample_env, 'stock_list') else 50,
                'num_features': len(env_config.get('features', [])) if env_config.get('features') else 5
            }
            policy = TradingPolicy
        else:
            # æ ‡å‡†ç­–ç•¥éœ€è¦æ­£ç¡®çš„net_archæ ¼å¼
            if algorithm == 'PPO' and isinstance(net_arch, dict):
                policy_kwargs = {'net_arch': net_arch}
            else:
                policy_kwargs = {'net_arch': net_arch if isinstance(net_arch, list) else [256, 256]}
            policy = 'MlpPolicy'

        # åˆ›å»ºæ¨¡å‹
        if algorithm == 'SAC':
            model_kwargs.update({
                'buffer_size': model_config.get('buffer_size', 1000000),
                'tau': model_config.get('tau', 0.005),
                'target_update_interval': model_config.get('target_update_interval', 1),
                'learning_starts': model_config.get('learning_starts', 100)
            })
            self.model = SAC(policy, self.train_env, policy_kwargs=policy_kwargs, **model_kwargs)

        elif algorithm == 'PPO':
            model_kwargs.update({
                'n_steps': model_config.get('n_steps', 2048),
                'n_epochs': model_config.get('n_epochs', 10),
                'clip_range': model_config.get('clip_range', 0.2),
                'ent_coef': model_config.get('ent_coef', 0.0)
            })
            self.model = PPO(policy, self.train_env, policy_kwargs=policy_kwargs, **model_kwargs)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")

        logger.info(f"æ¨¡å‹åˆ›å»ºå®Œæˆ - ç®—æ³•: {algorithm}, è®¾å¤‡: {model_kwargs['device']}")

    def setup_callbacks(self) -> List[BaseCallback]:
        """è®¾ç½®è®­ç»ƒå›è°ƒ"""
        callbacks = []

        callback_config = self.config.get('callbacks', {})

        # è®­ç»ƒæŒ‡æ ‡ç›‘æ§å›è°ƒ - æ”¾åœ¨æœ€å‰é¢ä»¥ç¡®ä¿åŠæ—¶ç›‘æ§
        if callback_config.get('enable_training_metrics', True):
            metrics_callback = TrainingMetricsCallback(
                log_interval=callback_config.get('metrics_log_interval', 1000),
                eval_interval=callback_config.get('metrics_eval_interval', 10000),
                verbose=1
            )
            # ä¼ é€’é…ç½®ä¿¡æ¯ç»™å›è°ƒ
            metrics_callback.set_config(self.config)
            callbacks.append(metrics_callback)

        # è¯„ä¼°å›è°ƒ
        if callback_config.get('enable_eval', True):
            eval_callback = EvalCallback(
                self.eval_env,
                best_model_save_path=f"models/best_model",
                log_path="logs/eval_logs",
                eval_freq=callback_config.get('eval_freq', 10000),
                n_eval_episodes=callback_config.get('n_eval_episodes', 5),
                deterministic=True,
                render=False,
                verbose=1
            )
            callbacks.append(eval_callback)

        # æ£€æŸ¥ç‚¹å›è°ƒ
        if callback_config.get('enable_checkpoint', True):
            checkpoint_callback = CheckpointCallback(
                save_freq=callback_config.get('save_freq', 50000),
                save_path="models/checkpoints",
                name_prefix="rl_model"
            )
            callbacks.append(checkpoint_callback)

        # å›æ’¤æ—©åœå›è°ƒ
        if callback_config.get('enable_drawdown_stopping', True):
            drawdown_callback = DrawdownStoppingCallback(
                max_drawdown=callback_config.get('max_training_drawdown', 0.15),
                patience=callback_config.get('drawdown_patience', 100),
                verbose=1
            )
            callbacks.append(drawdown_callback)

        # TensorBoardå›è°ƒ
        if callback_config.get('enable_tensorboard', True):
            tb_callback = TensorBoardCallback(verbose=1)
            callbacks.append(tb_callback)

        return callbacks

    def train(self) -> None:
        """æ‰§è¡Œè®­ç»ƒ"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        self.training_start_time = datetime.now()

        # è®­ç»ƒé…ç½®
        train_config = self.config['training']
        total_timesteps = train_config.get('total_timesteps', 1000000)

        # è®¾ç½®éšæœºç§å­
        if 'seed' in train_config:
            set_random_seed(train_config['seed'])

        # è®¾ç½®å›è°ƒ
        callbacks = self.setup_callbacks()

        try:
            # å¼€å§‹è®­ç»ƒ
            self.model.learn(
                total_timesteps=total_timesteps,
                callback=callbacks,
                log_interval=train_config.get('log_interval', 10),
                progress_bar=True
            )

            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = f"models/final_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.model.save(final_model_path)
            logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

        except KeyboardInterrupt:
            logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            # ä¿å­˜å½“å‰æ¨¡å‹
            interrupt_model_path = f"models/interrupted_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.model.save(interrupt_model_path)
            logger.info(f"ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupt_model_path}")

        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise RuntimeError(f"è®­ç»ƒå¤±è´¥: {e}")

        training_time = datetime.now() - self.training_start_time
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time}")

    def evaluate(self, model_path: str = None, test_data: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨å½“å‰æ¨¡å‹
            test_data: æµ‹è¯•æ•°æ®ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨éªŒè¯ç¯å¢ƒ

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")

        # åŠ è½½æ¨¡å‹
        if model_path:
            model_config = self.config['model']
            algorithm = model_config.get('algorithm', 'SAC').upper()

            if algorithm == 'SAC':
                eval_model = SAC.load(model_path)
            elif algorithm == 'PPO':
                eval_model = PPO.load(model_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        else:
            eval_model = self.model

        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        if test_data is not None:
            env_config = self.config['environment']
            eval_env = PortfolioEnv(
                data=test_data,
                initial_cash=env_config.get('initial_cash', 1000000),
                lookback_window=env_config.get('lookback_window', 30),
                transaction_cost=env_config.get('transaction_cost', 0.003),
                max_drawdown_threshold=env_config.get('max_drawdown_threshold', 0.15),
                reward_penalty=env_config.get('reward_penalty', 2.0),
                features=env_config.get('features')
            )
        else:
            eval_env = self.eval_env.envs[0]

        # æ‰§è¡Œè¯„ä¼°
        n_eval_episodes = self.config.get('evaluation', {}).get('n_episodes', 5)
        episode_rewards = []
        episode_lengths = []

        for episode in range(n_eval_episodes):
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0

            while True:
                action, _ = eval_model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                episode_length += 1

                if terminated or truncated:
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

            logger.info(f"è¯„ä¼°å›åˆ {episode + 1}: å¥–åŠ± = {episode_reward:.4f}, é•¿åº¦ = {episode_length}")

        # è·å–ç»„åˆæ€§èƒ½
        portfolio_performance = eval_env.get_portfolio_performance()

        # æ•´ç†è¯„ä¼°ç»“æœ
        evaluation_results = {
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'portfolio_performance': portfolio_performance
        }

        # ä¿å­˜è¯„ä¼°ç»“æœ
        results_path = f"results/evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
        with open(results_path, 'w', encoding='utf-8') as f:
            yaml.dump(evaluation_results, f, allow_unicode=True, default_flow_style=False)

        logger.info(f"è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜: {results_path}")

        return evaluation_results

    def run_full_pipeline(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒ-è¯„ä¼°æµæ°´çº¿

        Returns:
            è®­ç»ƒå’Œè¯„ä¼°ç»“æœ
        """
        logger.info("å¼€å§‹è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿...")

        try:
            # 1. åˆå§‹åŒ–æ•°æ®
            train_data, valid_data, test_data = self.initialize_data()

            # 2. åˆ›å»ºç¯å¢ƒ
            self.create_environments(train_data, valid_data)

            # 3. åˆ›å»ºæ¨¡å‹
            self.create_model()

            # 4. æ‰§è¡Œè®­ç»ƒ
            self.train()

            # 5. è¯„ä¼°æ¨¡å‹
            evaluation_results = self.evaluate(test_data=test_data)

            # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            pipeline_results = {
                'config': self.config,
                'training_time': str(datetime.now() - self.training_start_time),
                'evaluation_results': evaluation_results,
                'data_info': {
                    'train_shape': train_data.shape,
                    'valid_shape': valid_data.shape,
                    'test_shape': test_data.shape
                }
            }

            # ä¿å­˜å®Œæ•´ç»“æœ
            results_path = f"results/pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
            with open(results_path, 'w', encoding='utf-8') as f:
                yaml.dump(pipeline_results, f, allow_unicode=True, default_flow_style=False)

            logger.info(f"å®Œæ•´æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼Œç»“æœå·²ä¿å­˜: {results_path}")

            return pipeline_results

        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            raise RuntimeError(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")

        finally:
            # æ¸…ç†èµ„æº
            if self.train_env:
                self.train_env.close()
            if self.eval_env:
                self.eval_env.close()


def load_config(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        raise RuntimeError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {e}")


if __name__ == "__main__":
    # ç¤ºä¾‹é…ç½®
    example_config = {
        'data': {
            'data_root': os.path.expanduser("/Users/shuzhenyi/.qlib/qlib_data"),
            'provider_uri': {
                '1min': os.path.expanduser("/Users/shuzhenyi/.qlib/qlib_data/cn_data_1min"),
                'day': os.path.expanduser("/Users/shuzhenyi/.qlib/qlib_data/cn_data")
            },
            'market': 'csi300',
            'stock_limit': 30,
            'start_time': '2020-01-01',
            'end_time': '2023-12-31',
            'train_start': '2020-01-01',
            'train_end': '2022-12-31',
            'valid_start': '2023-01-01',
            'valid_end': '2023-06-30',
            'test_start': '2023-07-01',
            'test_end': '2023-12-31',
            'freq': 'day',
            'fields': ['$close', '$open', '$high', '$low', '$volume']
        },
        'environment': {
            'initial_cash': 1000000,
            'lookback_window': 30,
            'transaction_cost': 0.003,
            'max_drawdown_threshold': 0.15,
            'reward_penalty': 2.0,
            'features': ['$close', '$open', '$high', '$low', '$volume']
        },
        'model': {
            'algorithm': 'SAC',
            'learning_rate': 3e-4,
            'batch_size': 256,
            'gamma': 0.99,
            'buffer_size': 1000000,
            'tau': 0.005,
            'use_custom_policy': False,
            'net_arch': [256, 256]
        },
        'training': {
            'total_timesteps': 500000,
            'n_envs': 4,
            'seed': 42,
            'log_interval': 10
        },
        'callbacks': {
            'enable_training_metrics': True,
            'metrics_log_interval': 1000,      # æ¯1000æ­¥è¾“å‡ºåŸºç¡€æŒ‡æ ‡
            'metrics_eval_interval': 10000,    # æ¯10000æ­¥è¾“å‡ºè¯¦ç»†æŒ‡æ ‡
            'enable_eval': True,
            'eval_freq': 10000,
            'n_eval_episodes': 3,
            'enable_checkpoint': True,
            'save_freq': 50000,
            'enable_drawdown_stopping': True,
            'max_training_drawdown': 0.15,
            'drawdown_patience': 100,
            'enable_tensorboard': True
        },
        'evaluation': {
            'n_episodes': 5
        },
        'log_level': 'INFO'
    }

    # ä¿å­˜ç¤ºä¾‹é…ç½®
    os.makedirs('configs', exist_ok=True)
    with open('configs/example_sac.yaml', 'w', encoding='utf-8') as f:
        yaml.dump(example_config, f, allow_unicode=True, default_flow_style=False)

    print("ç¤ºä¾‹é…ç½®å·²ä¿å­˜åˆ° configs/example_sac.yaml")
    print("è®­ç»ƒå™¨ç±»å·²å®ç°ï¼Œå¯é€šè¿‡ä»¥ä¸‹æ–¹å¼ä½¿ç”¨:")
    print("trainer = RLTrainer(config)")
    print("results = trainer.run_full_pipeline()")