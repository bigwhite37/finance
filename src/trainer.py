"""
å¼ºåŒ–å­¦ä¹ è®­ç»ƒæµæ°´çº¿
é›†æˆQlibæ•°æ®åŠ è½½ã€ç¯å¢ƒåˆ›å»ºã€æ¨¡å‹è®­ç»ƒã€è¯„ä¼°å’Œå›æµ‹
"""
import os
import yaml
import logging
from datetime import datetime
from typing import Dict, Any, Optional, List, Tuple
import numpy as np
import pandas as pd
import torch
from stable_baselines3 import SAC, PPO
from stable_baselines3.common.env_util import make_vec_env
from stable_baselines3.common.vec_env import DummyVecEnv, SubprocVecEnv
from stable_baselines3.common.callbacks import (
    EvalCallback, StopTrainingOnRewardThreshold,
    CheckpointCallback, BaseCallback
)
from stable_baselines3.common.monitor import Monitor
from stable_baselines3.common.utils import set_random_seed
import gymnasium as gym
from tqdm import tqdm

from data_loader import QlibDataLoader, split_data
from env import PortfolioEnv
from model import TradingPolicy, RiskAwareRewardWrapper, PortfolioMetrics

logger = logging.getLogger(__name__)


class TrainingQualityAnalyzer:
    """è®­ç»ƒè´¨é‡åˆ†æå™¨ï¼Œç”¨äºè¯„ä¼°è®­ç»ƒå‚æ•°ã€ä»£ç ã€æ¨¡å‹è´¨é‡"""

    def __init__(self):
        self.metrics_history = []
        self.quality_scores = {}

    def analyze_training_stability(self, rewards: List[float], window_size: int = 50) -> Dict[str, float]:
        """
        åˆ†æè®­ç»ƒç¨³å®šæ€§

        Args:
            rewards: å¥–åŠ±å†å²
            window_size: æ»‘åŠ¨çª—å£å¤§å°ï¼ˆé»˜è®¤50è€Œé100ï¼‰

        Returns:
            ç¨³å®šæ€§æŒ‡æ ‡å­—å…¸
        """
        if len(rewards) < 10:  # æœ€å°‘éœ€è¦10ä¸ªå¥–åŠ±å€¼
            return {
                'stability_score': 0.0,
                'trend_score': 0.0,
                'variance_score': 0.0,
                'slope': 0.0,
                'variance': 0.0
            }

        # åŠ¨æ€è°ƒæ•´çª—å£å¤§å°é¿å…è¿‡å¤§
        actual_window_size = min(window_size, max(10, len(rewards) // 2))

        # å¦‚æœæ•°æ®ä¸è¶³çª—å£å¤§å°ï¼Œç›´æ¥ç”¨æ‰€æœ‰æ•°æ®
        if len(rewards) < actual_window_size:
            moving_avg = [np.mean(rewards)]
        else:
            # è®¡ç®—æ»‘åŠ¨å¹³å‡
            moving_avg = []
            for i in range(actual_window_size, len(rewards) + 1):
                window_rewards = rewards[i-actual_window_size:i]
                moving_avg.append(np.mean(window_rewards))

        if len(moving_avg) < 2:
            # å¦‚æœæ»‘åŠ¨å¹³å‡ä¸è¶³2ä¸ªç‚¹ï¼Œç›´æ¥åˆ†æåŸå§‹å¥–åŠ±
            x = np.arange(len(rewards))
            slope = np.polyfit(x, rewards, 1)[0] if len(rewards) > 1 else 0.0
            variance = np.var(rewards)
            mean_reward = np.mean(rewards)
        else:
            # è¶‹åŠ¿åˆ†æ - çº¿æ€§å›å½’æ–œç‡
            x = np.arange(len(moving_avg))
            slope = np.polyfit(x, moving_avg, 1)[0]
            variance = np.var(moving_avg)
            mean_reward = np.mean(moving_avg)

        # æ”¹è¿›çš„è¶‹åŠ¿è¯„åˆ†ï¼šè€ƒè™‘è´Ÿå¥–åŠ±æƒ…å†µ
        if mean_reward < 0:
            # å¯¹äºè´Ÿå¥–åŠ±ï¼Œæ–œç‡ä¸ºæ­£ï¼ˆè¶‹å‘0æˆ–æ­£å€¼ï¼‰å¾—é«˜åˆ†
            trend_score = max(0, min(1, (slope + 0.1) / 0.2))
        else:
            # å¯¹äºæ­£å¥–åŠ±ï¼Œæ–œç‡ä¸ºæ­£å¾—é«˜åˆ†
            trend_score = max(0, min(1, (slope + 0.01) / 0.02))

        # é’ˆå¯¹ç™¾åˆ†ç‚¹å°ºåº¦çš„æ–¹å·®è¯„åˆ†ï¼šç›´æ¥ç”¨æ ‡å‡†å·®è¯„ä¼°
        std_dev = np.sqrt(variance)
        # å¯¹äºç™¾åˆ†ç‚¹å°ºåº¦å¥–åŠ±ï¼Œæ ‡å‡†å·®è¶…è¿‡2.0è®¤ä¸ºæ˜¯é«˜æ–¹å·®
        variance_score = max(0, min(1, 1 - std_dev / 3.0))  # æ ‡å‡†å·®è¶…è¿‡3.0å¾—åˆ†ä¸º0

        # ç»¼åˆç¨³å®šæ€§å¾—åˆ†
        stability_score = 0.6 * trend_score + 0.4 * variance_score

        return {
            'stability_score': stability_score,
            'trend_score': trend_score,
            'variance_score': variance_score,
            'slope': slope,
            'variance': variance
        }

    def analyze_convergence_quality(self, losses: List[float]) -> Dict[str, float]:
        """
        åˆ†ææ”¶æ•›è´¨é‡

        Args:
            losses: æŸå¤±å†å²

        Returns:
            æ”¶æ•›è´¨é‡æŒ‡æ ‡
        """
        if len(losses) < 100:
            return {
                'convergence_score': 0.0,
                'oscillation_score': 0.0,
                'improvement_ratio': 0.0,
                'oscillation_ratio': 0.0
            }

        # è®¡ç®—æŸå¤±ä¸‹é™è¶‹åŠ¿
        recent_losses = losses[-100:]
        early_losses = losses[:100] if len(losses) >= 200 else losses[:len(losses)//2]

        improvement = (np.mean(early_losses) - np.mean(recent_losses)) / (np.mean(early_losses) + 1e-8)
        convergence_score = max(0, min(1, improvement))

        # åˆ†æéœ‡è¡ç¨‹åº¦
        loss_diff = np.diff(recent_losses)
        oscillation = np.std(loss_diff) / (np.mean(np.abs(loss_diff)) + 1e-8)
        oscillation_score = max(0, min(1, 1 - oscillation / 10))

        return {
            'convergence_score': convergence_score,
            'oscillation_score': oscillation_score,
            'improvement_ratio': improvement,
            'oscillation_ratio': oscillation
        }

    def analyze_hyperparameter_quality(self, config: Dict[str, Any]) -> Dict[str, float]:
        """
        åˆ†æè¶…å‚æ•°è´¨é‡

        Args:
            config: é…ç½®å­—å…¸

        Returns:
            è¶…å‚æ•°è´¨é‡è¯„åˆ†
        """
        scores = {}

        # å­¦ä¹ ç‡è¯„ä¼°
        lr = config.get('model', {}).get('learning_rate', 3e-4)
        if 1e-5 <= lr <= 1e-2:
            scores['learning_rate_score'] = 1.0
        elif 1e-6 <= lr <= 1e-1:
            scores['learning_rate_score'] = 0.7
        else:
            scores['learning_rate_score'] = 0.3

        # æ‰¹æ¬¡å¤§å°è¯„ä¼°
        batch_size = config.get('model', {}).get('batch_size', 256)
        if 64 <= batch_size <= 512:
            scores['batch_size_score'] = 1.0
        elif 32 <= batch_size <= 1024:
            scores['batch_size_score'] = 0.8
        else:
            scores['batch_size_score'] = 0.5

        # ç½‘ç»œæ¶æ„è¯„ä¼°
        net_arch = config.get('model', {}).get('net_arch', [256, 256])
        if isinstance(net_arch, list) and 2 <= len(net_arch) <= 4:
            if all(64 <= size <= 512 for size in net_arch):
                scores['network_arch_score'] = 1.0
            else:
                scores['network_arch_score'] = 0.7
        else:
            scores['network_arch_score'] = 0.5

        # ç¯å¢ƒå‚æ•°è¯„ä¼°
        env_config = config.get('environment', {})
        transaction_cost = env_config.get('transaction_cost', 0.003)
        if 0.001 <= transaction_cost <= 0.01:
            scores['transaction_cost_score'] = 1.0
        else:
            scores['transaction_cost_score'] = 0.6

        # ç»¼åˆè¯„åˆ†
        scores['overall_hyperparameter_score'] = np.mean(list(scores.values()))

        return scores

    def generate_quality_report(self,
                              rewards: List[float],
                              losses: List[float],
                              config: Dict[str, Any],
                              portfolio_values: List[float],
                              env_data=None) -> str:
        """
        ç”Ÿæˆè®­ç»ƒè´¨é‡æŠ¥å‘Š

        Args:
            rewards: å¥–åŠ±å†å²
            losses: æŸå¤±å†å²
            config: é…ç½®
            portfolio_values: ç»„åˆä»·å€¼å†å²

        Returns:
            è´¨é‡æŠ¥å‘Šå­—ç¬¦ä¸²
        """
        report = []
        report.append("=" * 80)
        report.append("è®­ç»ƒè´¨é‡åˆ†ææŠ¥å‘Š")
        report.append("=" * 80)

        # ç¨³å®šæ€§åˆ†æ
        if rewards:
            stability = self.analyze_training_stability(rewards)
            report.append(f"\nğŸ“Š è®­ç»ƒç¨³å®šæ€§åˆ†æ:")
            report.append(f"  ç»¼åˆç¨³å®šæ€§å¾—åˆ†: {stability['stability_score']:.3f}")
            report.append(f"  è¶‹åŠ¿å¾—åˆ†: {stability['trend_score']:.3f}")
            report.append(f"  æ–¹å·®å¾—åˆ†: {stability['variance_score']:.3f}")
            report.append(f"  å¥–åŠ±è¶‹åŠ¿æ–œç‡: {stability['slope']:.6f}")

            if stability['stability_score'] > 0.8:
                report.append("  âœ… è®­ç»ƒç¨³å®šæ€§è‰¯å¥½")
            elif stability['stability_score'] > 0.6:
                report.append("  âš ï¸  è®­ç»ƒç¨³å®šæ€§ä¸€èˆ¬ï¼Œå»ºè®®è°ƒæ•´å­¦ä¹ ç‡")
            else:
                report.append("  âŒ è®­ç»ƒä¸ç¨³å®šï¼Œå»ºè®®æ£€æŸ¥è¶…å‚æ•°è®¾ç½®")

        # æ”¶æ•›è´¨é‡åˆ†æ
        if losses:
            convergence = self.analyze_convergence_quality(losses)
            report.append(f"\nğŸ¯ æ”¶æ•›è´¨é‡åˆ†æ:")
            report.append(f"  æ”¶æ•›å¾—åˆ†: {convergence['convergence_score']:.3f}")
            report.append(f"  éœ‡è¡å¾—åˆ†: {convergence['oscillation_score']:.3f}")
            report.append(f"  æ”¹è¿›æ¯”ç‡: {convergence['improvement_ratio']:.3f}")

            if convergence['convergence_score'] > 0.7:
                report.append("  âœ… æ¨¡å‹æ”¶æ•›è‰¯å¥½")
            elif convergence['convergence_score'] > 0.4:
                report.append("  âš ï¸  æ”¶æ•›é€Ÿåº¦è¾ƒæ…¢ï¼Œå¯è€ƒè™‘è°ƒæ•´å­¦ä¹ ç‡")
            else:
                report.append("  âŒ æ”¶æ•›å›°éš¾ï¼Œå»ºè®®æ£€æŸ¥æ¨¡å‹æ¶æ„æˆ–æ•°æ®è´¨é‡")

        # è¶…å‚æ•°è´¨é‡åˆ†æ
        hyperparams = self.analyze_hyperparameter_quality(config)
        report.append(f"\nâš™ï¸  è¶…å‚æ•°è´¨é‡åˆ†æ:")
        report.append(f"  å­¦ä¹ ç‡å¾—åˆ†: {hyperparams['learning_rate_score']:.3f}")
        report.append(f"  æ‰¹æ¬¡å¤§å°å¾—åˆ†: {hyperparams['batch_size_score']:.3f}")
        report.append(f"  ç½‘ç»œæ¶æ„å¾—åˆ†: {hyperparams['network_arch_score']:.3f}")
        report.append(f"  äº¤æ˜“æˆæœ¬å¾—åˆ†: {hyperparams['transaction_cost_score']:.3f}")
        report.append(f"  ç»¼åˆè¶…å‚æ•°å¾—åˆ†: {hyperparams['overall_hyperparameter_score']:.3f}")

        # ç»„åˆè¡¨ç°åˆ†æ - éœ€è¦è°ƒè¯•æ•°æ®æ¥æºé—®é¢˜
        if portfolio_values and len(portfolio_values) > 1:
            report.append(f"\nğŸ’° ç»„åˆè¡¨ç°åˆ†æ (DEBUG):")
            report.append(f"  æ•°æ®ç‚¹æ€»æ•°: {len(portfolio_values)}")
            report.append(f"  å‰10ä¸ªå€¼: {portfolio_values[:10]}")
            report.append(f"  å10ä¸ªå€¼: {portfolio_values[-10:]}")
            report.append(f"  æœ€å°å€¼: {min(portfolio_values):,.0f}")
            report.append(f"  æœ€å¤§å€¼: {max(portfolio_values):,.0f}")
            
            # ç­‰æƒæŠ•èµ„baselineè®¡ç®—
            # æ·»åŠ è°ƒè¯•ä¿¡æ¯
            report.append(f"\nğŸ” ç¯å¢ƒæ•°æ®è°ƒè¯•ä¿¡æ¯:")
            if env_data is None:
                report.append(f"  ç¯å¢ƒæ•°æ®: None")
            else:
                report.append(f"  ç¯å¢ƒæ•°æ®ç±»å‹: {type(env_data)}")
                report.append(f"  ç¯å¢ƒå±æ€§: {[attr for attr in dir(env_data) if not attr.startswith('_')][:10]}")
                if hasattr(env_data, 'data'):
                    report.append(f"  æ•°æ®å½¢çŠ¶: {env_data.data.shape}")
                    report.append(f"  æ•°æ®åˆ—: {list(env_data.data.columns)[:10]}")
                if hasattr(env_data, 'time_index'):
                    report.append(f"  æ—¶é—´ç´¢å¼•é•¿åº¦: {len(env_data.time_index)}")
                if hasattr(env_data, 'stock_list'):
                    report.append(f"  è‚¡ç¥¨åˆ—è¡¨: {env_data.stock_list}")
            
            if not config or 'data' not in config or 'custom_stocks' not in config['data']:
                raise RuntimeError("é…ç½®ä¿¡æ¯ä¸è¶³ï¼Œæ— æ³•è®¡ç®—baseline")
                
            stocks = config['data']['custom_stocks']
            num_stocks = len(stocks)
            
            if env_data is not None:
                # ä½¿ç”¨ç¯å¢ƒæ•°æ®è®¡ç®—æ›´ç²¾ç¡®çš„ç­‰æƒbaseline
                report.append(f"\nğŸ“Š ç­‰æƒæŠ•èµ„Baseline:")
                report.append(f"  è‚¡ç¥¨æ± : {stocks}")
                report.append(f"  è‚¡ç¥¨æ•°é‡: {num_stocks}")
                
                # è·å–ç¯å¢ƒçš„å†å²æ•°æ®
                has_data = (hasattr(env_data, 'data') and 
                           hasattr(env_data, 'time_index') and 
                           hasattr(env_data, 'stock_list'))
                
                report.append(f"  æ•°æ®å¯ç”¨æ€§æ£€æŸ¥: {has_data}")
                if hasattr(env_data, 'data'):
                    report.append(f"  æœ‰dataå±æ€§: True, å½¢çŠ¶: {getattr(env_data.data, 'shape', 'N/A')}")
                else:
                    report.append(f"  æœ‰dataå±æ€§: False")
                if hasattr(env_data, 'time_index'):
                    report.append(f"  æœ‰time_indexå±æ€§: True, é•¿åº¦: {len(getattr(env_data, 'time_index', []))}")
                else:
                    report.append(f"  æœ‰time_indexå±æ€§: False")
                if hasattr(env_data, 'stock_list'):
                    report.append(f"  æœ‰stock_listå±æ€§: True, å†…å®¹: {getattr(env_data, 'stock_list', [])}")
                else:
                    report.append(f"  æœ‰stock_listå±æ€§: False")
                
                if has_data:
                    # è®¡ç®—ç­‰æƒç»„åˆåœ¨ç›¸åŒæ—¶é—´æ®µçš„è¡¨ç°
                    equal_weight = 1.0 / num_stocks
                    initial_value = portfolio_values[0]
                    
                    # ä½¿ç”¨å®é™…è‚¡ç¥¨ä»·æ ¼æ•°æ®è®¡ç®—ç­‰æƒæ”¶ç›Š
                    start_idx = max(0, len(env_data.time_index) - len(portfolio_values))
                    end_idx = len(env_data.time_index) - 1
                    
                    report.append(f"  æ—¶é—´èŒƒå›´: ç´¢å¼• {start_idx} -> {end_idx}")
                    
                    if start_idx < end_idx and hasattr(env_data, 'data'):
                        # æ£€æŸ¥æ•°æ®åˆ—
                        close_column = None
                        for col in ['$close', 'close', 'Close', '$Close']:
                            if col in env_data.data.columns:
                                close_column = col
                                break
                        
                        if not close_column:
                            raise RuntimeError(f"æ•°æ®ä¸­æ‰¾ä¸åˆ°ä»·æ ¼åˆ—ï¼Œå¯ç”¨åˆ—: {list(env_data.data.columns)}")
                        
                        # è·å–èµ·å§‹å’Œç»“æŸä»·æ ¼
                        start_time = env_data.time_index[start_idx]
                        end_time = env_data.time_index[end_idx]
                        
                        report.append(f"  ä½¿ç”¨ä»·æ ¼åˆ—: {close_column}")
                        report.append(f"  æ—¶é—´èŒƒå›´: {start_time} -> {end_time}")
                        
                        total_return = 0.0
                        valid_stocks = 0
                        stock_details = []
                        
                        for stock in stocks:
                            if (stock, start_time) not in env_data.data.index:
                                stock_details.append(f"{stock}: èµ·å§‹æ—¶é—´æ•°æ®ç¼ºå¤±")
                                continue
                            if (stock, end_time) not in env_data.data.index:
                                stock_details.append(f"{stock}: ç»“æŸæ—¶é—´æ•°æ®ç¼ºå¤±")
                                continue
                                
                            start_price = env_data.data.loc[(stock, start_time), close_column]
                            end_price = env_data.data.loc[(stock, end_time), close_column]
                            
                            if start_price <= 0 or end_price <= 0:
                                stock_details.append(f"{stock}: ä»·æ ¼æ— æ•ˆ ({start_price}->{end_price})")
                                continue
                                
                            stock_return = (end_price / start_price - 1)
                            total_return += stock_return * equal_weight
                            valid_stocks += 1
                            stock_details.append(f"{stock}: {start_price:.2f}->{end_price:.2f} ({stock_return:+.2%})")
                        
                        # æ˜¾ç¤ºè‚¡ç¥¨è¯¦æƒ…
                        for detail in stock_details:
                            report.append(f"  {detail}")
                        
                        if valid_stocks == 0:
                            raise RuntimeError("æ²¡æœ‰æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®")
                        
                        # é‡æ–°å½’ä¸€åŒ–æƒé‡
                        total_return = total_return * num_stocks / valid_stocks
                        baseline_final_value = initial_value * (1 + total_return)
                        baseline_return = total_return * 100
                        
                        report.append(f"  æœ‰æ•ˆè‚¡ç¥¨æ•°: {valid_stocks}/{num_stocks}")
                        report.append(f"  ç­‰æƒç­–ç•¥æœŸæœ«ä»·å€¼: {baseline_final_value:,.0f}")
                        report.append(f"  ç­‰æƒç­–ç•¥æ€»æ”¶ç›Šç‡: {baseline_return:+.2f}%")
                    else:
                        raise RuntimeError(f"æ—¶é—´ç´¢å¼•èŒƒå›´æ— æ•ˆ: {start_idx} -> {end_idx}")
                        
                else:
                    # å¦‚æœç¯å¢ƒæ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨æ”¶ç›Šç‡æ–¹æ³•è®¡ç®—baseline
                    if hasattr(env_data, 'return_history') and len(env_data.return_history) > 0:
                        avg_market_return = np.mean(list(env_data.return_history))
                        baseline_final_value = portfolio_values[0] * (1 + avg_market_return * len(portfolio_values))
                        baseline_return = (baseline_final_value / portfolio_values[0] - 1) * 100
                        
                        report.append(f"  ç­‰æƒç­–ç•¥æœŸæœ«ä»·å€¼ (ä¼°ç®—): {baseline_final_value:,.0f}")
                        report.append(f"  ç­‰æƒç­–ç•¥æ€»æ”¶ç›Šç‡ (ä¼°ç®—): {baseline_return:+.2f}%")
                        report.append(f"  (ä½¿ç”¨ç¯å¢ƒå¹³å‡æ”¶ç›Šä¼°ç®—)")
                    else:
                        # å¦‚æœç¯å¢ƒæ•°æ®ä¸å¯ç”¨ï¼Œä½¿ç”¨æ”¶ç›Šç‡æ–¹æ³•è®¡ç®—baseline
                        report.append(f"\n  ä½¿ç”¨æ”¶ç›Šç‡æ–¹æ³•è®¡ç®—ç­‰æƒbaseline...")
                        
                        import sys
                        import os
                        sys.path.append('src')
                        import qlib
                        from qlib.data import D
                        
                        # é‡æ–°åˆå§‹åŒ–qlibå’Œè·å–æ•°æ®
                        if 'data' not in config or 'provider_uri' not in config['data']:
                            raise RuntimeError("é…ç½®ä¸­ç¼ºå°‘æ•°æ®è·¯å¾„ä¿¡æ¯")
                            
                        qlib.init(provider_uri=config['data']['provider_uri']['day'], region='cn')
                        
                        # ä½¿ç”¨æ­£ç¡®çš„è®­ç»ƒæ—¶é—´èŒƒå›´
                        start_time = config['data']['train_start']
                        end_time = config['data']['train_end']
                        
                        # è·å–åŸå§‹ä»·æ ¼æ•°æ®ï¼ˆä¸ä½¿ç”¨æ ‡å‡†åŒ–ï¼‰
                        data = D.features(
                            instruments=stocks,
                            fields=['$close'],
                            start_time=start_time,
                            end_time=end_time,
                            freq='day'
                        )
                        
                        if data.empty:
                            raise RuntimeError("æ— æ³•è·å–è‚¡ç¥¨ä»·æ ¼æ•°æ®")
                            
                        report.append(f"  åŸå§‹æ•°æ®å½¢çŠ¶: {data.shape}")
                        report.append(f"  æ—¶é—´èŒƒå›´: {start_time} -> {end_time}")
                        
                        # è·å–æ—¶é—´ç´¢å¼•
                        time_index = data.index.get_level_values(1).unique().sort_values()
                        
                        # æ ¹æ®portfolio_valuesçš„é•¿åº¦è®¡ç®—å¯¹åº”çš„æ—¶é—´æ®µ
                        portfolio_steps = len(portfolio_values)
                        total_days = len(time_index)
                        
                        # portfolio_stepsæ˜¯è®­ç»ƒæ­¥æ•°ï¼Œä¸ç­‰äºäº¤æ˜“å¤©æ•°
                        # éœ€è¦æ ¹æ®ç¯å¢ƒçš„max_stepsæ¥è®¡ç®—å®é™…å¯¹åº”çš„å¤©æ•°
                        if 'environment' in config and 'max_steps' in config['environment']:
                            max_episode_steps = config['environment']['max_steps']
                            # è®¡ç®—å®é™…ä½¿ç”¨çš„äº¤æ˜“å¤©æ•°ï¼ˆå–è¾ƒå°å€¼ï¼‰
                            actual_trading_days = min(portfolio_steps, max_episode_steps, total_days)
                        else:
                            # å¦‚æœæ²¡æœ‰é…ç½®ä¿¡æ¯ï¼Œä½¿ç”¨å¯ç”¨å¤©æ•°
                            actual_trading_days = min(portfolio_steps, total_days)
                        
                        if actual_trading_days <= 1:
                            raise RuntimeError(f"è®¡ç®—çš„äº¤æ˜“å¤©æ•°å¤ªå°‘: {actual_trading_days}")
                        
                        # ä½¿ç”¨æœ€è¿‘çš„actual_trading_dayså¤©è®¡ç®—baseline
                        baseline_start_idx = total_days - actual_trading_days
                        baseline_end_idx = total_days - 1
                        
                        baseline_start_time = time_index[baseline_start_idx]
                        baseline_end_time = time_index[baseline_end_idx]
                        
                        report.append(f"  æ€»è®­ç»ƒæ­¥æ•°: {portfolio_steps}")
                        report.append(f"  å¯ç”¨äº¤æ˜“å¤©æ•°: {total_days}")
                        report.append(f"  å®é™…è®¡ç®—å¤©æ•°: {actual_trading_days}")
                        report.append(f"  baselineè®¡ç®—æ—¶é—´æ®µ: {baseline_start_time} -> {baseline_end_time}")
                        
                        # è®¡ç®—æ¯åªè‚¡ç¥¨çš„æ”¶ç›Šç‡
                        equal_weight = 1.0 / num_stocks
                        total_return = 0.0
                        valid_stocks = 0
                        
                        for stock in stocks:
                            if (stock, baseline_start_time) not in data.index or (stock, baseline_end_time) not in data.index:
                                report.append(f"  {stock}: ç¼ºå°‘æ—¶é—´ç‚¹æ•°æ®")
                                continue
                                
                            start_price = data.loc[(stock, baseline_start_time), '$close']
                            end_price = data.loc[(stock, baseline_end_time), '$close']
                            
                            if start_price <= 0 or end_price <= 0:
                                report.append(f"  {stock}: ä»·æ ¼æ— æ•ˆ ({start_price:.2f}->{end_price:.2f})")
                                continue
                            
                            stock_return = (end_price / start_price - 1)
                            total_return += stock_return * equal_weight
                            valid_stocks += 1
                            
                            report.append(f"  {stock}: {start_price:.2f}->{end_price:.2f} ({stock_return:+.2%})")
                        
                        if valid_stocks == 0:
                            raise RuntimeError("æ²¡æœ‰æœ‰æ•ˆçš„è‚¡ç¥¨æ•°æ®ç”¨äºè®¡ç®—baseline")
                        
                        # é‡æ–°å½’ä¸€åŒ–æƒé‡ï¼ˆå¦‚æœæœ‰è‚¡ç¥¨æ•°æ®ç¼ºå¤±ï¼‰
                        if valid_stocks < num_stocks:
                            total_return = total_return * num_stocks / valid_stocks
                            report.append(f"  æƒé‡å½’ä¸€åŒ–: {valid_stocks}/{num_stocks}åªè‚¡ç¥¨æœ‰æ•ˆ")
                        
                        baseline_final_value = portfolio_values[0] * (1 + total_return)
                        baseline_return = total_return * 100
                        
                        report.append(f"  âœ… æˆåŠŸè®¡ç®—ç­‰æƒbaseline:")
                        report.append(f"  æœ‰æ•ˆè‚¡ç¥¨æ•°: {valid_stocks}/{num_stocks}")
                        report.append(f"  ç­‰æƒç­–ç•¥æœŸæœ«ä»·å€¼: {baseline_final_value:,.0f}")
                        report.append(f"  ç­‰æƒç­–ç•¥æ€»æ”¶ç›Šç‡: {baseline_return:+.2f}%")
            else:
                raise RuntimeError("æ²¡æœ‰å¯ç”¨çš„ç¯å¢ƒæ•°æ®è®¡ç®—baseline")
            
            # æš‚æ—¶ç”¨ç®€å•è®¡ç®—æŸ¥çœ‹é—®é¢˜
            if len(portfolio_values) > 1:
                initial_value = portfolio_values[0]
                final_value = portfolio_values[-1]
                total_return = (final_value / initial_value - 1) * 100
                
                report.append(f"\n  === ç­–ç•¥ vs Baseline å¯¹æ¯” ===")
                report.append(f"  åˆå§‹å€¼: {initial_value:,.0f}")
                report.append(f"  æœ€ç»ˆå€¼: {final_value:,.0f}")
                report.append(f"  ç­–ç•¥æ€»æ”¶ç›Šç‡: {total_return:+.2f}%")
                
                # ä¸baselineå¯¹æ¯”
                if 'baseline_return' in locals():
                    excess_return = total_return - baseline_return
                    report.append(f"  è¶…é¢æ”¶ç›Š (vs ç­‰æƒ): {excess_return:+.2f}%")
                    if excess_return > 0:
                        report.append(f"  âœ… ç­–ç•¥è·‘èµ¢ç­‰æƒbaseline")
                    else:
                        report.append(f"  âŒ ç­–ç•¥è·‘è¾“ç­‰æƒbaseline")
                
                # ç®€å•å›æ’¤è®¡ç®—æ¥æ‰¾é—®é¢˜
                peak = portfolio_values[0]
                max_drawdown = 0
                for value in portfolio_values:
                    if value > peak:
                        peak = value
                    drawdown = (peak - value) / peak
                    if drawdown > max_drawdown:
                        max_drawdown = drawdown
                        
                report.append(f"  æœ€å¤§å›æ’¤: {max_drawdown:.2%}")
                report.append(f"  å³°å€¼: {peak:,.0f}")
                
                # æ‰¾å‡ºå¯¼è‡´æœ€å¤§å›æ’¤çš„å€¼
                for i, value in enumerate(portfolio_values):
                    if value > peak:
                        peak = value
                    drawdown = (peak - value) / peak
                    if abs(drawdown - max_drawdown) < 0.001:
                        report.append(f"  æœ€å¤§å›æ’¤å‘ç”Ÿåœ¨ç´¢å¼•{i}: å³°å€¼{peak:,.0f} -> å½“å‰{value:,.0f}")
                        break

        # æ€»ä½“å»ºè®®
        report.append(f"\nğŸ” æ€»ä½“å»ºè®®:")

        if rewards and len(rewards) > 100:
            recent_performance = np.mean(rewards[-100:])
            early_performance = np.mean(rewards[:100])

            if recent_performance > early_performance * 1.1:
                report.append("  âœ… æ¨¡å‹æŒç»­æ”¹è¿›ï¼Œè®­ç»ƒæ•ˆæœè‰¯å¥½")
            elif recent_performance > early_performance * 0.9:
                report.append("  âš ï¸  æ¨¡å‹æ€§èƒ½è¶‹äºç¨³å®šï¼Œå¯è€ƒè™‘è°ƒæ•´æ¢ç´¢ç­–ç•¥")
            else:
                report.append("  âŒ æ¨¡å‹æ€§èƒ½ä¸‹é™ï¼Œå»ºè®®æ£€æŸ¥è¿‡æ‹Ÿåˆæˆ–æ•°æ®æ³„éœ²é—®é¢˜")

        report.append("=" * 80)

        return "\n".join(report)


class DrawdownStoppingCallback(BaseCallback):
    """å¤šå°ºåº¦å›æ’¤æ—©åœå›è°ƒï¼Œæ”¯æŒæ¢¯åº¦é€’å¢è€å¿ƒå’Œæ¨¡å‹å¿«ç…§"""

    def __init__(self, max_drawdown: float = 0.20, base_patience: int = 100, 
                 warmup_steps: int = 50000, verbose: int = 0):
        super().__init__(verbose)
        self.max_drawdown = max_drawdown
        self.base_patience = base_patience
        self.warmup_steps = warmup_steps
        
        # å¤šå°ºåº¦æ£€æµ‹çª—å£
        self.short_window = 20   # çŸ­æœŸæ£€æµ‹
        self.medium_window = 100 # ä¸­æœŸæ£€æµ‹
        self.long_window = 200   # é•¿æœŸæ£€æµ‹
        
        # å¤šå°ºåº¦å†å²è®°å½•
        self.short_history = []
        self.medium_history = []
        self.long_history = []
        
        # è¿è§„è®¡æ•°å™¨
        self.short_violations = 0
        self.medium_violations = 0
        self.long_violations = 0
        
        # æ¢¯åº¦é€’å¢è€å¿ƒæœºåˆ¶
        self.current_patience = base_patience
        self.consecutive_violations = 0
        
        # æœ€ä½³çŠ¶æ€å¿«ç…§
        self.best_portfolio_value = 0
        self.best_model_step = 0

    def _on_step(self) -> bool:
        """å¤šå°ºåº¦å›æ’¤æ£€æµ‹å’Œæ¢¯åº¦é€’å¢æ—©åœ"""
        # é¢„çƒ­æœŸé—´ä¸æ‰§è¡Œæ—©åœ
        if self.num_timesteps < self.warmup_steps:
            return True
            
        # è·å–ç¯å¢ƒä¿¡æ¯
        if not hasattr(self.training_env, 'get_attr'):
            return True
            
        current_drawdowns = self.training_env.get_attr('current_drawdown')
        portfolio_values = self.training_env.get_attr('total_value')
        
        if not current_drawdowns or not portfolio_values:
            return True
            
        # è®¡ç®—å¹³å‡æŒ‡æ ‡
        avg_drawdown = np.mean(current_drawdowns)
        avg_portfolio_value = np.mean(portfolio_values)
        
        # æ›´æ–°æœ€ä½³çŠ¶æ€
        if avg_portfolio_value > self.best_portfolio_value:
            self.best_portfolio_value = avg_portfolio_value
            self.best_model_step = self.num_timesteps
        
        # å¤šå°ºåº¦å†å²æ›´æ–°
        self._update_histories(avg_drawdown)
        
        # å¤šå°ºåº¦æ£€æµ‹
        violation_detected = self._multi_scale_detection()
        
        if violation_detected:
            self.consecutive_violations += 1
            # æ¢¯åº¦é€’å¢è€å¿ƒï¼šè¿ç»­è¿è§„æ—¶æŒ‡æ•°å‡å°‘è€å¿ƒ
            patience_factor = max(0.1, 0.8 ** (self.consecutive_violations // 10))
            current_patience = int(self.base_patience * patience_factor)
            
            if self.consecutive_violations >= current_patience:
                if self.verbose > 0:
                    short_dd = np.mean(self.short_history) if self.short_history else 0
                    medium_dd = np.mean(self.medium_history) if self.medium_history else 0
                    long_dd = np.mean(self.long_history) if self.long_history else 0
                    
                    logger.warning(f"å¤šå°ºåº¦å›æ’¤è¶…é™è§¦å‘æ—©åœ:")
                    logger.warning(f"  çŸ­æœŸå›æ’¤({self.short_window}æ­¥): {short_dd:.2%}")
                    logger.warning(f"  ä¸­æœŸå›æ’¤({self.medium_window}æ­¥): {medium_dd:.2%}")
                    logger.warning(f"  é•¿æœŸå›æ’¤({self.long_window}æ­¥): {long_dd:.2%}")
                    logger.warning(f"  è¿ç»­è¿è§„: {self.consecutive_violations}æ­¥")
                    logger.warning(f"  å½“å‰è€å¿ƒ: {current_patience}")
                
                return False
        else:
            # è¿è§„ç¼“è§£æ—¶é‡ç½®è®¡æ•°å™¨
            self.consecutive_violations = max(0, self.consecutive_violations - 2)
        
        return True
    
    def _update_histories(self, avg_drawdown: float):
        """æ›´æ–°å¤šå°ºåº¦å†å²è®°å½•"""
        self.short_history.append(avg_drawdown)
        self.medium_history.append(avg_drawdown)
        self.long_history.append(avg_drawdown)
        
        # ç»´æŒçª—å£å¤§å°
        if len(self.short_history) > self.short_window:
            self.short_history.pop(0)
        if len(self.medium_history) > self.medium_window:
            self.medium_history.pop(0)
        if len(self.long_history) > self.long_window:
            self.long_history.pop(0)
    
    def _multi_scale_detection(self) -> bool:
        """å¤šå°ºåº¦è¿è§„æ£€æµ‹"""
        violation = False
        
        # çŸ­æœŸæ£€æµ‹ï¼šå¿«é€Ÿå“åº”
        if len(self.short_history) >= 10:
            short_avg = np.mean(self.short_history)
            if short_avg > self.max_drawdown * 1.2:  # çŸ­æœŸé˜ˆå€¼æ›´ä¸¥æ ¼
                self.short_violations += 1
                violation = True
            else:
                self.short_violations = max(0, self.short_violations - 1)
        
        # ä¸­æœŸæ£€æµ‹ï¼šå¹³è¡¡æ£€æµ‹
        if len(self.medium_history) >= 30:
            medium_avg = np.mean(self.medium_history)
            if medium_avg > self.max_drawdown:
                self.medium_violations += 1
                violation = True
            else:
                self.medium_violations = max(0, self.medium_violations - 1)
        
        # é•¿æœŸæ£€æµ‹ï¼šè¶‹åŠ¿ç¡®è®¤
        if len(self.long_history) >= 50:
            long_avg = np.mean(self.long_history)
            if long_avg > self.max_drawdown * 0.8:  # é•¿æœŸé˜ˆå€¼ç¨å¾®å®½æ¾
                self.long_violations += 1
                violation = True
            else:
                self.long_violations = max(0, self.long_violations - 1)
        
        # ç»¼åˆåˆ¤æ–­ï¼šä»»ä¸€å°ºåº¦æŒç»­è¿è§„å³ä¸ºè¿è§„
        return (self.short_violations >= 5 or 
                self.medium_violations >= 10 or 
                self.long_violations >= 20)


class TrainingMetricsCallback(BaseCallback):
    """è®­ç»ƒæŒ‡æ ‡ç›‘æ§å›è°ƒï¼Œå®šæœŸè¾“å‡ºè®­ç»ƒç›¸å…³æŒ‡æ ‡"""

    def __init__(self,
                 log_interval: int = 1000,
                 eval_interval: int = 10000,
                 verbose: int = 1):
        """
        åˆå§‹åŒ–è®­ç»ƒæŒ‡æ ‡å›è°ƒ

        Args:
            log_interval: æŒ‡æ ‡è¾“å‡ºé—´éš”ï¼ˆtimestepsï¼‰
            eval_interval: è¯¦ç»†è¯„ä¼°é—´éš”ï¼ˆtimestepsï¼‰
            verbose: è¯¦ç»†ç¨‹åº¦
        """
        super().__init__(verbose)
        self.log_interval = log_interval
        self.eval_interval = eval_interval

        # æŒ‡æ ‡å†å²è®°å½• - ç»Ÿä¸€çš„portfolio_values_historyç”¨äºä¸€è‡´çš„è´¨é‡æŠ¥å‘Š
        self.episode_rewards = []
        self.episode_lengths = []
        self.portfolio_values_history = []  # ç»Ÿä¸€çš„ç»„åˆä»·å€¼å†å²
        self.drawdowns = []
        self.actions_history = []

        # ç»Ÿè®¡ä¿¡æ¯
        self.last_log_step = 0
        self.last_eval_step = 0
        self.episode_count = 0

        # æ€§èƒ½æŒ‡æ ‡
        self.best_mean_reward = -np.inf
        self.best_portfolio_value = 0
        self.worst_drawdown = 0

        # è´¨é‡åˆ†æå™¨
        self.quality_analyzer = TrainingQualityAnalyzer()
        self.losses_history = []

        logger.info(f"è®­ç»ƒæŒ‡æ ‡ç›‘æ§å·²å¯ç”¨ - æ—¥å¿—é—´éš”: {log_interval}, è¯„ä¼°é—´éš”: {eval_interval}")

    def _on_step(self) -> bool:
        """æ¯æ­¥æ‰§è¡Œçš„ç›‘æ§é€»è¾‘"""
        current_step = self.num_timesteps

        # æ”¶é›†ç¯å¢ƒæŒ‡æ ‡
        self._collect_env_metrics()

        # å®šæœŸè¾“å‡ºåŸºç¡€æŒ‡æ ‡
        if current_step - self.last_log_step >= self.log_interval:
            self._log_basic_metrics(current_step)
            self.last_log_step = current_step

        # å®šæœŸè¿›è¡Œè¯¦ç»†è¯„ä¼°
        if current_step - self.last_eval_step >= self.eval_interval:
            self._log_detailed_metrics(current_step)
            self._log_quality_analysis(current_step)
            self.last_eval_step = current_step

        return True

    def _collect_env_metrics(self):
        """æ”¶é›†ç¯å¢ƒæŒ‡æ ‡ï¼ŒåŒ…æ‹¬CVaRç­‰æ–°é£é™©æŒ‡æ ‡"""
        if not hasattr(self.training_env, 'get_attr'):
            return

        # è·å–åŸºç¡€ç¯å¢ƒçŠ¶æ€
        total_values = self.training_env.get_attr('total_value')
        drawdowns = self.training_env.get_attr('current_drawdown')
        
        # è·å–æ–°å¢çš„é£é™©æŒ‡æ ‡
        cvar_values = self.training_env.get_attr('cvar_value')
        rolling_peaks = self.training_env.get_attr('rolling_peak')

        if len(total_values) > 0:
            # æ”¹è¿›ï¼šåªè®°å½•ç¬¬ä¸€ä¸ªç¯å¢ƒçš„æ•°æ®ï¼Œé¿å…å¤šç¯å¢ƒæ··æ·†
            # è¿™æ ·å¯ä»¥å¾—åˆ°ä¸€ä¸ªè¿ç»­çš„ç»„åˆä»·å€¼åºåˆ—
            self.portfolio_values_history.append(total_values[0])  # åªå–ç¬¬ä¸€ä¸ªç¯å¢ƒ
            self.drawdowns.append(drawdowns[0])  # åªå–ç¬¬ä¸€ä¸ªç¯å¢ƒ

            # åŸºç¡€æŒ‡æ ‡è®°å½•åˆ°TensorBoard
            self.logger.record('env/mean_total_value', np.mean(total_values))
            self.logger.record('env/max_total_value', np.max(total_values))
            self.logger.record('env/mean_drawdown', np.mean(drawdowns))
            self.logger.record('env/max_drawdown', np.max(drawdowns))
            
            # æ–°å¢é£é™©æŒ‡æ ‡
            if cvar_values:
                self.logger.record('risk/mean_cvar', np.mean(cvar_values))
                self.logger.record('risk/max_cvar', np.max(cvar_values))
            
            if rolling_peaks:
                self.logger.record('env/mean_rolling_peak', np.mean(rolling_peaks))
                
            # è®¡ç®—Calmaræ¯”ç‡ï¼ˆå¹´åŒ–æ”¶ç›Š/æœ€å¤§å›æ’¤ï¼‰
            if len(self.portfolio_values_history) > 252:  # è‡³å°‘ä¸€å¹´æ•°æ®
                recent_values = self.portfolio_values_history[-252:]
                annual_return = (recent_values[-1] / recent_values[0]) - 1
                max_dd_period = np.max(self.drawdowns[-252:]) if len(self.drawdowns) >= 252 else np.max(self.drawdowns)
                if max_dd_period > 0:
                    calmar_ratio = annual_return / max_dd_period
                    self.logger.record('performance/calmar_ratio', calmar_ratio)

            # æ›´æ–°æœ€ä½³æŒ‡æ ‡
            current_max_value = np.max(total_values)
            current_worst_drawdown = np.max(drawdowns)

            if current_max_value > self.best_portfolio_value:
                self.best_portfolio_value = current_max_value

            if current_worst_drawdown > self.worst_drawdown:
                self.worst_drawdown = current_worst_drawdown

    def _log_basic_metrics(self, current_step: int):
        """è¾“å‡ºåŸºç¡€è®­ç»ƒæŒ‡æ ‡"""
        if self.verbose < 1:
            return

        # è·å–æœ€è¿‘çš„å¥–åŠ±ä¿¡æ¯
        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
            recent_episodes = list(self.model.ep_info_buffer)[-10:]  # æœ€è¿‘10ä¸ªå›åˆ
            recent_rewards = [ep['r'] for ep in recent_episodes]
            recent_lengths = [ep['l'] for ep in recent_episodes]

            mean_reward = np.mean(recent_rewards)
            mean_length = np.mean(recent_lengths)

            # æ›´æ–°æœ€ä½³å¥–åŠ±
            if mean_reward > self.best_mean_reward:
                self.best_mean_reward = mean_reward

            # è¾“å‡ºåŸºç¡€æŒ‡æ ‡
            logger.info(f"[æ­¥éª¤ {current_step:,}] "
                       f"å¹³å‡å¥–åŠ±: {mean_reward:.4f} | "
                       f"å¹³å‡å›åˆé•¿åº¦: {mean_length:.1f} | "
                       f"æœ€ä½³å¥–åŠ±: {self.best_mean_reward:.4f}")

            # å¦‚æœæœ‰ç»„åˆä»·å€¼æ•°æ®
            if self.portfolio_values_history:
                recent_values = self.portfolio_values_history[-100:]  # æœ€è¿‘100ä¸ªå€¼
                current_value = recent_values[-1] if recent_values else 0
                value_change = ((current_value / recent_values[0]) - 1) * 100 if len(recent_values) > 1 else 0

                logger.info(f"[æ­¥éª¤ {current_step:,}] "
                           f"å½“å‰ç»„åˆä»·å€¼: {current_value:,.0f} | "
                           f"ä»·å€¼å˜åŒ–: {value_change:+.2f}% | "
                           f"æœ€å¤§å›æ’¤: {self.worst_drawdown:.2%}")

    def _log_detailed_metrics(self, current_step: int):
        """è¾“å‡ºè¯¦ç»†è®­ç»ƒæŒ‡æ ‡"""
        if self.verbose < 1:
            return

        logger.info("=" * 80)
        logger.info(f"è¯¦ç»†è®­ç»ƒæŠ¥å‘Š - æ­¥éª¤ {current_step:,}")
        logger.info("-" * 80)

        # æ¨¡å‹å­¦ä¹ ç‡ç­‰å‚æ•°
        if hasattr(self.model, 'learning_rate'):
            current_lr = self.model.learning_rate
            if callable(current_lr):
                current_lr = current_lr(1.0)  # è·å–å½“å‰å­¦ä¹ ç‡
            logger.info(f"å½“å‰å­¦ä¹ ç‡: {current_lr:.2e}")

        # å›åˆç»Ÿè®¡
        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
            episodes = list(self.model.ep_info_buffer)
            if len(episodes) >= 10:
                rewards = [ep['r'] for ep in episodes[-50:]]  # æœ€è¿‘50ä¸ªå›åˆ
                lengths = [ep['l'] for ep in episodes[-50:]]

                logger.info(f"å›åˆç»Ÿè®¡ (æœ€è¿‘50å›åˆ):")
                logger.info(f"  å¹³å‡å¥–åŠ±: {np.mean(rewards):.4f} Â± {np.std(rewards):.4f}")
                logger.info(f"  å¥–åŠ±èŒƒå›´: [{np.min(rewards):.4f}, {np.max(rewards):.4f}]")
                logger.info(f"  å¹³å‡é•¿åº¦: {np.mean(lengths):.1f} Â± {np.std(lengths):.1f}")

        # ç»„åˆæ€§èƒ½ç»Ÿè®¡
        if self.portfolio_values_history:
            recent_values = self.portfolio_values_history[-1000:]  # æœ€è¿‘1000ä¸ªå€¼
            if len(recent_values) > 1:
                initial_value = recent_values[0]
                current_value = recent_values[-1]
                total_return = (current_value / initial_value - 1) * 100

                # è®¡ç®—æ³¢åŠ¨ç‡
                returns = np.diff(recent_values) / recent_values[:-1]
                volatility = np.std(returns) * np.sqrt(252) * 100  # å¹´åŒ–æ³¢åŠ¨ç‡

                logger.info(f"ç»„åˆæ€§èƒ½ç»Ÿè®¡:")
                logger.info(f"  åˆå§‹ä»·å€¼: {initial_value:,.0f}")
                logger.info(f"  å½“å‰ä»·å€¼: {current_value:,.0f}")
                logger.info(f"  æ€»æ”¶ç›Šç‡: {total_return:+.2f}%")
                logger.info(f"  å¹´åŒ–æ³¢åŠ¨ç‡: {volatility:.2f}%")
                logger.info(f"  æœ€å¤§ä»·å€¼: {self.best_portfolio_value:,.0f}")

        # å›æ’¤ç»Ÿè®¡
        if self.drawdowns:
            recent_drawdowns = self.drawdowns[-1000:]
            logger.info(f"å›æ’¤ç»Ÿè®¡:")
            logger.info(f"  å½“å‰å›æ’¤: {recent_drawdowns[-1]:.2%}")
            logger.info(f"  å¹³å‡å›æ’¤: {np.mean(recent_drawdowns):.2%}")
            logger.info(f"  æœ€å¤§å›æ’¤: {np.max(recent_drawdowns):.2%}")

        # è®­ç»ƒç¨³å®šæ€§æŒ‡æ ‡
        if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
            metrics = self.model.logger.name_to_value
            if 'train/loss' in metrics:
                logger.info(f"è®­ç»ƒæŸå¤±: {metrics['train/loss']:.6f}")
            if 'train/policy_gradient_loss' in metrics:
                logger.info(f"ç­–ç•¥æ¢¯åº¦æŸå¤±: {metrics['train/policy_gradient_loss']:.6f}")
            if 'train/value_loss' in metrics:
                logger.info(f"ä»·å€¼å‡½æ•°æŸå¤±: {metrics['train/value_loss']:.6f}")

        logger.info("=" * 80)

    def _log_quality_analysis(self, current_step: int):
        """è¾“å‡ºè®­ç»ƒè´¨é‡åˆ†æ"""
        # æ”¶é›†è®­ç»ƒæŸå¤±
        if hasattr(self.model, 'logger') and hasattr(self.model.logger, 'name_to_value'):
            metrics = self.model.logger.name_to_value
            if 'train/loss' in metrics:
                self.losses_history.append(metrics['train/loss'])

        # æ”¶é›†å¥–åŠ±å†å²
        rewards_history = []
        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
            episodes = list(self.model.ep_info_buffer)
            rewards_history = [ep['r'] for ep in episodes]

        # ç”Ÿæˆè´¨é‡æŠ¥å‘Š
        if len(rewards_history) > 50 and len(self.portfolio_values_history) > 50:
            # è·å–é…ç½®ä¿¡æ¯ï¼ˆéœ€è¦ä»çˆ¶ç±»ä¼ é€’ï¼‰
            config = getattr(self, 'config', {})

            # å°è¯•è·å–ç¯å¢ƒæ•°æ®è¿›è¡Œè°ƒè¯•
            env_data = None
            debug_info = []
            
            debug_info.append(f"training_envç±»å‹: {type(self.training_env)}")
            debug_info.append(f"training_envå±æ€§: {[attr for attr in dir(self.training_env) if not attr.startswith('_')][:15]}")
            
            if hasattr(self.training_env, 'envs') and len(self.training_env.envs) > 0:
                # VecEnvæƒ…å†µ
                env_data = self.training_env.envs[0]
                debug_info.append(f"ä»VecEnvè·å–: {type(env_data)}")
            elif hasattr(self.training_env, 'env'):
                # MonitoråŒ…è£…çš„æƒ…å†µ
                env_data = self.training_env.env
                debug_info.append(f"ä»Monitorè·å–: {type(env_data)}")
            else:
                # ç›´æ¥ç¯å¢ƒ
                env_data = self.training_env
                debug_info.append(f"ç›´æ¥ç¯å¢ƒ: {type(env_data)}")
                
            # è¿›ä¸€æ­¥è§£åŒ…Monitor
            if hasattr(env_data, 'env'):
                env_data = env_data.env
                debug_info.append(f"è¿›ä¸€æ­¥è§£åŒ…: {type(env_data)}")
                
            # å°è¯•é€šè¿‡get_attrè·å–ç¯å¢ƒæ•°æ®
            if env_data is None and hasattr(self.training_env, 'get_attr'):
                env_attrs = self.training_env.get_attr('data')
                if len(env_attrs) > 0:
                    # åˆ›å»ºä¸€ä¸ªä¸´æ—¶å¯¹è±¡æ¥å­˜å‚¨æ•°æ®
                    class TempEnv:
                        def __init__(self):
                            self.data = env_attrs[0]
                            self.time_index = self.training_env.get_attr('time_index')[0]
                            self.stock_list = self.training_env.get_attr('stock_list')[0]
                    env_data = TempEnv()
                    debug_info.append(f"é€šè¿‡get_attråˆ›å»ºä¸´æ—¶ç¯å¢ƒ")

            # å°†è°ƒè¯•ä¿¡æ¯åŠ å…¥æŠ¥å‘Š
            for info in debug_info:
                logger.info(f"ç¯å¢ƒè°ƒè¯•: {info}")

            quality_report = self.quality_analyzer.generate_quality_report(
                rewards=rewards_history,
                losses=self.losses_history,
                config=config,
                portfolio_values=self.portfolio_values_history,  # ä½¿ç”¨ç»Ÿä¸€å†å²è®°å½•
                env_data=env_data
            )

            logger.info("\n" + quality_report)

            # ä¿å­˜è´¨é‡æŠ¥å‘Šåˆ°æ–‡ä»¶
            report_path = f"logs/quality_report_{current_step}.txt"
            with open(report_path, 'w', encoding='utf-8') as f:
                f.write(quality_report)

    def set_config(self, config: Dict[str, Any]):
        """è®¾ç½®é…ç½®ä¿¡æ¯ä¾›è´¨é‡åˆ†æä½¿ç”¨"""
        self.config = config

    def _on_training_end(self) -> None:
        """è®­ç»ƒç»“æŸæ—¶çš„æ€»ç»“"""
        logger.info("=" * 80)
        logger.info("è®­ç»ƒå®Œæˆ - æœ€ç»ˆç»Ÿè®¡")
        logger.info("-" * 80)

        if hasattr(self.model, 'ep_info_buffer') and len(self.model.ep_info_buffer) > 0:
            all_episodes = list(self.model.ep_info_buffer)
            all_rewards = [ep['r'] for ep in all_episodes]
            all_lengths = [ep['l'] for ep in all_episodes]

            logger.info(f"æ€»å›åˆæ•°: {len(all_episodes)}")
            logger.info(f"å¹³å‡å¥–åŠ±: {np.mean(all_rewards):.4f}")
            logger.info(f"æœ€ä½³å¥–åŠ±: {np.max(all_rewards):.4f}")
            logger.info(f"æœ€å·®å¥–åŠ±: {np.min(all_rewards):.4f}")
            logger.info(f"å¥–åŠ±æ ‡å‡†å·®: {np.std(all_rewards):.4f}")

        if self.portfolio_values_history:
            logger.info(f"æœ€ç»ˆç»„åˆä»·å€¼: {self.portfolio_values_history[-1]:,.0f}")
            logger.info(f"æœ€ä½³ç»„åˆä»·å€¼: {self.best_portfolio_value:,.0f}")

        if self.drawdowns:
            logger.info(f"æœ€å¤§å›æ’¤: {self.worst_drawdown:.2%}")

        logger.info("=" * 80)


class TensorBoardCallback(BaseCallback):
    """è‡ªå®šä¹‰TensorBoardå›è°ƒï¼Œè®°å½•é¢å¤–æŒ‡æ ‡"""

    def __init__(self, verbose: int = 0):
        super().__init__(verbose)

    def _on_step(self) -> bool:
        """è®°å½•è‡ªå®šä¹‰æŒ‡æ ‡"""
        if hasattr(self.training_env, 'get_attr'):
            # è·å–ç¯å¢ƒæŒ‡æ ‡
            total_values = self.training_env.get_attr('total_value')
            drawdowns = self.training_env.get_attr('current_drawdown')

            # è®°å½•åˆ°TensorBoard
            if len(total_values) > 0:
                self.logger.record('env/mean_total_value', np.mean(total_values))
                self.logger.record('env/max_total_value', np.max(total_values))
                self.logger.record('env/mean_drawdown', np.mean(drawdowns))
                self.logger.record('env/max_drawdown', np.max(drawdowns))

        return True


class RLTrainer:
    """å¼ºåŒ–å­¦ä¹ è®­ç»ƒå™¨"""

    def __init__(self, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–è®­ç»ƒå™¨

        Args:
            config: è®­ç»ƒé…ç½®å­—å…¸
        """
        self.config = config
        self.setup_logging()
        self.setup_directories()

        # åˆå§‹åŒ–ç»„ä»¶
        self.data_loader = None
        self.train_env = None
        self.eval_env = None
        self.model = None

        # è®­ç»ƒçŠ¶æ€
        self.training_start_time = None
        self.best_reward = -np.inf
        self.training_history = []

    def setup_logging(self):
        """è®¾ç½®æ—¥å¿—"""
        log_level = getattr(logging, self.config.get('log_level', 'INFO').upper())
        logging.basicConfig(
            level=log_level,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler(f"logs/training_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"),
                logging.StreamHandler()
            ]
        )

    def setup_directories(self):
        """åˆ›å»ºå¿…è¦ç›®å½•"""
        dirs = ['models', 'logs', 'tensorboard', 'results']
        for dir_name in dirs:
            os.makedirs(dir_name, exist_ok=True)

    def initialize_data(self) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨å¹¶è·å–è®­ç»ƒæ•°æ®

        Returns:
            è®­ç»ƒé›†ã€éªŒè¯é›†ã€æµ‹è¯•é›†
        """
        logger.info("åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨...")

        data_config = self.config['data']
        self.data_loader = QlibDataLoader(data_config.get('data_root'))

        # åˆå§‹åŒ–Qlib
        provider_uri = data_config.get('provider_uri')
        self.data_loader.initialize_qlib(provider_uri)

        # è·å–è‚¡ç¥¨åˆ—è¡¨
        market = data_config.get('market', 'csi300')
        stock_limit = data_config.get('stock_limit', 50)
        custom_stocks = data_config.get('custom_stocks', None)
        stock_list = self.data_loader.get_stock_list(market, stock_limit, custom_stocks)

        logger.info(f"è·å–{len(stock_list)}åªè‚¡ç¥¨ç”¨äºè®­ç»ƒ")

        # åŠ è½½æ•°æ®
        data = self.data_loader.load_data(
            instruments=stock_list,
            start_time=data_config['start_time'],
            end_time=data_config['end_time'],
            freq=data_config.get('freq', 'day'),
            fields=data_config.get('fields')
        )

        # åˆ†å‰²æ•°æ®
        train_data, valid_data, test_data = split_data(
            data,
            data_config['train_start'], data_config['train_end'],
            data_config['valid_start'], data_config['valid_end'],
            data_config['test_start'], data_config['test_end']
        )

        logger.info(f"æ•°æ®åˆ†å‰²å®Œæˆ - è®­ç»ƒ: {train_data.shape}, éªŒè¯: {valid_data.shape}, æµ‹è¯•: {test_data.shape}")

        return train_data, valid_data, test_data

    def create_environments(self, train_data: pd.DataFrame, valid_data: pd.DataFrame) -> None:
        """
        åˆ›å»ºè®­ç»ƒå’Œè¯„ä¼°ç¯å¢ƒ

        Args:
            train_data: è®­ç»ƒæ•°æ®
            valid_data: éªŒè¯æ•°æ®
        """
        logger.info("åˆ›å»ºè®­ç»ƒå’Œè¯„ä¼°ç¯å¢ƒ...")

        env_config = self.config['environment']

        def make_env(data: pd.DataFrame, rank: int = 0) -> gym.Env:
            """åˆ›å»ºç¯å¢ƒå·¥å‚å‡½æ•°"""
            def _init():
                env = PortfolioEnv(
                    data=data,
                    initial_cash=env_config.get('initial_cash', 1000000),
                    lookback_window=env_config.get('lookback_window', 30),
                    transaction_cost=env_config.get('transaction_cost', 0.003),
                    max_drawdown_threshold=env_config.get('max_drawdown_threshold', 0.15),
                    reward_penalty=env_config.get('reward_penalty', 2.0),
                    features=env_config.get('features'),
                    max_steps=env_config.get('max_steps')
                )

                # æ·»åŠ MonitoråŒ…è£…å™¨ç”¨äºè®°å½•
                log_dir = f"logs/env_logs/rank_{rank}"
                os.makedirs(log_dir, exist_ok=True)
                env = Monitor(env, log_dir)

                # è®¾ç½®éšæœºç§å­
                env.reset(seed=rank)
                return env

            return _init

        # åˆ›å»ºè®­ç»ƒç¯å¢ƒ
        train_config = self.config['training']
        n_envs = train_config.get('n_envs', 4)

        if n_envs > 1:
            self.train_env = SubprocVecEnv([
                make_env(train_data, i) for i in range(n_envs)
            ])
        else:
            self.train_env = DummyVecEnv([make_env(train_data, 0)])

        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        self.eval_env = DummyVecEnv([make_env(valid_data, 999)])

        logger.info(f"ç¯å¢ƒåˆ›å»ºå®Œæˆ - è®­ç»ƒç¯å¢ƒæ•°é‡: {n_envs}")

    def create_model(self) -> None:
        """åˆ›å»ºå¼ºåŒ–å­¦ä¹ æ¨¡å‹"""
        logger.info("åˆ›å»ºå¼ºåŒ–å­¦ä¹ æ¨¡å‹...")

        model_config = self.config['model']
        algorithm = model_config.get('algorithm', 'SAC').upper()

        # è·å–ç¯å¢ƒä¿¡æ¯
        sample_env = self.train_env.envs[0] if hasattr(self.train_env, 'envs') else self.train_env.unwrapped

        # æ¨¡å‹å‚æ•° - ç§»é™¤GPUå­¦ä¹ ç‡æ”¾å¤§é€»è¾‘
        learning_rate = model_config.get('learning_rate', 3e-4)
        logger.info(f"ä½¿ç”¨å­¦ä¹ ç‡: {learning_rate}")

        model_kwargs = {
            'learning_rate': learning_rate,  # ä¸å†äººä¸ºæ”¾å¤§å­¦ä¹ ç‡
            'batch_size': model_config.get('batch_size', 256),
            'gamma': model_config.get('gamma', 0.99),
            'verbose': 1,
            'tensorboard_log': "tensorboard/",
            'device': 'cuda' if torch.cuda.is_available() else 'cpu'
        }

        # ç­–ç•¥å‚æ•°
        net_arch = model_config.get('net_arch', [256, 256])

        # å¦‚æœä½¿ç”¨è‡ªå®šä¹‰ç­–ç•¥
        if model_config.get('use_custom_policy', False):
            env_config = self.config['environment']
            policy_kwargs = {
                'lookback_window': env_config.get('lookback_window', 30),
                'num_stocks': len(sample_env.stock_list) if hasattr(sample_env, 'stock_list') else 50,
                'num_features': len(env_config.get('features', [])) if env_config.get('features') else 5
            }
            policy = TradingPolicy
        else:
            # æ ‡å‡†ç­–ç•¥éœ€è¦æ­£ç¡®çš„net_archæ ¼å¼
            if algorithm == 'PPO' and isinstance(net_arch, dict):
                policy_kwargs = {'net_arch': net_arch}
            else:
                policy_kwargs = {'net_arch': net_arch if isinstance(net_arch, list) else [256, 256]}
            policy = 'MlpPolicy'

        # åˆ›å»ºæ¨¡å‹
        if algorithm == 'SAC':
            model_kwargs.update({
                'buffer_size': model_config.get('buffer_size', 1000000),
                'tau': model_config.get('tau', 0.005),
                'target_update_interval': model_config.get('target_update_interval', 1),
                'learning_starts': model_config.get('learning_starts', 100),
                'ent_coef': model_config.get('ent_coef', 'auto')  # æ·»åŠ ç†µç³»æ•°æ”¯æŒ
            })
            self.model = SAC(policy, self.train_env, policy_kwargs=policy_kwargs, **model_kwargs)

        elif algorithm == 'PPO':
            model_kwargs.update({
                'n_steps': model_config.get('n_steps', 2048),
                'n_epochs': model_config.get('n_epochs', 10),
                'clip_range': model_config.get('clip_range', 0.2),
                'ent_coef': model_config.get('ent_coef', 0.0)
            })
            self.model = PPO(policy, self.train_env, policy_kwargs=policy_kwargs, **model_kwargs)

        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")

        logger.info(f"æ¨¡å‹åˆ›å»ºå®Œæˆ - ç®—æ³•: {algorithm}, è®¾å¤‡: {model_kwargs['device']}")

    def setup_callbacks(self) -> List[BaseCallback]:
        """è®¾ç½®è®­ç»ƒå›è°ƒ"""
        callbacks = []

        callback_config = self.config.get('callbacks', {})

        # è®­ç»ƒæŒ‡æ ‡ç›‘æ§å›è°ƒ - æ”¾åœ¨æœ€å‰é¢ä»¥ç¡®ä¿åŠæ—¶ç›‘æ§
        if callback_config.get('enable_training_metrics', True):
            metrics_callback = TrainingMetricsCallback(
                log_interval=callback_config.get('metrics_log_interval', 1000),
                eval_interval=callback_config.get('metrics_eval_interval', 10000),
                verbose=1
            )
            # ä¼ é€’é…ç½®ä¿¡æ¯ç»™å›è°ƒ
            metrics_callback.set_config(self.config)
            callbacks.append(metrics_callback)

        # è¯„ä¼°å›è°ƒ
        if callback_config.get('enable_eval', True):
            eval_callback = EvalCallback(
                self.eval_env,
                best_model_save_path=f"models/best_model",
                log_path="logs/eval_logs",
                eval_freq=callback_config.get('eval_freq', 10000),
                n_eval_episodes=callback_config.get('n_eval_episodes', 5),
                deterministic=True,
                render=False,
                verbose=1
            )
            callbacks.append(eval_callback)

        # æ£€æŸ¥ç‚¹å›è°ƒ
        if callback_config.get('enable_checkpoint', True):
            checkpoint_callback = CheckpointCallback(
                save_freq=callback_config.get('save_freq', 50000),
                save_path="models/checkpoints",
                name_prefix="rl_model"
            )
            callbacks.append(checkpoint_callback)

        # å›æ’¤æ—©åœå›è°ƒ
        if callback_config.get('enable_drawdown_stopping', True):
            drawdown_callback = DrawdownStoppingCallback(
                max_drawdown=callback_config.get('max_training_drawdown', 0.20),
                base_patience=callback_config.get('drawdown_base_patience', 100),
                warmup_steps=callback_config.get('drawdown_warmup_steps', 50000),
                verbose=1
            )
            callbacks.append(drawdown_callback)

        # TensorBoardå›è°ƒ
        if callback_config.get('enable_tensorboard', True):
            tb_callback = TensorBoardCallback(verbose=1)
            callbacks.append(tb_callback)

        return callbacks

    def train(self) -> None:
        """æ‰§è¡Œè®­ç»ƒ"""
        logger.info("å¼€å§‹è®­ç»ƒ...")
        self.training_start_time = datetime.now()

        # è®­ç»ƒé…ç½®
        train_config = self.config['training']
        total_timesteps = train_config.get('total_timesteps', 1000000)

        # è®¾ç½®éšæœºç§å­
        if 'seed' in train_config:
            set_random_seed(train_config['seed'])

        # è®¾ç½®å›è°ƒ
        callbacks = self.setup_callbacks()

        try:
            # å¼€å§‹è®­ç»ƒ
            self.model.learn(
                total_timesteps=total_timesteps,
                callback=callbacks,
                log_interval=train_config.get('log_interval', 10),
                progress_bar=True
            )

            # ä¿å­˜æœ€ç»ˆæ¨¡å‹
            final_model_path = f"models/final_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.model.save(final_model_path)
            logger.info(f"æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_model_path}")

        except KeyboardInterrupt:
            logger.info("è®­ç»ƒè¢«ç”¨æˆ·ä¸­æ–­")
            # ä¿å­˜å½“å‰æ¨¡å‹
            interrupt_model_path = f"models/interrupted_model_{datetime.now().strftime('%Y%m%d_%H%M%S')}"
            self.model.save(interrupt_model_path)
            logger.info(f"ä¸­æ–­æ¨¡å‹å·²ä¿å­˜: {interrupt_model_path}")

        except Exception as e:
            logger.error(f"è®­ç»ƒè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
            raise RuntimeError(f"è®­ç»ƒå¤±è´¥: {e}")

        training_time = datetime.now() - self.training_start_time
        logger.info(f"è®­ç»ƒå®Œæˆï¼Œè€—æ—¶: {training_time}")

    def evaluate(self, model_path: str = None, test_data: pd.DataFrame = None) -> Dict[str, Any]:
        """
        è¯„ä¼°æ¨¡å‹æ€§èƒ½

        Args:
            model_path: æ¨¡å‹è·¯å¾„ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨å½“å‰æ¨¡å‹
            test_data: æµ‹è¯•æ•°æ®ï¼Œå¦‚æœNoneåˆ™ä½¿ç”¨éªŒè¯ç¯å¢ƒ

        Returns:
            è¯„ä¼°ç»“æœå­—å…¸
        """
        logger.info("å¼€å§‹æ¨¡å‹è¯„ä¼°...")

        # åŠ è½½æ¨¡å‹
        if model_path:
            model_config = self.config['model']
            algorithm = model_config.get('algorithm', 'SAC').upper()

            if algorithm == 'SAC':
                eval_model = SAC.load(model_path)
            elif algorithm == 'PPO':
                eval_model = PPO.load(model_path)
            else:
                raise ValueError(f"ä¸æ”¯æŒçš„ç®—æ³•: {algorithm}")
        else:
            eval_model = self.model

        # åˆ›å»ºè¯„ä¼°ç¯å¢ƒ
        if test_data is not None:
            env_config = self.config['environment']
            eval_env = PortfolioEnv(
                data=test_data,
                initial_cash=env_config.get('initial_cash', 1000000),
                lookback_window=env_config.get('lookback_window', 30),
                transaction_cost=env_config.get('transaction_cost', 0.003),
                max_drawdown_threshold=env_config.get('max_drawdown_threshold', 0.15),
                reward_penalty=env_config.get('reward_penalty', 2.0),
                features=env_config.get('features'),
                max_steps=env_config.get('max_steps')
            )
        else:
            eval_env = self.eval_env.envs[0]

        # æ‰§è¡Œè¯„ä¼°
        n_eval_episodes = self.config.get('evaluation', {}).get('n_episodes', 5)
        episode_rewards = []
        episode_lengths = []

        for episode in range(n_eval_episodes):
            obs, _ = eval_env.reset()
            episode_reward = 0
            episode_length = 0

            while True:
                action, _ = eval_model.predict(obs, deterministic=True)
                obs, reward, terminated, truncated, info = eval_env.step(action)
                episode_reward += reward
                episode_length += 1

                if terminated or truncated:
                    break

            episode_rewards.append(episode_reward)
            episode_lengths.append(episode_length)

            logger.info(f"è¯„ä¼°å›åˆ {episode + 1}: å¥–åŠ± = {episode_reward:.4f}, é•¿åº¦ = {episode_length}")

        # è·å–ç»„åˆæ€§èƒ½
        portfolio_performance = eval_env.get_portfolio_performance()

        # æ•´ç†è¯„ä¼°ç»“æœ
        evaluation_results = {
            'episode_rewards': episode_rewards,
            'episode_lengths': episode_lengths,
            'mean_reward': np.mean(episode_rewards),
            'std_reward': np.std(episode_rewards),
            'mean_length': np.mean(episode_lengths),
            'portfolio_performance': portfolio_performance
        }

        # ä¿å­˜è¯„ä¼°ç»“æœ
        results_path = f"results/evaluation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
        with open(results_path, 'w', encoding='utf-8') as f:
            yaml.dump(evaluation_results, f, allow_unicode=True, default_flow_style=False)

        logger.info(f"è¯„ä¼°å®Œæˆï¼Œç»“æœå·²ä¿å­˜: {results_path}")

        return evaluation_results

    def run_full_pipeline(self) -> Dict[str, Any]:
        """
        è¿è¡Œå®Œæ•´çš„è®­ç»ƒ-è¯„ä¼°æµæ°´çº¿

        Returns:
            è®­ç»ƒå’Œè¯„ä¼°ç»“æœ
        """
        logger.info("å¼€å§‹è¿è¡Œå®Œæ•´è®­ç»ƒæµæ°´çº¿...")

        try:
            # 1. åˆå§‹åŒ–æ•°æ®
            train_data, valid_data, test_data = self.initialize_data()

            # 2. åˆ›å»ºç¯å¢ƒ
            self.create_environments(train_data, valid_data)

            # 3. åˆ›å»ºæ¨¡å‹
            self.create_model()

            # 4. æ‰§è¡Œè®­ç»ƒ
            self.train()

            # 5. è¯„ä¼°æ¨¡å‹
            evaluation_results = self.evaluate(test_data=test_data)

            # 6. ç”Ÿæˆç»¼åˆæŠ¥å‘Š
            pipeline_results = {
                'config': self.config,
                'training_time': str(datetime.now() - self.training_start_time),
                'evaluation_results': evaluation_results,
                'data_info': {
                    'train_shape': train_data.shape,
                    'valid_shape': valid_data.shape,
                    'test_shape': test_data.shape
                }
            }

            # ä¿å­˜å®Œæ•´ç»“æœ
            results_path = f"results/pipeline_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.yaml"
            with open(results_path, 'w', encoding='utf-8') as f:
                yaml.dump(pipeline_results, f, allow_unicode=True, default_flow_style=False)

            logger.info(f"å®Œæ•´æµæ°´çº¿æ‰§è¡Œå®Œæˆï¼Œç»“æœå·²ä¿å­˜: {results_path}")

            return pipeline_results

        except Exception as e:
            logger.error(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")
            raise RuntimeError(f"æµæ°´çº¿æ‰§è¡Œå¤±è´¥: {e}")

        finally:
            # æ¸…ç†èµ„æº
            if self.train_env:
                self.train_env.close()
            if self.eval_env:
                self.eval_env.close()


def load_config(config_path: str) -> Dict[str, Any]:
    """
    åŠ è½½é…ç½®æ–‡ä»¶

    Args:
        config_path: é…ç½®æ–‡ä»¶è·¯å¾„

    Returns:
        é…ç½®å­—å…¸
    """
    try:
        with open(config_path, 'r', encoding='utf-8') as f:
            config = yaml.safe_load(f)
        return config
    except Exception as e:
        raise RuntimeError(f"åŠ è½½é…ç½®æ–‡ä»¶å¤±è´¥ {config_path}: {e}")
