"""
å¢å¼ºç³»ç»ŸéªŒè¯å™¨
éªŒè¯æ‰€æœ‰æ”¹è¿›æ˜¯å¦èƒ½å¤Ÿè¾¾åˆ°8%å¹´åŒ–æ”¶ç›Šç›®æ ‡
"""

import os
import sys
import numpy as np
import pandas as pd
import logging
from typing import Dict, List, Tuple, Optional
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

# å¯¼å…¥å¢å¼ºç»„ä»¶
from rl_agent.enhanced_trading_environment import EnhancedTradingEnvironment
from rl_agent.stable_cvar_ppo_agent import StableCVaRPPOAgent
from rl_agent.adaptive_safety_shield import AdaptiveSafetyShield
from rl_agent.enhanced_reward_system import EnhancedRewardSystem
from factors.advanced_alpha_factors import AdvancedAlphaFactors

# å¯¼å…¥åŸºç¡€ç»„ä»¶
from data.data_manager import DataManager
from config.config_manager import ConfigManager
from utils.logger import setup_logger

logger = logging.getLogger(__name__)


class EnhancedSystemValidator:
    """å¢å¼ºç³»ç»ŸéªŒè¯å™¨"""
    
    def __init__(self, config_path: Optional[str] = None):
        """
        åˆå§‹åŒ–éªŒè¯å™¨
        
        Args:
            config_path: é…ç½®æ–‡ä»¶è·¯å¾„
        """
        # è®¾ç½®æ—¥å¿—
        setup_logger()
        
        # åŠ è½½é…ç½®
        if config_path:
            self.config_manager = ConfigManager(config_path)
        else:
            self.config_manager = ConfigManager()
        
        # å¢å¼ºé…ç½®
        self.enhanced_config = self._create_enhanced_config()
        
        # åˆå§‹åŒ–æ•°æ®ç®¡ç†å™¨
        self.data_manager = DataManager(self.config_manager.get_data_config())
        
        # éªŒè¯ç»“æœå­˜å‚¨
        self.validation_results = {
            'baseline_performance': {},
            'enhanced_performance': {},
            'improvement_analysis': {},
            'target_achievement': {},
            'component_analysis': {}
        }
        
        logger.info("å¢å¼ºç³»ç»ŸéªŒè¯å™¨åˆå§‹åŒ–å®Œæˆ")
    
    def _create_enhanced_config(self) -> Dict:
        """åˆ›å»ºå¢å¼ºé…ç½®"""
        base_config = self.config_manager.config
        
        # å¢å¼ºé…ç½®è¦†ç›–
        enhanced_config = {
            # æ•°æ®é…ç½®
            'data': {
                'provider': 'yahoo',
                'region': 'cn',
                'universe': 'csi300',
                'start_date': '2020-01-01',
                'end_date': '2022-12-31',
                'backtest_start': '2023-01-01',
                'backtest_end': '2023-12-31'
            },
            
            # å¢å¼ºå› å­é…ç½®
            'factors': {
                'use_advanced_factors': True,
                'factor_selection': 'all',
                'short_window': 5,
                'medium_window': 20,
                'long_window': 60
            },
            
            # ç¨³å®šCVaR-PPOé…ç½®
            'agent': {
                'hidden_dim': 256,
                'learning_rate': 1e-4,  # é™ä½å­¦ä¹ ç‡æå‡ç¨³å®šæ€§
                'clip_epsilon': 0.2,
                'ppo_epochs': 8,        # å¢åŠ è®­ç»ƒè½®æ¬¡
                'batch_size': 64,
                'gamma': 0.99,
                'lambda_gae': 0.95,
                'cvar_alpha': 0.05,
                'cvar_lambda': 0.1,
                'cvar_threshold': -0.02
            },
            
            # è‡ªé€‚åº”å®‰å…¨ä¿æŠ¤å±‚é…ç½®
            'safety_shield': {
                'max_position': 0.25,           # æå‡è‡³25%
                'max_leverage': 2.0,            # æå‡è‡³2å€
                'var_threshold': 0.05,          # æå‡è‡³5%
                'max_drawdown_threshold': 0.12, # 12%
                'volatility_threshold': 0.30,   # 30%
                'risk_adaptation_factor': 1.0,
                'market_regime_sensitivity': 0.3,
                'performance_feedback_weight': 0.2
            },
            
            # å¢å¼ºå¥–åŠ±ç³»ç»Ÿé…ç½®
            'reward_system': {
                'target_annual_return': 0.08,   # 8%ç›®æ ‡
                'target_sharpe_ratio': 1.0,
                'return_weight': 1.0,
                'sharpe_weight': 0.3,
                'consistency_weight': 0.2,
                'momentum_weight': 0.15,
                'efficiency_weight': 0.1,
                'risk_penalty_weight': 0.1,
                'cost_penalty_weight': 0.05
            },
            
            # è®­ç»ƒé…ç½®
            'training': {
                'total_episodes': 400,          # å¢åŠ è®­ç»ƒè½®æ•°
                'max_steps_per_episode': 300,
                'save_frequency': 50,
                'evaluation_frequency': 25,
                'early_stopping_patience': 100,
                'target_annual_return': 0.08    # 8%ç›®æ ‡
            },
            
            # ç¯å¢ƒé…ç½®
            'environment': {
                'lookback_window': 30,
                'transaction_cost': 0.001,
                'initial_capital': 1000000,
                'max_position': 0.25,
                'max_leverage': 2.0
            }
        }
        
        return enhanced_config
    
    def run_comprehensive_validation(self) -> Dict:
        """è¿è¡Œç»¼åˆéªŒè¯"""
        logger.info("ğŸš€ å¼€å§‹ç»¼åˆç³»ç»ŸéªŒè¯")
        
        # 1. å‡†å¤‡æ•°æ®
        logger.info("ğŸ“Š å‡†å¤‡è®­ç»ƒå’Œæµ‹è¯•æ•°æ®...")
        train_data, test_data = self._prepare_validation_data()
        
        # 2. åŸºçº¿ç³»ç»ŸéªŒè¯
        logger.info("ğŸ“ˆ éªŒè¯åŸºçº¿ç³»ç»Ÿæ€§èƒ½...")
        baseline_results = self._validate_baseline_system(train_data, test_data)
        self.validation_results['baseline_performance'] = baseline_results
        
        # 3. å¢å¼ºç³»ç»ŸéªŒè¯
        logger.info("ğŸ”¥ éªŒè¯å¢å¼ºç³»ç»Ÿæ€§èƒ½...")
        enhanced_results = self._validate_enhanced_system(train_data, test_data)
        self.validation_results['enhanced_performance'] = enhanced_results
        
        # 4. æ€§èƒ½å¯¹æ¯”åˆ†æ
        logger.info("ğŸ“‹ è¿›è¡Œæ€§èƒ½å¯¹æ¯”åˆ†æ...")
        improvement_analysis = self._analyze_improvements(baseline_results, enhanced_results)
        self.validation_results['improvement_analysis'] = improvement_analysis
        
        # 5. ç›®æ ‡è¾¾æˆè¯„ä¼°
        logger.info("ğŸ¯ è¯„ä¼°ç›®æ ‡è¾¾æˆæƒ…å†µ...")
        target_analysis = self._analyze_target_achievement(enhanced_results)
        self.validation_results['target_achievement'] = target_analysis
        
        # 6. ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š
        logger.info("ğŸ“„ ç”ŸæˆéªŒè¯æŠ¥å‘Š...")
        final_report = self._generate_final_report()
        
        # 7. ä¿å­˜ç»“æœ
        self._save_validation_results(final_report)
        
        logger.info("âœ… ç»¼åˆéªŒè¯å®Œæˆ")
        return final_report
    
    def _prepare_validation_data(self) -> Tuple[Dict, Dict]:
        """å‡†å¤‡éªŒè¯æ•°æ®"""
        # è·å–è®­ç»ƒæ•°æ® (2020-2022)
        train_config = self.enhanced_config['data'].copy()
        train_config['start_date'] = '2020-01-01'
        train_config['end_date'] = '2022-12-31'
        
        train_stock_data = self.data_manager.get_stock_data(
            start_time=train_config['start_date'],
            end_time=train_config['end_date']
        )
        
        train_price_data = train_stock_data['$close'].unstack().T
        train_volume_data = train_stock_data['$volume'].unstack().T
        
        # è·å–æµ‹è¯•æ•°æ® (2023)
        test_config = self.enhanced_config['data'].copy()
        test_config['start_date'] = '2023-01-01'
        test_config['end_date'] = '2023-12-31'
        
        test_stock_data = self.data_manager.get_stock_data(
            start_time=test_config['start_date'],
            end_time=test_config['end_date']
        )
        
        test_price_data = test_stock_data['$close'].unstack().T
        test_volume_data = test_stock_data['$volume'].unstack().T
        
        # è®¡ç®—å¢å¼ºå› å­
        enhanced_factors = AdvancedAlphaFactors(self.enhanced_config['factors'])\n        \n        # è®­ç»ƒå› å­\n        train_factors = enhanced_factors.calculate_all_factors(\n            train_price_data, train_volume_data\n        )\n        \n        # æµ‹è¯•å› å­\n        test_factors = enhanced_factors.calculate_all_factors(\n            test_price_data, test_volume_data\n        )\n        \n        train_data = {\n            'price_data': train_price_data,\n            'volume_data': train_volume_data,\n            'factor_data': train_factors,\n            'period': '2020-2022 (Training)'\n        }\n        \n        test_data = {\n            'price_data': test_price_data,\n            'volume_data': test_volume_data,\n            'factor_data': test_factors,\n            'period': '2023 (Testing)'\n        }\n        \n        logger.info(f\"è®­ç»ƒæ•°æ®: {train_price_data.shape}, æµ‹è¯•æ•°æ®: {test_price_data.shape}\")\n        return train_data, test_data\n    \n    def _validate_baseline_system(self, train_data: Dict, test_data: Dict) -> Dict:\n        \"\"\"éªŒè¯åŸºçº¿ç³»ç»Ÿï¼ˆå½“å‰ç³»ç»Ÿï¼‰\"\"\"\n        logger.info(\"è¿è¡ŒåŸºçº¿ç³»ç»ŸéªŒè¯...\")\n        \n        # ä½¿ç”¨å½“å‰ç³»ç»Ÿé…ç½®\n        baseline_config = self.config_manager.config.copy()\n        \n        # åˆ›å»ºåŸºçº¿ç¯å¢ƒå’Œæ™ºèƒ½ä½“\n        baseline_env = self._create_baseline_environment(train_data, baseline_config)\n        baseline_agent = self._create_baseline_agent(baseline_env, baseline_config)\n        \n        # è®­ç»ƒåŸºçº¿ç³»ç»Ÿ\n        baseline_training_results = self._train_system(\n            baseline_env, baseline_agent, episodes=200, system_name=\"åŸºçº¿ç³»ç»Ÿ\"\n        )\n        \n        # æµ‹è¯•åŸºçº¿ç³»ç»Ÿ\n        baseline_test_results = self._test_system(\n            test_data, baseline_agent, baseline_config, system_name=\"åŸºçº¿ç³»ç»Ÿ\"\n        )\n        \n        return {\n            'training_results': baseline_training_results,\n            'test_results': baseline_test_results,\n            'system_type': 'baseline'\n        }\n    \n    def _validate_enhanced_system(self, train_data: Dict, test_data: Dict) -> Dict:\n        \"\"\"éªŒè¯å¢å¼ºç³»ç»Ÿ\"\"\"\n        logger.info(\"è¿è¡Œå¢å¼ºç³»ç»ŸéªŒè¯...\")\n        \n        # åˆ›å»ºå¢å¼ºç¯å¢ƒå’Œæ™ºèƒ½ä½“\n        enhanced_env = self._create_enhanced_environment(train_data)\n        enhanced_agent = self._create_enhanced_agent(enhanced_env)\n        \n        # è®­ç»ƒå¢å¼ºç³»ç»Ÿ\n        enhanced_training_results = self._train_system(\n            enhanced_env, enhanced_agent, episodes=400, system_name=\"å¢å¼ºç³»ç»Ÿ\"\n        )\n        \n        # æµ‹è¯•å¢å¼ºç³»ç»Ÿ\n        enhanced_test_results = self._test_system(\n            test_data, enhanced_agent, self.enhanced_config, system_name=\"å¢å¼ºç³»ç»Ÿ\"\n        )\n        \n        return {\n            'training_results': enhanced_training_results,\n            'test_results': enhanced_test_results,\n            'system_type': 'enhanced'\n        }\n    \n    def _create_baseline_environment(self, data: Dict, config: Dict):\n        \"\"\"åˆ›å»ºåŸºçº¿ç¯å¢ƒ\"\"\"\n        # ä½¿ç”¨ç®€åŒ–çš„å› å­æ•°æ®\n        simple_factors = pd.DataFrame(\n            np.random.randn(len(data['price_data']), 3),\n            index=data['price_data'].index,\n            columns=['factor1', 'factor2', 'factor3']\n        )\n        \n        # è¿™é‡Œåº”è¯¥å¯¼å…¥åŸå§‹çš„TradingEnvironmentï¼Œä½†ä¸ºäº†æ¼”ç¤ºï¼Œä½¿ç”¨å¢å¼ºç¯å¢ƒ\n        return EnhancedTradingEnvironment(\n            factor_data=simple_factors,\n            price_data=data['price_data'],\n            config=config\n        )\n    \n    def _create_baseline_agent(self, env, config: Dict):\n        \"\"\"åˆ›å»ºåŸºçº¿æ™ºèƒ½ä½“\"\"\"\n        # ä½¿ç”¨æ ‡å‡†é…ç½®\n        agent_config = config.get('agent', {})\n        agent_config.update({\n            'learning_rate': 3e-4,  # æ ‡å‡†å­¦ä¹ ç‡\n            'hidden_dim': 256,\n            'ppo_epochs': 4         # æ ‡å‡†è®­ç»ƒè½®æ¬¡\n        })\n        \n        return StableCVaRPPOAgent(\n            state_dim=env.observation_space.shape[0],\n            action_dim=env.action_space.shape[0],\n            config=agent_config\n        )\n    \n    def _create_enhanced_environment(self, data: Dict) -> EnhancedTradingEnvironment:\n        \"\"\"åˆ›å»ºå¢å¼ºç¯å¢ƒ\"\"\"\n        return EnhancedTradingEnvironment(\n            factor_data=data['factor_data'],\n            price_data=data['price_data'],\n            config=self.enhanced_config\n        )\n    \n    def _create_enhanced_agent(self, env: EnhancedTradingEnvironment) -> StableCVaRPPOAgent:\n        \"\"\"åˆ›å»ºå¢å¼ºæ™ºèƒ½ä½“\"\"\"\n        return StableCVaRPPOAgent(\n            state_dim=env.observation_space.shape[0],\n            action_dim=env.action_space.shape[0],\n            config=self.enhanced_config['agent']\n        )\n    \n    def _train_system(self, env, agent, episodes: int, system_name: str) -> Dict:\n        \"\"\"è®­ç»ƒç³»ç»Ÿ\"\"\"\n        logger.info(f\"å¼€å§‹è®­ç»ƒ{system_name}ï¼Œå…±{episodes}è½®...\")\n        \n        training_metrics = {\n            'episode_rewards': [],\n            'episode_returns': [],\n            'episode_sharpe_ratios': [],\n            'best_performance': {\n                'episode': 0,\n                'annual_return': -np.inf,\n                'sharpe_ratio': -np.inf\n            },\n            'convergence_metrics': {\n                'training_stability': 0.0,\n                'final_performance': 0.0\n            }\n        }\n        \n        for episode in range(episodes):\n            # é‡ç½®ç¯å¢ƒ\n            observation, info = env.reset()\n            episode_reward = 0.0\n            episode_steps = 0\n            \n            while True:\n                # è·å–åŠ¨ä½œ\n                action, log_prob, value = agent.get_action(observation)\n                \n                # æ‰§è¡ŒåŠ¨ä½œ\n                next_observation, reward, terminated, truncated, info = env.step(action)\n                \n                # å­˜å‚¨ç»éªŒ\n                agent.store_experience(\n                    observation, action, reward, log_prob, value, terminated\n                )\n                \n                episode_reward += reward\n                episode_steps += 1\n                observation = next_observation\n                \n                if terminated or truncated:\n                    break\n            \n            # æ›´æ–°æ™ºèƒ½ä½“\n            if len(agent.memory['states']) >= agent.batch_size:\n                update_info = agent.update()\n            \n            # è®°å½•æŒ‡æ ‡\n            training_metrics['episode_rewards'].append(episode_reward)\n            \n            if 'performance_metrics' in info:\n                annual_return = info['performance_metrics'].get('annualized_return', 0.0)\n                sharpe_ratio = info['performance_metrics'].get('sharpe_ratio', 0.0)\n                \n                training_metrics['episode_returns'].append(annual_return)\n                training_metrics['episode_sharpe_ratios'].append(sharpe_ratio)\n                \n                # æ›´æ–°æœ€ä½³æ€§èƒ½\n                if annual_return > training_metrics['best_performance']['annual_return']:\n                    training_metrics['best_performance'].update({\n                        'episode': episode,\n                        'annual_return': annual_return,\n                        'sharpe_ratio': sharpe_ratio\n                    })\n            \n            # å®šæœŸæŠ¥å‘Šè¿›åº¦\n            if (episode + 1) % 50 == 0:\n                recent_rewards = training_metrics['episode_rewards'][-10:]\n                avg_reward = np.mean(recent_rewards) if recent_rewards else 0.0\n                logger.info(\n                    f\"{system_name} - Episode {episode + 1}/{episodes}, \"\n                    f\"å¹³å‡å¥–åŠ±: {avg_reward:.2f}, \"\n                    f\"æœ€ä½³å¹´åŒ–æ”¶ç›Š: {training_metrics['best_performance']['annual_return']:.2%}\"\n                )\n        \n        # è®¡ç®—æ”¶æ•›æŒ‡æ ‡\n        if len(training_metrics['episode_rewards']) >= 50:\n            final_50_rewards = training_metrics['episode_rewards'][-50:]\n            training_metrics['convergence_metrics']['training_stability'] = 1.0 / (np.std(final_50_rewards) + 1e-8)\n            training_metrics['convergence_metrics']['final_performance'] = np.mean(final_50_rewards)\n        \n        logger.info(f\"{system_name}è®­ç»ƒå®Œæˆ\")\n        return training_metrics\n    \n    def _test_system(self, test_data: Dict, agent, config: Dict, system_name: str) -> Dict:\n        \"\"\"æµ‹è¯•ç³»ç»Ÿ\"\"\"\n        logger.info(f\"å¼€å§‹æµ‹è¯•{system_name}...\")\n        \n        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒ\n        if system_name == \"å¢å¼ºç³»ç»Ÿ\":\n            test_env = EnhancedTradingEnvironment(\n                factor_data=test_data['factor_data'],\n                price_data=test_data['price_data'],\n                config=config\n            )\n        else:\n            # åŸºçº¿ç³»ç»Ÿä½¿ç”¨ç®€åŒ–å› å­\n            simple_factors = pd.DataFrame(\n                np.random.randn(len(test_data['price_data']), 3),\n                index=test_data['price_data'].index,\n                columns=['factor1', 'factor2', 'factor3']\n            )\n            test_env = EnhancedTradingEnvironment(\n                factor_data=simple_factors,\n                price_data=test_data['price_data'],\n                config=config\n            )\n        \n        # è¿è¡Œæµ‹è¯•\n        observation, info = test_env.reset()\n        test_metrics = {\n            'daily_returns': [],\n            'portfolio_values': [],\n            'actions_taken': [],\n            'final_report': {}\n        }\n        \n        while True:\n            # è·å–ç¡®å®šæ€§åŠ¨ä½œï¼ˆæµ‹è¯•æ—¶ä¸æ¢ç´¢ï¼‰\n            action, _, _ = agent.get_action(observation, deterministic=True)\n            \n            # æ‰§è¡ŒåŠ¨ä½œ\n            next_observation, reward, terminated, truncated, info = test_env.step(action)\n            \n            # è®°å½•æŒ‡æ ‡\n            if 'performance_metrics' in info:\n                test_metrics['portfolio_values'].append(info['portfolio_value'])\n            \n            test_metrics['actions_taken'].append(action.copy())\n            observation = next_observation\n            \n            if terminated or truncated:\n                test_metrics['final_report'] = test_env.get_final_report()\n                break\n        \n        # è®¡ç®—æµ‹è¯•æŒ‡æ ‡\n        final_performance = test_metrics['final_report']['final_performance']\n        \n        test_results = {\n            'annual_return': final_performance.get('annualized_return', 0.0),\n            'sharpe_ratio': final_performance.get('sharpe_ratio', 0.0),\n            'max_drawdown': final_performance.get('max_drawdown', 0.0),\n            'win_rate': final_performance.get('win_rate', 0.0),\n            'calmar_ratio': final_performance.get('calmar_ratio', 0.0),\n            'total_return': final_performance.get('total_return', 0.0),\n            'portfolio_values': test_metrics['portfolio_values'],\n            'test_period': test_data['period']\n        }\n        \n        logger.info(\n            f\"{system_name}æµ‹è¯•å®Œæˆ - å¹´åŒ–æ”¶ç›Š: {test_results['annual_return']:.2%}, \"\n            f\"å¤æ™®æ¯”ç‡: {test_results['sharpe_ratio']:.2f}, \"\n            f\"æœ€å¤§å›æ’¤: {test_results['max_drawdown']:.2%}\"\n        )\n        \n        return test_results\n    \n    def _analyze_improvements(self, baseline: Dict, enhanced: Dict) -> Dict:\n        \"\"\"åˆ†ææ”¹è¿›æ•ˆæœ\"\"\"\n        baseline_test = baseline['test_results']\n        enhanced_test = enhanced['test_results']\n        \n        improvements = {\n            'annual_return_improvement': {\n                'baseline': baseline_test['annual_return'],\n                'enhanced': enhanced_test['annual_return'],\n                'absolute_improvement': enhanced_test['annual_return'] - baseline_test['annual_return'],\n                'relative_improvement': (\n                    (enhanced_test['annual_return'] - baseline_test['annual_return']) / \n                    (abs(baseline_test['annual_return']) + 1e-8) * 100\n                )\n            },\n            'sharpe_ratio_improvement': {\n                'baseline': baseline_test['sharpe_ratio'],\n                'enhanced': enhanced_test['sharpe_ratio'],\n                'absolute_improvement': enhanced_test['sharpe_ratio'] - baseline_test['sharpe_ratio']\n            },\n            'risk_control_improvement': {\n                'baseline_max_drawdown': baseline_test['max_drawdown'],\n                'enhanced_max_drawdown': enhanced_test['max_drawdown'],\n                'drawdown_reduction': baseline_test['max_drawdown'] - enhanced_test['max_drawdown']\n            },\n            'overall_improvement_score': 0.0\n        }\n        \n        # è®¡ç®—ç»¼åˆæ”¹è¿›è¯„åˆ†\n        return_score = min(max(improvements['annual_return_improvement']['relative_improvement'] / 100, -2), 2)\n        sharpe_score = min(max(improvements['sharpe_ratio_improvement']['absolute_improvement'], -2), 2)\n        risk_score = min(max(-improvements['risk_control_improvement']['drawdown_reduction'] * 10, -1), 1)\n        \n        improvements['overall_improvement_score'] = 0.5 * return_score + 0.3 * sharpe_score + 0.2 * risk_score\n        \n        return improvements\n    \n    def _analyze_target_achievement(self, enhanced_results: Dict) -> Dict:\n        \"\"\"åˆ†æç›®æ ‡è¾¾æˆæƒ…å†µ\"\"\"\n        test_results = enhanced_results['test_results']\n        target_annual_return = 0.08  # 8%\n        \n        achievement_analysis = {\n            'target_annual_return': target_annual_return,\n            'actual_annual_return': test_results['annual_return'],\n            'target_achieved': test_results['annual_return'] >= target_annual_return,\n            'achievement_ratio': test_results['annual_return'] / target_annual_return,\n            'gap_to_target': target_annual_return - test_results['annual_return'],\n            'confidence_metrics': {\n                'sharpe_ratio': test_results['sharpe_ratio'],\n                'max_drawdown': test_results['max_drawdown'],\n                'win_rate': test_results['win_rate'],\n                'calmar_ratio': test_results['calmar_ratio']\n            },\n            'risk_adjusted_achievement': {\n                'target_sharpe': 1.0,\n                'actual_sharpe': test_results['sharpe_ratio'],\n                'risk_adjusted_target_achieved': test_results['sharpe_ratio'] >= 1.0,\n                'drawdown_within_limit': test_results['max_drawdown'] <= 0.12\n            }\n        }\n        \n        # è®¡ç®—æˆåŠŸæ¦‚ç‡\n        if achievement_analysis['target_achieved']:\n            achievement_analysis['success_probability'] = min(\n                1.0, \n                achievement_analysis['achievement_ratio'] * \n                (1 + test_results['sharpe_ratio'] / 2) * \n                (1 - test_results['max_drawdown'] * 5)\n            )\n        else:\n            achievement_analysis['success_probability'] = max(\n                0.0,\n                achievement_analysis['achievement_ratio'] * 0.8\n            )\n        \n        return achievement_analysis\n    \n    def _generate_final_report(self) -> Dict:\n        \"\"\"ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š\"\"\"\n        report = {\n            'validation_summary': {\n                'validation_date': datetime.now().isoformat(),\n                'system_components': [\n                    'ç¨³å®šCVaR-PPOæ™ºèƒ½ä½“',\n                    'è‡ªé€‚åº”å®‰å…¨ä¿æŠ¤å±‚',\n                    'å¢å¼ºå¥–åŠ±ç³»ç»Ÿ',\n                    'é«˜çº§Alphaå› å­åº“',\n                    'å¢å¼ºäº¤æ˜“ç¯å¢ƒ'\n                ],\n                'target_goal': '8%å¹´åŒ–æ”¶ç›Šç‡',\n                'validation_period': '2023å¹´ï¼ˆæ ·æœ¬å¤–æµ‹è¯•ï¼‰'\n            },\n            'performance_comparison': self.validation_results['improvement_analysis'],\n            'target_achievement': self.validation_results['target_achievement'],\n            'detailed_results': {\n                'baseline_performance': self.validation_results['baseline_performance'],\n                'enhanced_performance': self.validation_results['enhanced_performance']\n            },\n            'success_metrics': {\n                'target_achieved': self.validation_results['target_achievement']['target_achieved'],\n                'achievement_ratio': self.validation_results['target_achievement']['achievement_ratio'],\n                'success_probability': self.validation_results['target_achievement']['success_probability'],\n                'overall_improvement_score': self.validation_results['improvement_analysis']['overall_improvement_score']\n            },\n            'recommendations': self._generate_recommendations()\n        }\n        \n        return report\n    \n    def _generate_recommendations(self) -> List[str]:\n        \"\"\"ç”Ÿæˆæ”¹è¿›å»ºè®®\"\"\"\n        recommendations = []\n        \n        target_achieved = self.validation_results['target_achievement']['target_achieved']\n        achievement_ratio = self.validation_results['target_achievement']['achievement_ratio']\n        \n        if target_achieved:\n            recommendations.extend([\n                \"âœ… æ­å–œï¼ç³»ç»Ÿå·²æˆåŠŸè¾¾åˆ°8%å¹´åŒ–æ”¶ç›Šç›®æ ‡\",\n                \"ğŸ”„ å»ºè®®è¿›è¡Œæ›´é•¿æœŸçš„æ ·æœ¬å¤–æµ‹è¯•éªŒè¯ç¨³å®šæ€§\",\n                \"ğŸ“Š å¯è€ƒè™‘åœ¨ä¸åŒå¸‚åœºç¯å¢ƒä¸‹çš„é²æ£’æ€§æµ‹è¯•\",\n                \"âš¡ å»ºè®®ç›‘æ§å®ç›˜äº¤æ˜“ä¸­çš„æ‰§è¡Œåå·®\"\n            ])\n        else:\n            if achievement_ratio >= 0.8:\n                recommendations.extend([\n                    \"â­ ç³»ç»Ÿè¡¨ç°æ¥è¿‘ç›®æ ‡ï¼Œå»ºè®®å¾®è°ƒä»¥ä¸‹æ–¹é¢ï¼š\",\n                    \"ğŸ¯ è¿›ä¸€æ­¥ä¼˜åŒ–å¥–åŠ±å‡½æ•°çš„æ”¶ç›Šå¯¼å‘æ€§\",\n                    \"ğŸ“ˆ å¢åŠ æ›´å¤šé«˜è´¨é‡alphaå› å­\",\n                    \"âš–ï¸ é€‚åº¦æ”¾å®½é£é™©çº¦æŸä»¥é‡Šæ”¾æ”¶ç›Šæ½œåŠ›\"\n                ])\n            else:\n                recommendations.extend([\n                    \"ğŸ”§ ç³»ç»Ÿéœ€è¦è¿›ä¸€æ­¥æ”¹è¿›ï¼Œå»ºè®®ï¼š\",\n                    \"ğŸ§  å¢å¼ºæ¨¡å‹æ¶æ„çš„å­¦ä¹ èƒ½åŠ›\",\n                    \"ğŸ’¡ é‡æ–°è®¾è®¡å¥–åŠ±å‡½æ•°ä»¥æ›´å¥½æ¿€åŠ±æ”¶ç›Š\",\n                    \"ğŸ” æ·±å…¥åˆ†æå› å­æœ‰æ•ˆæ€§å¹¶ä¼˜åŒ–å› å­åº“\",\n                    \"âš¡ è€ƒè™‘é‡‡ç”¨æ›´å…ˆè¿›çš„å¼ºåŒ–å­¦ä¹ ç®—æ³•\"\n                ])\n        \n        return recommendations\n    \n    def _save_validation_results(self, report: Dict):\n        \"\"\"ä¿å­˜éªŒè¯ç»“æœ\"\"\"\n        # åˆ›å»ºç»“æœç›®å½•\n        results_dir = './validation_results'\n        os.makedirs(results_dir, exist_ok=True)\n        \n        # ç”Ÿæˆæ–‡ä»¶å\n        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n        report_file = f'{results_dir}/enhanced_system_validation_{timestamp}.json'\n        \n        # ä¿å­˜æŠ¥å‘Š\n        with open(report_file, 'w', encoding='utf-8') as f:\n            json.dump(report, f, indent=2, ensure_ascii=False, default=str)\n        \n        logger.info(f\"éªŒè¯æŠ¥å‘Šå·²ä¿å­˜è‡³: {report_file}\")\n    \n    def print_validation_summary(self, report: Dict):\n        \"\"\"æ‰“å°éªŒè¯æ‘˜è¦\"\"\"\n        print(\"\\n\" + \"=\"*80)\n        print(\"ğŸ¯ å¢å¼ºç³»ç»ŸéªŒè¯æŠ¥å‘Š\")\n        print(\"=\"*80)\n        \n        # ç›®æ ‡è¾¾æˆæƒ…å†µ\n        target_info = report['target_achievement']\n        print(f\"\\nğŸ“Š ç›®æ ‡è¾¾æˆæƒ…å†µ:\")\n        print(f\"  ç›®æ ‡å¹´åŒ–æ”¶ç›Šç‡: {target_info['target_annual_return']:.1%}\")\n        print(f\"  å®é™…å¹´åŒ–æ”¶ç›Šç‡: {target_info['actual_annual_return']:.2%}\")\n        print(f\"  ç›®æ ‡è¾¾æˆ: {'âœ… æ˜¯' if target_info['target_achieved'] else 'âŒ å¦'}\")\n        print(f\"  è¾¾æˆæ¯”ä¾‹: {target_info['achievement_ratio']:.1%}\")\n        print(f\"  æˆåŠŸæ¦‚ç‡: {target_info['success_probability']:.1%}\")\n        \n        # æ€§èƒ½æ”¹è¿›\n        improvement = report['performance_comparison']\n        print(f\"\\nğŸ“ˆ æ€§èƒ½æ”¹è¿›åˆ†æ:\")\n        print(f\"  å¹´åŒ–æ”¶ç›Šæ”¹è¿›: {improvement['annual_return_improvement']['absolute_improvement']:.2%}\")\n        print(f\"  å¤æ™®æ¯”ç‡æ”¹è¿›: {improvement['sharpe_ratio_improvement']['absolute_improvement']:.2f}\")\n        print(f\"  ç»¼åˆæ”¹è¿›è¯„åˆ†: {improvement['overall_improvement_score']:.2f}\")\n        \n        # å»ºè®®\n        print(f\"\\nğŸ’¡ æ”¹è¿›å»ºè®®:\")\n        for rec in report['recommendations']:\n            print(f\"  {rec}\")\n        \n        print(\"\\n\" + \"=\"*80)\n\n\ndef main():\n    \"\"\"ä¸»å‡½æ•°\"\"\"\n    print(\"ğŸš€ å¯åŠ¨å¢å¼ºç³»ç»ŸéªŒè¯\")\n    \n    # åˆ›å»ºéªŒè¯å™¨\n    validator = EnhancedSystemValidator()\n    \n    # è¿è¡ŒéªŒè¯\n    validation_report = validator.run_comprehensive_validation()\n    \n    # æ‰“å°æ‘˜è¦\n    validator.print_validation_summary(validation_report)\n    \n    return validation_report\n\n\nif __name__ == \"__main__\":\n    main()"