#!/usr/bin/env python
"""
æœ€ç»ˆè®­ç»ƒç³»ç»Ÿæµ‹è¯• - éªŒè¯æ‰€æœ‰ä¼˜åŒ–æªæ–½
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))
import numpy as np
import logging
from config import get_default_config

# è®¾ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

def analyze_config():
    """åˆ†æå½“å‰é…ç½®çš„åˆç†æ€§"""
    config = get_default_config()
    
    print("ğŸ”§ å½“å‰é…ç½®åˆ†æ:")
    print("=" * 50)
    
    # æ™ºèƒ½ä½“é…ç½®
    agent_config = config['agent']
    print(f"ğŸ¤– æ™ºèƒ½ä½“é…ç½®:")
    print(f"  - ç½‘ç»œè§„æ¨¡: {agent_config['hidden_dim']} (è¾ƒå°ï¼Œæœ‰åˆ©äºç¨³å®šæ€§)")
    print(f"  - å­¦ä¹ ç‡: {agent_config['learning_rate']} (éå¸¸ä¿å®ˆ)")
    print(f"  - PPO clip: {agent_config['clip_epsilon']} (ä¸¥æ ¼é™åˆ¶)")
    print(f"  - è®­ç»ƒè½®æ•°: {agent_config['ppo_epochs']} (æœ€å°‘)")
    print(f"  - æ‰¹æ¬¡å¤§å°: {agent_config['batch_size']} (æœ€å°)")
    print(f"  - CVaRæƒé‡: {agent_config['cvar_lambda']} (æœ€å°)")
    
    # å®‰å…¨çº¦æŸé…ç½®
    shield_config = config['safety_shield']
    print(f"\nğŸ›¡ï¸ å®‰å…¨çº¦æŸé…ç½®:")
    print(f"  - æœ€å¤§æ æ†: {shield_config['max_leverage']} (å·²æ”¾å®½)")
    print(f"  - å•è‚¡ä»“ä½: {shield_config['max_position']} (å·²æ”¾å®½)")
    print(f"  - VaRé˜ˆå€¼: {shield_config['var_threshold']} (å·²æ”¾å®½)")
    print(f"  - å›æ’¤é˜ˆå€¼: {shield_config['max_drawdown_threshold']} (å·²æ”¾å®½)")
    
    # è®­ç»ƒé…ç½®
    training_config = config['training']
    print(f"\nğŸ“š è®­ç»ƒé…ç½®:")
    print(f"  - æ€»episode: {training_config['total_episodes']}")
    print(f"  - æ›´æ–°é¢‘ç‡: {training_config['update_frequency']}")
    print(f"  - ä¿å­˜é¢‘ç‡: {training_config['save_frequency']}")
    
    return config

def run_stability_test():
    """è¿è¡Œç¨³å®šæ€§æµ‹è¯•"""
    print("\nğŸ§ª è¿è¡Œç¨³å®šæ€§æµ‹è¯•...")
    
    try:
        from data import DataManager
        from factors import FactorEngine
        from rl_agent import TradingEnvironment, CVaRPPOAgent, SafetyShield
        
        config = get_default_config()
        
        # æµ‹è¯•æ•°æ®åŠ è½½
        print("ğŸ“Š æµ‹è¯•æ•°æ®åŠ è½½...")
        data_manager = DataManager(config['data'])
        factor_engine = FactorEngine(config['factors'])
        
        # è·å–å°æ ·æœ¬æ•°æ®
        instruments = data_manager._get_universe_stocks('2019-01-01', '2023-12-31')[:10]
        stock_data = data_manager.get_stock_data(
            instruments=instruments,
            start_time='2023-01-01',
            end_time='2023-02-28'
        )
        
        if stock_data.empty:
            print("âŒ æ•°æ®åŠ è½½å¤±è´¥")
            return False
        
        price_data = stock_data['$close'].unstack()
        volume_data = stock_data['$volume'].unstack()
        factor_data = factor_engine.calculate_all_factors(price_data, volume_data)
        
        print(f"âœ… æ•°æ®åŠ è½½æˆåŠŸ: ä»·æ ¼{price_data.shape}, å› å­{factor_data.shape}")
        
        # æµ‹è¯•ç¯å¢ƒåˆ›å»º
        print("ğŸŒ æµ‹è¯•ç¯å¢ƒåˆ›å»º...")
        environment = TradingEnvironment(factor_data, price_data, config['environment'])
        
        # æµ‹è¯•æ™ºèƒ½ä½“åˆ›å»º
        print("ğŸ¤– æµ‹è¯•æ™ºèƒ½ä½“åˆ›å»º...")
        state_dim = environment.observation_space.shape[0]
        action_dim = environment.action_space.shape[0]
        agent = CVaRPPOAgent(state_dim, action_dim, config['agent'])
        
        # æµ‹è¯•å®‰å…¨ä¿æŠ¤å±‚
        print("ğŸ›¡ï¸ æµ‹è¯•å®‰å…¨ä¿æŠ¤å±‚...")
        safety_shield = SafetyShield(config['safety_shield'])
        
        print(f"âœ… ç»„ä»¶åˆ›å»ºæˆåŠŸ: çŠ¶æ€ç»´åº¦={state_dim}, åŠ¨ä½œç»´åº¦={action_dim}")
        
        # è¿è¡ŒçŸ­æœŸè®­ç»ƒæµ‹è¯•
        print("ğŸƒ è¿è¡ŒçŸ­æœŸè®­ç»ƒæµ‹è¯•...")
        test_rewards = []
        
        for episode in range(3):
            state, info = environment.reset()
            episode_reward = 0
            steps = 0
            
            for step in range(min(10, len(price_data)-1)):  # æœ€å¤š10æ­¥
                action, log_prob, value, cvar_estimate = agent.get_action(state)
                safe_action = safety_shield.shield_action(action, info, price_data, environment.portfolio_weights)
                
                next_state, reward, terminated, truncated, info = environment.step(safe_action)
                agent.store_transition(state, safe_action, reward, value, log_prob, terminated or truncated, cvar_estimate)
                
                state = next_state
                episode_reward += reward
                steps += 1
                
                if terminated or truncated:
                    break
            
            test_rewards.append(episode_reward)
            print(f"  Episode {episode+1}: å¥–åŠ±={episode_reward:.4f}, æ­¥æ•°={steps}")
        
        # æµ‹è¯•ç½‘ç»œæ›´æ–°
        print("ğŸ”„ æµ‹è¯•ç½‘ç»œæ›´æ–°...")
        try:
            update_stats = agent.update()
            loss = update_stats.get('total_loss', 0)
            lr = update_stats.get('learning_rate', 0)
            
            if np.isfinite(loss) and loss < 1e6:  # æŸå¤±å€¼åˆç†
                print(f"âœ… æ›´æ–°æˆåŠŸ: æŸå¤±={loss:.6f}, å­¦ä¹ ç‡={lr:.2e}")
                return True
            else:
                print(f"âš ï¸ æŸå¤±å€¼å¼‚å¸¸: {loss}")
                return False
                
        except Exception as e:
            print(f"âŒ æ›´æ–°å¤±è´¥: {e}")
            return False
            
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def provide_recommendations():
    """æä¾›è®­ç»ƒå»ºè®®"""
    print("\nğŸ’¡ è®­ç»ƒå»ºè®®:")
    print("=" * 50)
    
    print("ğŸ¯ å¦‚æœæµ‹è¯•é€šè¿‡ï¼Œå¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒ:")
    print("   python main.py --mode train")
    
    print("\nğŸ“ˆ ç›‘æ§æŒ‡æ ‡:")
    print("   - å¹³å‡å¥–åŠ±åº”è¯¥é€æ¸ä¸Šå‡")
    print("   - çº¦æŸè§¦å‘æ¬¡æ•°åº”è¯¥å‡å°‘")
    print("   - Sharpeæ¯”ç‡åº”è¯¥æ”¹å–„")
    print("   - æŸå¤±å€¼åº”è¯¥ä¿æŒåœ¨åˆç†èŒƒå›´(< 1000)")
    
    print("\nâš ï¸ å¦‚æœä»æœ‰é—®é¢˜:")
    print("   1. è¿›ä¸€æ­¥é™ä½å­¦ä¹ ç‡åˆ° 1e-6")
    print("   2. å‡å°‘ç½‘ç»œè§„æ¨¡åˆ° 32")
    print("   3. å¢åŠ æ›´å¤šæ•°å€¼ç¨³å®šæ€§æ£€æŸ¥")
    print("   4. è€ƒè™‘ä½¿ç”¨æ›´ç®€å•çš„å¥–åŠ±å‡½æ•°")
    
    print("\nğŸ”§ é«˜çº§ä¼˜åŒ–é€‰é¡¹:")
    print("   - ä½¿ç”¨é¢„è®­ç»ƒçš„å› å­æƒé‡")
    print("   - å®æ–½è¯¾ç¨‹å­¦ä¹ (Curriculum Learning)")
    print("   - æ·»åŠ ç»éªŒå›æ”¾ç¼“å†²åŒº")
    print("   - ä½¿ç”¨æ›´ç¨³å®šçš„ä¼˜åŒ–å™¨(å¦‚RMSprop)")

def main():
    """ä¸»å‡½æ•°"""
    print("ğŸš€ Aè‚¡å¼ºåŒ–å­¦ä¹ é‡åŒ–äº¤æ˜“ç³»ç»Ÿ - æœ€ç»ˆæµ‹è¯•")
    print("=" * 60)
    
    # åˆ†æé…ç½®
    config = analyze_config()
    
    # è¿è¡Œç¨³å®šæ€§æµ‹è¯•
    success = run_stability_test()
    
    # æä¾›å»ºè®®
    provide_recommendations()
    
    # æ€»ç»“
    print("\n" + "=" * 60)
    if success:
        print("ğŸ‰ ç³»ç»Ÿæµ‹è¯•é€šè¿‡ï¼å¯ä»¥å¼€å§‹æ­£å¼è®­ç»ƒã€‚")
        print("ğŸ’ª æ‰€æœ‰ä¼˜åŒ–æªæ–½å·²å®æ–½ï¼Œç³»ç»Ÿç¨³å®šæ€§å¤§å¹…æå‡ã€‚")
    else:
        print("âš ï¸ ç³»ç»Ÿä»éœ€è¿›ä¸€æ­¥ä¼˜åŒ–ã€‚")
        print("ğŸ”§ å»ºè®®æŒ‰ç…§ä¸Šè¿°å»ºè®®è¿›è¡Œè°ƒæ•´ã€‚")
    
    return success

if __name__ == "__main__":
    main()